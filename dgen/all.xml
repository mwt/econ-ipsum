</article>
<abstract> <p>We introduce a game of complete information with multiple principals and multiple common agents. Each agent makes a decision that can affect the payoffs of all principals. Each principal offers monetary transfers to each agent conditional on the action taken by the agent. We characterize pure-strategy equilibria and we provide conditions--in terms of game balancedness--for the existence of an equilibrium with an efficient outcome. Games played through agents display a type of strategic inefficiency that is absent when either there is a unique principal or there is a unique agent.</p> </abstract>
<abstract> <p>This paper develops a general method for constructing exactly similar tests based on the conditional distribution of nonpivotal statistics in a simultaneous equations model with normal errors and known reduced-form covariance matrix. These tests are shown to be similar under weak-instrument asymptotics when the reduced-form covariance matrix is estimated and the errors are non-normal. The conditional test based on the likelihood ratio statistic is particularly simple and has good power properties. Like the score test, it is optimal under the usual local-to-null asymptotics, but it has better power when identification is weak.</p> </abstract>
<abstract> <p>The block bootstrap is the best known bootstrap method for time-series data when the analyst does not have a parametric model that reduces the data generation process to simple random sampling. However, the errors made by the block bootstrap converge to zero only slightly faster than those made by first-order asymptotic approximations. This paper describes a bootstrap procedure for data that are generated by a Markov process or a process that can be approximated by a Markov process with sufficient accuracy. The procedure is based on estimating the Markov transition density nonparametrically. Bootstrap samples are obtained by sampling the process implied by the estimated transition density. Conditions are given under which the errors made by the Markov bootstrap converge to zero more rapidly than those made by the block bootstrap.</p> </abstract>
<abstract> <p>In this paper we focus on strategic voting behavior when both an election and a signaling motivation affect voters' behavior. We analyze a model of elections with two candidates competing on a one-dimensional policy space. Voters are privately and imperfectly informed about a common shock affecting the electorate's preferences. Candidates are assumed to choose policy in response to information gleaned from election results and according to exogenous factors that may lead to polarization in candidates' policy choices. We analyze a subset of symmetric equilibria in which strategies are symmetric to candidates' names and private signals (CSS equilibria). We show that signaling and election motivations pull voters to vote in different directions. We provide conditions that show the relation between the amount of information aggregated in the election and the motivation that influences voting behavior the most. Finally, we show that when candidates are responsive and polarized, all CSS equilibria are inefficient in the limit.</p> </abstract>
<abstract> <p> In this paper we derive the asymptotic properties of within groups (WG), GMM, and LIML estimators for an autoregressive model with random effects when both T and N tend to infinity. GMM and LIML are consistent and asymptotically equivalent to the WG estimator. When T/N → 0 the fixed T results for GMM and LIML remain valid, but WG, although consistent, has an asymptotic bias in its asymptotic distribution. When T/N tends to a positive constant, the WG, GMM, and LIML estimators exhibit negative asymptotic biases of order 1/T, 1/N, and 1/(2N - T), respectively. In addition, the crude GMM estimator that neglects the autocorrelation in first differenced errors is inconsistent as T/N → c &gt; 0, despite being consistent for fixed T. Finally, we discuss the properties of a random effects pseudo MLE with unrestricted initial conditions when both T and N tend to infinity. </p> </abstract>
<abstract> <p>We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score.</p> </abstract>
<abstract> <p>An isotone pure strategy equilibrium exists in any game of incomplete information in which each player's action set is a finite sublattice of multidimensional Euclidean space, types are multidimensional and atomless, and each player's interim expected payoff function satisfies two "nonprimitive conditions" whenever others adopt isotone pure strategies: (i) single-crossing in own action and type and (ii) quasi-supermodularity in own action. Conditions (i), (ii) are satisfied in supermodular and log-supermodular games given affiliated types, and in games with independent types in which each player's ex post payoff satisfies supermodularity in own action and nondecreasing differences in own action and type. This result is applied to provide the first proof of pure strategy equilibrium existence in the uniform price auction when bidders have multi-unit demand, nonprivate values, and independent types.</p> </abstract>
<abstract> <p>This paper develops new econometric methods to infer hospital quality in a model with discrete dependent variables and nonrandom selection. Mortality rates in patient discharge records are widely used to infer hospital quality. However, hospital admission is not random and some hospitals may attract patients with greater unobserved severity of illness than others. In this situation the assumption of random admission leads to spurious inference about hospital quality. This study controls for hospital selection using a model in which distance between the patient's residence and alternative hospitals are key exogenous variables. Bayesian inference in this model is feasible using a Markov chain Monte Carlo posterior simulator, and attaches posterior probabilities to quality comparisons between individual hospitals and groups of hospitals. The study uses data on 74,848 Medicare patients admitted to 114 hospitals in Los Angeles County from 1989 through 1992 with a diagnosis of pneumonia. It finds the smallest and largest hospitals to be of the highest quality. There is strong evidence of dependence between the unobserved severity of illness and the assignment of patients to hospitals, whereby patients with a high unobserved severity of illness are disproportionately admitted to high quality hospitals. Consequently a conventional probit model leads to inferences about quality that are markedly different from those in this study's selection model.</p> </abstract>
<abstract> <p>We present a general method for computing the set of supergame equilibria in infinitely repeated games with perfect monitoring and public randomization. We present a three-stage algorithm that constructs a convex set containing the set of equilibrium values, constructs another convex set contained in the set of equilibrium values, and produces strategies that support them. We explore the properties of this algorithm by applying it to familiar games.</p> </abstract>
<abstract> <p> I characterize the implications of the common prior assumption for finite orders of beliefs about beliefs at a state and show that in finite models, the only such implications are those stemming from the weaker assumption of a common support. More precisely, given any finite N and any finite partitions model where priors have the same support, there is another finite partitions model with common priors that has the same nth order beliefs and knowledge for all n ≤ N. </p> </abstract>
<abstract> <p>The paper analyzes the impact of the initial condition on the problem of testing for unit roots. To this end, we derive a family of optimal tests that maximize a weighted average power criterion with respect to the initial condition. We then investigate the relationship of this optimal family to popular tests. We find that many unit root tests are closely related to specific members of the optimal family, but the corresponding members employ very different weightings for the initial condition. The popular Dickey-Fuller tests, for instance, put a large weight on extreme deviations of the initial observation from the deterministic component, whereas other popular tests put more weight on moderate deviations. Since the power of unit root tests varies dramatically with the initial condition, this paper explains the results of comparative power studies of unit root tests. The results allow a much deeper understanding of the merits of particular tests in specific circumstances, and a guide to choosing which statistics to use in practice.</p> </abstract>
<abstract> <p>We consider the problem of constructing a portfolio of finitely many assets whose returns are described by a discrete joint distribution. We propose mean-risk models that are solvable by linear programming and generate portfolios whose returns are nondominated in the sense of second-order stochastic dominance. Next, we develop a specialized parametric method for recovering the entire mean-risk efficient frontiers of these models and we illustrate its operation on a large data set involving thousands of assets and realizations.</p> </abstract>
<abstract> <p>In certain auction, search, and related models, the boundary of the support of the observed data depends on some of the parameters of interest. For such nonregular models, standard asymptotic distribution theory does not apply. Previous work has focused on characterizing the nonstandard limiting distributions of particular estimators in these models. In contrast, we study the problem of constructing efficient point estimators. We show that the maximum likelihood estimator is generally inefficient, but that the Bayes estimator is efficient according to the local asymptotic minmax criterion for conventional loss functions. We provide intuition for this result using Le Cam's limits of experiments framework.</p> </abstract>
<abstract> <p>We present estimators for nonparametric functions that are nonadditive in unobservable random terms. The distributions of the unobservable random terms are assumed to be unknown. We show that when a nonadditive, nonparametric function is strictly monotone in an unobservable random term, and it satisfies some other properties that may be implied by economic theory, such as homogeneity of degree one or separability, the function and the distribution of the unobservable random term are identified. We also present convenient normalizations, to use when the properties of the function, other than strict monotonicity in the unobservable random term, are unknown. The estimators for the nonparametric function and for the distribution of the unobservable random term are shown to be consistent and asymptotically normal. We extend the results to functions that depend on a multivariate random term. The results of a limited simulation study are presented.</p> </abstract>
<abstract> <p>In this study we consider a labor market matching model where firms post wage-tenure contracts and workers, both employed and unemployed, search for new job opportunities. Given workers are risk averse, we establish there is a unique equilibrium in the environment considered. Although firms in the market make different offers in equilibrium, all post a wage-tenure contract that implies a worker's wage increases smoothly with tenure at the firm. As firms make different offers, there is job turnover, as employed workers move jobs as the opportunity arises. This implies the increase in a worker's wage can be due to job-to-job movements as well as wage-tenure effects. Further, there is a nondegenerate equilibrium distribution of initial wage offers that is differentiable on its support except for a mass point at the lowest initial wage. We also show that relevant characteristics of the equilibrium can be written as explicit functions of preferences and the other market parameters.</p> </abstract>
<abstract> <p>Weak nonparametric restrictions are developed, sufficient to identify the values of derivatives of structural functions in which latent random variables are nonseparable. These derivatives can exhibit stochastic variation. In a microeconometric context this allows the impact of a policy intervention, as measured by the value of a structural derivative, to vary across people who are identical as measured by covariates. When the restrictions are satisfied quantiles of the distribution of a policy impact across people can be identified. The identification restrictions are local in the sense that they are specific to the values of the covariates and the specific quantiles of latent variables at which identification is sought. The conditions do not include the commonly required independence of latent variables and covariates. They include local versions of the classical rank and order conditions and local quantile insensitivity conditions. Values of structural derivatives are identified by functionals of quantile regression functions and can be estimated using the same functionals applied to estimated quantile regression functions.</p> </abstract>
<abstract> <p>This paper proposes an estimation method for a repeated auction game under the presence of capacity constraints. The estimation strategy is computationally simple as it does not require solving for the equilibrium of the game. It uses a two stage approach. In the first stage the distribution of bids conditional on state variables is estimated using data on bids, bidder characteristics, and contract characteristics. In the second stage, an expression of the expected sum of future profits based on the distribution of bids is obtained, and costs are inferred based on the first order condition of optimal bids. We apply the estimation method to repeated highway construction procurement auctions in the state of California between May 1996 and May 1999. In this market, previously won uncompleted contracts reduce the probability of winning further contracts. We quantify the effect of intertemporal constraints on bidders' costs and on bids. Due to the intertemporal effect and also to bidder asymmetry, the auction can be inefficient. Based on the estimates of costs, we quantify efficiency losses.</p> </abstract>
<abstract> <p>This paper analyzes the specification and identification of causal multivariate duration models. We focus on the case in which one duration concerns the point in time a treatment is initiated and we are interested in the effect of this treatment on some outcome duration. We define "no anticipation of treatment" and relate it to a common assumption in biostatistics. We show that (i) no anticipation and (ii) randomized treatment assignment can be imposed without restricting the observational data. We impose (i) but not (ii) and prove identification of models that impose some structure. We allow for dependent unobserved heterogeneity and we do not exploit exclusion restrictions on covariates. We provide results for both single-spell and multiple-spell data. The timing of events conveys useful information on the treatment effect.</p> </abstract>
<abstract> <p>We study the problem of the existence and uniqueness of solutions to the Bellman equation in the presence of unbounded returns. We introduce a new approach based both on consideration of a metric on the space of all continuous functions over the state space, and on the application of some metric fixed point theorems. With appropriate conditions we prove uniqueness of solutions with respect to the whole space of continuous functions. Furthermore, the paper provides new sufficient conditions for the existence of solutions that can be applied to fairly general models. It is also proven that the fixed point coincides with the value function and that it can be approached by successive iterations of the Bellman operator.</p> </abstract>
<abstract> <p>It has been known that, in aggregating infinite utility streams, there does not exist any social welfare function, which satisfies the axioms of Pareto, intergenerational equity, and continuity. We show that the impossibility result persists even without imposing the continuity axiom, and in frameworks allowing for more general domains of utilities than those used in the existing literature.</p> </abstract>
<abstract> <p>In econometrics there are many occasions where knowledge of the structural relationship among dependent variables is required to answer questions of interest. This paper gives identification and estimation results for nonparametric conditional moment restrictions. We characterize identification of structural functions as completeness of certain conditional distributions, and give sufficient identification conditions for exponential families and discrete variables. We also give a consistent, nonparametric estimator of the structural function. The estimator is nonparametric two-stage least squares based on series approximation, which overcomes an ill-posed inverse problem by placing bounds on integrals of higher-order derivatives.</p> </abstract>
<abstract> <p> This paper presents new identification conditions for the mixed proportional hazard model. In particular, the baseline hazard is assumed to be bounded away from 0 and ∞ near t = 0. These conditions ensure that the information matrix is nonsingular. The paper also presents an estimator for the mixed proportional hazard model that converges at rate N&lt;sup&gt;-1/2&lt;/sup&gt;. </p> </abstract>
<abstract> <p>We provide easy to verify sufficient conditions for the consistency and asymptotic normality of a class of semiparametric optimization estimators where the criterion function does not obey standard smoothness conditions and simultaneously depends on some nonparametric estimators that can themselves depend on the parameters to be estimated. Our results extend existing theories such as those of Pakes and Pollard (1989), Andrews (1994a), and Newey (1994). We also show that bootstrap provides asymptotically correct confidence regions for the finite dimensional parameters. We apply our results to two examples: a 'hit rate' and a partially linear median regression with some endogenous regressors.</p> </abstract>
<abstract> <p>We develop a model of monetary exchange where, as in the random matching literature, agents trade bilaterally and not through centralized markets. Rather than assuming they match exogenously and at random, however, we determine who meets whom as part of the equilibrium. We show how to formalize this process of directed matching in dynamic models with double coincidence problems, and present several examples and applications that illustrate how the approach can be used in monetary theory. Some of our results are similar to those in the random matching literature; others differ significantly.</p> </abstract>
<abstract> <p> The paper studies bilateral contracting between one principal and N agents when each agent's utility depends on the principal's unobservable contracts with other agents. We show that allowing deviations to menu contracts from which the principal chooses bounds equilibrium outcomes in a wide class of bilateral contracting games without imposing ad hoc restrictions on the agents' beliefs. This bound yields, for example, competitive convergence as N → ∞ in environments in which an appropriately-defined notion of competitive equilibrium exists. We also examine the additional restrictions arising in two common bilateral contracting games: the "offer game" in which the principal makes simultaneous offers to the agents, and the "bidding game" in which the agents make simultaneous offers to the principal. </p> </abstract>
<abstract> <p>In sequential bargaining models without outside options, each player's bargaining power is ultimately determined by which player will make an offer and when. This paper analyzes a sequential bargaining model in which players may hold different beliefs about which player will make an offer and when. Excessive optimism about making offers in the future can cause delays in agreement. The main result states that, despite this, if players will remain sufficiently optimistic for a sufficiently long future, then in equilibrium they will agree immediately. This result is also extended to other canonical models of optimism.</p> </abstract>
<abstract> <p>A nonparametric, residual-based block bootstrap procedure is proposed in the context of testing for integrated (unit root) time series. The resampling procedure is based on weak assumptions on the dependence structure of the stationary process driving the random walk and successfully generates unit root integrated pseudo-series retaining the important characteristics of the data. It is more general than previous bootstrap approaches to the unit root problem in that it allows for a very wide class of weakly dependent processes and it is not based on any parametric assumption on the process generating the data. As a consequence the procedure can accurately capture the distribution of many unit root test statistics proposed in the literature. Large sample theory is developed and the asymptotic validity of the block bootstrap-based unit root testing is shown via a bootstrap functional limit theorem. Applications to some particular test statistics of the unit root hypothesis, i.e., least squares and Dickey-Fuller type statistics are given. The power properties of our procedure are investigated and compared to those of alternative bootstrap approaches to carry out the unit root test. Some simulations examine the finite sample performance of our procedure.</p> </abstract>
<abstract> <p>Many refinements of Nash equilibrium yield solution correspondences that do not have closed graph in the space of payoffs or information. This has significance for implementation theory, especially under complete information. If a planner is concerned that all equilibria of his mechanism yield a desired outcome, and entertains the possibility that players may have even the slightest uncertainty about payoffs, then the planner should insist on a solution concept with closed graph. We show that this requirement entails substantial restrictions on the set of implementable social choice rules. In particular, when preferences are strict (or more generally, hedonic), while almost any social choice function can be implemented in undominated Nash equilibrium, only monotonic social choice functions can be implemented in the closure of the undominated Nash correspondence.</p> </abstract>
<abstract> <p>This paper provides deterministic approximation results for stochastic processes that arise when finite populations recurrently play finite games. The processes are Markov chains, and the approximation is defined in continuous time as a system of ordinary differential equations of the type studied in evolutionary game theory. We establish precise connections between the long-run behavior of the discrete stochastic process, for large populations, and its deterministic flow approximation. In particular, we provide probabilistic bounds on exit times from and visitation rates to neighborhoods of attractors to the deterministic flow. We sharpen these results in the special case of ergodic processes.</p> </abstract>
<abstract> <p> This paper analyzes the linear regression model y = xβ + ε with a conditional median assumption med(ε ǀ z) = 0, where z is a vector of exogenous instrument random variables. We study inference on the parameter β when y is censored and x is endogenous. We treat the censored model as a model with interval observation on an outcome, thus obtaining an incomplete model with inequality restrictions on conditional median regressions. We analyze the identified features of the model and provide sufficient conditions for point identification of the parameter β. We use a minimum distance estimator to consistently estimate the identified features of the model. We show that under point identification conditions and additional regularity conditions, the estimator based on inequality restrictions is √N-normal and we derive its asymptotic variance. One can use our setup to treat the identification and estimation of endogenous linear median regression models with no censoring. A Monte Carlo analysis illustrates our estimator in the censored and the uncensored case. </p> </abstract>
<abstract> <p>A popular way to account for unobserved heterogeneity is to assume that the data are drawn from a finite mixture distribution. A barrier to using finite mixture models is that parameters that could previously be estimated in stages must now be estimated jointly: using mixture distributions destroys any additive separability of the log-likelihood function. We show, however, that an extension of the EM algorithm reintroduces additive separability, thus allowing one to estimate parameters sequentially during each maximization step. In establishing this result, we develop a broad class of estimators for mixture models. Returning to the likelihood problem, we show that, relative to full information maximum likelihood, our sequential estimator can generate large computational savings with little loss of efficiency.</p> </abstract>
<abstract> <p>With cheap talk, more can be achieved by long conversations than by a single message--even when one side is strictly better informed than the other. ("Cheap talk" means plain conversation--unmediated, nonbinding, and payoff-irrelevant.) This work characterizes the equilibrium payoffs for all two-person games in which one side is better informed than the other and cheap talk is permitted.</p> </abstract>
<abstract> <p> This paper considers tests for structural instability of short duration, such as at the end of the sample. The key feature of the testing problem is that the number, m, of observations in the period of potential change is relatively small--possibly as small as one. The well-known F test of Chow (1960) for this problem only applies in a linear regression model with normally distributed iid errors and strictly exogenous regressors, even when the total number of observations, n + m, is large. We generalize the F test to cover regression models with much more general error processes, regressors that are not strictly exogenous, and estimation by instrumental variables as well as least squares. In addition, we extend the F test to nonlinear models estimated by generalized method of moments and maximum likelihood. Asymptotic critical values that are valid as n → ∞ with m fixed are provided using a subsampling-like method. The results apply quite generally to processes that are strictly stationary and ergodic under the null hypothesis of no structural instability. </p> </abstract>
<abstract> <p>This paper develops a dynamic industry model with heterogeneous firms to analyze the intra-industry effects of international trade. The model shows how the exposure to trade will induce only the more productive firms to enter the export market (while some less productive firms continue to produce only for the domestic market) and will simultaneously force the least productive firms to exit. It then shows how further increases in the industry's exposure to trade lead to additional inter-firm reallocations towards more productive firms. The paper also shows how the aggregate industry productivity growth generated by the reallocations contributes to a welfare gain, thus highlighting a benefit from trade that has not been examined theoretically before. The paper adapts Hopenhayn's (1992a) dynamic industry model to monopolistic competition in a general equilibrium setting. In so doing, the paper provides an extension of Krugman's (1980) trade model that incorporates firm level productivity differences. Firms with different productivity levels coexist in an industry because each firm faces initial uncertainty concerning its productivity before making an irreversible investment to enter the industry. Entry into the export market is also costly, but the firm's decision to export occurs after it gains knowledge of its productivity.</p> </abstract>
<abstract> <p> Cointegrated bivariate nonstationary time series are considered in a fractional context, without allowance for deterministic trends. Both the observable series and the cointegrating error can be fractional processes. The familiar situation in which the respective integration orders are 1 and 0 is nested, but these values have typically been assumed known. We allow one or more of them to be unknown real values, in which case Robinson and Marinucci (2001, 2003) have justified least squares estimates of the cointegrating vector, as well as narrow-band frequency-domain estimates, which may be less biased. While consistent, these estimates do not always have optimal convergence rates, and they have nonstandard limit distributional behavior. We consider estimates formulated in the frequency domain, that consequently allow for a wide variety of (parametric) autocorrelation in the short memory input series, as well as time-domain estimates based on autoregressive transformation. Both can be interpreted as approximating generalized least squares and Gaussian maximum likelihood estimates. The estimates share the same limiting distribution, having mixed normal asymptotics (yielding Wald test statistics with χ &lt;sup&gt;2&lt;/sup&gt; null limit distributions), irrespective of whether the integration orders are known or unknown, subject in the latter case to their estimation with adequate rates of convergence. The parameters describing the short memory stationary input series are √n-consistently estimable, but the assumptions imposed on these series are much more general than ones of autoregressive moving average type. A Monte Carlo study of finite-sample performance is included. </p> </abstract>
<abstract> <p>We consider an infinite-horizon exchange economy with incomplete markets and collateral constraints. As in the two-period model of Geanakoplos and Zame (2002), households can default on their liabilities at any time, and financial securities are only traded if the promises associated with these securities are backed by collateral. We examine an economy with a single perishable consumption good, where the only collateral available consists of productive assets. In this model, competitive equilibria always exist and we show that, under the assumption that all exogenous variables follow a Markov chain, there also exist stationary equilibria. These equilibria can be characterized by a mapping from the exogenous shock and the current distribution of financial wealth to prices and portfolio choices. We develop an algorithm to approximate this mapping numerically and discuss ways to implement the algorithm in practice. A computational example demonstrates the performance of the algorithm and shows some quantitative features of equilibria in a model with collateral and default.</p> </abstract>
<abstract> <p> We propose an estimation method for models of conditional moment restrictions, which contain finite dimensional unknown parameters (θ) and infinite dimensional unknown functions (h). Our proposal is to approximate h with a sieve and to estimate θ and the sieve parameters jointly by applying the method of minimum distance. We show that: (i) the sieve estimator of h is consistent with a rate faster than n&lt;sup&gt;-1/4&lt;/sup&gt; under certain metric; (ii) the estimator of θ is √n consistent and asymptotically normally distributed; (iii) the estimator for the asymptotic covariance of the θ estimator is consistent and easy to compute; and (iv) the optimally weighted minimum distance estimator of θ attains the semiparametric efficiency bound. We illustrate our results with two examples: a partially linear regression with an endogenous nonparametric part, and a partially additive IV regression with a link function. </p> </abstract>
<abstract> <p>We consider the bootstrap unit root tests based on finite order autoregressive integrated models driven by iid innovations, with or without deterministic time trends. A general methodology is developed to approximate asymptotic distributions for the models driven by integrated time series, and used to obtain asymptotic expansions for the Dickey-Fuller unit root tests. The second-order terms in their expansions are of stochastic orders O&lt;sub&gt;p&lt;/sub&gt;(n&lt;sup&gt;-1/4&lt;/sup&gt;) and O&lt;sub&gt;p&lt;/sub&gt;(n&lt;sup&gt;-1/2&lt;/sup&gt;), and involve functionals of Brownian motions and normal random variates. The asymptotic expansions for the bootstrap tests are also derived and compared with those of the Dickey-Fuller tests. We show in particular that the bootstrap offers asymptotic refinements for the Dickey-Fuller tests, i.e., it corrects their second-order errors. More precisely, it is shown that the critical values obtained by the bootstrap resampling are correct up to the second-order terms, and the errors in rejection probabilities are of order o(n&lt;sup&gt;-1/2&lt;/sup&gt;) if the tests are based upon the bootstrap critical values. Through simulations, we investigate how effective is the bootstrap correction in small samples.</p> </abstract>
<abstract> <p>We provide a simple behavioral definition of 'subjective mixture' of acts for a large class of (not necessarily expected-utility) preferences. Subjective mixtures enjoy the same algebraic properties as the 'objective mixtures' used to great advantage in the decision setting introduced by Anscombe and Aumann (1963). This makes it possible to formulate mixture-space axioms in a fully subjective setting. For illustration, we present simple subjective axiomatizations of some models of choice under uncertainty, including Bewley's model of choice with incomplete preferences (2002).</p> </abstract>
<abstract> <p>Demand analysis has a classic application to agricultural statistics. Empirical findings, however, obtained from different countries often show great discrepancies. This multi-country study suggests that such discrepancies in many cases are due not only to the application of different statistical methods, to differences in respect of geographical position, consumption habits, etc., but are the result of the structural disparity in the general economic situation, country by country. Accordingly special attention has been paid to the influence on the average demand elasticities caused by varying income level.</p> </abstract>
<abstract> <p> Cet article constitue la mise au point et le développement d'une communication antérieure sur le même sujet [4]. Il concerne le problème posé par l'incompatibilité de la règle de vente au coût marginal préconisée par les Welfare Economics, et les conditions d'équilibre budgétaire imposées par les Pouvoirs Publics aux entreprises nationalisées. La méthode consiste en une ``maximation de Pareto'' appliquée à un modèle général dont les liaisons de structure comportent à la fois des liaisons entre les quantités et des liaisons entre les valeurs. Les résultats obtenus, rassemblés dans la section 8, sont commentés et comparés aux solutions antérieurement proposées à ce problème. Un résumé de cet article est constitué par les sections 1, 3, et 8, qui peuvent être lues sans prendre connaissance des calculs. Le modèle développé dans la section 2 a pour objet de faciliter l'interprétation donnée dans la section 3, et d'expliquer plus clairement l'intervention des prix et revenus dans les liaisons du système; mais la maximation exposée dans la section 5 est effectuée sur le modèle transformé qui, à la fin de la section 4, rassemble sous une forme condensée les liaisons du système. La section 9, enfin, à l'occasion de la critique faite de la politique ``traditionnelle'' des monopoles publics, commente certains points particuliers des résultats exposés dans la section 8. </p> </abstract>
<abstract> <p>Existing theory of consumer demand does not contain a body of theorems purporting to explain the consumption behavior of individuals when their preferences are changed, either autonomously or by advertising and other form of selling effort. The objective of this paper is to present a theory of consumer demand with variable preferences. The assumption that the individual consumer has a unique ordinal utility index function is replaced by the assumption that he has a family of ordinal utility functions; advertising expenditures by the sellers of commodities are assumed to determine which one of these ordinal utility functions is to be maximized. From these assumptions are derived a number of theoretical relations which measurements defining advertising elasticities of demand must satisfy. The relations involving shifts in demand and advertising elasticities of demand are shown to be analogues of the theorems of consumer demand under fixed preferences. For example, a theorem corresponding to the well-known Leontief-Hicks theorem on aggregate commodities has been worked out, thus showing that, under certain advertising conditions, a group to commodities may be treated as a single good.</p> </abstract>
<abstract> <p>The saddle point plays a very important role in the theory of games, in general equilibrium theory in economics, and in linear and nonlinear programming problems. This point is equivalent to a stationary solution of a special type of system of differential equations. The writer has constructed a high-speed electro-analog computer, which is not only useful as a practical programming solver, but which also gives a very interesting model of the economic equilibrium.</p> </abstract>
<abstract> <p>A set of conditions for a social welfare function, somewhat less severe than those required by K. Arrow, is shown to lead, like his, to a contradiction.</p> </abstract>
<abstract> <p>This paper examines the influence of ratio transformations on correlation and regression estimates. After a discussion of the "spurious" ratio correlation problem necessary and sufficient conditions are adduced for the correlation between two series with a common denominator to equal the partial correlation between numerator series with the deflating variable's influence held constant. So far as regression coefficients are concerned, it is shown what conditions must be fulfilled to obtain best linear unbiased least squares estimates when the data are in ratio form. These conditions will be more frequently fulfilled with cross-section than with time series data. Some empirical properties of Chenery's test of the capacity principle are then re-evaluated in light of these technical conditions. Chenery's work illustrates the point that ratio transformations on time series usually require circumspection when the data are cross-sectional.</p> </abstract>
<abstract> <p>The temporal variations in a few input-output coefficients for the antifriction bearing industry are noted. It appears that these variations are due to cyclical phenomena rather than to price changes, technological change, or variations in product mix. The coefficients for the individual plants in the industry tend to move parallel to the coefficients for the industry as a whole, but there are large differences among the plant coefficients at any given time. Some of these differences are associated with the degree of integration of the plants and the type of products being made, but most of the inter-plant differences cannot be explained with conventional census-type data.</p> </abstract>
<abstract> <p>The method used here represents an attempt to eliminate the influence of unspecified factors on the dependent variate in multiple regression analysis. It is applied in constructing a production function for British coal mining over the period 1943-53 and in estimating the elasticities of output with regardto labour and to horsepower, the latter indicating mechanisation. The statistical and economic significance of the results is examined.</p> </abstract>
<abstract> <p>Papers and discussion published in the form of abstracts are indicated by "abs." Papers reported by title are indicated by "T." The number following the date of publication indicates the volume number, and the numbers following the colon indicate the pages. References subordinate to the principal reference are listedin brackets.</p> </abstract>
<abstract> <p>The volume is indicated by the number which appears before the colon, an "s" being added when reference is made to a supplement. The number following the colon is the first page of the reference. It is in italics for abstracts, and is followed by a "R" for book reviews and by an "E" for errata</p> </abstract>
<abstract> <p>This is a study of factors responsible for the wide cross-sectional differences in the past and current rates of use of hybrid seed corn in the United States. Logistic growth functions are fitted to the data by states and crop reporting districts, reducing differences among areas to differences in estimates of the three parameters of the logistic: origins, slopes, and ceilings. The lag in the development of adaptable hybrids for particular areas and the lag in the entry of seed producers into these areas (differences in origins) are explained on the basis of varying profitability of entry, "profitability" being a function of market density, and innovation and marketing cost. Differences in the long-run equilibrium use of hybrid corn (ceilings) and in the rates of approach to that equilibrium (slopes) are explained, at least in part, by differences in the profitability of the shift from open pollinated to hybrid varieties in different parts of the country. The results are summarized and the conclusion is drawn that the process of innovation, the process of adapting and distributing a particular invention to different markets and the rate at which it is accepted by entrepreneurs are amenable to economic analysis.</p> </abstract>
<abstract> <p>A comparison of elasticities for food, clothing, housing, and miscellaneous items with respect to total expenditure and family size, based on regression analyses of about 40 surveys from about 30 countries. The elasticities are found to be similar but not equal. Engel's law, formulated in 1857, is confirmed by all surveys.</p> </abstract>
<abstract> <p>"It is, therefore, necessary to abandon substitutes and attempt to predict the short run consumption-income relation itself...."</p> </abstract>
<abstract> <p>The primary purpose of this study is to develop a model based on linear programming and input-output techniques which will assist in the formulation of development programs for underdeveloped areas. I. The elements of the development problem and the data available are considered from the point of view of selecting the most significant structural relationships for a formal model. II. A nonlinear programming model is presented which is designed particularly to analyze the choice between self-sufficiency and international specialization and the resulting investment patterns under various types of restrictions. III. A method of solving the programming problem by a simple iterative procedure is suggested. It takes advantage of the structure of the matrix of activities and the limited number of primary factors involved. Convergence of the solution is demonstrated. IV. A development model for Southern Italy is constructed for purposes of illustration, based on studies of consumption, investment, and input structure and assumptions as to the remaining parameters. A solution for a hypothetical development program is given, showing the method of solution and the effect of variation in several of the parameters. V. The implications of the results for the solution of a more general model are considered. The relationship of the interindustry model to sector analyses and the role of each in actual planning situations are discussed.</p> </abstract>
<abstract> <p>This article applies linear programming to a field hitherto untouched by it. Competitive bidding for serial bonds issued by governments and other public authorities is currently based on the "net interest cost" method. The authors examine and describe the factors which enter into net interest cost and provide a method for adjusting those variables most subject to the control of the bond bidders. Since the bidder presenting the lowest net interest cost to the issuing authority wins the issue, the object of these adjustments is to minimize a given bidder's net interest cost. The linear programming problem which arises from the minimization requirement is of a form which admits of an explicit solution. This solution is given and a practical method for its computation is provided.</p> </abstract>
<abstract> <p>This article studies the implications of complementarity on the problem of the long-term forecast. The conditions for maintaining long-run equilibrium between factor demand and factor supply are derived and illustrated with the help of a three-factor model, so as to bring out the particular problem of foreign trade. Equilibrium for all of the three factors appears possible only if the parameters of the system satisfy a set of specific relations or certain policy variables are introduced. Finally the complete system as used by the Central Planning Bureau is presented, together with the numerical values chosen for the parameters.</p> </abstract>
<abstract> <p>A framework for a static theory of consumer behavior is advanced which will not only subsume the standard indifference curve approach and the technique of "revealed preference," but will also admit a more general class of behavior patterns. The proposed theory analyzes consumer behavior from two related aspects, the first being that concerned with the consumer's subjective point of view as to his actions in the market, and the second being concerned with the economist's point of view as an objective observer of the consumer's market behavior. The paper deals with the n-commodity case, and makes no restrictive assumptions as to the regularity of the commodity space, existence of a utility function, continuity of consumption, or non-satiation in taste. In addition to the usual postulates for static analysis, only two behavioral axioms are assumed: (a) a revealed preference transitivity type axiom, and (b) a "no-price illusion" axiom (somewhat similar to the usual "no-money illusion" assumption). Demand theorems analogous to those commonly derived in consumer theory are proved by means of a generalized revealed preference approach and by the use of convex set theory in Euclidean n-space.</p> </abstract>
<abstract> <p>The Austrian theory relating capital, output (income), and input (labour and land) measures the time dimension of capital by a single interval, the period of production. Only in special cases, such as the maturing of wine and the growing of timber, is the exact theory satisfactory, while its use as an approximation to reality is open to criticism. The objections can be removed by introducing further time measures--the variances of the input and output distributions over time, and the higher statistical moments. In the exact, general case the relations between the sets of moments, the rate of return, the real wage rate, and a suitably defined production function determine output, input, and the value of capital. The ratios of capital to output, and wage and rent bills to income, are analysed. Suitable approximations depending upon the moments and the rate of return are available.</p> </abstract>
<abstract> <p>This paper is concerned with improved techniques of program planning, particularly as they apply to the scheduling of activities over time within an organization or economy in which the activities must share in the use of limitedamounts of various commodities. The contemplated use of electronic computers forrapidly computing programs and the assumptions underlying the mathematical model are discussed. The paper is concluded by an illustrative example.</p> </abstract>
<abstract> <p>Activities (or production processes) are considered as building blocks out of which a technology is constructed. Postulates are developed by which activities may be combined. The main part of the paper is concerned with the discrete type model and the use of a linear maximization function for finding the "optimum" program. The mathematical problem associated with this approach is developed first in general notation and then in term sof a dynamic system of equations expressed in matrix notation. Typical problems from the fields of inter-industryrelations, transportation nutrition, warehouse storage, and air transport are given in the last section.</p> </abstract>
<abstract> <p>The present paper deals with the general problem of sequential choice among several actions, where at each stage the options available are to stop and take a definite action or to continue sampling for more information. There are costs attached to taking inappropriate action and to sampling. A characterization of the optimum solution is obtained first under very general assumptions as to the distribution of the successive observations and the costs of sampling; then moredetailed results are given for the case where the alternative actions are finitein number, the observations are drawn under conditions of random sampling, and the cost depends only on the number of observations. Explicit solutions are given for the case of two actions, random sampling, and linear cost functions.</p> </abstract>
<abstract> <p>This paper covers one phase of a regression analysis of 117 bank stocks for the period 1945-52, which was conducted primarily to shed light on some current problems of banks in the capital markets. Since doubts have arisen concerning the reliability of regression analysis in such an application, this paper considers evidence of heterogeneity in the assumed underlying bank stock population and of auto-correlation in the residuals. Finally, it experiments with models designed in the hope of eliminating the worst effects of heterogeneity and auto-correlation.</p> </abstract>
<abstract> <p>Given the amounts required of a certain commodity in each of T future periods of time, we examine the problem of how production should be scheduled over time in order to satisfy these requirements at the lowest possible cost. A general solution to this problem is exhibited under certain "reasonable" assumptions concerning the nature of production and storage costs, and a convenient graphical method of securing the solution is outlined. One significant aspect of the solution is that under certain conditions (and especially if the cost of carrying inventories is "large" relative to the marginal cost of production), the optimal plan consists of a sequence of plans covering successive intervals of the entire horizon and having the following property: the production schedule for each interval is identical with the optimum plan for the corresponding interval considered separately; it depends, in other words, on requirements within the interval itself, but is essentially independent of requirements for earlier or later periods. It is shown that these results have significant implications concerning the length of the expectation and planning horizon relevant to current production decisions and that they throw some light on the rationale of certain observed business practices.</p> </abstract>
<abstract> <p>The experiment reported here involved the running of a set of complete information duopoly games in which subjects were permitted to communicate by means of written messages before each decision. There are a number of interesting questions which arise in situations of this sort: What proportion of the time do subjects agree on a joint decision? How do their actions vary depending on whether or not agreement has been reached? How frequently are decisions Pareto optimal? Do any standard solution concepts, such as the Nash solution, joint profit maximization or equal split of profits, characterize the Pareto optimal outcomes? The results are highly gratifying as they strongly confirm one's intuition: Agreement is present in a substantial majority of decisions, and Pareto optimality is a feature of most agreements. These results are some indication that the rationality assumptions made by economists are not unreasonable.</p> </abstract>
<abstract> <p>This paper estimates a model of the United Kingdom's monetary sector comprising supply and demand functions for various types of monetary assets including currency, bank reserves, demand deposits, time deposits, and total deposits. Themodel is "closed" by a simple distributed lag version of the quantity theory of money. Since the matrix of endogenous variables turns out to be triangular, the model is superficially recursive and thus OLS may be a consistent, FIML estimator of the system. In case the other conditions for recursiveness are not met and the system is genuinely interdependent, TSLS estimates are also presented. Various interest rates and income elasticities are calculated. They conform broadly with previous single equation estimates of their magnitude.</p> </abstract>
<abstract> <p>This paper concerns itself with the following problem: Suppose the true production function is of the CES type with constant returns to scalle. If we fit an unrestricted Cobb-Douglas production function instead, what is the nature of the bias in the estimate of the returns to scale parameter?</p> </abstract>
<abstract> <p>The pure theory of consumer demand is a thoroughly developed topic, more so than is its use in providing extra information to improve estimates of demand parameters. In this paper some aspects of the Slutsky-Hicks-Allen theory of demand are used in conjunction with multivariate statistical analysis to provide estimates of elasticities of domestic demand for several New Zealand meats. These estimates are more efficient, in a statistical sense, and considerably superior for prediction purposes than are corresponding results obtained by more standard methods. In addition, the utility maximizationtheory is tested to see if it is an hypothesis capable of explaining observed demand for the meats considered.</p> </abstract>
<abstract> <p>This paper offers a qualitative theoretical analysis of the error that may arise when a linear programming calculation is used to solve a problem involving some degree of nonlinearity. Six propositions are developed: (1) a linear approximation to a nonlinear program will not necessarily yield the true maximum; (2) it need not provide an answer better than a randomly chosen initial solution; (3) it may not even provide the best possible corner solution; (4) a reduction in the curvature of the profit surface does not always guarantee improvement in the accuracy of a linear approximation; (5) proximity of the initial point to the maximum need not increase the accuracy of the linear approximation; and (6) only if the objective function is monotone throughout can we be assured that a linear approximation will yield results which represent an improvement over the initial point. The paper also describes sampling experiments in which the correct solution of a quadratic programming problem subject to linear constraints was compared with the solution of a linear programming problem obtained by replacing the quadratic maximand by its tangent hyperplane at the initial point and by other linear approximations. In general, the linear programming calculations did not yield results very close to the true maximum nor did the approximation improve substantially as the curvature of the objective function was reduced.</p> </abstract>
<abstract> <p>The product life cycle is a well known concept in marketing theory. It is generally assumed to comprise four phases. So far, however, quantitative evidence of this shape of the product life cycle has been missing. This paper deals with methods of estimating the parameters of such life cycles and two methods of estimating product life cycles are given. An iterative method is developed that is of independent value as well as providing a good starting point for Marquardt's generalized least squares method. It is shown that there exist complementary and substitutive effects between the product under consideration and the sales of related products sold by the same company.</p> </abstract>
<abstract> <p>Necessary and sufficient conditions for the addivity of ordinal utility functions are well known. In this paper a necessary and sufficient condition for the addivity of von Neumann-Morgenstern utility functions is presented; this condition is summarized in the "strong additivity axiom". A "weak addivity axiom" is introduced, and it is shown that an individual's preferences satisfy this axiom if and only if his von Neumann-Morgenstern utility function is either additive or "log additive". The extension of these results to the continuous consumption case is indicated.</p> </abstract>
<abstract> <p>This paper presents a method for eliminating management bias from production functions fitted to cross-section data on multiproduct enterprises. The method is applied to a sample of peasant farms in Rhodesia. The estimates are used to calculate marginal productivities, to examine the efficiency of allocation in the sample, to assess the relative importance of factors in leading to increases in output, and to examine the characteristics of farms with a better than average performance.</p> </abstract>
<abstract> <p>Hannan in "Regression for Time Series" [4] proposed an interesting method of estimating regression coefficients using spectral techniques, and later in "The Estimation of Relationship Involving Distributed Lags" [5] applied the method to the estimation of parameters in a distributed lag model. This paper first illustrates that Hannan's method in his former article is asymptotically equal to Aitken's least-squares estimation in which the covariance matrix of the regression residual is estimated in a certain consistent manner from the calculated residuals. Next it proves that Hannan's method in his latter article is asymptotically equal to maximum likelihood estimation of the distributed lag model. Hannan's method is useful when the investigator's a priori knowledge about the stochastic process of the residual is minimal. But if the process can be specified to be, say, a first order or second order autoregressive system, it is desirable to use such knowledge in estimation. For such a case, the paper proposes an estimator of the distributed lag model based on the Gauss-Newton iterative method and proves it to be asymptotically equal to the maximum likelihood estimator. The paper evaluates the asymptotic distribution of two other estimators of the distributed lag model, including the one proposed by Klein [7].</p> </abstract>
<abstract> <p>This article is concerned with the theory of the "bottleneck" problem experienced by a developing economy, say that of an underdeveloped country which is trying to achieve specified production goals over a fixed time period, where there is competition for restricted resources and limited external aid, including for example competition between the needs for consumer goods and capital goods. The problem is viewed as a generalized type of nonlinear programming problem. Mathematically, a basic difficulty in such a model is of course its enormous size; and it is desirable to find methods of testing approximate solutions. One such test is provided by the duality theory of nonlinear programming, which is shown to apply to this situation, so that the problem can be regarded as either of two distinct problems, which are shown to have the same optimal solution. Economically, these two problems may be described in terms of determining the optimal procedure at any given time during the period in terms of what has already happened, or alternatively in terms of what is required to happen subsequent to that time. The two problems involve respectively maximization and minimization, so that, in particular, upper and lower bounds for the value of production can be calculated for any particular production plan.</p> </abstract>
<abstract> <p>A system of six equations is presented to explain quarterly movements in the U.S. trade cycle over a 21 year period in terms of five exogenous variables--hourly rate of money wages, public expenditure, taxation, exports, and bank deposits. All other variables--consumption, various types of investment, inventory changes, imports--are endogenous. The development of the theoretical ideas contained in the system is traced. The model is tested against data for the period involved, and an application is made to post-war data. The data used and its sources are indicated.</p> </abstract>
<abstract> <p>Statistical inference, from observations to economic behavior parameters, can be made in two steps:inference from the observations to the parameters of the assumed joint distribution of the observations, and inference from that distribution to the parameters of the structural equations describing economic behavior. The latter problem of inference, described by the term "identification problem," is discussed in this article in an expository manner, drawing on other more original work for concepts and theorems, using a number of examples drawn partly from econometric literature.</p> </abstract>
<abstract> <p>Existing bilateral trade agreements often carry the restriction that hard currency or gold must be used to settle trade balances that have exceeded specified margins of credit. By means of a scheme of mutual cancellation of balances among several countries, it is hoped to get a larger volume of trade within the framework of a system of bilateral agreements containing such restrictive provisions. The simple mathematics of a compensation scheme of M. H. Ekker is presented. The main principle is the minimization, in the square, of the sum of balances under realistic types of constraints.</p> </abstract>
<abstract> <p>This article sets out an analysis of interest rates in a simplified model market with only four, sharply distinguished, types of asset. There are shown to be three layers to the analysis: the basic pattern of demand and supply of assets of different types, the demand and supply of money, and the state of expectations. The analysis follows the lines opened up by Keynes and elaborates upon them at some points. It does not fit into static equilibrium theory, but is intended to link up with the analysis of an economy undergoing continuous change and development.</p> </abstract>
<abstract> <p>The purpose of this study is to establish a formula which could be considered, in some of its aspects at least, as expressing the losses resulting from the defects of an economic situation as compared with a given optimal situation (taken as reference). Use is made of the amount by which the sum of individual incomes in the second situation could be increased so that all satisfactions are made equal to their values in the first situation. The expression arrived at has a definite sign and is null for the reference situation, minimum (under certain constraints) for an optimal situation.</p> </abstract>
<abstract> <p>It is commonly accepted that Pareto's law holds well for the middle range of income distribution but not for smaller and larger income classes. Although it is comparatively easy to analyze the larger incomes statistically, the econometric approach is essential for analyzing the distribution of smaller incomes. Data are presented for income distribution in 108 of the 274 communes (villages, towns, and cities) in Hokkaido, Japan, including data for the smallerincomes below the tax limit. Using these data and Pareto's curve, the author attempts to estimate income distribution as a whole in Japan. The extreme positive skewness of the income distribution is noted, and appropriate methods of curve fitting are explored.</p> </abstract>
<abstract> <p>A basic assumption of the theory of games is that each player correctly perceives the payoff functions of the other players. This assumption seems highly unrealistic, and it is dropped in this paper and replaced by the assumption that each player has a perception of the payoff function of each of the other players; these perceptions may be incorrect. It is shown that this leads to an analogue of characteristic function theory in which each player has a subjective characteristic function which he believes represents the strength of the several coalitions. It is proposed that the coalitions be treated directly as outcomes and that individual preferences among them be ascertained. If the von Neumann utility axioms plus one other are met, then it can be argued that a simple transformation of the resulting utility function is a player's subjective characteristic function. The beginnings of an equilibrium theory are outlined for this more general model; this theory reduces to a known one when there are no misperceptions. The value of this generalized equilibrium theory is severely limited by two strong assumptions which are made.</p> </abstract>
<abstract> <p> The structure X&lt;sub&gt;i&lt;/sub&gt; = U&lt;sub&gt;i&lt;/sub&gt; + α Ui-1 with the U&lt;sub&gt;i&lt;/sub&gt; identically and independently distributed (and X&lt;sub&gt;i&lt;/sub&gt; observable) is analyzed with respect to the identifiability of the parameter α and the distribution of U. The so-called linear structural relationship is examined briefly. </p> </abstract>
<abstract> <p>In the theory of preferences underlying utility theory it is generally assumed that the indifference relation is transitive, and this leads to equivalence classes of indifferent elements or, equally, to indifference curves. It has been pointed out that utility is not perfectly discriminable, as such a theory necessitates. In this paper intransitive indifference relations are admitted and a class of them are axiomatized. This class is shown to be substantially equivalent to a utility theory in which there are just noticeable difference functions which state for any value of utility the change in utility so that the change is just noticeable. In the case of risk represented by a linear utility function over a mixture space, the precise form of the function is examined in detail.</p> </abstract>
<abstract> <p>An empirical cost function for fuel consumption in air transportation is obtained from a consideration of the engineering determinants of the relationship between physical inputs and physical outputs. The accuracy of fuel consumption estimates so obtained is considered in the light of operating data for Northeast Airlines. Marginal cost functions are derived which facilitate theanalysis of the impact upon cost, not only of changes in output and factor prices, but also of changes in quality of the product and the techniques of production. The advantages and limitations of the approach are indicated.</p> </abstract>
<abstract> <p>It is not always irrelevant whether one deals with stock or flow variables in economic analysis. Some simple dynamic models are shown in which the choice between stock and flow variables becomes essential. It is also not true that the liquidity preference theory of interest is identical with the loanable funds theory of interest. This problem is intimately connected with the distinctions between stock and flow analysis. These two interest theories are shown to be different in simple dynamic models.</p> </abstract>
<abstract> <p>Entrepreneurs may form their plans on the basis of expected prices equal to most probable prices plus or minus risk allowances. The allowance may be interpreted as a certainty equivalent of subjective probability calculations, and may be given exact formulation under some conditions. The risk allowance will be positive whenever entrepreneurs are willing to sacrifice expected returns for a narrowing of dispersion of possible returns. Diminishing marginal utility of money will explain such conservatism. Limited capital resources also tend to make entrepreneurs behave in a conservative manner.</p> </abstract>
<abstract> <p>A number of observed cases are reported of doubly logarithmic rectilinearity with least square fits (1) in the phenomenon of brand-names (including trade- names) of manufacturers and distributors in the United States (1946) in reference to (a) the varying number of concerns that use the same brand-name, (b) the varying number of brand-names in cities of origin of different population sizes, (c) the varying preferences of families for different brands of like commodities, (d) the varying number of different items under like brands; and (2) in the closely related phenomenon of (a) the varying number of different manufacturers of like goods, (b) the varying number of factories in cities of different population size, and (c) the varying number of manufacturers with different numbers of warehouses and other subsidiary organizations.</p> </abstract>
<abstract> <p>Optimal inventory policy is first derived for a simple model in which the future (and constant) demand flow and other relevant quantities are known in advance. This is followed by the study of uncertainty models--a static and a dynamic one--in which the demand flow is a random variable with a known probability distribution. The best maximum stock and the best reordering point are determined as functions of the demand distribution, the cost of making an order, and the penalty of stock depletion.</p> </abstract>
<abstract> <p>A numerical evaluation of the "dead loss" associated with a non-optimal situation (in the Pareto sense) of an economic system is sought. Use is made of the intrinsic price systems associated with optimal situations of whose existence a noncalculus proof is given. A coefficient of resource utilization yielding measures of the efficiency of the economy is introduced. The treatment is based on vector-set properties in the commodity space.</p> </abstract>
<abstract> <p>A distinction is drawn between a sales contract and an employment contract, and a formal model is constructed exhibiting this distinction. By introducing a definition of rational behavior, a method is obtained for determining under what conditions an employment contract will rationally be preferred to a sales contract, and what limits will rationally be placed on the authority of an employer in an employment contract. The relationship of this model to certain other theories of planning under uncertainty is discussed.</p> </abstract>
<abstract> <p>Methods are given for calculating the effect on the stability of a pure multiplier system of certain simple kinds of shifts in the lag pattern of consumption responses. In particular, other things being equal (and the "other things" must be specified in detail), increases in average lag slow up the rate of return to equilibrium after disturbance. Increased dispersion of the timing of consumption responses has the same effect; thus a lengthening of the average lag combined with a concentration of the lag pattern may have a combined result going either way. Of course, any approximately realistic shift in the consumption lag distribution, such as might result from a change in the amount of terms of instalment credit, would not be of the simple type treated here. However, fairly general variations might be approximated by a combination of one-sided and symmetric displacement as discussed below.</p> </abstract>
<abstract> <p>The Keynesian definition of involuntary unemployment has given rise to much controversy. According to this definition it is useless to discuss problems of involuntary unemployment within a consistent economic model if one of its equations is the classical supply function of labor, because there can be no involuntary unemployment within such a model. Similar problems arise whenever one tries to demonstrate the possibility of involuntary individual economic decisions within a given model of economic behavior. It is the contention of the present writer that the notion of involuntary economic decisions, to become meaningful, must be derived from a comparison between alternative economic models, or frameworks, under which society may collectively choose to operate.</p> </abstract>
<abstract> <p>This paper takes issue with Don Patinkin, who recently argued that the simultaneous equations of classical economic theory are necessarily inconsistent, and that the classical attempt to determine real prices in the real sector of the economy and absolute prices in the monetary sector involves logical contradictions. Since a large part of his argument hinges on a misunderstanding of just what it was that the classical school assumed, the present paper restates the classical theory so as to emphasize its postulational bases. A simple example is then employed to demonstrate that it is quite possible to set up a consistent classical system in which relative prices are determined in the real sector independently of absolute prices in the monetary sector. Next the details of Patinkin's analysis are examined for the flaws that led him to believe that such a system could not be set up. The paper closes with some observations on the ways in which determinacy can be built into a system and on the generally unsatisfactory state of equilibrium theory.</p> </abstract>
<abstract> <p>The application of the method of least squares is discussed for the case in which a large number of unknowns have to be estimated; in particular, the application to family diet surveys is considered. Methods are indicated for shortening the analysis and investigating the importance of different variables by a step-by-step procedure. Suggestions are made on the planning of surveys in the light of the methods investigated here.</p> </abstract>
<abstract> <p>Data for the French gas industry is studied under the assumption that the demandfor gas by a consumer is a function of the price of gas and of the disposable income of the consumer. The demand equation is shown to be identified within themodel defined by the demand equation, the equation of production, and the equilibrium equations. Elasticities of demand with respect to price and disposable income are obtained and confidence regions are calculated and interpreted.</p> </abstract>
<abstract> <p>After introducing some basic concepts and three postulates on rational choice, it is proposed to show that if the economists' theory of assets is completed by a fourth postulate on rational choice, then utility can be defined as a quantity whose mathematical expectation is maximized by the rational man. In this sense, utility is "measurable" and "manageable." These results are inspired by von Neumann's and Morgenstern's discussion of utility in Theory of Games and Economic Behavior; an attempt is made to sketch some relations between their approach and the present one. It is shown in conclusion that while gambling is compatible with the four postulates, the "love of danger" is not; and a property of the maximum mathematical expectation of utility is conjectured.</p> </abstract>
<abstract> <p>The Leontief matrices of inter-industry transactions are large, a row for each industry in a nation. It would be desirable to invert such matrices of an order of from 100 to 200. The present paper suggests using the sum of a power series to approximate the inverse of a Leontief matrix with any desired degree of accuracy. This requires many more multiplications than do such direct methods as the Gauss-Doolittle process. But the method proposed in this paper is especially well adapted to automatic computation on the new electronic machines,in which case the large number of multiplications is not serious. The main advantage of the proposed method is that it provides an upper bound to the error of any element in the estimated inverse. A short cut method is also indicated for computing the approximation when the number of terms of the power series needed to obtain the desired degree of accuracy is large.</p> </abstract>
<abstract> <p>A new treatment is presented of a classical economic problem, one which occurs in many forms, as bargaining, bilateral monopoly, etc. It may also be regarded as a nonzero-sum two-person game. In this treatment a few general assumptions are made concerning the behavior of a single individual and of a group of two individuals in certain economic environments. From these, the solution (in the sense of this paper) of the classical problem may be obtained. In the terms of game theory, values are found for the game.</p> </abstract>
<abstract> <p>"Adjusted concavity" of demand curves as a criterion for comparing a firm's output under discriminatory and simple monopoly pricing is examined. An alternative criterion, the "slope ratio," is developed and its relation to the problem of output change under discriminatory pricing is explained. The relationship of the two criteria is set forth and a graphical presentation of both is given for comparative purposes. Finally, a simple illustrative problem is attacked using the two alternative criteria.</p> </abstract>
<abstract> <p>A. Wald has presented a model of production and a model of exchange and proofs of the existence of an equilibrium for each of them. Here proofs of the existence of an equilibrium are given for an integrated model of production, exchange and consumption. In addition the assumptions made on the technologies of producers and the tastes of consumers are significantly weaker than Wald's. Finally a simplification of the structure of the proofs has been made possible through use of the concept of an abstract economy, a generalization of that of a game.</p> </abstract>
<abstract> <p>This paper considers the problem of determining an optimal price policy for a firm over a number of planning periods, taking into account the effect of its present policy on the demand curves which will confront it in the future. The possibilities of shifts, lags, and kinks in the demand functions are allowed for. A partial solution is obtained by using activity analysis, and is applied to the recent controversy between the "normal cost" and "marginal cost" pricing rules</p> </abstract>
<abstract> <p>The behavior of economic units engaged in the production and distribution of flaxseed and linseed oil in the United States is summarized in a system of linear stochastic equations. The latter contains such jointly dependent, observable economic variables as production, stocks, and consumption of the above commodities. The particular equation system studied reflects the investigator's desire to explain quarterly inventory fluctuations in these commodities and is formulated in the light of market conditions prevailing during the period 1926-1939. Primarily for the latter reason, no claim of predictive usefulness under present conditions is made for the estimates of these equations. The paper concludes with an evaluation of the estimates.</p> </abstract>
<abstract> <p>The conditions under which a devaluation will improve a country's balance of payments--stability conditions of exchange rate adjustments--have been extensively dealt with in literature for a two-country system under static conditions. This paper represents an attempt to analyze the complications arising from the introduction of more countries into the system under dynamic conditions. The dynamic stability conditions for an n-counutry model have been derived in mathematical terms through a system of difference equations under quite general conditions in the Appendix. Special pain has been taken in the text to interpret these mathematical conditions in economic terms for a more restricted three-country model. Three cases of exchange rate adjustments have been analyzed. It has been found that, provided that each country taken by itself is stable (i.e., that a depreciation of its currency will improve its balance of payments, all other countries' par values remaining unchanged), a sufficient condition for the stability of a three-country system is that, for any pair of countries, the absolute value of the product of the effects on each others' balances, each acting alone, is smaller than the product of the effects on their own balances.</p> </abstract>
<abstract> <p>This paper is addressed to the problems of estimating output capabilities for an entire economy. It represents a one-industry experiment in relying primarily upon engineering data for this purpose, rather then upon time series informationalone. The use of linear programming, combined with the shift in emphasis upon sources of information, holds out the promise of greater forecasting reliabilitythan is otherwise attainable.</p> </abstract>
<abstract> <p>A Monte Carlo sampling technique is employed to compare small sample properties of limited-information--single-equation, least squares, and instrumental variables estimates. Two versions of an essentially two equation model are each used to generate 100 sets of observations over 20 time periods.</p> </abstract>
<abstract> <p> Reservations systems charged with selling a fixed quantity of a highly perishable commodity through many agents present an instructive example of decision making and communication in teams. Section 2 discusses the optimal limit of sales when the probability distributions of late cancellations and no-shows and of the number of standby passengers are known. An example was calculated on the basis of Γ distributions, which had been found to fit these distributions adequately. (The statistical analysis of these empirical distributions is not included here.) Section 3 describes the existing airlines reservations systems, Section 4 discusses their effect on the sales-limit problem. Section 5 considers the rationale of these communication rules. In terms of a simplified model the conditions are studied for which a decentralized quota system is as efficient as the prevailing centralized ``sell and record'' system. </p> </abstract>
<abstract> <p>For the larger industrial countries--and especially the United States--there seems to exist a fairly strong correlation between the level of industrial production and the volume of non-agricultural employment. This correlation tendsto show that the productivity of labour, in a given country, depends primarily upon the size of its economy. For smaller industrial countries, in which foreigntrade often plays a much more important role, the correlation between productionand employment no longer applies. In the present article, an attempt is made to show that this correlation may be generalized, on the assumption that the productivity of labour (and the size of the economy), is a given country, depends not only upon the number of persons engaged in production, but also on the intensity of its participation in the international trade. It is hoped that the results here presented may help to clear the ground for the measurement of the economic effects of trade policies, for the elaboration of longterm plans, and perhaps also for the study of such regional arrangements as Common Markets or Free Trade Areas.</p> </abstract>
<abstract> <p>The inevitable errors made in empirically quantifying a function system used to represent an economy will carry over and affect estimates derived through use of the system. This may be especially serious if relatively small errors can cumulate to produce large consequences. Some concern with this problem in connection with interindustry relations analysis has been expressed, in part because of the high order of the linear equation systems used. The propagation of errors through computations using interindustry structural matrices is examined. It is concluded that, with reasonable care in quantification, interindustry relations estimates can be made with confidence that structural matrix errors are not only non-cumulative but compensating in effect. The power of this approach is enhanced by an inherent ability to minimize the undesirable effects of data imperfections.</p> </abstract>
<abstract> <p>This paper seeks to survey the literature in economics, philosophy, mathematics, and statistics on the subject of choice among alternatives the consequences of which are not certain. Attention is centered on the suggested modes of describing uncertainty and on the theories of rational and actual behavior of in individuals making choices.</p> </abstract>
<abstract> <p>This paper attempts to estimate savings equations from sample survey data combining economic, demographic, and attitudinal variables. Savings as a function of income, family size, liquid assets, income change, and age of family head is estimated separately for home-owners and renters. The residuals from these equations are then studied in relation to the perceived degree of permanence of income change, income expectations, and general economic outlook. In addition, equations are estimated for a pooled group of home-owners and renters, with a dummy home-ownership variable, and for different income change groups.</p> </abstract>
<abstract> <p>The demand for intermediate commodities is related to the demand to the final commodities in the production of which they enter. It is shown how their elasticities are determined by the technical conditions of production as well as the market for the final products. In particular, positive price elasticities may appear if the fixed cost is an important part of the production expenses. The model is fairly simple but can easily be generalized.</p> </abstract>
<abstract> <p>Attempts to estimate demand equations can, roughly, be classified into two groups. To the first group belong those studies which concentrate on an empirically acceptable explanation of demand for individual commodities, while the overall relationships between the quantities demanded of all commodities in the budget remain in the background. The second group of studies is chiefly concerned with the allocation aspect of consumer demand and has complete systems of demand equations as its object. The overall restrictions on demand equations provided by the theory of consumers' choice play a dominant role in these studies. The following article belongs to the second group. It considers a complete system of demand equations with coefficients which satisfy the theoretical restrictions in an exact way. The estimation of these coefficients is complicated by the nonlinear nature of the restrictions and the interdependence between the equations. These difficulties are not insurmountable as will be clear from the article itself.</p> </abstract>
<abstract> <p>In this paper the linear aggregation coefficient is defined as a measure of the degree of perfectness of a linear aggregation function with respect to a given basic function, and its properties are analyzed in relation to its dual, the linear correlation coefficient.</p> </abstract>
<abstract> <p>Economic production systems may break up into subeconomies of goods and processes that can function independently of each other. This article first explores the various kinds of decompositions that may exist in production schemes, nonlinear as well as linear. Some recent techniques developed in graph theory are adopted to ascertain the decompositions possible for a given production system and the precedence relations between the subeconomies of the decomposition. Finally, I show how the concept of "tearing" from simultaneous equations theory might be used to analyze square input-output models for potential decompositions.</p> </abstract>
<abstract> <p>Estimates of parameters from cross section data are often introduced into time series regression as known with certainty, which leads to conditional estimates of the time series regression. This paper develops a method of pooling cross section and time series data from the Bayesian point of view, to estimate all the parameters simultaneously. It is shown that the parameters which are common to both the regressions will have on the average sharper posterior distributions. It is also demonstrated that the traditional method often leads to underestimates of the standard errors of the time series estimates. The method is applied to estimate a statistical demand function for the U.S. based on cross section and time series data given in Tobin [18].</p> </abstract>
<abstract> <p>Most growth models, whether they be of the Keynesian-Kaldor, Harrodian, or Solow-Swan type, ignore or at least minimize the role of the money supply in the process of accumulation and growth. In general, real factors rather than monetary phenomena are emphasized. There has been little success in developing a theory of capital accumulation and growth which unites Keynesian marginal efficiency and liquidity preference concepts. Instead, full employment is often made a precondition of the analysis. Tobin has, at least, attempted to study the relationship between money and growth but his system is defective since it omits the construction of a demand for capital schedule by entrepreneurs that can be formulated independently of the savings propensity and portfolio decisions of households. (An independent investment demand function--the essence of the static Keynesian system--is often omitted in growth analysis.) This paper shows Tobin's model is applicable only to nonmonetary Say's Law economies, and attempts to remedy the defects of such an analysis. In a modern monetary, market-oriented economy, full employment is likely to be neither automatic nor a position of stable equilibrium (as Phillips curves imply a highly unstable full employment price level). To assume full employment as a precondition is to remove the problem of the role of the money supply in the process of accumulation and growth from the real world. This paper presents an analysis which allows the examination of the role of money within the context of a Keynesian system permitting independent savings, investment and liquidity preference functions. It does not make full employment a precondition of the model.</p> </abstract>
<abstract> <p>It is argued here that in the presence of joint production, ordinary least squares regression, OLS, gives inconsistent estimates. An alternative estimation procedure is therefore developed. The application of the powerful tool of canonical correlation analysis for econometric problems has so far been rather limited because of certain problems. An attempt is made in the first part of this paper to extend the analysis in order to handle a greater variety of problems. A New way of looking at the canonical variables is suggested while developing an estimation technique for joint production functions. In the second part, empirical examples and a comparison with OLS are included. The forecasting accuracy is also compared, illustrating the superiority of the suggested estimation procedure.</p> </abstract>
<abstract> <p>In an earlier article published in the International Economic Review (1963) Liviatan proposed a consistent estimator for a distributed lag model. In the present note we analyze the bias of this estimator to order 1/T, T being the number of observations.</p> </abstract>
<abstract> <p>A corporate profits function is first developed in which gross profits depend positively on unlagged current dollar sales and capacity utilization rates and negatively on lagged sales. This function is tested for all two-digit manufacturing industries. It is found that the estimates differ substantially but similar results can be grouped into primarily competitive and primarily oligopolistic industries. Further testing with unit labor costs as an additional independent variable shows that competitive industries pass on increased labor costs in the form of higher prices much more readily than do oligopolistic industries. Finally, an aggregate econometric model is used to show that an overall increase in unit labor costs is reflected almost entirely in higher prices and decreases corporate profits very little.</p> </abstract>
<abstract> <p>Previous studies have recognized that occupational mobility varies with age, but studies have been impeded by imperfect measures of income to which career choosers respond. In this article, cohort changes for six age groups are viewed as resulting from responses to unobserved relative incomes in each of five decades. By an iterative procedure, least-squares estimates can be obtained for ratios of age group response coefficients, and relative incomes are estimable down to a logarithmic transformation. Properties of the estimators are investigated. The analysis is applied to the U.S. and regions, and inferences are made about changes in the aggregate supply elasticity of farm operators and other conditions likely to prevail in the future.</p> </abstract>
<abstract> <p>A new system of inequality indicators is introduced in the paper. It has been elaborated for measuring income inequalities, but can serve as inequality measures for other phenomena as well. The new indicators not only measure the degree of the inequality but are also suitable for measuring its economic motivation. Properties of the new measures, including large sample properties of their estimators, are discussed. the case of the lognormal distribution is of special interest.</p> </abstract>
<abstract> <p>In this paper we show that a bargaining game will yield a negotiated solution with certain reasonable properties if the rules of the game are appropriately restricted. The basic idea is to provide an incentive for all the group components to engage in a process of concessions until the point where some agreement is reached. The incentive consists of the threat of a preannounced "imposed" solution which will be enforced if no settlement can be reached.</p> </abstract>
<abstract> <p>An example of a twice, continuously, differentiable utility function whose generated demand functions are not differentiable everywhere is given, along with necessary and sufficient conditions for such differentiability.</p> </abstract>
<abstract> <p>This paper uses input-output studies to compare the structure of production in four countries: the United States, Japan, Norway, and Italy. It first analyses the nature of interdependence as revealed by the pattern of interindustry flows and the extent of similarity in each country. Differences in the cost structure and use of each type of commodity are then measured, and some conclusions are drawn as to their origin.</p> </abstract>
<abstract> <p>The present paper is devoted to the question of the dynamic stability of a perfectly competitive market with the price adjustment rate proportionate to excess demand. Although the problem dates back to the nineteenth century, a general answer is not yet available. In none of the cases studied has the system been found to be unstable. The same is true of the further results presented in "On the Stability of Competitive Equilibrium, II," by K. J. Arrow, H. D. Block, and L. Hurwicz scheduled to appear in the next issue of Econometrica. The latter paper extends the results of the present article in several directions.</p> </abstract>
<abstract> <p>The problem of simultaneous equation bias is examined in the context of the Cobb-Douglas production function and firm decision functions, assuming competition. Two extreme models are specified, depending on the nature of the disturbances in the relations. In one model, simultaneous equation bias does not occur; in the other model, it does occur. Asymptotic "least squares" estimates (estimates that would be obtained given an infinite sample) are presented for the latter case. In this case, (1) corrections for the bias of least squares estimates are derived for some specifications, (2) it is shown that there exists a fairly pronounced tendency for the least squares estimated elasticity sum to approach one, regardless of the true value of the elasticities.</p> </abstract>
<abstract> <p>The existence and uniqueness of competitive equilibrium in Graham's model of international trade is proved by using Kakutani's fixed point theorem. The method of this proof is sufficiently general that it may be applied to other competitive models in which Graham's restrictive assumptions have been considerably relaxed.</p> </abstract>
<abstract> <p>Much controversy about causality and related concepts has arisen in the philosophical literature, and the haze surrounding the discussion has spread to other fields, among them econometrics. This brief outline stresses certain general principles and ideas that are relevant for actual research activity in the natural and social sciences. The main points are: (1) The concept of causality is indispensable and fundamental to all science; (2) The controversial issues are not latent in the concept of causality itself, but in certain questionable hypotheses, so-called "laws of causality," such as the universal scope of causality, the certainty (irrevocability) of a cause-effect relationship, the connection of causality with theories of induction, the principles of determinism and freedom of the will, etc; (3) A definition of causality which seems to be adequate from both common-sense and scientific points of view is suggested with reference to the well-known situation of the controlled experiment; (4) Some general remarks on statistical methods from the viewpoint of the dual distinction between descriptive and explanatory analysis and between experimental and nonexperimental observations are made; (5-6) Some comments are offered on causal relations as a tool in econometrics, with special regard to the rationale of different types of economic models.</p> </abstract>
<abstract> <p>An attempt is made here to carry the concept of the standard error of forecast, which has long been used with single equations, into the realm of the complete econometric model. The analysis is begun by reviewing the problem and developing the error formula for the single equation situation. This background is then used for approaching the analogous problem for the full model. The equation system forecasts a vector of endogenous variables which are essentially random variables with probability distributions. Hence a vector of standard errors is desirable. Formulas for estimating the elements of this vector are worked out in matrix form convenient for computation. The problem of presenting the forecast as a vector of distributions is discussed in terms of confidence intervals and tolerance intervals. Finally the reduced form, as distinct from the full structure method of estimating the forecast distributions, is considered.</p> </abstract>
<abstract> <p>This study involves an application of linear programming. The model, though hypothetical, is constructed in terms of relevance to characteristic multi-product and process operations, such as those found in the oil industry. All important phases, production, manufacturing (refining), transportation and marketing, are covered. Through tax considerations, financial as well as the more usual price, cost, and technological conditions, are included. The dual theorem of linear programming is used in a manner which makes it relevant to sensitivity analysis involved in studying technological and structural changes. This suggests possible uses of the dual theorem to areas other than transportation and classical decentralization analysis which have, to date, received major attention in the programming literature. By means of the dual formulation and specified computational procedures detailed in this paper considerable simplification in sensitivity analysis is achieved. A general theorem is established (easily applied in this and similar cases) by means of which an optimal solution to the dual is obtained without the need for matrix inversion or calculating an optimal simplex tableau. Finally, the solution and analysis are carried out with respect to a "kinked" functional which is only piecewise linear. A prototype is thus provided for handling a certain class of nonlinear programming problems which are also relevant to important areas of economic analysis.</p> </abstract>
<abstract> <p>In linear regression models in which the disturbances are autocorrelated, it is often assumed that these are given by a Markov process. This article investigates the loss of efficiency of estimators of the regression parameters when there are certain types of specification bias concerning the disturbances. Specifically, it points out that the loss of efficiency can be serious if the initial conditions of the process which are assumed to be true are in fact not true. In particular, if the process is incorrectly assumed to be stationary, then although the estimation procedure which is presumed to yield the best linear unbiased estimators will produce unbiased estimators, their joint efficiency may nevertheless be close to zero.</p> </abstract>
<abstract> <p>This study maintains that the American economy is characterized by a universe with more than one basic autoregressive population structure. Agreement with this hypothesis is tested by the analysis of the statistical properties of a number of economic time series. A comparison is then made with the results of other investigators of this problem, and finally further elaborations on the nature of these basic autoregressive population structures are entertained.</p> </abstract>
<abstract> <p>To test the vulnerability of the U.S. economy to depression, an econometric model has been built, which seeks to put the automatic stabilizers in a realistic macroeconomic setting. This model makes it possible to estimate the impact of alternative time patterns of decline of fixed investment and government purchases on the quarterly national income accounts, particularly on GNP. The model is also used to discover the implications of new potential policies, particularly improvements in the automatic stabilizers. A simulation approach is employed to study the effects of the error terms in the equations, both to see the forecasting potential of a model of this sort and to test whether errors lead to cumulative deviations from the expected path of the system.</p> </abstract>
<abstract> <p>This study analyzes liquidity functions (demand for money) in the American Economy. Total money holdings and idle balances are treated as alternative variables. In Part I an aggregative liquidity function is analyzed twice, once using interest rates and wealth as independent variables and once using last year's idle balances as a third independent variable. Very good fits were obtained in both cases. In Part II the liquidity function is disaggregated by major holders. Only one independent variable, the rate of interest, is used. Most emphasis is on year-to-year changes. Estimates obtained in this way are compared to those of a naive model. Finally the elasticity of the liquidity functions at various levels of interest rates is analyzed. Neither the data nor theoretical considerations give any reason for expecting a liquidity trap.</p> </abstract>
<abstract> <p>This paper discusses the relative merits of ordinary least squares estimation and several simultaneous-equations methods for econometric purposes. Most of the evidence appealed to is in the form of Monte Carlo experiments, because there is as yet little small sample theory of the properties of the different estimators.</p> </abstract>
<abstract> <p>This paper offers reasons for believing that experience with applications of existing simultaneous equations procedures does not yet furnish a basis for reliably appraising their practical usefulness. The need to consider simultaneity of relations in conjunction with other sources of specification error is stressed. Prediction with knowledge of some endogenous variables is briefly discussed.</p> </abstract>
<abstract> <p>Since the publication of Haavelmo's well-known papers on simultaneous equation estimation in 1943-44, the literature on this subject has dealt mainly with overidentified relationships. Practically all the econometric models estimated by the simultaneous equation approach are overidentified, the contention being that economic theory or a priori information would require the exclusion of a sufficient number of variables from a given relationship so that it becomes overidentified. It is argued in this paper that the contrary is true and that economic theory would require the inclusion of many more variables than those found in the existing econometric models so that the structural relationships included in these models are likely to be underidentified. Apparently "reasonable" overidentified structures have been obtained probably only because the specification errors arising from omission of relevant variables have cancelled one another. A consequence of the prevalence of underidentified structures is that the least squares reduced from equations are likely to be the best forecasting equations.</p> </abstract>
<abstract> <p>The choice between single-equation and equation-systems methods of estimation is often based on improper criteria and unjustified claims. On the one hand, advocates of the equation-systems approach probably overstated their case in the early years of enthusiastic development. While the gains to be realized from the use of more powerful statistical methods are quite modest, they are, nonetheless, real and should not be neglected. On the other hand, coefficient-by-coefficient comparisons of structural estimates by the two approaches are not suitable in many situations. There is need for comparisons of summary statistics of a whole system. In particular, reduced form coefficients may show large differences even though component structural coefficients appear to be close. In addition, the efficiency properties of single equation least squares estimates do not hold under transformation from structural to reduced form coefficients. These points are clearly revealed in a number of recent Monte Carlo studies.</p> </abstract>
<abstract> <p> In this paper we discuss an application of the competitive mechanisms to mathematical programming. The competitive method was first investigated by Professor Paul A. Samuelson for linear programming and was later applied to concave programming by Professors Kenneth J. Arrow and Leonid Hurwicz. We shall consider two formulations of Walras's tâtonnement processes in order to solve programming problems and show that both processes are globally stable for general programming problems in which neither the concavity of the maximands nor the convexity of the feasible sets is necessarily assumed. </p> </abstract>
<abstract> <p>Expenditure elasticities are generally obtained by assuming certain forms such as the semi-log, double-log or probit relation for the Engel curves and then estimating them from family budget data by the conventional method of least squares. This procedure is by no means the best, particularly when grouped data are available. In this paper an alternative method is proposed for estimating the Engel elasticities from two types of concentration curves which together describes on an average the consumption pattern of the community.</p> </abstract>
<abstract> <p>A dual interpretation of the Leontief dynamic input-output system suggests the problem: What is the relationship between the stability of the output system and the stability of the dual? If the output system is stable, the dual must be unstable and vice versa, at least for the closed system. For the open system the statement is true except under rather implausible circumstances.</p> </abstract>
<abstract> <p>This article reviews the empirical nature of isoclines or expansion paths from production functions derived from experiments. The algebraic and geometric nature of isoclines is explained for conditions under which interaction is both present and absent among the production factors analyzed, the economic implication of the various families of isoclines also is explained. The production functions for which the isoclines are derived come from physical producing units, rather than from firms or industries.</p> </abstract>
<abstract> <p>This paper reports results which verify the general proposition that, where each unit of a time series is an average of points within that unit, the effect of such averaging will be to introduce a positive first-order serial correlationin the first differences of such a series even where the original series is a random chain. The findings here reported are the result of an investigation undertaken after Professor Holbrook Working pointed out to me that certain erroneous conclusions had resulted from failure to recognize that such a disturbance had been caused by this averaging process in one of several time series analyzed in a paper by the late Herbert E. Jones and myself, entitled "Some A Posteriori Probabilities in Stock Market Action," which was published in the July, 1937, number of Econometrica, pages 280-294.</p> </abstract>
<abstract> <p>A model for choices among uncertain alternatives is developed in which preference between pure alternatives and likelihood judgments between events areassumed to be independent probabilistic processes. It is, in some respects, a probabilistic version of utility models of the von Neumann-Morgenstern type. Certain plausible notions about subjective probability are shown to imply a very simple discrimination function between events in which the probability of choice depends only upon differences of subjective probabilities. Similarly, the expected utility hypothesis is shown to imply that preference discrimination depends upon utility differences, and the form of that discrimination function is determined. Questions of empirical verification are discussed.</p> </abstract>
<abstract> <p>This article describes the geometric tools which can be employed for the qualitative analysis of second order difference equations. By showing how linearequations can be investigated by geometric methods it suggests how some nonlinear equations can also be handled with the aid of these tools. This is illustrated by a geometric restatement of the Hicksain (nonlinear second order difference equation) trade cycle model.</p> </abstract>
<abstract> <p>In the framework of the Theory of Games several attempts have been made to solvethe problem of how to act in bargaining situations. None of these attempts seemsto be completely satisfactory. This paper presents the report of an empirical investigation. An experiment which tested the reactions of students actually playing a bargaining game is discussed. The data from this experiment are used to support the view that "caution" as a personal characteristic of the player must be considered in any satisfactory solution of the problem.</p> </abstract>
<abstract> <p>Demand and supply depend in general not only on current prices but also on expectations. If it is assumed that expectations are formed from past prices in the adaptive way suggested by Cagan and others, it is shown that under certain circumstances the resulting system of multiple markets has dynamic stability.</p> </abstract>
<abstract> <p>In this note we are concerned with a technical point concerning the Cobb-Douglasfunction fitted to productivity data. We derive a new variance formula for the estimated marginal productivity. This formula is based on assumptions which we consider to be more realistic than those underlying the variance formula in current use which may lead to sizeable errors in estimation.</p> </abstract>
<abstract> <p>This is a study of the varying relative importance of different factors determining the import demand function for burlap in the U.S.A. as between different time periods based on regression analysis. The limited information method and the least squares method are applied to the same model and a case study is made of their mutual consistency.</p> </abstract>
<abstract> <p> This paper considers the problem of rural-urban land conversions as a special case of the more general one of how the equilibrium location of firms in a von Thünen plain changes with changes in the conditions of demand and supply for the commodities they produce. After treating the problem of equilibrium location rather generally, a model involving specific assumptions about the relevant functional relationships is developed in detail. Some tentative conclusions about the direction of land conversions are reached on the basis of the probable magnitudes of a few strategic parameters. </p> </abstract>
<abstract> <p>A set of linear expenditure functions for k commodity groups comprising all consumers' expenditure is constructed which contains 2k + 1 regression coefficients, one of them common to all k equations. The regression coefficientsare obtained by the least squares method, and their economic meaning is discussed. The method is applied to expenditure per head on 11 commodity groups in the United Kingdom, 1948-57. Estimates for income elasticities of demand and for shifts in demand are obtained, and their reliability is examined.</p> </abstract>
<abstract> <p>Voting is presented as an n-person majority game, in which preferences are ordinal. A condition on the preferences, substantially weaker than one postulated by D. Black, is shown to be sufficient for "stability" in such games.</p> </abstract>
<abstract> <p>The mathematical foundations of rational behavior, in the sense of a transitive ordering of alternatives, are developed without making any assumptions about the special character of the set of alternatives from which choices are made. It is shown that the ordering of alternatives may be characterized by a utility function, where utility is represented by a vector with real-valued components, such vectors being ordered lexicographically (like the words in a dictionary). If an axiom permitting comparison of intensities of preference is admitted, such a utility index must be unique up to transformations (such as proportionality transformations) preserving group operations. A purely topological axiom, called the Axiom of Substitution, is shown to imply that utility is real-valued (i.e., that the above vector has only one component).</p> </abstract>
<abstract> <p>This paper considers the notion of additivity in connection with direct and indirect utility functions. The implications of direct and indirect additivity for demand elasticities are derived. An empirical comparison of a directly additive with an indirectly additive preference system suggests that direct additivity is more realistic.</p> </abstract>
<abstract> <p>Consider a group of consumption goods industries in a process of economic growth. It is often assumed that the production of each sector will expand proportionately to the income elasticity of demand for its products. This simple rule may need some modification if capital can substitute for labour to different degrees in different sectors, and total capital stock grows at a rate which is different from the rate of growth of total labour input. Different rates of technical progress may also give rise to a need for modifying the above mentioned simple rule. Approximate formulas for these modifications are worked out in such a way that they can be applied for quantitative evaluations. Some rough numerical illustrations are offered in the concluding section of the paper.</p> </abstract>
<abstract> <p>Alternative approaches to the measurement of capacity are possible. Economic aspects of capacity measurement, as contrasted with pure engineering considerations, involve the introduction of cost considerations and limitations imposed by interdependence of different sectors of the economy. Some approaches to capacity measurement in terms of cost functions are explored, particularly as properties of a probit total cost function. A method of aggregating individual capacities, taking into account the interdependence of an input-output model, is outlined. Capacity measures, so obtained, are useful as descriptive statistics of economic efficiency and as explanatory variables in modern theories of investment behavior.</p> </abstract>
<abstract> <p>This paper investigates Böhm-Bawerk's idea of a preference for advancing the timing of future satisfactions from a somewhat different point of view. It is shown that simple postulates about the utility function of a consumption program for an infinite future logically imply impatience at least for certain broad classes of programs. The postulates assert continuity, sensitivity, stationarity of the utility function, the absence of intertemporal complementarity, and the existence of a best and a worst program. The more technical parts of the proof are set off in starred sections.</p> </abstract>
<abstract> <p>The paper shows that the output-investment ratio as well as the income-investment ratio is determined by the technological conditions of increasing production. The relation between sectorial allocation and physical composition of investment is shown. The question of maximising the rate of increase of national product or of national income, respectively, is discussed subject to the condition that minimal consumption requirements be satisfied.</p> </abstract>
<abstract> <p>Fractile Graphical Analysis is a new method of statistical analysis which provides an effective summary of information particularly useful in situations where the data do not permit a description in terms of a few parameters relating to the distribution; it also provides a graphical way of testing differences between groups. This method can be used for any variate which can be ranked. In this paper, the use of this method is illustrated for the comparison of economic data--relating either to the same population at different points of time or to different populations--by means of examples taken from the Indian National Sample Survey.</p> </abstract>
<abstract> <p>In this paper, we explicitly introduce consumption into the von Neumann models of economic growth and show that the two alternative Marx-von Neumann and Walras-von Neumann models--in both of which the effects of prices, the rate of interest, and the real wage rate on the consumption coefficients are allowed--have balanced growth solutions. The relation between the rate of interest and the rate of balanced growth is discussed also. It should be noted, however, that the models posses one degree of freedom; once the real wage rate is given, all prices, the rate of interest, and the rate of growth are determined. The rate of growth thus determined is a variant of Harrod's warranted rate of growth. The real wage rate is fixed at the level where the warranted rate and the natural rate of growth are equal.</p> </abstract>
<abstract> <p>Contrary to arguments advanced by others, it is shown in this note that, in general, a stochastic variation for a given relationship need not be accentuated in simultaneous models. Any statement to that effect must be based on a theory about shocks.</p> </abstract>
<abstract> <p>The vague and often teleological LeChatelier principle of thermodynamics can be formulated as an unambiguous mathematical theorem concerned with elements of the definite matrices associated with maximizing problems. It thus has found many applications in economic theory: e.g., in the study of how the constraints imposed by rationing diminish the price elasticity of a maximizing demander. The present paper shows that the LeChatelier principle can also be extended to Leontief-Metzler-Mosak systems by virtue of the special properties of their off-diagonal elements, and even though they do not have the symmetric and definite matrices characteristic of a maximum problem. Hence, the principle may be applicable to analysis of input-output, multisectoral Keynesian multiplier systems, and general demand analysis involving gross substitutes.</p> </abstract>
<abstract> <p>Koyck's method for distributed lags works on the assumption that successive lag coefficients decrease geometrically. The present paper generalizes his approach to a family of J-shaped or unimodal lag distributions given by the Pascal distributions (of which the geometric is a special case). There is an unsystematic discussion of the problem of estimation; and the final section considers the circumstances under which a moving average can be transformed into a finite recursion.</p> </abstract>
<abstract> <p>This paper illustrates the results obtained by adapting the model proposed by the authors in an earlier paper [6] to incorporate the method of estimating durability proposed by Nerlove [3]. It is found that in the cases examined the estimates of durability obtained in this way are very low. Possible reasons for these results are briefly indicated but are not examined empirically.</p> </abstract>
<abstract> <p>This paper, which in part serves as a common introduction to the two papers following in this issue, attempts to define the meaning of the "casual interpretability of a parameter" in a system of simultaneous linear relationships. It attempts, moreover, to expound a basis for interpreting the parameters of a nonrecursive or interdependent system casually. This is done in terms of an underlying causal chain system to which the interdependent system is either an approximation or a description of the equilibrium state.</p> </abstract>
<abstract> <p>When interdependent models are regarded as approximations to recursive models, a specification error is made. What is the importance of that specification error for the maximum likelihood estimation procedure? This question is analyzed for the case where lagged endogenous variables are treated as current on the grounds that the lag is small relative to the observation period. It is argued that the appropriate estimation procedure is qualitatively different from that which ignores the specification error, however small the omitted lag may be. It is contended that as the lag approaches zero the limit form of the likelihood function is not the likelihood of the limit form of the model. The article is, however, exploratory and the analysis lacks complete rigor.</p> </abstract>
<abstract> <p>The parting of the ways between causal chain (recursive) and interdependent (nonrecursive) systems is reviewed from the point of view of explanatory relations specified in terms of conditional expectations. On the customary assumptions, a causal chain system is designed so that its relations both in the original form and in the reduced form can be specified in terms of conditional expectations, whereas the relations of interdependent systems allow such specification only in the reduced form. A third type of model is discussed, called conditional causal chains, which formally is similar to interdependent systems, with the important difference that the behavioural relations of the original system are specified in terms of conditional expectations.</p> </abstract>
<abstract> <p>This article deals with the construction of price and quantity index numbers for an arbitrary number of periods (or georgraphical units) which satisfy the requirement that the total sum of squares of the discrepancies between true and index-constructed cross-values is minimized. Special attention is paid to the aggregation problem which arises when this method is applied to a group of commodities as well as to subgroups.</p> </abstract>
<abstract> <p>The problem dealt with in this article is whether we can indicate, with the help of measured economic concepts, the rate of savings--as a function of time--which maximizes utility over time. The author believes that his attempt has been unsuccessful, but hopes that the nature of the difficulties encountered may be of some help in future attempts to solve this problem--a problem regarding the most important decision to be taken for any development program.</p> </abstract>
<abstract> <p>In linear programming we assume that all the parameters of the problem, i.e., the coefficients of the objective function, the inequalities and the availabilities are known numbers. This is frequently a not very realistic assumption. In stochastic linear programming the parameters become random variables, i.e., we know only their distribution. In the passive approach to stochastic linear programming the distribution of the objective function is approximated and decisions are based upon this distribution. In the active approach the decision variables are the amounts of resources to be devoted to the various activities.</p> </abstract>
<abstract> <p>The central purpose of this article is to produce a model that will explain both fluctuations and growth through the operation of endogenous forces. It thus attempts a synthesis of business cycle theories of the Tinbergen type and growth theories of the Harrod type. The model stresses those factors that, in the opinion of the author, explain the great depression of the 1930's and the continued growth (subject to minor fluctuations) of the economy since the war. It does not seek to explain the minor fluctuations that have occurred in the last decade. This article has been arranged, at the cost of some repetition, to meet the requirements of non-mathematical and mathematical readers.</p> </abstract>
<abstract> <p>Two problems in the allocation of indivisible resources are discussed. Both can be interpreted as problems of assigning plants to locations. The first problem, in which cost of transportation between plants is ignored, is found to be a linear programming problem, with which is associated a system of rents that sustains an optimal assignment. The recognition of cost of interplant transportation in the second problem introduces complications which call for more laborious and largely unexplored computations and which also appear to defeat the price system as a means of sustaining an optimal assignment.</p> </abstract>
<abstract> <p> The classical method of least-squares estimation of the coefficients α&lt;/b&gt; in the (matrix) equation y = Zα + e yields estimators α̂ = Ay = + Ae. This method, however, employs only one of a class of transformation matrices, A, which yield this result; namely, the special case where A = (Z′Z)-1Z′. As is well known, the consistency of the estimators, α̂, requires that all of the variables whose sample values are represented as elements of the matrix Z be asymptotically uncorrelated with the error terms, e. In recent years some rather elaborate methods of obtaining consistent and otherwise optimal estimators of the coefficients α have been developed. In this paper we present a straightforward generalization of classical linear estimation which leads to estimates of α which possess optimal properties equivalent to those of existing limited-information single-equation estimators, and which is pedagogically simpler and less expensive to apply.3 </p> </abstract>
<abstract> <p>This paper considers optimization problems in which some or all variables must take on integral values. An ability to solve such problems would be valuable in itself, and would also allow us to handle certain kinds of heretofore intractable "economies of scale." We do not present an automatic algorithm for solving such problems. Rather we present a general approach susceptible to individual variations, depending upon the problem and the judgment of the user. Two moderate-size examples are presented to illustrate the method.</p> </abstract>
<abstract> <p>An important problem in econometric theory requires inversion of a Leontief matrix of inter-industry transactions, the order of such a matrix being equal to the number of industries comprising the given economy. For Leontief matrices of high order, to avoid accumulation of round-off errors inherent in direct processes of matrix inversion such as the Gauss-Doolittle method, F. V. Waugh [11] proposed a certain power series (Neuman's Series) which approximates the inverse of a Leontief matrix to any desired degree of accuracy. The present paper indicates several other power series inversions of the Leontief matrix each of which converges more rapidly than the series roposed by Waugh. Application of the new method to power series inversion of other kinds of matrices is also indicated.</p> </abstract>
<abstract> <p>To determine an optimum economic policy one needs: (a) a welfare function valuing "target" variables; (b) a model, describing the effect of policy "instruments" on the targets; and (c) limits within which the variables are allowed to vary. The welfare function is derived by "imaginary interviewing of the policy-makers." It is linearized in intervals or "facets." Linear programming indicates the optimum policy within each facet of the welfare function. A policy optimal with respect to all facets around it is the absolute optimum. By way of an appendix, a survey is given of the Multiplex Method of programming, developed by Professor R. Frisch, and particularly suited to this type of analysis.</p> </abstract>
<abstract> <p>This article is concerned with the interpretation of econometric equation systems as systems of macrorelations. If an aggregate consumption function, say, is part of such a system and if it is also regarded as the macroecnomic analogue of a set of consumption functions of individual households, what is then the relationship between the macroeconomic consumption function and the underlying microeconomic consumption functions? The analysis, which is partly statistical, is carried out for linear equation systems.</p> </abstract>
<abstract> <p>This paper is a complement to Morishima's [9]. It generalizes the usual dynamic Leontief model by removing the restrictions that all stocks of capital goods be fully utilized, and it generalizes the usual price theory of such models by allowing prices to change and to be expected to change. The main analytical tool used is linear programming and the well-known relation between the dual variables and competitive shadow-prices. Some conclusions are reached about the ability of a system of current and futures prices to police an efficient intertemporal allocation of resources under stringent simplifying assumptions.</p> </abstract>
<abstract> <p>The Brouwer fixed point theorem is used to prove the existence of a competitive equilibrium under more general assumptions than those which have been required in earlier discussions.</p> </abstract>
<abstract> <p>This article illustrates briefly by a practical example the computation of some generalized classical linear estimates of coefficients in a structural equation.</p> </abstract>
<abstract> <p> This paper is a sequel to ``On the Stability of the Competitive Equilibrium, I,'' by K. J. Arrow and L. Hurwicz. It extends the results of ``I'' in several directions. In particular, it provides a proof of stability in the large (and not merely locally) when all goods are gross substitutes; this result is found to be valid for processes where the price adjustment rate is a continuous sign-preserving, but not necessarily proportionate, function of excess demand. The paper deals both with systems where one of the commodities plays the role of numéraire and with systems where all commodities are treated symmetrically. </p> </abstract>
<abstract> <p>This article is concerned with the differences in the amplitudes executed by the endogenous variables in a stochastic difference equation model. It is shown that the differences in the amplitudes are partially a function of the structure of the model. An application to a bi-sector Hicksian trade cycle model is provided as an illustration.</p> </abstract>
<abstract> <p>The consequences of using ordinary Single Equation Least Squares in estimating regression parameters are examined in a case where the parameters relate to a household expenditure study in which the statistical model underlying the study is really a set of simultaneous equations.</p> </abstract>
<abstract> <p>This paper discusses the possible use of the Aeracom-type analogcomputer in solving dynamic economic models. The first part presents an industry inventor model as an illustration and shows solutions that were obtained by use of the Aeracom for various parameter values. The latter part discusses the types of equations that can be handled by the Aeracom and the general applicability of the computer to economic models.</p> </abstract>
<abstract> <p>This paper deals with the effects of investment in one region or country upon income in all regions of an n-region system, and with the relations between these income movements and the pattern of trade among the various regions or countries. It includes both a static system of n equations based upon the the usual definition of income and a corresponding dynamic system based upon the assumption that the output of a given region or country tends to rise when demand exceeds supply and to contract when supply exceeds demand. Under the assumed conditions, it is shown that stability of the system may be described in terms of Hicks's "conditions of perfect stability." The Hicks conditions, in turn, are dependent upon the marginal propensities to spend of the various regions. Throughout the discussion of the static problems, the system is assumed to be dynamically stable.</p> </abstract>
<abstract> <p>The concept of the multiplier is extended to an economy composed of a number of sectors, such as countries, regions, industries, classes, and functional groups, or individuals, firms, and governments. The economy should be divided to such anextent as to maximize the stability of the parameters. If all the participating groups have the same marginal propensity to spend, the conventional multiplier formula is obtained. Some properties of such a system are described--in particular, the conditions for dynamic stability; the effect of changes in relative prices is also introduced.</p> </abstract>
<abstract> <p>The problem of the identifiability of a linear relation between variables subject to error is examined in two different two-variable models in which the errors are independent of the "true" variables. In one model the errors are specified to be jointly normally distributed, and in the other to be stochastically independent. In both cases necessary and sufficient conditions for identifiability are found. A summary of previous results is included.</p> </abstract>
<abstract> <p>In the theory of the firm under perfect competition a period which measures the effect of the rate of interest on relative prices of inputs and outputs should be called the period of production. Alternatively, this period can be defined as the difference between an output period and an input period, each with respect to the plans of a marginal entrepreneur about to start a new firm. The output period is the weighted mean of times to future sales; similarly, the input period is the weighted mean of times to future purchases. The value of the period of production is considered here for certain simple cases where it can be simply related to the average periods of the capital goods being used. These results provide approximations for more realistic cases where the average period of a capital good is less easily defined. Subsequently we examine the variations in this period of production when the rate of interest changes. If the results are applied to an economy in which each firm produces only one commodity, the complete period of production can then be defined for each commodity, and this determines the effect of changes in the rate of interest on the ratio of the price of this commodity to an index of prices of the primary factors of production, labour and land. Finally, some rough calculations are made to estimate the order of magnitudes of actual complete periods of production.</p> </abstract>
<abstract> <p>Short cut computational methods are developed for solving systems whose matrices may be generally described as block triangular.</p> </abstract>
<abstract> <p>By taking account of obvious and inescapable limitations on the functioning of the accelerator, we explain some of the chief characteristics of the cycle, notably its failure to die away, along with the fact that capital stock is usually either in excess or in short supply. By a succession of increasingly complex models, the nature and methods of analyzing nonlinear cycle models is developed. The roles of lags and of secular evolution are illustrated. In each case the system's equilibrium position is unstable, but there exists a stable limit cycle toward which all motions tend.</p> </abstract>
<abstract> <p>This paper reports the results of a statistical investigation of consumer response to shifts in the prices of butter and margarine. The quantity data are shipments to retail stores of a chain in Rhode Island, the price data are retail prices. It is believed that the market and production conditions during the period under study were such that a relatively simple statistical is justified.</p> </abstract>
<abstract> <p>A simple electric circuit is described that determines the equilibrium prices and quantities that will result in a static model when a number of interdependent trading units stand ready to buy or sell a homogeneous good, according to known trading functions, and when there are significant freight costs per unit between each trading unit and every other. This circuit is compared as a method of solution with digital computers and electronic differential analyzers. The importance of these possibilities to traditional value theory is indicated.</p> </abstract>
<abstract> <p>This paper contains a simple dynamic theory of demand in which distinctions are drawn between consumption and net investment and also between actual and equilibrium levels of consumption. On this basis analyses are made for clothing and for household durable goods for the interwar and postwar periods. No significant differences are found in the responses of the two periods and methods are given for analysing together the annual data available for the earlier period and the quarterly data available for the later one.</p> </abstract>
<abstract> <p>The problem of statistical estimation of the parameters of Leontief and other linear models is discussed. The discussion is based on the assumptions that in such interdependent models there is one variable which may be regarded more or less as exogenous and that the random errors may be effectively described by means of a normal probability distribution. It is found that in certain circumstances the least squares method of estimation when applied directly to these models gives rise to a large bias and underestimation of the random variation. Furthermore, the results differ the more from those obtained by maximum likelihood estimation the larger the degree of interdependence and the larger the relative importance of the residual variation in the models. The analysis assumes certain results of the theory of Leontief models and of simultaneous equations methods of estimation in particular and of the theory of statistical estimation and regression in general.</p> </abstract>
<abstract> <p>The standard theorems on the Leontief System are proved in a new and elementary way. These concern the existence of a unique nonnegative solution, duality properties, efficiency and profitability, and the generalization to allow substitution of the inputs to an industry.</p> </abstract>
<abstract> <p>The purpose of this paper is to test a theory of investment behavior based on the neoclassical theory of optimal capital accumulation. This theory of investment behavior determines the demand for capital services, the relationship between changes in demand and actual investment expenditures, and investment for replacement purposes. Investment functions based on the theory are fitted to data for fifteen sub-industries for manufacturing in the United States for the period 1947-1960 and groupings of these industries into total durables, total nondurables, and total manufacturing. The empirical results provide new evidence on the relationship of determinants of investment expenditures to instruments of economic policy. These results suggest an important role for the tax structure and the cost of capital in the determination of investment expenditures. Investment functions for groupings of the sub-industries are tested for the presence of errors of aggregation. The use of such aggregates results in substantial errors of aggregation.</p> </abstract>
<abstract> <p>This paper investigates the interrelationships among Samuelson's correspondence principle, quasi-dominant diagonal matrices, and Morishima matrices, in the context of a qualitatively specified environment. In addition, the stability properties of the Morishima case are studied under the competitive assumptions that Walras' Law holds and excess demand functions are homogeneous of degree zero in prices.</p> </abstract>
<abstract> <p>This article gives necessary and sufficient conditions for the existence of a finite maximum of a quadratic functional. The functional is the present value of a revenue stream in discrete time over an infinite horizon. Both scalar and vector versions of the problem are solved. It is shown that the problem always has a solution for a sufficiently high finite discount rate. Some conditions to ensure the nonnegativity of the solution are also presented. The origin of the problem is finding the sequence of outputs that will maximize the present value of the net return of a monopolist who sells k related products with related demands described by a set of k nth order linear difference equations.</p> </abstract>
<abstract> <p>Pontryagin's Principle can be and has been used in inventory theory, production theory, capital theory and growth theory. The idea presented in this paper shows how the principle can be used also when a firm operates in a market economy or a country in a world market. The resulting jumps in the state variables--amount of capital, amount sold of a commodity, etc.--will disappear through a reinterpretation of the system. The idea is that time is one of the variables and the speed of time can be controlled. By stopping time and letting the other variables change, one can get jumps with respect to time.</p> </abstract>
<abstract> <p>The difficulty of obtaining empirical coefficients of long-run economic relations stems from the fact that long-run equilibrium values of economic variables are unobservable. To overcome this difficulty, assumptions are usually made about the relationships between observed and the unobserved long-run variables. Such assumptions have led to distributed lag analysis. In an earlier paper I have shown that such an approach is likely to give erroneous results, for the adjustment process may not be consistent with the comparative statics solutions for the short-run variables. In the present paper, the earlier analysis is supplemented and it is also shown how the problem can be overcome and how the long-run coefficients can be estimated even though the long-run equilibrium values are unobserved. The basic deviation from other formulations of this subject is that adjustment equations are used to described only those movements of variables about which comparative statics has nothing to say. In this way, contradictions between comparative statics and ad hoc dynamic formulations are avoided.</p> </abstract>
<abstract> <p>The theory of economic models of decentralization is developed from a point of view quite different from the virtual planning phases of economic systems as displayed by market mechanism type procedures. The Central unit transmits appropriate information to each divisional unit which then acts (commits resources) according to optimization with respect to variables under its control. Our theory presents a natural hierarchy of coherently decentralized systems which is based on the increasing amounts of information which are to be transmitted to divisional units for proper decentralized decision making. In general, this information involves more than prices alone and the notion of preemptive goals is introduced for this purpose by means of examining dual convex programming problems. Employing our precise specifications, uniqueness of divisional optima (in contrast to certain trivial cases of decentralization) is not required. Thus, additional flexibility is available to divisions without violating company goals. Further we show that our procedure is a robust one in the sense that approximate fulfillment of the preemptive goals, e.g., non-optimal but close to optimal divisional solutions, results in small deviations from optimal profit.</p> </abstract>
<abstract> <p>Most dynamic theories of the monetary policy mechanism, and reserve position doctrine in particular, specify a homogeneous banking system response to monetary policy actions. This article argues that the micro models of bank behavior implicit in these monetary theories remain untested and perhaps misspecified. An experimental model of a single bank's weekly balance sheets adjustments is specified and estimated with the purpose of improving the micro foundations of the theory of monetary dynamics.</p> </abstract>
<abstract> <p>A new method is proposed for deriving skew distributions of business firm sizes from the assumption of Gibrat's Law. The growth of the firm is decomposed into an industry-wide component and an individual component, the latter governed by a one-period Markov process. The model is fitted to data on the recent growth of large American firms.</p> </abstract>
<abstract> <p>This paper asserts and demonstrates an equivalence that exists between the utility function implied by a general form of the standard aggregate consumption function and a subclass of the general class of additive utility functions used in the optimal savings literature.</p> </abstract>
<abstract> <p>A multivariate errors-in-variables model is used to analyze the propensity to consume from each of several sources of income for a sample of 621 families in a three year (1960-1962) panel study. The analysis provides estimates of the relationships among the transitory components of the various types of income (contemporary and intertemporal) as well as the marginal propensities to consume out of the permanent component of income from each source. Total family income is much more stable than its components, as there is a tendency for changes in head's income to be offset by opposite changes in wife's and transfer income. The marginal propensity to consume out of the permanent component of both the head's and wife's labor income is .9, a result consistent with Friedman's for aggregate income, and implying that growing labor force participation of wives should have no effect on the long run savings rate.</p> </abstract>
<abstract> <p>The paper discusses why certain commonly used two-step procedures give estimators which are asymptotically less efficient than the maximum likelihood estimator when there are lagged dependent variables among the regressors. This sort of problem is often encountered in the estimation of distributed lag models with serial correlation in the residuals.</p> </abstract>
<abstract> <p>The problem of an optimal tax-subsidy scheme under which a socially optimal point is maintained through the competitive market mechanism in the presence of external economies of scale is discussed. The scheme is compared to the doctrine developed by Marshall, Pigou, Kahn, and Lerner.</p> </abstract>
<abstract> <p>This paper presents formulae for the standard error of forecast of a single equation and the covariance matrix of forecasts of a complete system of equations that are appropriate when the exogenous variables in the forecast period are stochastic. The problems of defining forecast intervals and multidimensional forecast regions are also discussed.</p> </abstract>
<abstract> <p>We introduce and analyze a class of variable elasticity of substitution (VES) production functions for which the substitution parameter varies linearly with the capital-labor ratio around the intercept term of unity. The VES function contains as special cases the more important special cases of the well known CES function. In terms of some familiar economic relationships, the VES posits a linear view of the world in contrast to the log-linear view posited by the CES function.</p> </abstract>
<abstract> <p>Once the coefficients of an econometric model have been estimated, the dynamic properties of the resulting system of equations are frequently of interest. In this paper Fourier methods are used to obtain the spectrum matrix of the endogenous variables of the Klein-Goldberger model. The power spectra and coherence and phase relationships implied by the model are derived for selected endogenous variables and are compared with previous results.</p> </abstract>
<abstract> <p>This paper is designed to make revealed preference an operational tool of research on utility theory. The preference relations are shown in Boolean matrices and digraphs to facilitate the processing of a large quantity of empirical data and to classify different types and degrees of consumer rationality.</p> </abstract>
<abstract> <p>In this paper, a continuous time model for a monopoly firm of the Evans' type, encompassing operations, investments, and output prices, is formulated as an optimal control problem. In the model the objective of the firm is to maximize, subject to various constraints, the integral of production profits less interest and investment costs over a finite decision-making interval, plus the value of the capacity at the end of the period. The state variables are capacity, debt, and output price; the controls are the scale of operation, rate of purchase of new capacity, and rate of change of the output price. Final capacity, price, and debt are control parameters. There are several inequality constraints. Using results in control theory, the optimal controls are characterized for a model basically linear in structure. It shows that the one case suggested by Evans for further analysis is a trivial problem. These results are interpreted using the properties of the value equation. In addition, the control model is formulated alternatively as a mathematical programming problem. Solutions may then be computed by publishing algorithms.</p> </abstract>
<abstract> <p>This paper develops approximations of the Gram-Charlier type to the cumulative distribution function of the instrumental variables estimator on classical assumptions. In the special case where there are only two endogenous variables in the estimated equation, exact values of the cumulative distribution function are computed by numerical integration and compared with the approximations. Although the error in the approximation depends critically on the parameters of the stochastic model, the approximation is good for the special case even for small sample size over a wide range of values of the parameters.</p> </abstract>
<abstract> <p>This paper attempts to test empirically the strong axiom of the revealed preference theory using consumer food panel data. For the purpose of the test the theory is recast in determinantal form to facilitate computer programming. Each family is first checked for complete consistency in all purchases. If it should fail to so behave, the second step is to ascertain the maximal subset of purchases that is consistent. Finally, we ascertain the maximal subset that is not inconsistent.</p> </abstract>
<abstract> <p>In 1948, when seeking estimates of costs of alternative weapon systems, the potentially embarrassing error of relating costs to rates of output while ignoring another relevant variable, quantity of items produced, was made obvious by access to the airframe production data analyzed in this paper. By 1949 the present paper had been completed for the RAND Corporation, but reliance on "military classified" data and sources prevented open publication at that time. Although the sources and data have been declassified for several years, it seems appropriate to publish the paper in its original form now that the phenomenon is being incorporated in formal economic theory.</p> </abstract>
<abstract> <p>The properties of various forms of Engel functions satisfying the additivity criterion are investigated. It is suggested that a form of relationship used by H. Working, but recently neglected, offers on balance great advantages. A logical generalisation of it yields a flexible function from which a good fit may be expected for most commodity groups.</p> </abstract>
<abstract> <p>Economists are generally agreed that higher rate levels have increased the automatic stabilizing properties of the federal personal income tax in the United States. Nonetheless, empirical attempts at estimating the degree of built-in tax flexibility have largely concentrated upon the response of income tax yield to changes in the tax base while disregarding such feedback effects as are apt to occur in a system of simultaneous equations representing the American economy. This paper specifies an econometric model for the U. S., estimates the coefficients of the structural equations, and utilizes the results to calculate the amount of built-in flexibility attributable to the federal income tax since World War II.</p> </abstract>
<abstract> <p>Monte Carlo techniques are used to study the first order autoregressive time series model with unknown level, slope, and error variance. The effect of lagged variables on inference, estimation, and prediction is described, using results from the classical normal linear regression model as a standard. In particular, use of the t and x^2 distributions as approximate sampling distributions is verified for inference concerning the level and residual error variance. Bias in the least squares estimate of the slope is measured, and two bias corrections are evaluated. Least squares chained prediction is studied, and attempts to measure the success of prediction and to improve on the least squares technique are discussed.</p> </abstract>
<abstract> <p>A queueing model--together with a cost structure--is presented, which envisages the imposition of tolls on newly arriving customers. It is shown that frequently this is a strategy which might lead to the attainment of social optimality.</p> </abstract>
<abstract> <p>A pure exchange economy is considered without the assumption of convex preferences. It is shown that the divergence from equilibrium due to non-convexity is bounded in a fashion independent of the number of traders. For a sufficiently large number of traders there are configurations arbitrarily close to equilibrium.</p> </abstract>
<abstract> <p>The problem of prediction of a subset of the endogenous variables conditioned on the balance of the endogenous variables, as well as the predetermined variables, is discussed in the context of a general linear model with normal disturbances. It is shown that the conditional least-squares predictor is unbiased, correcting Srinivasan's [9] treatment of Waugh's paper [10] in a less general setting. In the course of the argument, a simple derivation of the multivariate t density is presented. Srinivasan's [9] remark that consistency of an estimator does not imply its asymptotic unbiasedness (in one of the common senses of that term) is illustrated by two examples, one with a discrete and one with a continuous distribution. They show that neither asymptotic unbiasedness nor zero asymptotic variance is necessary for consistency. The example with a continuous distribution is the two-stage least-squares estimator.</p> </abstract>
<abstract> <p>The usual least squares estimator of a set of reduced form parameters is unbiased. We show here that a certain conditional least squares estimator is also unbiased, and record some related distribution theory.</p> </abstract>
<abstract> <p>A mixed model of regression with error components is proposed as one of possible interest for combining cross section and time series data. For known variances, it is shown that Aitken estimators and covariance estimators are in one sense asymptotically equivalent, even though the Aitken estimators are more efficient in small samples. Turning to unknown variance components, Zellner-type iterative estimators are compared with covariance estimators. Here, few small sample properties are obtained. However, it is shown that covariance and Zellner-type estimators have equivalent asymptotic distributions and equivalent limits of sequences of first and second order moments for weakly nonstochastic regressors. For the model analyzed, the theoretical results obtained, as well as ease of computation, tend to support traditional covariance estimators of the regression parameters. An additional interesting result presented in an appendix is that ordinary least squares estimates of the β's (ignoring the error components) have unbounded asymptotic variances. On efficiency grounds, this argues rather strongly for some care in combining data from alternative sources in regression analysis.</p> </abstract>
<abstract> <p>A broad class of separable programming problems admits a degree of decentralization. The propositions below state conditions under which optima of the global problem imply optimal solutions to the subproblems and vice versa.</p> </abstract>
<abstract> <p>From knowledge of budget proportions, income elasticities, and the income elasticity of the marginal utility of income, point estimates of all direct and cross demand elasticities with respect to price may be easily computed for want-independent goods. Estimates obtained in this manner are without standard errors. One method of ascertaining the reliability of such estimates is to compare them, under varying conditions, with corresponding estimates whose standard errors are known, as is done herein.</p> </abstract>
<abstract> <p>The variance of solutions to stochastic linear dynamic systems tends to a finite limit if the system is damped. Limit cycles are possible in linear systems, but the corresponding solutions would have increasing variance in the stochastic case. On the other hand, nonlinear systems with limit cycles may have solutions with bounded variance in the stochastic case. A numerical example illustrating the limit cycles of Kaldor's nonlinear trade cycle model is shown to have solutions with bounded variance when the system is subjected to random shocks. The variance series is periodic with one-half the cyclical duration of the mean series. The amplitude of the mean series decreases over time. In a linear system, the mean series would have the same cyclical pattern as the nonstochastic solution.</p> </abstract>
<abstract> <p>Conditions of independence among factors in a product set are investigated when preferences apply to probability distributions defined on the product set.</p> </abstract>
<abstract> <p>In this paper a family of functional iterations is introduced. One member of this family is the Newton-Raphson method and another member, obtained from a generalization of Steffensen's method to a system of equations, has been considered in [7]. The general member of the family is derived from a regula falsi construction, due to Gauss, for a particular choice of points in the iteration. From the computational point of view, all the members of the family of iterations, except the Newton-Raphson method, have the property that the partial derivatives of the system of equations are used almost never if a computing device with unlimited precision is utilized. Further, the asymptotic speed of convergence for any member is at least of order two. In view of the difficulties of obtaining the functional form of the second order partials of the likelihood function for general linear and nonlinear simultaneous systems, the method proposed here may be recommended in the computation of full information maximum likelihood estimates. Even if the partials of the system of equations are easily calculated, then some member of the family may still lead to convergence if the Newton-Raphson method does not. Practically speaking, the proposed method can be used to determine an approximate solution and this approximate solution will be closer to the solution if the precision of the computations is higher.</p> </abstract>
<abstract> <p>This paper presents an heuristic introduction to the harmonic analysis of a class of nonstationary processes and develops the estimation techniques required for such processes in Brillinger and Hatanaka [9]. The introduction is carried out via the notion of a frequency component for both a stationary and a nonstationary process. This form of presentation has the advantage of leading directly to the definitions of power spectrum, cross-spectrum and partial cross-spectrum for stationary processes, and their moving analogues for nonstationary processes. The problem of the formulation and estimation, through harmonic analysis, of a system of simultaneous equations is also considered.</p> </abstract>
<abstract> <p>This paper first takes a rather pessimistic look at what has been accomplished in recent years in understanding the "price mechanism." It then takes up two points in some detail. First, it is shown that stationary expectations do not ensure the convergence of all equilibrium paths on to a steady state in a neoclassical model with heterogeneous capital goods (an appendix works an example). Secondly, a tatonnement process is outlined and discussed for an economy with constant returns to scale.</p> </abstract>
<abstract> <p>This paper begins by formulating a finite horizon linear programming model for economic development. The formulation allows for heterogeneous capital goods and for nonnegativity constraints upon investment in each sector. It is then proved that a certain set of conditions are sufficient to ensure that an optimal solution to this T period, finite horizon plan will also coincide with an optimal solution during the first T periods of an infinite horizon plan. Among the restrictive conditions imposed to prove this sufficiency theorem are the following: gradualist consumption paths, no primary factors that cannot themselves be produced within the economy, a Leontief technology, and a characterization of the optimal finite horizon solution as one in which the terminal investment and output levels are positive. An illustrative numerical example is provided.</p> </abstract>
<abstract> <p>In this article we analyze the moment matrix, to order 1/T^2(T being the number of observations), of the two-stage least-squares estimators of coefficients in two different equations which are part of a complete set of structural equations.</p> </abstract>
<abstract> <p>Drawing up a medium term economic plan usually involves a complicated interaction between the planning ministry and representatives of the various industries, firms, or departments. Each economic agent works in his own environment with at best incomplete information about the other agents. Yet somehow the economic system as a whole is typically able to move toward an operational plan which is satisfactory even when judged by the criterion of complete information. This paper examines the properties of one particular theoretical model of economic planning in which the center transmits information via a system of quotas.</p> </abstract>
<abstract> <p>In an economy with an arbitrary number of consumers and an arbitrary number of commodities, some public and some private, I propose a generalization of Lindahl's equilibrium solution, and prove an existence theorem for it. A particular generalization of the "core" of an exchange equilibrium to an economy with public goods is proposed, and it is shown that a Lindahl equilibrium allocation is also a core allocation.</p> </abstract>
<abstract> <p>This paper examines the implications of the position that simultaneous equation models are limiting approximations to nonsimultaneous ones as time lags go to zero, observations being on averages or sums of variables over time. Necessary conditions that a given simultaneous model can be the limit of such processes in a reasonable way are derived. This leads to tests on the specification of the model and of submodels thereof, which tests examine the interrelations between equations.</p> </abstract>
<abstract> <p>Sufficient conditions are given for the existence of continuous numerical representations (utilities) for partially ordered topological spaces.</p> </abstract>
<abstract> <p> The problem to be considered in this paper is that in a linear regression model, y = Xβ + ε (where X is n × k of rank r ≤ k), the disturbance vector ε′ = (&lt;tex-math&gt;$\varepsilon _{1},\varepsilon _{2},...,\varepsilon _{n}$&lt;/tex-math&gt;) is distributed according to the null hypothesis, H0, as multivariate normal with mean vector 0 and variance-covariance matrix proportional to &lt;tex-math&gt;$\Sigma _{0}$&lt;/tex-math&gt;, against the alternative hypothesis, H&lt;sub&gt;1&lt;/sub&gt;, that it is distributed as multivariate normal with mean vector 0 and variance-covariance matrix proportional to Σ &lt;sub&gt;1&lt;/sub&gt;. Three test statistics, &lt;tex-math&gt;$s_{1},s_{2}$&lt;/tex-math&gt;, and s&lt;sub&gt;3&lt;/sub&gt;, all functions of estimated disturbances from the fitted regression are proposed to test the hypothesis H&lt;sub&gt;0&lt;/sub&gt;. It is shown (in Section 3) that all three tests based on &lt;tex-math&gt;$s_{1},s_{2}$&lt;/tex-math&gt;, and s&lt;sub&gt;3&lt;/sub&gt; are unbiased and that the test T(1) based on s&lt;sub&gt;1&lt;/sub&gt; is most powerful. In Section 4, ε is assumed to have a special covariance structure, namely, a first order stationary Markov process, uniform covariance structure, and moving average of order one, and the general results obtained in Section 3 are simplified. It is also shown that the hypothesis H&lt;sub&gt;0&lt;/sub&gt;, in general, cannot be tested and that only an implication of it can be tested. Section 5 contains three numerical illustrations comparing the results proposed in this study with the Durbin-Watson procedure. </p> </abstract>
<abstract> <p>A value share is defined as the ratio of the expenditure of the ith commodity to total expenditure. Some value shares go up and others go down in successive periods, and in this article the problem is raised whether one can define transitions from the ith share to the jth in an attractive way. The procedure is applied to Dutch data in the period 1921-1963.</p> </abstract>
<abstract> <p>This paper reports on a study of the timing of the economic impact of government defense procurement. By assuming that the letting of new orders signals the beginning and shipments signal the end of the impact of defense procurement, this research investigates the effects of changes in product mix and capacity utilization on the duration of such impacts. The effects of changes in product mix and capacity utilization on inventory accumulation are also investigated.</p> </abstract>
<abstract> <p>The object of this note is to generalize a result of Dummet and Farquharson [3] on the existence of a choice set, i.e., the existence of an alternative socially considered to be "best" under a system of majority voting. In the first section the motivation of the exercise is explained and the relevance of the problem is discussed. In the second, the generalization is stated in the shape of a theorem and is proved. In the third, some observations are made on the nature of the theorem and its relation to conditions of transitivity of majority decision.</p> </abstract>
<abstract> <p>Previous studies have examined dynamic systems that are decomposable into independent subsystems. This article treats of systems that are nearly decomposable--systems with matrices whose elements, except within certain submatrices along the main diagonal, approach zero in the limit. Such a system can be represented as a superposition of (1) a set of independent subsystems (one for each submatrix on the diagonal) and (2) an aggregate system having one variable for each subsystem. This superposition separates short-run from long-run dynamics and justifies the ignoring of "weak" linkages in partial equilibrium studies.</p> </abstract>
<abstract> <p>This paper considers the question of whether simultaneous equation estimation is possible in view of the fact that specification errors are always made in the construction of models which are supposed only to hold approximately. It is shown that, so far as consistency is concerned, good approximations give good results, but that different estimators have different sensitivities to specification errors of the types considered. It follows that the choice of an estimator may depend crucially on its properties in such situations. The problem leads naturally to a consideration of the positions of T. C. Liu and H. Wold on the prevalence of underidentification and recursiveness, respectively. The concept of recursiveness is generalized as is the Proximity Theorem, and it is shown that a position intermediate between the two just mentioned is highly tenable, such a position permitting the use of simultaneous equation estimators. The whole problem turns out to be related to the question of almost unilateral coupling of dynamic systems and some conjectures are presented on the possibility of generalizing the recent aggregation and partition theorem of H. Simon and A. Ando.</p> </abstract>
<abstract> <p>A variety of functional forms have been suggested, in the past, as suitable for describing distributions of income. Some have been derived from models "explaining" the generation of an income distribution, while others are claimed only to fit observations reasonably well. One which has not been widely considered is the sech square distribution. This distribution has certain useful characteristics, such as simple Lorenz measures of inequality and a simple method of graphical analysis, which make it a useful tool in examining and comparing distributions of income. The differential equation from which the sech square distribution is derived can be varied to allow a wide range of different distribution forms to be fitted. A similarity exists between this distribution function and the Pareto and Champernowne distribution functions. Some of the characteristics of the latter distribution are discussed in the paper.</p> </abstract>
<abstract> <p>The central focus of this article is a derivation of stochastic utility functions from the basic postulates of stimulus sampling learning theory. Generalizations of the results to multiple choice situations are discussed, as well as relations to the entropy of a set of alternatives and to Luce's choice axiom.</p> </abstract>
<abstract> <p>This article examines the relationship between the price of commodities and ther total labour content when the productive contributions of all other factors are imputed to labour. The transformation of one basis of valuation into the other, which entails the use of Leontief matrices, may be distorted by errors of aggregation. We examine the probable limits of these errors and the conditions for their total absence.</p> </abstract>
<abstract> <p>Nonlinear programming is a numerical technique of computing the "optimum levels" of "activities" for an organization or enterpreneur wishing to maximize an "objective function" (say, profit). While with "linear programming" profit is a "linear function" of the activity levels which are linearly constrained, the present methods permit nonlinear functional relationships of a particular type.</p> </abstract>
<abstract> <p>We consider in this note the bias of the residual variance estimator in simultaneous equations according to the k-class method. An unbiased estimator is formulated.</p> </abstract>
<abstract> <p>K. J. Arrow's general possibility theorem is justly criticized and revised by J. H. Blau. But Blau's revised theorem is based on by far stronger assumptions than Arrow's, and, therefore, seems to lose a great deal of its practical significance. This paper aims to show that Blau's criticism is not so serious, as it appears and that the essence of Arrow's argument remains virtually unchanged and equally challenging to the interested social scientist.</p> </abstract>
<abstract> <p> Some of the recent contributions to the problem of the stability of a competitive economy are surveyed. Emphases are laid on the uses of economic laws such as Walras' law, the homogeneity of demand functions with respect to prices, etc., in proving stability, and on the development of models of nontâtonnement processes of adjustment in the market. </p> </abstract>
<abstract> <p>Two issues are the subject of analysis in this paper. First, there is a theoretical discussion of the elasticity of import demand for a single commodity. It is shown that there is a strong presumption that import demand is elastic. Secondly, an empirical demand function is obtained for the United States import demand for raw apparel wool.</p> </abstract>
<abstract> <p> This paper presents first a general theory of a capitalistic optimum and a model illustrating its essential features, and, secondly, the empirical justification of this model, and its principal applications. Under very general conditions it is possible to show that we cannot expect, from an indefinite increase of available real capital, an indefinite increase of real national income consumed per inhabitant, and that there is an optimum amount of capital for which the real income per inhabitant is maximum. The conditions under which this maximum is attained are given. The general model, which is presented, and, in particular, its exponential variety, appear quite remarkably confirmed by all presently available empirical data, with respect to both the hypotheses and the results. A very simple expression of consumed real income is given in terms of the rate of interest i and the rate of growth ρ. </p> </abstract>
<abstract> <p>This paper presents a utility analysis of personal saving in which the only storable asset, capital, exposes its holder to the risk of capital gain or loss. The consequences of this risk and the effect of other parameters upon the optimal consumption policy is analyzed by means of dynamic programming.</p> </abstract>
<abstract> <p>A theory of how best to aggregate or simplify a given detailed model of simultaneous equations in the reduced form is proposed, assuming that the model is to be used for prediction purposes. Results are obtained for aggregating exogenous or endogenous variables, or both, including the cases where the aggregation is to be in partitioning form, and where variables are to be eliminated from the model. A Bayes decision theory approach is taken.</p> </abstract>
<abstract> <p>Effects of instalment buying on the demand for durable goods and economic instability are closely related to the borrowing behavior of individual credit users. The present paper examines major factors associated with consumer borrowing and discusses the relevance of our findings to some formal hypotheses in economic theory. The relationship of installment buying to the level of economic activity and the predictive power of estimated relationships are also investigated.</p> </abstract>
<abstract> <p>Parameters have been estimated for an amended version of Eisner's distributed lag investment function. Like Eisner, we found that accelerator effects prove to be concentrated among firms evidencing moderate to rapid growth in sales and gross fixed assets; however, significant portions of the variance in current capital expenditures are also accounted for by differences in the previous rate of growth of gross fixed assets. Profits again appear to be a proxy variable while our estimate of the strength of the relationship between differences in depreciation ratios and the variance in gross capital expenditures tends to be weaker than Eisner's similar estimate.</p> </abstract>
<abstract> <p>According to Wold, information on some of the variables at time t cannot be used for prediction of the values of the remaining variables at t, in a simultaneous equation system. This, however, is not the case with his implicit casual chain model. This paper considers the stochastic processes underlying the two models, uses the theory of canonical correlation to discuss Wold's criticism, and suggests the type of additional information necessary to remove these objections. It further shows how both these models are complementary to each other.</p> </abstract>
<abstract> <p>Population growth and economic growth are interdependent. Excessive population growth would cancel economic growth. The question is whether economic growth will prevent excessive population growth. An important facet of this problem is to estimate statistically the effects of economic growth on the birth rate.</p> </abstract>
<abstract> <p> The usual (N, t*)-definition of stability says that an equilibrium point x̄ is stable if there exists a finite time t* (dependent on a prescribed neighbourhood N of x̄) such that, for all t being not less than t*, x(t) remains in N. The least value, t̄*, of such t* may be a useful concept of dynamic economics, measuring the speed at which the equilibrium is established. In this paper we assume a multi-sectoral model of the Leontief type and obtain various formulas for the measurement of t̄*. </p> </abstract>
<abstract> <p>The utility of consumers' durables to the buyer stems primarily from the services which he expects to enjoy over a number of successive budget periods. Although the services are a time flow, the demand for the durable itself cannot be represented by the usual Marshallian flow demand functions of static character. The models which take the place of the traditional demand-supply scheme are of dynamic character even if the function of price as clearing the market is preserved. The notion of "excess demand" governing price changes adds a further dynamical element.</p> </abstract>
<abstract> <p>In this article we study the small sample properties of the so called general k-class estimators of simultaneous equations. Two members of the family of k-class estimators are found, one of which is unbiased to the degree of our approximation and the other possesses a minimum second moment around the true parameter value, again to the order of our approximation.</p> </abstract>
<abstract> <p>The authors examine the dynamic properties of the Klein-Goldberger model of the United States economy by extrapolating the exogenous variables and solving the equations on the IBM 650 for one hundred years. In this process no indication was found of oscillatory behavior. The introduction of random impulses of a reasonable order of magnitude, however, generates cycles which are comparable in their properties to those of the United States economy.</p> </abstract>
<abstract> <p>This article was written independently of Solow's [13] but is closely related to it. Under some plausible assumptions the author discusses various stability properties of a dynamic input-output system with a spectrum of techniques. Those economists who are interested in game theory, linear programming and input-output analysis may be charmed by Mrs. Robinson's model [9], a borderline model between von Neumann's theory of growth [8] and Marx's theory of reproduction. It is shown that her solution of a golden age is a particular solution of the dynamic Leontief model.</p> </abstract>
<abstract> <p>This article works through the basic theory of estimating the parameters of a complete economic model by full information maximum likelihood, and develops a simplified and condensed routine. The routine is then applied step by step to the estimation of the parameters of a known numerical model which has generated its own data. Various other methods of estimation are less costly, and some of these are tested both on the above synthetic data and on the actual data of a small Canadian model. The aim here is to attempt to make a rough efficiency appraisal, in terms of cost of estimation and bias, of the various methods tested.</p> </abstract>
<abstract> <p>This paper extends the results of Brown and von Neumann [3] to elucidate their significance in the study of the dynamic stability of a competitive market mechanism.</p> </abstract>
<abstract> <p>The Leontief dynamic model of the economy is frequently criticized for its lack of stability: outputs increase indefinitely with time, although final demand remains constant. If the time required for the adjustment of the stock of capital goods to the rate of output is greater than a theoretically determined minimum period, it is shown that a Leontief dynamic model based on backward difference equations is stable. The question is raised and answered as to whether difference or differential equations provide a better representation of a Leontief-type system.</p> </abstract>
<abstract> <p>This paper surveys the results of mostly recent research on optimal aggregate economic growth models, and comments on the difficulties encountered and on desirable directions of further research.</p> </abstract>
<abstract> <p>Several recent generalizations the geometrically declining distributed lag model are reviewed and the associated statistical estimation problems are discussed. Attention is drawn to the practical difficulties of distinguishing between different lag schemes and models on the basis of usual economic data. This leads to a discussion of the more general problem of estimating mixed autoregressive schemes with serially correlated disturbances and a review of the existing theoretical rationale for the various suggested distributed lag models.</p> </abstract>
<abstract> <p>Sufficient conditions are given for a general n person game to have a nonempty core. The conditions are a consequence of convexity of preferences if the game arises from an exchange economy. The proof of sufficiency is based on a finite algorithm, and makes no use of fixed point theorems.</p> </abstract>
<abstract> <p>This paper reports on the results of some sampling experiments on six structural equation estimators: direct least squares, two-stage least squares, Nagar's unbaised k-class estimator, limited-information maximum likelihood, three-stage least squares and full-information maximum likelihood. Sensitivity of the relative performances of the estimators to the parameter values chosen in the models and to the values of the exogenous variables used is studied. It is found that differences among the estimators are not great and that they are sensitive to the parameters used.</p> </abstract>
<abstract> <p>Given a set of economic agents and a set of coalitions, a non-empty family of subsets of the first set closed under the formation of countable unions and complements, an allocation is a countably additive function from the set of coalitions to the closed positive orthant of the commodity space. To describe preferences in this context, one can either introduce a positive, finite real measure defined on the set of coalitions and specify, for each agent, a relation of preference or difference on the closed positive orthant of the commodity space, or specify, for each coalition, a relation of preference or indifference on the set of allocations. This article studies the extent to which these two approaches are equivalent.</p> </abstract>
<abstract> <p>The LISE technique is extended to deal with certain subsystems of a full system which are of a special type. It is shown that any completely identified system can be uniquely decomposed into maximal subsystems of this type. The relation of the estimation procedure to certain classical statistical estimation procedures is discussed.</p> </abstract>
<abstract> <p>This simpler proof of a theorem of E. J. Hannan on the estimation of parameters in simultaneous stochastic equations emphasizes the equivalence between maximum likelihood and generalized variance ratio, includes a proof of a theorem on maximizing the ratio of two determinants |ANA|/|AMA| with respect to A, and brings out the restrictive assumption of Hannan's theorem in relating simultaneous equations system and canonical correlation theory.</p> </abstract>
<abstract> <p>This paper discusses a foreign trade optimization model, which minimizes domestic expenditures, subject to the conditions that both the export programs and the substitutable import programs belong to a convex set and that the net revenue from foreign exchange exceed a prescribed limit. It considers the splitting of this question into two separate programming problems: one for export activities, and one for import activities and import substitution. The interrelations between the optimal volume of foreign trade and the marginal exchange rate are shown.</p> </abstract>
<abstract> <p>In this paper we give necessary and sufficient conditions such that a decision maker who operates in a world in which finitely many states of nature can occur has an increasing, strictly concave utility function U(.) and a subjective probability measure P(.) and such that he chooses among acts with uncertain outcomes according to the expected value of U(.) (with respect to P(.)) which they offer him.</p> </abstract>
<abstract> <p>Wallace and Hussain (1969) considered the use of an error components regression model in the analysis of time series of cross-sections and developed an Aitken estimator of the coefficient vector based on an estimated variance-covariance matrix of error terms. In this paper, we have shown that under the set of assumptions adopted by Wallace and Hussain there are an infinite number of estimators which have the same asymptotic variance-covariance matrix as the Wallace-Hussain estimator and also that it is not possible to choose an estimator on the basis of asymptotic efficiency. We have developed an alternative estimator of the variance-covariance matrix of error terms and have used this estimator in developing a feasible "Aitken" type estimator for the coefficient vector. We have derived some small sample properties of this estimator and have compared them with those of other estimators of the coefficient vector.</p> </abstract>
<abstract> <p>The fact that long term interest rates have been higher on average than short rates in the twentieth century has often been interpreted in the term structure literature as evidence of the existence of positive "liquidity" or term premiums. The purpose of this paper is to point out that an average differential between long and short rates does not necessarily represent a differential between returns realized by holders of long versus short term bonds. In particular, it is shown that if short term rates are positively autocorrelated, interest rate differentials overstate differentials in realized rates of return. We conclude that liquidity or term premiums properly estimated from the Durand yield curve data are not consistent with the liquidity preference theory.</p> </abstract>
<abstract> <p>Consider a sequence of markets for goods and securities at successive dates, with no market at any date complete in the Arrow-Debreu sense. A concept of common expectations is proposed that requires traders to associate the same future prices to the same future exogenous events, but does not require them to agree on the (subjective) probabilities associated with those events. An equilibrium is a set of prices at the first date, a set of common price expectations for the future, and a consistent set of individual plans for consumers and producers such that, given the current prices and price expectations, each individual agent's plan is optimal for him, subject to an appropriate sequence of budget constraints. The existence of such an equilibrium is demonstrated under assumptions about technology and consumer preferences similar to those used in the typical Arrow-Debreu theory of complete markets. However, an equilibrium can fail to exist if some provision is not made for the elimination of "unprofitable" enterprises. The usual assumptions of "rationality" imply, in this model, that agents learn from experience and modify their expectations as Bayesians.</p> </abstract>
<abstract> <p>The methods of Cochrane and Orcutt orr Hildreth and Lu to correct linear regressions for first-order autoregression in the disturbances, as usually implemented, underestimate the standard errors of the regression coefficients whenever a lagged dependent variable is included. A convenient transformation is derived from the information matrix to remove this bias. The asymptotic standard error of the estimated serial coefficient is a useful coproduct of the analysis.</p> </abstract>
<abstract> <p>The estimation of a CES production function for real sectoral value added with factor augmenting technological change first is discussed, with emphasis on the possible effects of the deflation procedure utilized and on the attempt to estimate relatively long-run parameters. The estimates of that function for nine Chilean sectors then are examined with respect to the implied degree of sectoral flexibility, the absorption of surplus labor, the implications for linear assumptions about Chilean production functions, the distribution of income, and the degree of constraint on long-run growth due to a relative shortage of a primary factor.</p> </abstract>
<abstract> <p>On the basic assumption that individual consumption of a good is a stochastic phenomenon, the first part of this article shows that under general conditions market quantity demanded is asymptotically (as n, the number of individuals in the market, increases) distributed as normal with mean and variance a function of own price given all other prices and individual incomes. Next, by the use of integral transforms, it is shown that the unknown mean market demand function can be approximated by a specific functional form. The estimation problems involved with such a model are discussed in the last part of the paper.</p> </abstract>
<abstract> <p>The technique of partial aggregation is explored as a means of preserving the confidentiality of data while enabling research scholars to utilize the information for analytic purposes. For this purpose, two criteria are developed for evaluating the analytic consequences of partial aggregation: One measure indicates the degree of divergence or non-conformity between estimates produced by unaggregated data and partially aggregated data; and the other measure pertains to efficiency loss and expresses the fraction of the useful information in the unaggregated data which remains after the data have been grouped or partially aggregated. These measures are then applied in an experimental test using data from the Call Reports and the Income and Dividend Statements of nearly 5400 member banks of the Federal Reserve System. This experiment consists of evaluating the effect on twenty different regression models of three different levels of aggregation and seven different rules for arraying the data prior to aggregation.</p> </abstract>
<abstract> <p>This paper deals with simultaneous estimation of supply and demand functions for money in the United States. It gives special attention to supply formulations relating the money stock to maximum possible money stocks and to demand functions incorporating the product of national income and the rate of interest. The estimations test and attempt to improve on these formulations, and they provide evidence on the effects of changing measurement techniques of economic time series. Specifically, substantial differences emerge between estimates using quarterly averages of daily data on stock and flow variables and similar estimates using one-day end-of-quarter figures to characterize a series over a quarter. In the context of this investigation, quarterly average data appear superior in describing the true economic series, as at least some a priori judgments would suggest.</p> </abstract>
<abstract> <p>The relationship between the contingent commodity and asset approaches to consumer behavior under uncertainty is used in examining Slutsky equations for assets. The menu of assets describes an "attainable set" of contingent commodities; it is shown that a number of types of changes in the distributions of returns on assets leave this attainable set unaltered and that the effects of such changes in distributions on asset demands are simply related to the effects of a change in the asset price on demands.</p> </abstract>
<abstract> <p>Most growth models are based either on the assumption of fixed production coefficients for labour and capital or on the assumption of substitutability between factors. The present paper proposes a hypothesis which is a compromise between these extremes, viz., that any increment in production can be obtained by different combinations of increments in labour and capital inputs, whereas any piece of capital which is already installed will continue to be operated by a constant amount of labour throughout its life span. First, a "general model" is presented. Next, the model is solved in different special cases. In conclusion it is suggested that the proposed hypothesis would be particularly appropriate in studying the introduction of new techniques and the relationship between population growth, the rate of saving and "structural" unemployment.</p> </abstract>
<abstract> <p>Usually it is easier to obtain estimates for budget proportions and Engel elasticities than for elasticities with respect to price. By making certain want independence assumptions, the elasticities with respect to price can be deduced from the knowledge of budget proportions and Engel elasticities. In this connection the concept of the flexibility of the marginal utility of money is essential. A system of formulae describing these relations is given. In an appendix certain fundamental properties of the relations between marginal and proper choice field criteria are discussed.</p> </abstract>
<abstract> <p>Regression estimates from cross-section and time series sample data are often different. Some reasons why these discrepancies arise are presented, along with quantative results for three investment functions, explaining investment by gross internal funds and the firm's capital stock, and using individual firm observations. A substantial analytical advantage can be gained from having both time series and cross-sections for an identical group of firms so that the error variance structure for estimates based on both sorts of data can be analysed efficiently.</p> </abstract>
<abstract> <p>In a von Neumann model of an expanding economy, and in a similar model allowing for some final demand, a program is said to be maximal if it brings about the maximum feasible rate of growth. Is such a program efficient according to the criteria of welfare economics? We show that it need not be, and we study the correspondence between maximal and efficient balanced growth programs. Although a duality theorem associates a valuation rule to any maximal program, this rule does not always correspond to a natural decentralized price mechanism. Finally, a formula is given showing the relation between the rate of interest and the rate of growth for efficient programs.</p> </abstract>
<abstract> <p>The model presented here attempts to bridge the gap between optimal inventory policies for individual firms and the determination of aggregate industry inventory levels. The optimal solution for a single firm sets the conditions for market equilibrium. An iterative dynamic solution is then applied to predict market levels of inventories for specific commodities.</p> </abstract>
<abstract> <p>This paper uses a topic from multivariate analysis, i.e., canonical correlation theory, in the development of a generalized correlation coefficient for simultaneous equation systems. Canonical correlation theory within the framework of simultaneous equations is first presented and then the generalized correlation coefficient (the trace correlation) is developed from these results. The asymptotic variances of this static are presented and an application to a simultaneous equation system is given.</p> </abstract>
<abstract> <p>The maximum likelihood method for estimating relationships with limited dependent variables is generalized to include relationships in which the dependent variable, over some finite range, is not related to the independent variables.</p> </abstract>
<abstract> <p> Cet article présente dans un cadre rigoureux les propriétés des fonctions de consommation F qui ne dépendent que du revenu réel et des prix réels. Après avoir dégagé les relations existant entre les élasticités réelles et nominales, nous déterminons les classes de fonctions admissibles et, parmi elles, celles qui vérifient la théorie des choix. Enfin le problème de l'agrégation est abordé dans le cadre de ces classes. </p> </abstract>
<abstract> <p>In this paper estimates are made of the net effects of several demographic variables on the level of individuals' wages or salaries. The income data are taken from a cross-section sample survey which makes it possible for each income recipient to be classified by a number of different factors simultaneously. The method of analysis is based on techniques developed for analysing experimental data, and it is shown that certain problems which can be avoided by the use of efficient designs for controlled experiments will tend to arise in acute forms when the data are obtained from cross-section samples. In the latter part of the paper are presented distributions of expected incomes based on the simple model of income formation used in the preceding analysis. Distributions of residual incomes are also given and the properties of these residuals are examined. The discussion throws light on the role of some important demographic variables in shaping the distribution of employment incomes and on the validity of certain hypotheses which have been advanced to explain the distribution of incomes.</p> </abstract>
<abstract> <p>A computational procedure is given for finding the minimum of a quadratic function of variables subject to linear inequality constraints. The procedure is analogous to the Simplex Method for linear programming, being based on the Barankin-Dorfman procedure for this problem.</p> </abstract>
<abstract> <p>The preference for decentralization in the classic debates about planned production in a socialist economy has appeared more recently in the context of an organization that maximizes, subject to constraints, a scalar-valued function of its decisions. This article seeks a basis for the preference. A class of schemes for organizational decision-making is defined, and decentralized (in the classic sense), centralized, and unrestricted subclasses are considered. Criteria for ranking the schemes are obtained and applied in a simple illustrative organization. It is found that a general preference for one of the subclasses cannot be defended without further restricting the model substantially.</p> </abstract>
<abstract> <p>The problem of the identifiability of a structural equation under nonlinear and nonhomogeneous restrictions on its parameters is examined. The usual results for the linear and homogeneous case are generalized and some consequences for the use of a priori inequalities are derived.</p> </abstract>
<abstract> <p>The problem of international factor-price equalization is studied by using a technique which combines the Walrasian theory of general equilibrium with the theory of welfare economics. The principle of the complete or partial equalization was first enunciated by Eli Heckscher in his 1919 paper and was later elaborated by Bertil Ohlin, Paul A. Samuelson, and others. The present study shows in particular that the complete factor-price equalization occurs only in the cases in which the factor endowments in the countries are precisely the ones that arise in an equilibrium position of world trade where the factors of production as well as the commodities can move internationally.</p> </abstract>
<abstract> <p>Since econometric models have been utilized to generate quantitative forecasts of aggregate economic activity, it is of interest to determine how well the existing models have performed this function. While several analyses of the Klein-Goldberger and the Suits models have been completed, the forecasting performance of other models has not been analyzed. Since the Klein-Goldberger and Suits models have already been discussed, our attention will be focused on six quarterly models whose predictive performance has not been previously evaluated. Prior to performing each evaluation, the main characteristics of the particular model will be reviewed, but the equation system itself will not be presented. This procedure will provide perspectives about the scope of the model, the period to which it was fit, and the type of predictions that it generated. Before presenting this analysis, it is, however, necessary to discuss the variety of methods that may be utilized to evaluate these models, for there is no consensus on the one best procedure to be used, and to provide caveats that should be considered in interpreting the results.</p> </abstract>
<abstract> <p>For empirical implementation of the Cobb-Douglas function, it is customary to append a multiplicative lognormal disturbance and fit a linear regression in the logarithmic variables. When this is done, attention is shifted (apparently unwittingly) to the conditional median from the conditional mean which is ordinarily the prime target of study. The customary procedure may be modified to provide minimum variance unbiased estimation of the conditional median or conditional mean.</p> </abstract>
<abstract> <p>
               Le modèle présenté étudie l'usage de bons indexes sur les systèmes de prix comme procédé d'épargne et comme mode de financement des investissements. Les agents sont donc confrontés avec un problème de prévision de prix. On fait l'hypothèse qu'ils résolvent le problème par un critère de minimax. On montre que cette hypothèse comporte un équilibre des marchés tenus à chaque période, et une compatibilité des plans à long terme des agents économiques. Cet équilibre n'est pas un optimum en général; il en est un cependant si la variété des bons offerts est suffisamment étendue. 
            </p> </abstract>
<abstract> <p>In this paper, an extension of the theory of consumer behavior to cover situations when prices enter the utility function is considered. In Section 1 we formulate the problem. In Section 2 we derive a generalized Slutsky equation of which the traditional Slutsky equation is a special case. We investigate the class of utility functions that yield homogeneous of degree zero in prices and income demand functions and also see that these can't exist for a certain class of utility functions. We then establish theorems on "substitutable" and "complementary" commodities which are not, in general, valid for the traditional theory. Finally, a set of qualitative restrictions on demand functions is deduced, of which a subset is identical to the traditional restrictions.</p> </abstract>
<abstract> <p>Two hypotheses concerning short-run employment variation are put forth, one relating to the extent of specific investment in workers and the other to substitution with fixed factors. Both are used to explain occupational (skill) differences in short-run employment experience among railroad workers. Implications of these investments for the demand, variability, and timing of hours of work per man are developed and tested, and substitution parameters are estimated from cross-section data.</p> </abstract>
<abstract> <p>The construct of team decision theory developed by J. Marschak and R. Radner provides a useful framework for the conceptualization and evaluation of information within organizations. Quadratic teams concern team decision problems where the organizational objective criterion is represented by a quadratic function of the action and state variables. The purpose of this paper is to explicitly relate the analysis of quadratic team decision problems to recent research on optimal decision rules for aggregate planning--for example, as reported by Holt et al [5], Theil [9], van de Panne [10], and others. A numerical example is presented based on the well known microeconomic model of production and employment scheduling in a paint factory.</p> </abstract>
<abstract> <p>The comparative statics properties of qualitatively specified systems are investigated in two cases: the "purely qualitative" case studied by Lancaster and Gorman, and the case in which the coefficient matrix of the system is assumed to be stable. Samuelson's "correspondence principle" is shown to be of particular importance in the Morishima case, but of less importance when the Morishima conditions are violated.</p> </abstract>
<abstract> <p>
               La distinction classique entre biens individualisables et biens collectifs ne tient pas compte des biens et services, tels que la circulation automobile, dont la quantité est individualisable mais dont la qualité a un caractère collectif et dépend du niveau de la demande en raison d'effets externes. On étudie dans ce papier comment les conditions classiques de l'équilibre économique sont à modifier pour introduire cette catégorie de biens et quelles doivent être leurs règles de tarification. Les résultats théoriques obtenus sont ensuite appliqués au cas de la circulation automobile. 
            </p> </abstract>
<abstract> <p>
               Lévy-Lambert shows in [3], for a specific example, how the marginal cost pricing principle must be modified to take into account an imperfect environment. The author studies the case of two existing, perfectly substitutable facilities, namely a turnpike and a highway, connecting two given locations. It is well known that an optimal resource allocation can be attained only by levying a toll on users during congested hours. These tolls must be such that the private cost of the trip on each facility is equal to its marginal social cost. In practice, however, there are often constraints that prevent reaching this goal. Thus, what is the optimal toll to be charged on the turnpike, when it is impossible to levy a toll on the high way? We owe the answer to this question to Lévy-Lambert. The purposes of the present note are: (i) to derive explicitly Lévy-Lambert's result from a general equilibrium model and in the process to exhibit a minor qualification; (ii) to derive a few additional results, especially about the optimal redistribution of incomes; and (iii) to illustrate by a simple example a general methodology for analyzing ``second best'' models, already used with success in other fields. 
            </p> </abstract>
<abstract> <p>
               In this paper, the exact posterior distributions of the parameters of Haavelmo's model I is derived for locally uniform prior distributions. Marginal distributions of the parameters have been obtained for Haavelmo's data. Then the predictive probability density of the model is derived for given values of the exogenous variable, investment. In order to check some of the specifying assumptions, the model is expanded and analyzed under the assumption that the error terms are generated by a first order autoregressive scheme. Exact finite sample results are obtained and the posterior distributions are computed for Haavelmo's data. Conditional distributions of the parameters of the model are computed for given values of the autocorrelation parameter, ρ, in order to assess the effects of departures from our specifying assumptions. Another specifying assumption that is examined concerns the exogenous nature of investment. For this, Haavelmo's model II, in which investment is assumed to be endogenous, is used. Posterior distributions of the parameters of the model are computed for this model. The sensitiveness of the inference about the parameters of the model to the assumption that investment is exogeneous is studied by computing various conditional distributions for model II. It is seen that this assumption is very crucial for Haavelmo's data. Finally, two different prior distributions reflecting two different views about investment are introduced. The posterior distributions of the same parameter are then used to determine how one's prior belief is modified by the sample information. 
            </p> </abstract>
<abstract> <p>The object of this paper is to provide a systematic treatment of aggregation of individual welfare as a basis for social preference. Two polar cases of interpersonal comparability seem to have received all the attention in the literature so far. Either it is assumed that individual welfare measures are fully comparable, e.g., in Marshall [12], or that they are not comparable at all, e.g., in Robbins [15]. It is clear, however, that we frequently make judgments that are not consistent with noncomparability but which do not require full comparability. Part of the object of this paper is to examine the formal basis of such judgements and to develop a continuum of intermediate assumptions.</p> </abstract>
<abstract> <p>The construction of tests of model specification is considered from a general point of view. The results are applied to testing the serial independence of the disturbances in a regression model where some of the regressors are lagged dependent variables. It is shown that the asymptotic distribution of the lag-1 serial correlation coefficient calculated from the least-squares residuals differs from that of the coefficient calculated from the true disturbances. A consequence of this is that tests of serial independence based on the residuals from regression on fixed regressors are invalid when applied to models containing lagged dependent variables even when the null hypothesis of serial independence is true. Tests which are asymptotically valid for the large-sample case are suggested.</p> </abstract>
<abstract> <p> This article shows how to transform residuals from regression on an arbitrary set of k regressors to a set of values having the same joint distribution as the residuals from regression on a different set L of k regressors. Let d′ denote the value of the statistic &lt;tex-math&gt;$\Sigma (z_{t}-z_{t-1})^{2}/\Sigma z_{t}^{2}$&lt;/tex-math&gt; calculated from these values. It is shown that for a suitable choice of L the distribution of d′ is the same as that of &lt;tex-math&gt;$d_{U}$&lt;/tex-math&gt;, the significance values of which are tabulated in [1]. </p> </abstract>
<abstract> <p>In many situations involving interaction among parties, the individual welfare of each party can be improved by cooperation. This requires a cooperative decision, which specifies the course of action to be followed by each party, and a rule for sharing the total payoff resulting from that decision. In general, the total payoff will depend on the value of a random variable that is unknown at the time of the decision making. The payoffs are assumed to be in monetary terms while there is no restriction on the transfer of money between parties. This paper determines the sufficient conditions under which the cooperative decisions can be ordered according to their corresponding Pareto optimal fronts. The same ordering of the decisions can be obtained on the basis of the expected utility of a group, which has a particular utility function for the total payoff and probability distribution for the random variable. For the case of two parties, the conditions are determined under which the parties are willing to delegate the cooperative decision making to such a group on a provisional basis.</p> </abstract>
<abstract> <p>This paper extends the traditional analysis concerning the impact of factor accumulation on a country's terms of trade expounded in a two-by-two model to a world that consists of three countries, each producing three commodities with the help of three factors. It is shown that an "export-biased" factor growth must globally deteriorate the terms of trade of the expanding country--a result similar to that derived in the two-by-two model. An "import-biased" factor growth (of one factor only), however, will improve the growing country's terms of trade with respect to one country but worsen them with respect to the other. Consequently, the conventional view that an "import-biased" factor growth must raise the growing country's real income more than the rise in its output may not hold in a realistic world where trade is multilateral rather than bilateral.</p> </abstract>
<abstract> <p>The use of numerical techniques for solving dynamic nonlinear multisectoral models for development planning is analyzed. A four sector model with nonlinear welfare, production, and investment functions is developed and solved using conjugate gradient and neighboring extremal methods. The model draws on recent developments in nonlinear theoretical growth models and linear development planning models. Sensitivity tests on the turnpike properties of the model and on changes in the elasticity of substitution parameters in the production function are discussed. The numerical techniques used have their origins in control theory applications and exploit the dynamic structure of the planning model.</p> </abstract>
<abstract> <p>Gorman [2] has derived necessary and sufficient conditions for the existence of category expenditure functions which yield the optimal allocation of a consumer unit's income to each of a number of groups of commodities as functions of total income and group price indices. These conditions take the form of certain restrictions on the structure of the utility function. Gorman, however, did not address the problem of how these functions are derived. In this paper, we construct an algorithm (a budgeting procedure) for deriving the category expenditure functions and show that the necessary and sufficient condition for this procedure to be consistent is that the utility function be separable into homothetic parts.</p> </abstract>
<abstract> <p>The problem consists in maximizing a concave functional--given as an integral--on a class of functions. A method based on approximating optimal solutions by step functions is suggested; approximations are found by solving concave programming problems of a special type. Some convergence theorems are proved and an estimate of the error in terms of values of the objective functional is given.</p> </abstract>
<abstract> <p>The results of A. K. Sen and P. K. Pattanaik on sufficient conditions for the transitivity of simple majorities are extended to the case where it is not assumed that the individual's indifference relations are transitive.</p> </abstract>
<abstract> <p>In the model considered in this paper, a firm with several plants must supply several markets with known demands. Production costs are nonlinear while transportation costs between any plant-market pair are linear. The firm desires to minimize total production and transportation costs. The Kuhn-Tucker conditions and the dual to the transportation model are used to derive optimal conditions for this problem. These conditions are shown to be both necessary and sufficient if the production costs are convex at each plant, and are necessary otherwise. An algorithm is developed for reaching an optimal solution to the production-transportation problem for the convex case. This algorithm utilizes a decomposition approach. It iterates between a linear programming transportation problem which optimally allocates previously set plant production quantities to various markets and a routine which optimally sets plant production quantities to equate total marginal production costs including a shadow price representing the relative location cost determined from the transportation problem.</p> </abstract>
<abstract> <p>In this paper various methods for the estimation of simultaneous equation models with lagged endogenous variables and first order serially correlated errors are discussed. The methods differ in the number of instrumental variables used. The asymptotic and small sample properties of the various methods are compared, and the variables which must be included as instruments to insure consistent estimates are derived. A suggestion on how to estimate the approximate covariance matrix of the estimators is made.</p> </abstract>
<abstract> <p> The mini-general-equilibrium model of a two-factor, two-product economy with production functions homogeneous of the first degree provides verifiable propositions for a great many subjects in economics. The data for empirical verification, however, may not, and usually do not, meet the underlying assumptions. In this paper the effects of differences in the wage-rental ratios between the two sectors are traced. Let q&lt;sub&gt;j&lt;/sub&gt; be the wage-rental ratio in sector j; the paper deals with distortions of the kind &lt;tex-math&gt;$q_{1}=\gamma q_{2}$&lt;/tex-math&gt;. The admissible values for q&lt;sub&gt;1&lt;/sub&gt; and q&lt;sub&gt;2&lt;/sub&gt; are derived as functions of γ and the overall capital-labor ratio (k). Theorem 1 states the values of γ for which the sign of the difference in capital intensities is (i) preserved and (ii) reversed. Next we establish conditions under which the sign of the difference in factor shares is (i) in line with that of no distortion, (ii) reversed, (iii) a combination of (i) and (ii). Such conditions depend not only on γ and k but also on the elasticities of substitution (ES) in the two sectors. Theorem 2 summarizes the discussion on this subject. The proofs of the proposition leading to Theorems 1 and 2 are illustrated graphically. In order to analyze the effect on the sign of the supply function in this economy, the ES of products with respect to their prices is expressed in terms of the parameters of the individual production functions. The expression shows clearly the augmentation effect observed by Johnson [1] (i.e., in the absence of distortion, the ES between products is larger than those which exist between factors). Finally, Theorem 3 states conditions under which the supply function is increasing, declining, perfectly elastic, or a combination of these. </p> </abstract>
<abstract> <p>The exact sampling distributions of estimators of structural parameters of econometric models are unknown except for a few simple cases. In this situation two alternative approaches towards evaluating finite sample properties of various estimators have been adopted in the literature: (i) Monte Carlo experiments, and (ii) the approach pioneered by Nagar and his students in which the sampling error of an estimator is expressed as the sum of an infinite series of random variables, successive terms of which are of decreasing order of sample size in probability. It is claimed that the small sample properties of the estimator under consideration can be approximated by those of the first few terms of such an infinite series. This paper shows through examples that the Nagar approach can be misleading in the sense that it can yield an estimate for finite sample bias that differs from the true finite sample bias to the same order of sample size. And it can yield estimates of bias which are finite (infinite) while the true bias is infinite (finite). The paper also draws attention to some of the pitfalls to be avoided in studying the properties of an infinite sequence of random variables.</p> </abstract>
<abstract> <p>This paper discusses the existence of a competitive equilibrium when the assumption of perfect divisibility is not fulfilled for all the goods involved in the economy; we limit ourselves to the case of a pure exchange economy with a finite number of traders. Section 2 is devoted to the proof of an existence theorem when assuming that only two goods are exchanged in the economy; in Section 3 we build a counterexample to show the restrictive nature of the assumptions one has to make in order to generalize the above theorem.</p> </abstract>
<abstract> <p>Machines produce a constant flow of output when operating, and break down in a random process. The task of workers is to repair the broken machines. Servicing time is also a random process [2]. The limiting steady state probabilities and the relation between output and inputs are derived, and fitted to a Cobb-Douglas production function.</p> </abstract>
<abstract> <p>This paper determines the time series behavior of investment, output, and prices in a competitive industry with a stochastic demand. It is shown, first, that the equilibrium development for the industry solves a particular dynamic programming problem (maximization of "consumer surplus"). This problem is then studied to determine the characteristics of the equilibrium paths.</p> </abstract>
<abstract> <p>This paper discusses the role of congestion tolls in increasing the efficiency of use of commercial airports, in terms of a simple model of air transportation. In contrast to previous discussions, users and producers of transportation service (passengers and airlines) are explicitly distinguished. In the first version of the model, ticket prices are assumed--unrealistically--to be flexible and competitively determined; then perfect optimality is attainable by imposing an appropriate toll either on airlines or on passengers. In the more realistic second version of the model, ticket price is fixed above the competitive level. In the absence of a toll, there are two inefficiencies: the level of transportation is non-optimal, and it is produced inefficiently, using partially loaded airplanes. An appropriate toll on airlines can do much to correct both of these inefficiencies, and is always superior to the best toll on passengers.</p> </abstract>
<abstract> <p> The paper defines and analyzes a functional form for a one-output, many-factórs production function, which is homothetic (or homogeneous), and exhibits CRES; that is, its ES (Allen-Uzawa elasticities of substitution) vary along isoquants and differ as between pairs of factors, but the ES stand in fixed ratios everywhere. Given data on factor prices, quantities, and output, and assuming competitive costminimization, the parameters of CRESH are estimable from a system of log-linear equations, each containing at most three independent variables. The CES function, as well as its limiting forms (the Cobb-Douglas (σ = 1), Leontief (σ = 0), and linear (σ = ∞) functions) are special cases of CRESH. The Mukerji CRES function has an identical unit-isoquant surface, but it is not homothetic. Appendix A analyzes the Mukerji function. Appendix B derives the (implicit) CRESH cost function. </p> </abstract>
<abstract> <p>This paper is a natural outgrowth of recent studies in markets with a "continuum" of traders. An equivalence theorem for the core of such economies was obtained, under the assumption that the measure space of economic agents is atomless. We introduce a sufficient condition under which the preceding result can be extended to economies containing "atomic" traders. The condition bears on the measure of the atoms.</p> </abstract>
<abstract> <p>A new approach to the choice of econometric estimators, called small-sigma asymptotics, is introduced and applied to the choice of k-class estimators of the parameters of a single equation in a system of linear simultaneous stochastic equations. I find that when the degree of overidentification is no more than six, the two stage least squares estimator uniformly dominates the limited information maximum likelihood estimator in a certain sense. The small sigma method can be used on many problems in statistics and econometrics.</p> </abstract>
<abstract> <p>This paper presents an empirical analysis of the determinants of negotiated wage changes using a pooled sample of time series observations for fourteen Canadian manufacturing industries. Data on individual contracts are used in an attempt to allow for the discontinuity of wage adjustments given the predominance of collective bargaining and variable contract length. Using a nonlinear formulation, profit levels, the unemployment rate, the rate of change of prices, and other variables are found to be statistically significant.</p> </abstract>
<abstract> <p>The problem considered is that of the identification of a model of an economic system wherein the errors are not, as is often assumed, serially independent but are generated by a moving average process. It is assumed that estimation will be based only on the estimates of the first two moments of the variables involved and by making stationarity assumptions the problem is reduced to one of the unique factorization of spectra. It therefore has close associations with prediction theory.</p> </abstract>
<abstract> <p>In this study, a dynamic initial investment-borrowing model involving nonconvex investment effects and borrowing limitations is formulated as a discrete-time control problem. In the model, the firm's objective is to maximize, subject to constraints, the net worth of the firm over a finite decision-making period. Initial investment and borrowing are control parameters; and the scale of capacity use is the control variable. Investment costs, which reflect the "six-tenths" rule in particular, are nonconvex. Special considerations are thus involved in deriving the investment and borrowing rules. It is shown that the optimum must be at one of the following three points: (i) no investment and no borrowing, (ii) investment of just the endowment, and (iii) investment of the maximum amount possible. This result is especially important computationally, because the problem is convex at the points described by (ii) and (iii), and trivial at the origin. Therefore, the optimum may be computed by the use of published algorithms.</p> </abstract>
<abstract> <p>In this paper an analysis of optimum structural change and optimum trade policies for a developing economy is carried out in the context of a neoclassical three-sector open model of economic growth. The sectors are agriculture, light industry, and heavy industry, and the conditions for "staged" and "direct" patterns of development are derived. Some implications of a foreign exchange bottleneck are also derived.</p> </abstract>
<abstract> <p>The relationship between the time interval over which changes in a random variable are measured, and the variances of the changes for various intervals are examined with a view toward testing the independence of stock price changes. A statistical test is presented and subsequent analysis of changes for 1, 2,..., 12 month intervals suggests that prices do exhibit nonrandom behavior and in such cases the propensity is toward price reversals</p> </abstract>
<abstract> <p>An economic framework is presented for measurement of the net social benefits that can be attributed to development of a new outdoor recreation site, taking into consideration the influence that existing recreation developments have on the demand for services from the newly developed site. Methods are given for statistically estimating the empirical measures needed to apply the model, and an application is made to water-oriented outdoor recreation in Missouri. Results of the application suggest that investments in outdoor recreation can be evaluated under an objective economic decision criterion.</p> </abstract>
<abstract> <p>Several models for limited dependent variables (variables having a non-negligible probability of exactly equaling zero) are examined. Estimation in and discrimination among the various models are considered, followed by a small sampling experiment into the procedures and an example of their application.</p> </abstract>
<abstract> <p>In this paper an attempt is made to give precise expression to the conditions under which a profit maximizing firm with fixed research budget will choose each type of technical change (i.e., "neutral" and "nonneutral"). It was found that the optimal choice depends on the initial technology, relative factor prices, and relative costs of acquiring different types of technical change. The preferred technical change need not be exclusively of one sort (e.g., "neutral" change). Once "neutral" technical change becomes optimal, however, it remains so until there is a change in relative factor prices. On the other hand, adoption of a "biased" technical change may eventually cause "neutral" advance to become desired even in the absence of relative factor price change. Examination of the firm's decision criterion under the assumption that it is a monopsonistic buyer of factors of production, discloses that under identical initial conditions (i.e., relative factor prices and relative costs of alternative forms of technical change) the firm will prefer more "biased" technical change relative to the situation in which it purchases factors competitively. In particular, the firm will, under these condition, seek those "biased" technical changes which economize on the factor whose elasticity of supply is relatively smaller. Finally, it was also discovered that, contrary to previous suppositions, changes in the elasticity of substitution do affect the optimal capital-labor ration for each factor price combination in all cases but one.</p> </abstract>
<abstract> <p>Forecasting a vector of jointly dependent random variables frequently leads to the consideration of a confidence ellipsoid [6]. Joint confidence intervals can then be derived if the projections of the ellipsoid on a set of coordinate axes can be calculated. In this paper we derive an expression for such projections and then present the application to econometric forecasting.</p> </abstract>
<abstract> <p>This paper explores how far one can go in applying the modern theory of competitive equilibrium to the case of uncertainty. In the first part, the analyses of Arrow and Debreu are extended to the case in which different economic agents may have different information about the environment. The second part deals with the limitations of the Arrow-Debreu type of model, and discusses the difficulties associated with nonconvexities in the production of information, with information generated by spot markets, and with limitations on the computational capacities of economic agents. It is argued that the demand for liquidity arises from, among other things, the last two phenomena, and thus does not appear to be amenable to analysis by means of the "neoclassical" theory of competitive equilibrium.</p> </abstract>
<abstract> <p>The role of speculative short term capital movements in balance of payments adjustment and in exchange market stability is examined. A theory of speculative behavior with a distributed lag model of expectation formation at its core is developed and empirically tested using the Canadian data for the period 1952-1960. Tests of an alternative but generically similar specification of the model are also presented and discussed.</p> </abstract>
<abstract> <p>This paper presents a dynamic input-output model explicitly allowing for the durability and nontransferability of capital goods. The fact that capital goods are durable and hardly transferable once they have been installed in some factories requires that different vintages of capital goods should be distinguished. The traditional nonvintage input-output systems by Leontief and others are compared with our vintage model; it is seen that they are identical only when capital goods are "perfectly malleable" in the sense defined in the text. This is, of course, a very stringent and unrealistic condition. When it does not hold, the replacement coefficients cannot be constant and deviate from Leontief's (constant) flow coefficients. The capital coefficients obey the Sectoral Flexible Acceleration Principle instead of the simple one; it is seen that that level of activity of an industry i at which the acceleration principle ceases to work for a capital good j (because of loss of acceleration) depends on the age composition, as well as the total amount of the stocks of capital good j installed in industry i. It is seen, however, that the equilibrium rate of growth calculated from the traditional model would not be very far from the one calculated from our more general and realistic model; so that from the long run point of view, there is little difference between the two. This means that the vintage and nontransferabilit of capital goods would not be so far reaching in the long run growth analysis as they are in the discussion of cyclical movements.</p> </abstract>
<abstract> <p> In this paper it is shown that, when the disturbances &lt;tex-math&gt;$\varepsilon _{t}=y_{t}-\Sigma _{j=1}^{k}x_{tj}\beta _{j}$&lt;/tex-math&gt; in a regression model follow a first order autoregressive process with the parameter ρ generating the process, there exist cases where the classical least squares regression analysis as applied to the n — 1 observations: &lt;tex-math&gt;$y_{t}-\rho y_{t-1},x_{tj}-\rho x_{t-1,j}$&lt;/tex-math&gt; (j = 1, 2,..., k; t = 2,..., n), is less efficient than the classical least squares regression analysis as applied to the original n observations: &lt;tex-math&gt;$y_{t},x_{tj}(t=1,...,n)$&lt;/tex-math&gt;. It is shown that the efficiency of the former relative to the latter could be arbitrarily close to zero in some cases. </p> </abstract>
<abstract> <p>This paper investigates the household demand for four financial assets: marketable bonds, time and savings deposits at commercial banks, life insurance reserves, and savings accounts at other financial institutions--credit unions, savings and loan associations, and mutual savings banks. The focus of the analysis is on the substitution relationships among liquid assets, and between these assets and marketable securities. The explanatory variables used in the study are: income, wealth, and the yields on various assets included in household portfolios.</p> </abstract>
<abstract> <p>A syndicate is defined to be a group of individuals who must make a common decision under uncertainty that will result in a payoff to be shared jointly among them. In this paper, the normative criterion of Pareto optimality is employed to determine the construction, and conditions for existence, of a group utility function and a consensus of the members' probability assessments.</p> </abstract>
<abstract> <p>Fourier methods are used to investigate tests for the significance of the residuals from a regression (including an autoregression). Attention is concentrated on the relevant case where the exogenous variables in the regression have spectra concentrated at the origin of frequencies.</p> </abstract>
<abstract> <p>This note deals with Houthakker's approach of aggregating a production function for an industry. It tries to reverse Houthakker's procedure and to derived the distribution function of factor proportions from a neoclassical production function. Specifically, it derives the distribution function for a CES production function.</p> </abstract>
<abstract> <p>A model is constructed for three related time series: an inflow, a stock, and an outflow. The inflow and outflow time series are related via a truly stochastic distributed lag and the three time series are viewed as cumulations of a random number of units of randomly determined size. Among the interesting results are the serial correlations of the stock and outflow time series viewed singly and the cross and cross serial correlations between outflow in t and stock at t - 1, outflow in t, and inflow in t - 1, as well as between stock in t and inflow in t - i. These relations and others make it possible to evaluate the comparative advantage of lead series in forecasting lag series and as such may be of methodological value for the evaluation of stabilization policies. The model may be viewed as a description of several inflow-stock-outflow phenomena: trade credit and consumer credit processes, demand deposits of commercial banks, population and labor force, the formation and decay of aggregates of capital projects, and some aspects of the income-expenditure process.</p> </abstract>
<abstract> <p>A particular type of objective function is built in a linear programming version of the dynamic Leontief model so that the solution gives the path to accumulate the maximum value of capital stocks with the Neumann (turnpike) configuration in the terminal period of the program. In view of the author's turnpike theorem [8], leading the economy along this path is thought the safest method for a planning agency when desirable capital configuration in the future is unknown--as is usually the case. The model is applied to the Japanese economy for the periods 1955 to 1960, and the computed growth path is compared with the actual. Also, the relatively unstable nature of the dynamic Leontief model is shown by analysis of the computational results.</p> </abstract>
<abstract> <p>The response of manufacturers' inventory holdings to changes in the volume of sales and the backlog of unfilled orders is examined on a quarterly basis for the period 1948-55 within a buffer-stock flexible accelerator framework. The hypothesis that manufacturers successfully hedge against increases in the price of purchased materials, enlarging their stocks in advance of actual price increases, is rejected. By introducing explicitly the impact of prediction errors it is possible to infer that manufacturers tend to underestimate actual changes in sales volume, but by a surprisingly small amount. An analysis of discrepancies between desired and actual inventory holdings reveals that manufacturers tolerated sizable deficiencies in stocks throughout the Korean conflict.</p> </abstract>
<abstract> <p>In order to explain fairly simply how expectations are formed, we advance the hypothesis that they are essentially the same as the predictions of the relevant economic theory. In particular, the hypothesis asserts that the economy generally does not waste information, and that expectations depend specifically on the structure of the entire system. Methods of analysis, which are appropriate under special conditions, are described in the context of an isolated market with a fixed production lag. The interpretative value of the hypothesis is illustrated by introducing commodity speculation into the system.</p> </abstract>
<abstract> <p>The traditional method of estimating Engel curve parameters uses either (recorded) income or total expenditure as an independent variable in least squares analysis. Neither of these variables is, however, a satisfactory index of the true economic position of the family. This results in biased estimates of the income elasticities of the various consumption categories. The bias can, however, be eliminated in large samples by using both income and total expenditures in the estimation procedure. This can be accomplished by applying the method of "instrumental variables" to Engel curve analysis, with recorded income serving as the instrumental variable. Having formulated consistent estimation procedures, we use empirical data to investigate and analyze the direction and size of the biases in the traditional estimates of income elasticities of various commodity groups.</p> </abstract>
<abstract> <p>This note discusses a technical difficulty in the estimation of distributed lag models with autocorrelated disturbances.</p> </abstract>
<abstract> <p>This paper is an attempt to study optimal savings policy in a world where the growth rate of labor responds to economic factors. This modification makes the form of society's social welfare function important--its elasticity affects "real" economic variables. In addition, a rule for direct population control is investigated.</p> </abstract>
<abstract> <p> Consider a Bayesian decision problem in which F is the prior distribution over some parameter space T. If —ψ(d, t) is the product of the loss function and the likelihood function, then the Bayesian solution, d&lt;sub&gt;F&lt;/sub&gt;, maximizes &lt;tex-math&gt;$E_{F}(d)=\int _{T}\psi (d,t)dF(t)$&lt;/tex-math&gt;. Suppose {F&lt;sup&gt;n&lt;/sup&gt;} is a sequence of distribution functions that approach F&lt;sup&gt;0&lt;/sup&gt; in the sup-metric topology. Our main theorem gives conditions under which &lt;tex-math&gt;$d_{F^{n}}\rightarrow d_{F^{0}}$&lt;/tex-math&gt; and &lt;tex-math&gt;$E_{F^{0}}(d_{F^{n}})\rightarrow E_{F^{0}}(d_{F^{0}})$&lt;/tex-math&gt;. </p> </abstract>
<abstract> <p>The present study considers two duopolists each trying to outsell the other, under a non-negative profit constraint. Both advertisement and the customer-held stocks are brought into play. The optimal behavior for the duopolists is deduced from a mathematical model of differential games.</p> </abstract>
<abstract> <p>The paper discusses the problem of pooling cross-section and time-series data in the framework of likelihood functions and the associated posterior distributions and shows how this analysis can be useful in deciding whether or not to pool.</p> </abstract>
<abstract> <p>Different policies regarding allocation of foreign exchange to sectors producing grains, tractors, and machine tools give rise to different time profiles of consumption on account of irreversibility of investment. In this paper we try to find the best way this allocation may be done in order to optimize a social objective function over a finite and infinite planning horizon.</p> </abstract>
<abstract> <p>A linear programming model is used to estimate the cost of achieving certain air quality goals for the St. Louis airshed. The choice set of control methods is based on engineering data compiled by the author. The model is used to generate a set of alternative air quality levels for sulfur dioxide and particulates which can be attained at the same total cost as the goals initially considered. This frontier of trade-off possibilities is compared to a particular social indifference curve which is based on medical considerations. It is found that both curves are concave in the same direction.</p> </abstract>
<abstract> <p>Economies with indivisible commodities admit allocations that are nearly price equilibria, provided that the number of agents is large relative to the number of commodities. In the course of the proof, it is shown that a similar result is valid if agents are insensitive to small price changes.</p> </abstract>
<abstract> <p> In an earlier paper on "Interdependence as a Specification Error," Strotz has considered a recursive model with endogenous variables lagged by θ; letting θ go to 0, he argued that the likelihood function of the limit form of the model differed from the limit form of the likelihood function of the model. We show here that the two limits are obtained under different underlying assumptions; these alternative assumptions are brought out explicitly; under fixed assumptions, the two methods are shown to yield identical results. </p> </abstract>
<abstract> <p>A typical proof of the existence of a perfectly competitive market equilibrium employs an appropriately continuous price-to-price mapping that depends on excess demands. If trade tax-subsidy distortions are introduced into the model, the excess demand mappings may have disconnected image sets and destroy the continuity of the price-to-price mapping. This difficulty is overcome by developing a technique which explicitly takes account of the dependence of demand on both income and prices and simultaneously solves for equilibrium prices and income levels for all agents. This technique is then applied to establish two existence theorems for models of international trade with trade tax-subsidy distortions.</p> </abstract>
<abstract> <p>The parameters of a system of demand equations are estimated subject to the prior information of classical demand theory. The equations are estimated as a system using a variant of generalized least squares, the parametric restrictions being imposed by Lagrange multipliers. Tests of significance are given, both for individual restrictions and for the restrictions applied collectively. The method is applied to Barten's sixteen commodity consumer expenditure data for Holland. The work was done independently of R. H. Court's [6] similar treatment; however, there are significant differences in the method which warrant further discussion and the application is itself of some interest.</p> </abstract>
<abstract> <p>The purpose of this article is to suggest and demonstrate a procedure for testing economy wide linear programming models.The suggested procedure applies the linear programming model to an historical period. In general, the model's optimal solution will not be identical to the actual historical solution. Differences between the model's optimal and the economy's actual solutions could be explained by alternative sets of hypotheses--on the one hand, market imperfections, and on the other hand, defects in the linear programming model. Ordinary statistical tests could then be applied in testing the alternative hypotheses. In order to demonstrate the procedure, a linear programming model designed for prescribing an optimal economic structure for the Greek economy during the period 1954-61 is presented and tested.</p> </abstract>
<abstract> <p>Average cost functions for the life insurance industry, all of which show increasing and then constant returns, are estimated from cross section data for 237 companies. The special problems of measuring output, controlling for product mix, and accounting for the effect of rate of growth in output are examined and dealt with. The article concludes that average costs are constant beyond $100 million of premiums.</p> </abstract>
<abstract> <p>Bayesian and maximum likelihood methods and quarterly United States data are employed to estimate parameters of an aggregate consumption function, &lt;tex-math&gt;$C_{t}=\lambda C_{t-1}+(1-\lambda )kY_{t}+u_{t}-\lambda u_{t-1}$&lt;/tex-math&gt;, under four different assumptions about the error terms, &lt;tex-math&gt;$u_{t}-\lambda u_{t-1}$&lt;/tex-math&gt;, t = 1, 2,..., T. After a discussion of the results of estimation, Bayesian techniques for computing posterior odds on alternative models are described and applied to obtain posterior odds relating to different formulations of the consumption model with alternative prior probability density functions for the parameters.</p> </abstract>
<abstract> <p>In this paper, a first attempt is made to formulate a spatial equilibrium quadratic programming problem in its primal and "purified" dual forms. This formulation is in sharp contrast to the quadratic primal and dual forms formulated by Dorn [1] and Hanson [2]. Equivalence of the quantity and purified price formulations is formally proved. The proof is important because it permits the formulation of spatial equilibrium problems in terms of either the quantity domain or the price domain. The equivalence is then used to establish the mutually dual quadratic problems in quantify and price separately. The notion of a purified dual is extended to concave programming with linear inequality constraints to deal with spatial equilibrium involving nonlinear demand and supply functions.</p> </abstract>
<abstract> <p>Hitherto, studies of the impact of government taxation and expenditure on the distribution of national income or product among income classes have used arbitrary rules of thumb for the distribution of the benefits from pure public goods or bads. This article demonstrates the logically correct method for distribution of these public good benefits. Application of this method requires information on consumer preferences for public goods; since this information is unavailable, we illustrate the application of the method to the 1961 United States income distribution by showing how various distributions would result from various consumer preference structures.</p> </abstract>
<abstract> <p>This paper investigates the working of a multifunction tatonnement stability process in an actual economy by simulating the adjustment path of an abstract disequilibrium model in conjunction with an econometric equilibrium model, indicating that such a path is highly stable.</p> </abstract>
<abstract> <p>The choice of development period and consequent introduction time for a single innovation by an expected profit maximizing firm operating under conditions of rivalrous competition is studied. Factors taken into account by the firm are the increasing cost with compression of the development period, the reduction of profit opportunities with prolongation of the development period, and the probability of rival innovation and imitation which affect the potential rewards available to the firm. Comparisons is made with the timing that would be selected in the absence of rivalry. The effects of intense rivalry are also examined.</p> </abstract>
<abstract> <p>The nonlinear Kaldor theory of macroeconomic business cycles is combined with a classical growth mechanism to derive a deterministic model of business cycle phenomena. Sufficient conditions for the existence, uniqueness, and orbital asymptotic stability of a limit cycle are given. It is shown that the model also exhibits stochastic stability when the deterministic variables are randomly disturbed.</p> </abstract>
<abstract> <p>The central idea of the business cycle is of a pervasive cyclical movement of economic indicators. This paper shows that the concept of such simultaneous movements can be given a precise meaning by performing a principal component analysis of spectral density matrices or, with a different shade of meaning, coherence matrices. It suggests also a new method of computing spectral approximations for models that are nonlinear in their variables. The methods are applied to the Klein-Goldberger model for the United States.</p> </abstract>
<abstract> <p>The assumption that the rate of growth of population is exogenous is common to many growth models both of the descriptive and the optimal type. The present paper examines the consequences of optimally controlling population growth and discusses the resulting trade-off between expenditure on such control and on capital accumulation and consumption. In this context the assumption of constant returns to scale to capital and labor seems unrealistic and is relaxed. The problem thus becomes a two state variable control problem soluble by the application of Pontryagin's Maximum Principle.</p> </abstract>
<abstract> <p>This paper is concerned with the introduction of input or output targets in decentralized planning processes. The process which is studied is that of Dantzig-Wolf-Malinvaud. All the properties of a "good" scheme are present when the central board designates a neighborhood of a point of the production set (instead of an exact output target) where each firm or sector is constrained to maximize an index.</p> </abstract>
<abstract> <p>This paper attempts an international comparison of production structures, using the input-output framework. An earlier study in this field has shown that the production structures of advanced countries such as Italy, Japan, Norway, and the United States are similar, in spite of the wide differences in their levels of per capita income. This paper extends the analysis to a comparison of the production structure of India, a developing country with a very low per capita income, with those of the above developed countries. The result shows that in spite of the differences in the levels of development and per capita incomes, the similarity is preserved.</p> </abstract>
<abstract> <p>This empirical investigation is based on consumers' reports about their weekly purchases of everyday commodities. From the data we have corresponding price and consumption vectors of a number of individual demand functions and one mean demand function. By comparison with simulated demand functions, the correspondence between the price and consumption vectors is shown to be systematic and in agreement with the axioms of revealed preference.</p> </abstract>
<abstract> <p>The existence of pollution externalities calls for government intervention in developing policy measures that will improve social welfare. This paper suggests a method to predict and compare the short-run aggregate output, pollution, and labor input of a competitive industry facing various environmental policies. Assuming that the choice between alternative production technologies can only take place prior to the investment decision, the labor output ratios and the pollution output ratios are fixed in the short run but vary among plants; their distribution is the information used in the aggregation procedure. The performance of different environmental policies--taxes and standards--is examined and compared.</p> </abstract>
<abstract> <p>This paper describes a decentralized planning process which is a formalization of an iterative method proposed by Taylor [12] for the construction of an optimal plan in a socialist state. Further, it shows that this process is computationally and informationally more efficient than an alternative process proposed by Malinvaud [10] also as a formalization of Taylor's method.</p> </abstract>
<abstract> <p>This paper gives a constructive existence proof of a general equilibrium for a local public goods economy in which consumers are free to move among regions. The proof is based upon the Scarf algorithm for the computation of fixed points.</p> </abstract>
<abstract> <p>The meaning of exchange efficiency is examined in the context of an economy in which agents differ in their endowments of information. Definitions of efficiency, and of the core, are proposed which emphasize the role of communication. Opportunities for insurance are preserved by restricting communication, or in a market system by restricting insider trading, prior to the pooling of information for the purposes of production.</p> </abstract>
<abstract> <p>Axioms for an individual's preferences over time, taken from the present perspective, usually assume that the individual will live, or expects to live, throughout a given horizon span. This paper offers an axiomatization that explicitly recognizes the uncertainty of an individual's lifetime. It divides a horizon span into n periods and assumes that if death is not immediate then it will occur at the end of one of the periods. The theory is based on an unconditional preference relation over potential future consumption streams that accounts for uncertain lifetime, along with a conditional preference order that is based on the hypothesis that death will occur at the end of period i. There is a conditional order for each i from 1 to n. The utility representation involves an order-preserving utility function for each of the n conditional orders such that one potential consumption stream is unconditionally preferred to another if, and only if, the sum of the conditional utilities for the first stream exceeds the sum of the conditional utilities for the second. It is argued that the theory seems fairly reasonable only if probability of survival does not depend significantly on past consumption.</p> </abstract>
<abstract> <p>This paper shows how the Debreu-Sonnenschein theorem can be used to construct economies where disadvantageous reallocations of resources take place.</p> </abstract>
<abstract> <p>Using cross-sectional agricultural household accounting record data for 1967 and 1968, a linear logarithmic expenditure system, consisting of three commodities--leisure, agricultural commodities, and nonagricultural commodities--is estimated for the Province of Taiwan. The hypothesis of utility maximization, as well as other hypotheses on functional form, are tested. It is found that the empirical evidence is consistent with utility maximization and that household labor supply depends on both the composition and the size of the household as well as the wage rate and prices. The consumption demand, labor supply, and marketed surplus elasticities with espect to prices, income, household composition, and household endowment variables are also reported.</p> </abstract>
<abstract> <p>A set of standard dynamic disaggregated price equations are estimated to examine the relationship between changes in input prices and output prices. The equations perform satisfactorily by conventional criteria; however, when disaggregated by frequency, it is found that the high and low frequency components appear to satisfy different models. The differences are generally significant suggesting that the model is misspecified and that another lag distribution should be used. In particular, the sum of the lag coefficients for labor inputs is substantially larger when estimated with the low frequency component than the high. Therefore, such a price equation estimated during a regime of continued wage inflation would exhibit a much larger long run output price elasticity with respect to wages, than would one estimated during a period of stable or randomly fluctuating wages.</p> </abstract>
<abstract> <p>Very often, an index number used in an economic model has been constructed in two or more stages. If the two stage procedure gives the same answer as a single stage procedure, then Vartia calls the index number formula "consistent in aggregation." Paasche and Laspeyres indexes have this consistency in aggregation property, but these index number formulae are consistent only with very restrictive functional forms for the underlying aggregator (i.e., utility or production) function. The present paper shows that the class of superlative index number formulae has an approximate consistency in aggregation property, where a superlative index number formula is one which is consistent with a flexible functional form for the underlying aggregator function. The paper also contains some empirical examples which both illustrate the main theorem and also indicate that the chain principle for constructing index numbers is preferable to the fixed base method. Finally, the paper proves some theorems about the class of pseudo-superlative index numbers.</p> </abstract>
<abstract> <p>In the context of single equation nonlinear regressions involving continuously differentiable functions, the conditions under which polynomial approximations to general classes of nonlinear estimators can be obtained are derived. Attention is paid to the distribution of the approximations to the nonliear estimators and criteria are given for evaluating the approximation.</p> </abstract>
<abstract> <p>This paper considers a class of simultaneous equation models with both discrete and continuous random variables based on normally distributed latent random variables. The model set forth here contains the classical simultaneous equation model for continuous endogenous variables and more recent models for discrete endogenous variables as special cases of a more general model. Conditions for the existence of the model are developed. Identification criteria are provided and consistent estimators are proposed. The model set forth here is contrasted with the models of Goodman and Nerlove and Press.</p> </abstract>
<abstract> <p>The maximum likelihood estimate of the structural form of the errors in variable model with an instrumental variable is shown to be the median of the least squares, the reverse least squares, and the instrumental variables estimates, if all three have the same sign.</p> </abstract>
<abstract> <p>In this paper the concepts of identifiability and of exact and consistent estimability for econometric models are introduced in a rather general framework, and the relations between these concepts are investigated. As a main result we obtain conditions under which identifiability and consistency are equivalent almost everywhere.</p> </abstract>
<abstract> <p>The purpose of this paper is to describe a new gradient method for maximizing general functions. After a brief discussion of various known gradient methods the mathematical foundation is laid for the new algorithm which rests on maximizing a quadratic approximation to the function on a suitably chosen spherical region. The method requires no assumptions about the concavity of the function to be maximized and automatically modifies the step size in the lifet of the success of the quadratic approximation to the function. The paper further discusses some practical problems of implementing the algorithm and presents recent computational experience with it.</p> </abstract>
<abstract> <p>This paper derives a stochastic linear equation from factor analysis called factor analysis regression which is suggested as an alternative to classical least squares regression whenever least squares estimation is questionable or breaks down because of errors in the variables or multicollinearity. Statistical tests for the factor analysis regression are also suggested and an empirical example comparing factor-analysis regression with least squares is shown.</p> </abstract>
<abstract> <p>A procedure is proposed for partitioning a given matrix columnwise into a simpler or smaller matrix so that a certain quadratic cost criterion involving another symmetric matrix is minimized. The procedure, involving progressive mergers of subsets of data, is applied to two economic examples from the literature.</p> </abstract>
<abstract> <p>In this paper, we consider two basic aspects of demand analysis, with application to the demand for natural gas in the residential and commercial market. The more fundamental one consists in the formulation of a demand function for commodities--such as natural gas--whose consumption is technologically related to the stock of appliances. We believe that in such markets, the behavior of the consumer can be described best in terms of a dynamic mechanism. Related to this is the more specific problem of estimating the parameters of the demand function, when the demand model is cast in dynamic terms and when observations are drawn from a time series of cross sections. Accordingly, this paper is centered around these two major themes, although, as the title suggests, the emphasis is placed on the second one. In Section 1, we present the theoretical formulation of the dynamic model for gas. In Section 2, the results of the estimation of the gas model by ordinary least squares methods are presented. These results, together with more fundamental theoretical considerations, suggest a different approach. The essence of this approach, which is not restricted to the gas model, is discussed in Section 3, while two alternative procedures for estimating the coefficients of the dynamic model in the light of this new approach are proposed in Section 4. It is subsequently shown that the application of these procedures to the gas data produces results that are reasonable on the basis of a priori theoretical considerations.</p> </abstract>
<abstract> <p>The von Neumann-Morgenstern theory of games does not yield determinate solutions (corresponding to unique payoff vectors) for two-person variable-sum games and for n-person games. The present paper outlines a general theory of rational behavior in game situations which does yield determinate solutions for all classes of games. The theory is based on two classes of rationality postulates: those defining rational behavior as such, and those defining rational expectations concerning the other players' behavior. It is argued that this new approach greatly increases the possibilities for the application of game theory in economics and the other social sciences.</p> </abstract>
<abstract> <p>The basic theory upon which several well-known tests of the independence of least squares regression disturbances are based is reviewed and then applied to formulate a testing procedure which is inexpensive when a large-capacity, high-speed electronic computer is employed. Investigations to date indicate that the significance points of the von Neumann ratio, which can be obtained by this procedure, are accurate to the order required in practical work even when the number of degrees of freedom is small, and the test is always conclusive. This is encouraging since the results from attempts to apply the Durbin-Watson or the Theil-Nagar tests frequently may be inconclusive or of doubtful accuracy. Consideration is also given to the question of what to do when the null hypothesis of residual independence is rejected.</p> </abstract>
<abstract> <p>A model of a complete system of k linear expenditure functions introduced by Leser in 1960 [6, 7] has the advantage of great computational simplicity. An arbitrary specification, however, equating all cross elasticities of substitution results in unreliable estimate of price effects. In the present paper, a result in the theory of additive preferences [Houthakker, 5] is used in order to obtain a more plausible specification. This is achieved at the expense of an iterative procedure; however, the number of parameters (2k + 1) is identical in both models. A system of ten equations is fitted to Australian personal consumption data for the period 1949-50 to 1961-62.</p> </abstract>
<abstract> <p>The relationships between stock prices and volumes of sales are examined with the view that they are joint products of a single market mechanism. The results found here tend to support the notion that any model of the stock market which separates prices from volumes or vice versa will inevitably yield incomplete if not erroneous results.</p> </abstract>
<abstract> <p>It is shown geometrically and algebraically that a distortion in the factor market may make the transformation curve concave outward, either throughout or over a range, depending on the type of distortion. Also, computations are presented showing that it takes much factor market distortion to change the transformation curve appreciably.</p> </abstract>
<abstract> <p>This paper considers the hypothesis that commodities purchased on the market by consumers are inputs into the production of goods within the household. Its implications for the family of consumer demand functions whose arguments are real income and relative prices are drawn and compared with those of the hypothesis of additive separability. The paper closes with some examples of differences in commodity demand elasticities which are qualitatively consistent with the household production hypothesis and some comments upon how the latter might be utilized in empirical work.</p> </abstract>
<abstract> <p>In 1952 K. O. May presented a set of necessary and sufficient conditions for simple majority decision or, in other words, for direct democracy. The present article tries to investigate, by introducing some theorems on three-valued logic, logical conditions for a more general class of majority decision including representative system in its widest sense.</p> </abstract>
<abstract> <p> Le but de cet article est d'analyser la signification que revêtent l'élaboration et l'exécution des Plans français de développement au regard de la théorie économique. Distinct de la projection qui en fournit l'armature, le Plan constitue un ensemble d'objectifs dont la réalisation dépend de la combinaison du hasard et de la volonté. Son caractère actif l'apparente à une stratégie au sens de la théorie des jeux. L'élaboration du Plan est de type discrétionnaire, mais la nécessité de formuler des variantes conduit à rechercher pour certains problèmes (comme la durée du travail) des solutions formalisées. Du côté de l'exécution, les études en cours portent sur la définition d'un système de prix duaux correspondant à l'optimum. La fin de l'article est consacrée au traitement à accorder au taux de l'intérêt dans les modèles de croissance. </p> </abstract>
<abstract> <p>An exercise in the combined use of time series on consumers' expenditure in different countries, and in the use of first differences. A comparison of estimates from "within countries" and "between countries" regressions suggests that the former capture mostly short-run effects of income and price changes, the latter mostly long-run effects, and that the differences between these effects are highly significant both statistically and economically. In demand analysis it is therefore essential to specify the period of adjustment; there is no such thing as "the" elasticity of demand. Elasticities derived from time series for separate countries differ significantly from each other and from those derived from combined time series, but this does not invalidate the combined analysis because the estimates for separate countries are often unacceptable.</p> </abstract>
<abstract> <p>Survey data are used to investigate relations between current consumption of housing and ability to pay, the latter indicated not only by income, but also by likely past and future income as estimated from the individual's age and education.</p> </abstract>
<abstract> <p>An explicit formulation of the optimal strategy for all periods of dynamic linear decision rules is developed. This result is used in formulating the decision rules in feedback form. The corresponding feedback coefficients are computed for the paint factory case, described by Holt et al. [2].</p> </abstract>
<abstract> <p>This study deals with the problem of optimal investment allocation between capital goods and consumer goods industries in an underdeveloped country. Optimalization with respect to two criteria (full employment balanced growth and production of consumer goods) is achieved by using a new mathematical method: Pontryagin's Maximum Principle, and leads to the definition of an optimal investment policy. Numerical implications are described for the case of Algeria.</p> </abstract>
<abstract> <p>The mathematical isomorphism between the Walrasian multi-commodity price system and the Metzleric multi-region income system is not complete even in the well-behaved case of gross substitutes (for the stable price system) and superior goods (for the stable income system). A difference arises because of the price system's characteristic of zero-degree homogeneity which imposes additional constraints upon the inverse. The inverse of the price system has diagonal elements dominating single off-diagonal elements in both the same row and the same column, while the inverse of the income system has diagonal elements which are necessarily dominant only over single elements in the same column. This leads to a whole class of comparative statics propositions for the price system which have no counterpart in the income system and to economic implications about our ability to aggregate sectors or markets for qualitative analysis.</p> </abstract>
<abstract> <p>The optimal allocation of resources in New Zealand is analysed by applying linear programming methods to a general equilibrium model. A rich range of economic results--particularly in respect of bottlenecks--is obtained by examining near optimal and dual solutions. Changing the data by means of sensitivity analysis reveals the changes investment, production, the exchange rate, and wage policies that follow changes in export prices, import prices, technology, and the labour force. The effect of minimum and maximum wage policies is examined by adding special restraints to the dual problem.</p> </abstract>
<abstract> <p>In this paper we present several models which have been utilized to "explain" the variation of proportions. For these models, we review single equation estimation techniques which yield asymptotically efficient estimators. We then go on to consider the analysis of a set of correlated sample proportions and develop a joint estimation procedure which yields asymptotically efficient estimators. The joint estimation procedure, by taking account of heteroscedasticity as well as the correlations existing between proportions, produces estimators with smaller asymptotic variances than do single equation techniques which take account only of heteroscedasticity. Data on consumer decisions with respect to durable purchases and use of installment credit are analyzed to illustrate one use of these techniques in economics.</p> </abstract>
<abstract> <p>This paper provides a rigorous generalization of the well-known intuitive result that an equation in a two-equation model is in some sense identified if the variance of its disturbance is small relative to that of the other disturbance. The theorem is proved for the many-equation case.</p> </abstract>
<abstract> <p>This note has two purposes. Its first is to analyze the effects on the demand for each member of a group of firms producing substitute goods when there is a change in the number offered for sale. This analysis is confined to linear demand relations. The second purpose is to apply the analysis to a situation in which a group of firms either collude or merge. For certain linear demand relations it is shown that mergers will not result in a reduction of variety because of demand conditions.</p> </abstract>
<abstract> <p>During the first half of the twentieth century, utility theory has been subjected to considerable criticism and refinement. The present paper is concerned with certain experimental evidence and theoretical arguments suggesting that utility theory, whether cardinal or ordinal, cannot reflect, even to an approximation, the existing preferences of individuals in many economic situations. The argument is based on the necessity of transitivity for utility, observed circularities of preferences, and a theoretical framework that predicts circularities in the presence of preferences based on numerous criteria.</p> </abstract>
<abstract> <p>The theory of dynamic programming treats problems involving multi-stage processes by means of a transformation of the problem from the space of decisions to the space of functions. This is accomplished by deriving a functional equation whose solution is equivalent to the solution of the original problem. To illustrate this approach most clearly, free of extraneous analytic details, we consider a simple but nontrivial multi-stage investment problem. We show how exact solutions may be obtained in some cases, approximate solutions in others, and how these approximate solutions may be used to obtain more accurate solutions in the general case. Of particular importance is the decrease in the number of independent variables made possible by this approach. This is not only important from the theoretical standpoint, but is also of great value in reducing the cost in time and effort of numerical computation.</p> </abstract>
<abstract> <p>It is the purpose of the following study to review as far as possible not only the immense advantages but also the possible dangers in the use of mathematics in economics. This study was written and submitted to ECONOMETRICA before Professor Morgenstern sent the Council of the Econometric Society his recent suggestions regarding the conditions that ought to be required for election to the Fellows of the Society and in which he emphasized the danger in economics of a purely abstract orientation. The present study is concerned with this same problem. As one of the most eminent members of the Econometric Society wrote me regarding this study: "The only thing that troubles me is that most of this is so crystal clear, so self-evident that it nearly seems superfluous to bring it to the attention of the readers. However, when I think of the Kind of arguments and the kind of thinking that is currently going on, I realize that what appears self-evident to us does not have the same character for others."</p> </abstract>
<abstract> <p>An appropriate model for a market with many individually insignificant traders is one with a continuum of traders. Here it is proved that competitive equilibria exist in such markets, even though individual preferences may not be convex. Such a result is not true for markets with finitely many traders.</p> </abstract>
<abstract> <p>In an ordinary linear programming problem it is assumed that all the parameters (i.e., the coefficients of the objective function), the inequalities, and the resource availabilities are exactly known without errors. This assumption is relaxed in stochastic linear programming where some or all of the parameters are known only by their probability distributions. A distinction is generally drawn between the two approaches to stochastic linear programming: the passive (also termed "wait and see" approach) and the active (also termed "here and now" approach). In the passive approach the probability distribution of the objective function is derived explicitly or by numerical approximations, and decision rules are based on some features of this distribution. In the active approach additional decision variables are introduced indicating the amounts of various resources to be allocated to different activities. This paper analyzes a method of characterizing the distribution of the objective function values corresponding to the set of extreme points in the solution space for both these approaches of stochastic linear programming. Truncation refers to the selection of extreme points that are neighbors, so to say, to the optimal extreme point. The sensitivity of objective function values corresponding to truncated solutions is analyzed here in terms of stability properties, stability being measured in terms of variance. An application to an empirical economic problem where there are parametric variations in the coefficient matrix only is presented to illustrate the numerical problems and approximations involved in estimating the statistical distribution of the objective function. From an economic point of view the approach outlined here offers a theory of the second best, since it specifies the set of conditions under which a value of the objective function that corresponds to the optimum solution on the average may have higher instability than another value of the objective function that corresponds to a truncated solution, under the assumed conditions of stochastic linear programming.</p> </abstract>
<abstract> <p>The problem of price stabilization of primary commodities is dealt with here in the context of one commodity--tin. An attempt has been made to construct and estimate an econometric model with main emphasis on the forces that determine the demand for tin. Under the assumption that the forces which determine demand are beyond the control of any international commodity agreement, the model is simulated to determine policies which will soften the impact of changing demand on the producers of tin. The tools and policies suggested are in keeping with the powers of the existing International Tin Agreement.</p> </abstract>
<abstract> <p>The purpose of this paper is to introduce a class of distributed lag functions having the properties that an arbitrary distributed lag function may be approximated to any desired degree of accuracy by a member of this class and that the number of parameters required for a satisfactory approximation is less than that required for an equally good approximation by a finite distributed lag function.</p> </abstract>
<abstract> <p>In recent years, a number of power spectra have been estimated from economic data and the majority have been found to be of a similar shape. A number of implications of this shape are discussed, particular attention being paid to the reality of business cycles, stability and control problems, and model building.</p> </abstract>
<abstract> <p>The sampling properties of the usual estimator of marginal product from a Cobb-Douglas production function are studied under the assumption of normally distributed errors. The asymptotic normality of these estimators is demonstrated and certain special cases are considered for which the asymptotic distribution is particularly simple. An alternative estimator of marginal product is suggested which has both a smaller bias and greater precision than the usual estimator. Estimators of the variance-covariance matrix of all marginal product estimators mentioned are given. These are accurate to order n-1.</p> </abstract>
<abstract> <p>This article is concerned with the specification error implied by the use of a non-recursive model as a discrete approximation to a system of stochastic differential equations. One of the questions considered is whether, in view of this specification error, it is better, for the purpose of prediction, to ignore the over identifying restrictions and estimate the reduced form parameters directly. Another is the question of how to make the best use of the estimates of the parameters of the approximate model in order to obtain estimates of the parameters of the exact model.</p> </abstract>
<abstract> <p>A linear growth system is examined by applying a nonnegative output matrix to an age distribution of capital stock. The elements of the matrix are productivity rates and retention rates (depreciation rates). In this form, the matrix is irreducible and possesses a positive root which is greater in absolute value than any other root. Hence the well-known Perron-Frobenius theory can be applied. Then the matrix is generalized by decomposition into parts. Thes decomposition introduces interactions among capital sectors. Finally, the inverse of the maximal root is interpreted as a discount rate and the row eigenvector associated with that root as a set of implicit prices.</p> </abstract>
<abstract> <p>Arc elasticity is estimated for liquor by simply comparing state sales before and after price increases, standardized by states in which price did not change. The technique can be used wherever there are several economic units independent with respect to price changes; it allows causal interpretation and it permits comparison of various length-of-run elasticities.</p> </abstract>
<abstract> <p>In an earlier article Baumol and Gomory consider the problems created by the dual prices of an integer programming problem. They present a method for recomputing these dual prices so as to remedy some of these defects. This paper proposes an alternative view of the Baumol-Gomory recomputation process, which enables a reconciliation of the integer and linear programming properties. Moreover, we propose a further alteration of the Baumol-Gomory method that can be applied to cases in which nonzero prices are imputed to free goods in order to remove this property. We also show that even this method may not be entirely satisfactory in coping with the free goods problem.</p> </abstract>
<abstract> <p>It is known that if in a time series constructed from independent increments the individual items are replaced by, say, monthly averages, spurious correlation is introduced between successive first differences of the averages. The corresponding correlation for other statistics such as the mid-range is studied here. It is shown that the effect is even more pronounced when mid-ranges are used in place of averages.</p> </abstract>
<abstract> <p>Under certain conditions it is possible to define a meaningful aggregate of heterogeneous capital goods for purposes of long run analysis of production and distribution, even though no such aggregate can be meaningfully defined for the short run. This is shown to be the case in a recent model developed by Professor Solow for the study of a production function involving heterogeneous capital goods.</p> </abstract>
<abstract> <p>Quarterly time series for all manufacturing are used in this study to test the relationship between plant and equipment expenditures and (a) output and capacity, (b) the flow of internal funds, and (c) the level of corporate bond yields. The study makes use of a newly derived capacity index for all manufacturing and experiments with a number of different lag distributions.</p> </abstract>
<abstract> <p>This paper investigates the possibility of generalizing the classical theory of commodity markets to include uncertainty. It is shown that if uncertainty is considered as a commodity, it is possible to define a meaningful price concept, and to determine a price which makes supply equal to demand. However, if each participant seeks to maximize his utility, taking this price as given, the market will not in general reach a Pareto optimal state. If the market shall reach a Pareto optimal state, there must be negotiations between the participants, and it seems that the problem can best be analysed as an n-person cooperative game. The paper is written in the terminology of reinsurance markets. The theoretical model studied should be applicable also to stock exchanges and other markets where the participants seek to reach an optimal distribution of risk.</p> </abstract>
<abstract> <p>A utility theory is developed that parallels the von Neumann-Morgenstern utility theory, but makes no use of the assumption that preferences are complete (i.e., that any two alternatives are comparable).</p> </abstract>
<abstract> <p>This paper investigates an adjustment process for a pure exchange economy out of equilibrium, when trading is allowed at non-equilibrium prices. The "trading rule" postulated is that if a good is in excess demand at the going prices then trading ensures that no one will hold more of that good than he desires and vice versa when there is excess supply. An example of this mechanism is provided. It is shown that the system approaches an equilibrium.</p> </abstract>
<abstract> <p>One school of statistical thought holds that statistical decisions, when "rationally" made, are (and must be) made as if there were an a priori distribution on the states of Nature. Here "rational" means according to some set of axioms of "rational" choice of a decision function. In the present paper one aspect of one such axiom system is examined. The system is one of the simplest which has been proposed and can be regarded as a prototype of the others. It is argued that one of its axioms does not, upon closer scrutiny, appear very plausible and reasonable. It is demonstrated that this axiom requires the experimenter to have a preference among the states of Nature in advance of the experiment. Some related questions are discussed.</p> </abstract>
<abstract> <p>In this article we derive the exact finite sample frequency functions of the least squares and maximum likelihood estimators of the marginal propensity to consume, assuming the basic stochastic Keynesian model. The properties of these functions are considered in more detail for particular values of the parameters and sizes of sample. It is concluded that, for samples of 10 or more observations, generated by the model considered (with realistic values of the parameters), the maximum likelihood estimator of the marginal propensity to consume is the "better" general purpose estimator of this parameter.</p> </abstract>
<abstract> <p>The aggregate quarterly consumption functions suggested by Duesenberry-Eckstein-From (DEF) and by Zellner are recomputed and extended through 1960 on the basis of more recent data. While the actual "fit" of the DEF consumption function is substantially lower than previously reported, the coefficient estimates remain reasonably stable throughout the period. The Zellner consumption function fits well but gives rather low estimates of the long run marginal propensity to consume and a rather high and hard to interpret coefficient for the liquid assets variable. It is found also that the Durbin-Watson statistic presents a misleading picture of the amount of actual serial correlation in the residuals of these functions and an alternative nonparametric test is suggested.</p> </abstract>
<abstract> <p>The choice of methods of production as related to factor costs is formulated differently from the ordinary production function by means of a particular activity set. Continuous factor substitutability is maintained in an ex ante sense. The resulting formulation is more amenable to existing data and is applied via a distributed lag model.</p> </abstract>
<abstract> <p>In this paper, a 14-equation model of the United States potato industry is presented. Four of the equations contain endogenous variables lagged one time period. The solution to this system of first-order difference equations is presented to determine the system's stability. The stochastic stability is then investigated by obtaining estimates of the limiting variance-covariance matrix of endogenous variables. Thes matrix shows the cumulated effect of historical random shocks. This is followed by a similar study of the effect of erratic variation in exogenous variables. Next is a comparative static analysis, comparing actual values of variables with their stationary state values. The impact of the price support program on the industry is analyzed. Impact and stationary state multipliers are computed and short and long run effects of structural changes are evaluated.</p> </abstract>
<abstract> <p>The determining factor of consumption in budget analysis is not so much current income, but expected income or its capital value, which includes the capacity to earn income in the future. A budget survey of 68 Dutch farmers' families in the depression years 1935-1936 showed that most of them had dissavings. In the specification of the Engel curves for a number of items and for the aggregates of consumption expenditure by these farmers' families, the rental value of the cultivated land was introduced in addition to current income in order to represent expected income more adequately.</p> </abstract>
<abstract> <p>Johansen's analysis [4] of a vintage-capital growth model is extended by exploring the implications of alternative assumptions about expectations, with special attention being devoted to the polar assumptions of zero and perfect forsight; by introducing an arbitrary number of sectors; and by investigating the distributional aspects of the model.</p> </abstract>
<abstract> <p>Several related estimators of the parameters of a single equation of a simultaneous equation model are proposed. They are shown to be consistent and asymptotically efficient when the residual of the equation follows the first-order autoregressive process. One of these estimators, called MS2SLS, is designed to be consistent under a more general assumption about the stochastic process of the residual. A numerical analysis shows that the efficiency of MS2SLS is much higher than that of 2SLS under a general assumption about the stochastic process of the residual.</p> </abstract>
<abstract> <p>Analytic decision rules and horizon rules are developed for the warehousing problem. A stochastic warehousing problem is defined and solved, first by backwards induction, then by means of a forward working algorithm. The influence of holding costs and discount rates on forecasting horizons is illustrated.</p> </abstract>
<abstract> <p>The paper reformulates Houthakker's capacity method for quadratic programming in the framework of the simplex and dual methods for quadratic programming, thereby greatly reducing the conceptual and computational complexities of the method. It is shown that the method is applicable for all convex quadratic programming problems, including the case of a semi-definite matrix of the quadratic form and that of constraints in equality form. In the linear programming case the method reduces to a parametric version of the dual method.</p> </abstract>
<abstract> <p>For a catholic seasonal adjustment method for monthly economic time series, the general linear model and mutually independent random disturbances with zero mean and constant variance, in the special case with components consisting of twelve seasonal polynomials in t (time) of low degree and a nonseasonal polynomial in t of higher degree have been employed. A cogent set of test results consisting of best (minimum-variance) linear seasonal estimations and adjustments for the common logarithms of the monthly economic time series, "Shipments of Portland Cement in the United States, 1957-61," indicates that this is a theoretically and computationally promising approach now that large-capacity, high-speed electronic computers are available. The author has been attempting since 1959 to validate empirically the feasibility of this model. The history of statistical theories of seasonal adjustment is also briefly reviewed.</p> </abstract>
<abstract> <p>A strong type of turnpike theorem is proved for a planning model of a generalized dynamic Leontief system in which each industry has a polyhedral technology. Also, a dual turnpike theorem is proved for the shadow price of the original problem. This dual theorem shows the turnpike-like property of the shadow price with respect to the Neumann price ray. As a synthesis of the turnpike theorem and its dual, it is shown that all efficient paths of stocks are exactly on the so-called Neumann facet for most of the planning period in the case of polyhedral technology. McKenzie's recent results [3] are utilized in the course of the argument.</p> </abstract>
<abstract> <p>This paper presents a quarterly two-market model of the Swiss money and credit markets from 1947 to 1963. The model explains interest rates, the price level, and the quantity of money and bonds in terms of real wealth, real output, the monetary base, and the volume of government bonds and bank portfolios. The quarterly series of investment and output are derived from annual data by what is believed to be a new method of interpolation. The demand and supply functions are assumed to be linear in the first differences of logarithms. The parameters of the four equations are estimated by two-stage least squares. Of the 14 estimates all but one have the theoretically expected sign and all but two are significant at 5 per cent. The interest elasticities of money demand and bond supply are clearly negative, while the interest elasticities of money supply and bond demand, though the former is not significant, are positive. Estimating the same functions separately by ordinary least squares is shown to produce considerably different results; in particular, price and interest elasticities are much smaller in this case. The simultaneous estimates imply that price changes were mainly determined by investment activity and by changes in the monetary base, both with the expected positive sign. Interest rates, on the other hand, are positively affected by investment, output, and the supply of bonds by the government and the banking system, while the monetary base has a negative effect. The quantity of money is mainly, but not exclusively, determined by the monetary base. The volume of bonds, finally, is dominated by investment.</p> </abstract>
<abstract> <p>This paper investigates the demand for and supply of nonfarm residential mortgage credit in the United States. Yield adjustment in the home mortgage market is also considered. Among the explanatory variables used in this study are: the mortgage debt outstanding in the consumers' portfolio; the prices and yields involved in the borrowings and lendings in the mortgage and the nonmortage, long-term capital markets; the accumulation of savings in financial institutions; and the member banks' reserve position.</p> </abstract>
<abstract> <p>A computational procedure based on the results of Barankin and Dorfman [1], for minimising a convex quadratic function subject to linear constraints is developed. The applicability of the proposed procedure for linear programming problems is indicated.</p> </abstract>
<abstract> <p>In this paper, exact tests are proposed for testing the trend in the presence of autocorrelation and also for testing the trend and autocorrelation simultaneously in a first order Markov process. Also, the simultaneous confidence intervals associated with these tests are derived. These results are extended to a higher order Markov process.</p> </abstract>
<abstract> <p>A simplified model for interregional planning in agriculture appears to offer a method for solving linear programming problems of a given structure (each variable being represented in one constraint and in another constraint which includes all variables). This structure enables us to represent the model in a two dimensional space and to construct "potentiality curves," which constitute a first selection of the variables. The resolution with the remaining variables is easy. Thus, even large programmes can be solved by graphical representations and hand computations. The method proposed is, in fact, an application of a well-known algorithm the primal-dual method of Dantzig, Ford, and Fulkerson.</p> </abstract>
<abstract> <p>In this note we show the consistency of majority decisions under preference conditions that are more general than Single-Peaked Preferences (Arrow [1], Black [2]), Single-Caved Preferences (Inada [5]), Preferences separable into Two Groups (Inada [5]), and Latin-Square-less Preferences (Ward [12]). In the first part of the note, the underlying concepts and approach are introduced; in the second part the theorem is stated and proved; and in the third part its relationship with other sufficiency conditions is discussed.</p> </abstract>
<abstract> <p>A recent series of papers in Econometrica analyzed the effects of price stabilization upon producers. The analyses and conclusions of this series were remarkably similar to those of a series of papers published in the Quarterly Journal of Economics some twenty years ago, dealing with the effects of price stabilization upon consumers. That series demonstrated that price stabilization itself is neither a blessing nor a burden upon consumers. In a sense, each individual consumer loses if prices are stabilized at or above the simple, unweighted arithmetic average of the varying prices. In a similar sense, each individual consumer gains if prices are stabilized at or below the weighted means of the varying prices.</p> </abstract>
<abstract> <p>In this paper we have derived the demands for securities, under the assumption that (1) individuals choose their portfolios so as to maximize expected utility, (2) their utility functions are quadratic in the value of wealth, and (3) all individuals have the same probability distributions of future prices. This latter assumption probably is the most unrealistic one, but could be modified somewhat without affecting some of our conclusions. We have shown that, given these assumptions, the ratio of the quantity of security j to that of security j is the same for all individuals independently of their wealth and the parameters of their utility functions (providing that some of each security is held by every individual). The ratios of money holdings to wealth will differ among individuals, however. We also have shown that the elasticities of aggregate demand depend only upon the expected "yields" and the variances and covariances of the probability distributions of future prices. These demand relations were used in analyzing a problem in debt management, namely, "how can the composition of the government debt be altered in a way such as to hold gross national product a constant?" The answer to this question together with that to "how can the composition of the debt be altered in a way such as to keep the total current value of government debt a constant" leads us to conclude that--given our assumptions--variations in interest costs probably are so small that they can be neglected.</p> </abstract>
<abstract> <p>A Monte Carlo experiment is carried out to examine the small sample properties of ordinary least squares, indirect least squares, Hoch's, and Klein's estimates of the parameters of the Cobb-Douglas production function. A perfectly competitive model of firms in a single industry is considered in nine situations which differ in the behavior of the disturbances, the variability of inputs, and the position of the average firm. In each case 200 samples of size 20 and 200 samples of size 100 were obtained to approximate the sampling distribution of the various estimators.</p> </abstract>
<abstract> <p>A Laspeyres index of total input in the electric power industry was constructed. The nature of the bias imbedded in such an index, and its relation to the Paasche index and its bias, are investigated. Means to evaluate the actual size of the bias and means to reduce it are devised.</p> </abstract>
<abstract> <p>This paper discusses the stability of a multi-sector inventory cycle model incorporating inventories of inputs and, to some extent, of outputs. It emphasizes the effect on stability of adopting new assumptions about sales forecasting behavior; these new assumptions are embodied in a "customers' demand" forecasting model, presented in Section 1. It is argued that stability conclusions are somewhat sensitive to the forecasting assumptions adopted; so conclusions should be drawn with caution, given our lack of knowledge concerning actual forecasting techniques used by business.</p> </abstract>
<abstract> <p>A decision model in which information from a statistical observation is combined with a prior personal probability distribution via Bayes Theorem is sketched. A complication is introduced consisting of difficulties in exchange of information between the decision-maker and the statistician who analyzes the observation. Possible parcels of information which might be transmitted from statistician to decision-maker or client are listed. Some parcels of information are tentatively discussed and earlier discussions of others are cited.</p> </abstract>
<abstract> <p>In this paper is presented a simultaneous equation model that is non-triangular, its disturbances having a non-diagonal covariance matrix, and yet is causal and recursive, and whose underlying structural relations are also causal. The model is put forth as a counter-example to the Wold-Strotz claim that only triangular systems can be recursive and causal.</p> </abstract>
<abstract> <p>The quatratic function Q(x) to be maximized subject to linear constraints C'x &lt; d is considered as a function of the sources d. This leads to dual solution techniques, for example, the Houthakker capacity method and the Theilvan de Panne procedure. These algorithms are surveyed, some subtle points are clarified, and a new algorithm is illustrated.</p> </abstract>
<abstract> <p>Percentage changes in marginal utility are found to be invariant to utility transformations. They are quantifiable in the ordinary sense, and price and income elasticities can be expressed in terms of them. Definitions of necessity-luxury, independence and complementarity-substitutability in terms of the measurable utility changes lead to insights for empirical studies.</p> </abstract>
<abstract> <p>A nonnegative, finite distributed-lag model of the matrix multiplier is shown to be stable under exactly those conditions which impart stability to the first-order form of the model. Sufficient conditions on the first-order model are extended to cover the higher-order case. It is shown that a distributed-lag matrix multiplier is stable if, and only if, its corresponding first-order aggregation is also stable.</p> </abstract>
<abstract> <p>Our main objective is to demonstrate that there is only one "nice" way to measure technological change. Along the way, the unique "nice" measures of several other economic variables will be exhibited. We present axiom systems for indexes of several economic variables: inputs, outputs, prices, wages, inflation, and technological change. In addition to conventional smoothness and proportionality conditions, in each case an Invariance Axiom is proposed. For technological change this says, in a sense, that when there is no technological change there is no change in the index. One can prove that in each case there is a unique index satisfying the axioms, and this is a Divisia index. Since these continuous indexes are not generally independent of the path, the problem of how often to change index weights may be viewed in the light of a choice between invariance and independence. We show that the unique invariant measure of technological change is a natural generalization of Solow's measure to the case of many commodity types.</p> </abstract>
<abstract> <p>This paper contains an analysis of the pattern of comparative advantage in a model of growing international economy where intercountry productivity differentials are affected by, among other things, differences in economic life of machines in different countries.</p> </abstract>
<abstract> <p>This paper investigates the properties of a market for risky assets on the basis of a simple model of general equilibrium of exchange, where individual investors seek to maximize preference functions over expected yield and variance of yield on their portfolios. A theory of market risk premiums is outlined, and it is shown that general equilibrium implies the existence of so-called "market line," relating per dollar expected yield and standard deviation of yield. The concept of price of risk is discussed in terms of the slope of this line.</p> </abstract>
<abstract> <p>In this paper we consider the specification and estimation of the Cobb-Douglas production function model. After reviewing the "traditional" specifying assumptions for the model which are based on deterministic profit maximization, we develop a model in which profits are stochastic and in which maximization of the mathematical expectation of profits is posited. "Sampling theory" and Bayesian estimation techniques for this model are presented.</p> </abstract>
<abstract> <p>A model is presented for the derivation and implementation of an optimal linear decision rule for a firm producing and dealing in a number of interacting products, and possessing partial influence on their prices. The behavior of a multi-item production-inventory complex is represented by the dynamics of suitably defined state variables under the influence of decision rules that are linear in the state variables, but otherwise unspecified. The dynamic equations are stochastic owing to the presence of stochastic processes in the forcing terms. The statistical properties of these processes, together with the decision rules, determine the statistics of a functional describing the outcome. The optimum inventory decision is then derived as the "best" linear transformation on the past of the state variables, such that the mean value of the criterion functional is optimized subject to the system of constraints. A mechanism is also developed such that the optimal linear rule may be implemented.</p> </abstract>
<abstract> <p>A model of a pure exchange economy is investigated without the usual assumption of convex preference sets for the participating traders. The concept of core, taken from the theory of games, is applied to show that if there are sufficiently many participants, the economy as a whole will posses a solution that is sociologically stable--i.e., that cannot profitably be upset by any coalition of traders.</p> </abstract>
<abstract> <p>If one assumes (i) that individual economic units predict the values of random variables within a model by use of adaptive expectations functions of the type due to Koyck, (ii) that the parameters of these functions differ among individual economic units, and (iii) that "market expectations" are a weighted combination of individual expectations, then the function representing aggregate behavior is not a Koyck function, but rather some other function belonging to the general class of Pascal distributed lag functions.</p> </abstract>
<abstract> <p>This paper presents an analysis of the behavior of growth paths which are optimum with respect to the social welfare generated by consumption over a finite planning period. The principal result is the establishment of a (modified) golden rule turnpike property for such paths.</p> </abstract>
<abstract> <p>The estimation of the parameters of a regression equation or system that is nonlinear in the parameters or the dependent variables, or both, is described. This is achieved without recourse to linearization. The OLS and FIML estimates are obtained by a stable modification of the Newton method.</p> </abstract>
<abstract> <p>The assumptions underlying the application of econometric models to short-term forecasting are discussed and examined. It is suggested that the distinction between predictor variables estimated from outside information and predicted variables to be estimated by the equations of the model should follow practical rather than theoretical considerations. An illustration in the form of a simple prediction model for Ireland is given.</p> </abstract>
<abstract> <p>Linear probability functions are briefly discussed. The theory of linear discriminant functions in the two-class case is then presented. Such a function, which is a linear combination of observed variables, is used to classify each observed item into one of two classes. Certain identities between the two types of functions are obtained. Whereas the results from the two different models are the same, their distributional assumptions and derivations are quite different.</p> </abstract>
<abstract> <p>In the classical linear programming problem the behaviour of continuous, nonnegative variables subject to a system of linear inequalities is investigated. One possible generalization of this problem is to relax the continuity condition the variables. This paper presents a simple numerical algorithm for the solution of programming problems in which some or all of the variables can take only discrete values. The algorithm requires no special techniques beyond these used in ordinary linear programming, and lends itself to automatic computing. Its use is illustrated on two numerical examples.</p> </abstract>
<abstract> <p>In this article Gomory's method of solution of integer linear programming problems is described briefly (with an example of the method of solution). The bulk of the paper is devoted to a discussion of the dual prices and their relationship to the marginal yields of scarce indivisible resources and their efficient allocation.</p> </abstract>
<abstract> <p>This model is applicable to the flow of any kind of quantifiable transactions, and to matrices from three to more than one hundred actors, utlizing a 650 IBM electronic computer or similar equipment. The method develops a matrix of expected or baseline data from assumptions of complete indifference among the actors, and the actual amount of transactions in each direction for every pair of actors. It thus removes gross size effects and permits tentative inferences amount the distribution of preferences among pairs of larger groups of actors; about degrees of clustering or integration among actors; and about changes over time, if several matrices are used. It thus locates interesting pairs or groups for futher study. Import-export data are used as an example to show the detailed application of the method. For a given year and a group of countries, the import-export data can be arranged like a contingency table, except the diagonal cells are zero andthe ogerh entries are quantities of money, a continuous variable instead of a discrete variable. A model describing the data and techniques for the statitsitcal analysis are presented. This is a "null model" in the sense that the departures frm it are primary interest. The North Atlantic Area (1928) is used as an illustration.</p> </abstract>
<abstract> <p>We study the small sample properties of the simultaneous equation estimators by a Monte Carlo approach. The four methods of estimation considered are: least squares, two-stage least squares, unbiased and minimum-second-moment. The last of these four methods possesses the smallest second order sampling moments about the true parameter value in a majority of cases, while two-stage least squares shows the smallest bias in all cases. It is also found that the usual asymptotic standard errors of two-stage least squares give a rather satisfactory picture of the variability of the estimates about the true value. This is not true for the least squares method in all cases considered. Instead, it seems that the classical least squares standard errors measure the variability of the estimates about the biased expectation, not about the true value. In some cases this makes a very large difference.</p> </abstract>
<abstract> <p>Having estimated a linear regression with p coefficients, one may wish to test whether m additional observations belong to the same regression. This paper presents systematically the tests involved, relates the prediction interval (for m = 1) and the analysis of covariance (for m &gt; p) within the framework of general linear hypothesis (for any m), and extends the results to testing the equality between subsets of coefficients.</p> </abstract>
<abstract> <p>Global stability is proved for a dynamic competitive model with the assumption that goods are weak gross substitutes. Then local stability is proved when this assumption is weakened. A new Liapounov function is introduced.</p> </abstract>
<abstract> <p>Previous "short-cut" methods of solving linear programming problems have always had to revert to the simplex method, after the first two or three most promising activities have been located. The present paper shows that this is unnecessary, and presents a method of solving game theory and programming problems without using the simplex method. Despite its title the new method may, in large problems, involve the same amount of computing as the simplex method.</p> </abstract>
<abstract> <p>It is common to treat the money stock as exogenously determined in empirical investigations of the demand-for-money function. However, such neglect of the role of the commercial banking system in determining the money supply introduces simultaneous equations bias. In this paper, an aggregate money-supply function is derived in which an attempt is made to segregate the exogenous and endogenous aspects of the money stock. The coefficients of a structural model of the monetary sector which includes this supply relationship, an interest-responsive transactions demand function, and a reduced form income equation are estimated jointly for both the postwar and interwar periods. Structural supply and demand elasticities are computed, and it is shown that comparable single-equation elasticities are biased downward.</p> </abstract>
<abstract> <p>This paper reformulates the Samuelson model concerning competitive equilibrium among spatially separated markets. Assuming the existence of linear regional demand and supply relations, the problem of interconnected competitive markets is converted into a quadratic programming problem and a computational algorithm is specified which may be used to obtain directly and efficiently the optimum solution. The existence, uniqueness, and regularity of the solution are discussed, and an example is given to indicate the structure of the programming tableau.</p> </abstract>
<abstract> <p>Some sufficient conditions for the Simple Majority Decision Rule to be consistent are given in this paper. Professor Arrow has given in his book, Social Choice and Individual Values, two sufficient conditions for it. Our conditions are viewed as generalizations or modifications of his.</p> </abstract>
<abstract> <p>A natural generalization of least squares is proposed to estimate parameters in simultaneous linear equations. Full-information maximum likelihood is shown to be identical with this generalization. The extent to which other estimators deviate from the generalization is discussed. A paradox of Strotz is resolved, and application of canonical correlation theory to structural equations is indicated.</p> </abstract>
<abstract> <p>A two-sector model explains output, relative prices, factor employment and factor rewards in agriculture and non-agriculture. The exogenous variables include total factor supplies available to the economy, production function shifters and the proportion of agricultural inputs purchased from non-agriculture. Effects of agricultural changes on U.S. economic growth are estimated, with sensitivity tests based on measures of the variables for recent decades. Estimates are made of the contribution of each exogenous variable to annual decline of 2 to 3 percent in the farm labor force. A critical rate of technical advance in agriculture, below which there would be Malthusianism, is compared with observed rates of technical advance in U.S. agriculture. Alternative future values of the exogenous variables lead to projections to 1980 of the endogenous variables.</p> </abstract>
<abstract> <p>Every now and then, one encounters an allocation problem in which the "choice variable" is a point in an infinite-dimensional space. In such problems, the existence of an optimal choice among all permissible choices is not always assured. The present discussion is devoted to an investigation of this issue for a specific (but fairly common) class of allocation problems. Conditions for the existence of an optimal choice are derived and discussed.</p> </abstract>
<abstract> <p>This paper gives a non-linear growth model, which explains the development of an economy through stages somewhat similar to the Rostovian stages. Non-linearity is introduced by including the inaugmentable factor of land or natural resources in the production function along with labor and capital, and by recognizing that net saving is not a linear homogeneous function of income alone, but might be affected by the distribution of income and the interest rate and tends to be negative when per capita income is very low. Furthermore, population growth is assumed to follow a Neo-Malthusian pattern. The effects of non-neutral as well as neutral technical progress are discussed in this paper.</p> </abstract>
<abstract> <p> It has been known that the dynamic stability of multiple markets depends, when extrapolative expectations, gross substitutability, and tâtonnement are assumed, on the magnitude of the coefficients of expectation whose economic meaning is not necessarily clear. It is possible, however, to give an economic meaning to these coefficients and to estimate their magnitude, by giving a rational basis to extrapolative expectations. Rationality implies the use of economic theory, considering the cost of information and computation. Extrapolative expectations are derived as the prediction of the equilibrium by the use of estimated excess demand functions, and it is shown that the coefficients of expectations thus derived are such that the system of multiple markets is stable when gross substitutability and tâtonnement are also assumed. </p> </abstract>
<abstract> <p>This paper shows that when a homogeneous linear regression of a normally distributed variable Y on two normally distributed variables X and Z is deflated by Z, and when X and Y are uncorrelated, the deflated dependent variable Y/Z and independent variable X/Z are either uncorrelated or perfectly correlated. Thus, existing approximations to the covariance of these deflated variables are poor. A new approximation to this covariance is given that has the same defect for normally distributed variables, but that should otherwise be better than existing ones.</p> </abstract>
<abstract> <p>United States imports of manufactured products during the period 1947 to 1958 are examined in order to investigate the importance of changes in relative prices, ad valorem tariffs, and domestic production as explanatory variables. A cross section technique using first differences of product data over time is employed. Calculations were made after some stratifications of the data in order to illuminate additional aspects of the problem. Variable time lengths were used in the estimating process in an effort to evaluate the effect of time itself on the parameter estimates and also to determine whether peculiarities exist with respect to the particular years involved. The results were interpreted as reflecting institutional changes that have occurred in United States trade policy during this period.</p> </abstract>
<abstract> <p>The present paper considers, in a one-sector growth model, the relationship between investment and technical progress--in particular, the impact of the rate of investment on the level of technology. The notion of an optimal capital replacement period is developed and shown to depend on the rates of technical change and of investment. Finally, the model is used to evaluate a method which has previously been used to apportion increases in output per man-hour between investment and improvements in technology.</p> </abstract>
<abstract> <p>In this paper a game theoretic model will be developed for the determination of acreage allocation among four crops--wheat, corn, oats, and soybeans. The model will be functionally dependent upon statistical demand elasticity curves and will be applied to data for the period 1948-1958 for the United States. Elements of an individual farmer's strategy will be presented. In addition, the optimal game theoretic acreage allocation for the above period for total United States production will be compared with the actual acreage allocation for the four crops. The discussion throws light on the reasons for some of the variations in annual acreage for the above crops.</p> </abstract>
<abstract> <p>A multi-sector buffer-stock inventory model is developed in an attempt to resolve the problem of aggregation involved in deriving implications for the stability of the economy from a consideration of inventory practices of individual firms. It is demonstrated that stability depends upon a multitude of parameters, some of which are suppressed in aggregative model construction. The economy is necessarily unstable when perfect, if myopic expectations are assumed. With naive expectations stability becomes a definite possibility, particularly if firms attempt only a delayed adjustment of inventories to the equilibrium level. Although the empirical evidence marshaled in order to illustrate the application of the theorems does not prove sufficiently accurate to permit precise conclusions, it is apparent that the conditions for stability may well be satisfied for reasonable values of the system's parameters. Tax schemes which have been suggested as means of stabilizing fluctuations in inventory investment are appraised in the concluding section.</p> </abstract>
<abstract> <p>The classical hypothesis of consumer theory is that expenditures are regulated by a preference order, between all possible compositions of quantities of goods, which selects a best composition on every budget constraint determined by prices and total expenditure. A determination of a composition for every budget constraint defines an expenditure system. The hypothesis applies to any expenditure system, and asserts that it is associated in this way with some preference order. Here there is a statement of definitions and propositions which constitute a structural analysis of expenditure systems, and of the preference systems associated with them by the Samuelson principle of revealed preference. In accordance with the well-known investigation of Houthakker, and the subsequent one by Uzawa, focus is on the admissibility of the preference hypothesis in the usual form, involving a preference scale, for which representation by a numerical function is possible, rather than a general preference order. But there is also a more general analysis involving propositions of a type which appears to be quite new in the subject.</p> </abstract>
<abstract> <p>Applications of capital theory refer to a world of changing population, institutional processes, technology, and innovation. For example, an investment decision may be affected by some expected repetitive patterns of circumstances or by some seemingly irreversible processes threatened by obsolescences or stagnation. One may assume the principle of an eventually diminishing net returns flow. It seems desirable to treat the marginal efficiency of capital in terms of an initial investment and a net returns flow that varies with time. For a variety of such time dependent net returns flows the treatment of the marginal efficiency of capital is simplified by the Laplace transform technique illustrated in this article.</p> </abstract>
<abstract> <p>In this note we study the dynamic inventory problem for a follow-on provisioning in which the program length is subject to uncertainty with a known distribution. It is shown that under rather general cost conditions, the optimal policy is of the (S, s) type. This is true whether or not there exists a time lag in delivery provided that excess demand is always backlogged. The case of an infinite program horizon is also briefly discussed.</p> </abstract>
<abstract> <p>This note describes an electronic computer program for the computation of coefficients of a simultaneous stochastic system by various methods, including full-information maximum-likelihood.</p> </abstract>
<abstract> <p>This study involves the estimation of production functions of the Cobb-Douglas type in application to Indian industry. The few studies which have appeared so far on this particular aspect of Indian industry were concerned with the industrial sector as a whole based on cross-section data for industrial aggregates. Such production functions for the industrial sector as a whole can be regarded as some sort of an average of those for individual industries producing homogeneous products; but the divergences in this quantitative relationship among different types of individual industries depending on such factors as the degree of substitution that is possible between different factors of production, the capital intensity of the industry, etc., can be studied only when production functions are available separately for individual industries. In the present study, production functions for the industrial sector as a whole as well as for seven important industries in India are worked out based on cross-section data relating to individual firms for the two years 1951 and 1952. An interesting application of production functions of the Cobb-Douglas type has also been made to estimate the total capital employed in 1952 in Indian industry as a whole and in the three major industries of cotton, jute, and tea, separately.</p> </abstract>
<abstract> <p>This article is an attempt to determine whether firms can be assumed to behave as if they were using a rational inventory policy. A decision rule is deduced from a plausible model of a firm's cost structure and an attempt is made to see whether historical time series of firms' production, sales, and inventory data can be explained by such a decision rule.</p> </abstract>
<abstract> <p>The consumption of a durable good, automobiles, is treated as the annual depreciation of the stock of automobiles. This consumption of automobiles is related to income by utilizing data from each state for the two years 1940 and 1950 as cross-sectional observations.</p> </abstract>
<abstract> <p>The hypothesis considered is that the utility function can be written as &lt;tex-math&gt;$U=U[V^{A}(q_{a_{1}},...,q_{a_{\alpha}}),...,V^{M}(q_{m_{1}},...,q_{m_{\mu}})]$&lt;/tex-math&gt;, where the V's are "branch" utility functions depending on quantities of different commodities assigned to different branches (e.g., food, clothing). This hypothesis implies certain empirically meaningful and interesting conditions on the price coefficients of the demand functions. These conditions are, however, not subjected here to statistical test. A price index formula is next developed for measuring branch prices (e.g., the price of food, the price of clothing) and with the use of these indices a rationale is found for the individual doing his budgeting by first deciding how to allocate expenditure among the several branches and then making independent decisions as to how best to spend each branch allocation on the commodities within the branch. That this budgeting practice is both familiar and consistent with the proposed hypothesis is taken as evidence in support of the hypothesis and, therefore, of the implications for the price coefficients in demand functions.</p> </abstract>
<abstract> <p>The relationship between the input requirements of an industry and the multi-dimensional structure of the output of the industry is shown to be derivable from the microeconomic relationship between input and output structure for individual processes. Aggregation of the process functions for the industry is achieved through knowledge of the joint frequency distribution of the output dimensions of the processes. The principles are developed in terms of the case analysis of the fuel requirements of the trucking industry. Certain fundamental relationships are shown to exist, for the truck transportation industry, between average load, average length of haul, and number of hauls on the one side, and aggregate ton-miles, vehicle-miles, and tons on the other. Analogous relationships would hold in railroad and airplane transportation.</p> </abstract>
<abstract> <p>The problem of aggregating individual preference orderings to form a social ordering took a new turn when Arrow organized the subject abstractly. We study here his celebrated theorem that five plausible conditions on the method of aggregation are inconsistent. This theorem is in fact false in general, as a counter-example shows. When we increase the amount of disagreement which is allowed to occur, then the inconsistency is restored. The modified result preserves much of the impact of the original theorem.</p> </abstract>
<abstract> <p>The present paper is an attempt to relate the theory of wages, the theory of fiscal policy, and the theory of international trade to one another. The purpose is to illuminate the dilemma faced by certain Western European economies subjected to a threefold pressure from powerful labor unions asking for higher wages, from the Organization for European Economic Cooperation asking for freer trade, and from international competition. The medium is a simplified model marrying the elasticity and the absorption approaches to international trade theory and assuming that fiscal policy is successfully applied to keep the balance of trade equalling zero. The system of equations is solved for domestic output to which domestic employment is assumed to be in proportion. The effect upon the solution of manipulating domestic factor prices is determined. Available empirical measurements of demand elasticities are applied to the solution.</p> </abstract>
<abstract> <p>According to the static theory of decision-making under uncertainty, the policy maker will take that action that maximizes expected utility. In the dynamic theory several consecutive periods play a role, each of which is characterized by a certain action. The policy maker will then choose a maximizing strategy (i.e., a rule according to which all successive actions are determined by the information which is available at the time when the action has to be taken). This note is confined to the action in the first period of such a strategy. It is shown that, under certain conditions, the first-period action of the strategy which maximizes expected utility is identical with that of the strategy which neglects the uncertainty problem by maximizing utility under the condition that all uncertain elements are equal to their mean values.</p> </abstract>
<abstract> <p>In non-monetary neo-classical growth models, the equilibrium degree of capital intensity and correspondingly the equilibrium marginal productivity of capital and rate of interest are determined by "productivity and thrift," i.e., by technology and saving behavior. Keynesian difficulties, associated with divergence between warranted and natural rates of growth, arise when capital intensity is limited by the unwillingness of investors to acquire capital at unattractively low rates of return. But why should the community wish to save when rates of return are too unattractive to invest? This can be rationalized only if there are stores of value other than capital, with whose rates of retrun the marginal productivity of capital must compete. The paper considers monetary debt of the government as one alternative store of value and shows how enough saving may be channeled into this form to bring the warranted rate of growth of capital down to the natural rate. Equilibrium capital intensity and interest rates are then determined by portfolio behavior and monetary factors as well as by saving behavior and technology. In such an equilibrium, the real monetary debt grows at the natural rate also, either by deficit spending or by deflation. The stability of the equilibrium is also considered.</p> </abstract>
<abstract> <p>The existing hypothesis of dynamic stock adjustment is modified to allow for discontinuous adjustment observed in the actual behavior of the household toward durable goods purchases. It is proposed that the probability of purchase of durables is determined by the gap between desired and actual stock; and, given purchase, the amount of purchase is also determined by the gap between desired and actual stock. Difficulties of estimating such a model from cross-section data are indicated and a reformulation of the model is suggested. Regression equations are then specified and estimated from Survey of Consumer Finances data.</p> </abstract>
<abstract> <p> The "indirect utility function," a concept associated with Hotelling, Roy, Houthakker, and others, gives the maximized value of consumer's ordinal utility in function of the prices and income of his budget constraint: namely, &lt;tex-math&gt;$\phi ^{\ast}(p_{1}/I,...,P_{n}/I)=\text{Max}\,\phi (x_{1},...,x_{n})$&lt;/tex-math&gt; with respect to x's satisfying the budget constraint &lt;tex-math&gt;$\Sigma _{1}^{n}(p_{j}/I)x_{j}=1$&lt;/tex-math&gt;. &lt;tex-math&gt;$\text{Writing}\ p_{j}/I=y_{j},\Phi (Y)=-\phi ^{\ast}(Y)$&lt;/tex-math&gt;, it follows that &lt;tex-math&gt;$N(X;Y)=\phi (X)+\Phi (Y)\leq 0$&lt;/tex-math&gt; along &lt;tex-math&gt;$\Sigma _{1}^{n}\ x_{j}y_{j}=1$&lt;/tex-math&gt;, equalling zero only along the equilibrium demand relations X = X(Y), &lt;tex-math&gt;$Y=Y(X)\equiv X^{-1}(X)$&lt;/tex-math&gt;. It is shown that φ(X) and &lt;tex-math&gt;$\Phi (Y)$&lt;/tex-math&gt; are completely dual functions, one possessing all the general properties of the other. Just as φ's partial derivatives give &lt;tex-math&gt;$\phi _{i}/\phi _{j}=y_{i}/y_{j},\Phi \text{'}{\rm s}\ \text{give}\ \Phi _{i}/\Phi _{j}=x_{i}/x_{j}$&lt;/tex-math&gt;, etc. Numerous theorems are proved, such as: if either of φ and Φ has homothetic contours, so does the other; if both can be stretched into an additive form, they are both homothetic and belong to the so-called constant-elasticity—of-substitution family of Solow et al., a result already anticipated by Bergson in 1936; if the above can hold with no stretching required, we are in the "pure Bernoulli-Marshall" or Cobb-Douglas case of unitary own-elasticity and other demonstrated equivalent properties. Finally, dual functions of mixed variables, &lt;tex-math&gt;$\phi (y_{1},...,y_{r};x_{r+1},...,x_{n})$&lt;/tex-math&gt; and &lt;tex-math&gt;$\Phi (x_{1},...,y_{r};y_{r+1},...,y_{n})$&lt;/tex-math&gt;, are defined by Legendre transformations and the properties of "demand under rationing" are deduced from them. </p> </abstract>
<abstract> <p>A preference ordering is self-dual if it has a direct utility function that can be written in the same mathematical form as the corresponding indirect utility function. Samuelson has raised the question whether there are any such preference orderings in addition to the trivial one discussed by him. It is shown here that there exists at least one family of self-dual preference orderings, of which the direct and indirect addilog preference orderings are limiting cases. Although various properties of the demand functions derived from self-dual addilog can be demonstrated, the mathematical form of the utility functions has not been determined.</p> </abstract>
<abstract> <p>Zeuthen's multi-round solution to the bilateral monopoly problem is demonstrably restricted to bargainers who, at each round, almost perversely ignore the preceding succession of modified offers, and persist in a consideration of outcomes limited to total capitulation. Alternatively, the Zeuthen solution is a single-round solution. A Zeuthen-type model is developed and utilized to show that, whether single- or multi-round, the wage outcome is confined to a range delimited by those rates which maximize the expected gain from no-concession for each bargainer.</p> </abstract>
<abstract> <p>In estimating parameters of the Cobb-Douglas production function, assuming competition and profit maximization, the estimator to be employed depends on the specification of the behavior of the disturbance term in the production function. If this disturbance term is not transmitted to inputs, that is, if inputs are independent of this disturbance, then the least squares estimator is consistent; if the disturbance is fully transmitted to inputs, then a consistent estimator is obtained if some restrictions are imposed on the second moments of the disturbances in the system. A more general case may be specified, however, encompassing the above specifications as subcases. In this general case, the disturbance term may be only partially transmitted. If this occurs, then neither of the estimators noted above are consistent. In fairly general situations, these estimators furnish upper and lower bounds for the production function elasticity (in a one-input case) or for the sum of the elasticities (in the Q input case; Q any number). The consequences of each of these specifications, in terms of probability limits, are examined in some detail. This is carried out, first, for the one input case, and then the Q input case is discussed.</p> </abstract>
<abstract> <p>Two recent topics of growth economics, the Turnpike Theorem and the Relative Stability Theorem, are discussed on the basis of more general and realistic assumptions. Without the crucial assumption of primitivity, the "ergodic" properties asserted by those theorems are seen to be established only in the average sense. Our argument leads to extensions of the Kolmogoroff-Yoshida-Kakutani result into some nonlinear and non-unique cases.</p> </abstract>
<abstract> <p>This paper, a sequel to an earlier communication by the author [3], develops a computational procedure for minimising a class of separable convex functions subject to linear constraints.</p> </abstract>
<abstract> <p>The idea of this article is to use some concepts taken from information theory in order to evaluate the predictive quality of decompostion forecasts. In the present case, the forecast is the input structure of a given industry in a given year which is seen as a forecast of the input structure of that industry in a later year. It is a decomposition forecast because, as is well known in input-output analysis, input coefficients by industries add up t unity. Attention is paid to the predictive achievements of subgroups of input coefficients. The information measures defined are applied to ten input-output tables for the Netherlands, 1948-1957.</p> </abstract>
<abstract> <p>In estimating a function, certain assumptions are made about the random term. This paper deals with the influence of such assumptions on estimators for the function y = x&lt;sup&gt;α&lt;/sup&gt;. The case of a normally distributed random term, both in its multiplicative and additive forms, is considered here. The major part of the article is devoted to a hypothesis concerning a lognormally distributed random term, for which consistent estimators are derived.</p> </abstract>
<abstract> <p>The factor price equalization problem is approached by means of activity analysis. Conditions are determined in which the world prices of goods (alternatively, the world outputs of goods) will imply, by the profit conditions of competitive equilibrium, equal factor prices in all countries. The basic theorems are then applied to particular models of production.</p> </abstract>
<abstract> <p>This paper endeavors to present a simple model to explain the frequency distribution of income by adding "birth and death" considerations to the more usual random shock system type of model. The form of this model turns out to be a Gram-Charlier distribution subject to a linear logarithmic transformation, the parameters of which are meaningful and significant. A tentative method of fitting such curves is suggested and the values of the parameters discussed.</p> </abstract>
<abstract> <p>A fairly general dynamic Leontief System with discrete time periods is considered in which (a) alternative substitute type activities are allowed, (b) a bill of goods is given over time, and (c) the unknown quantities of activities satisfying the system are to be determined so as to minimize a specified linear objective function. The model has two features (a) the "general Leontief aspect" which assumes that there is one and only one output for each activity, and (b) a "dynamic (or one-way flow) aspect" which assumes that no inputs for an activity occur later in time than the time of its output. In the optimal solution certain activities appear in positive amounts and others in zero quantities. Once they are known, the quantitative solution is obtained readily by direct solution of the equations. Hence, the basic problem is to select the activities which appear in positive quantity in the optimal solution. The main results are (a) the "Samuelson Property" due to the Leontief aspect; i.e., the selection of these activities can be made independent of the bill of goods; (b) the local optimization property due to the inclusion also of the dynamic aspect; i.e., when the activities are ordered over time according to the time of production of their principal item, the selection of activities for the first k time periods, k = 1, 2,... can be made without consideration of the nature of activities producing in time periods later than k, moreover, if the coefficients in the linear objective function are interpreted as "costs" to perform a unit quantity of an activity, then the optimum selection for successive time periods can be obtained inductively as a sequence of local optimizations using an implicit cost for each time period by subtracting from original costs those implicit costs associated with earlier time periods. The two properties, together, lead to a special computational procedure which is more efficient than the ordinary procedure of the generalized simplex method and which, of course, is not applicable to a general linear programming problem. Numerically, the number of multiplications, required for the optimal solution by the conventional simplex method, is reduced by a factor p where p is the number of discrete time periods considered.</p> </abstract>
<abstract> <p>Cost structures of the kind usually posited in the economic theory of the firm do not seem to be readily ascertainable by an examination of business records. A somewhat different approach to the problem of optimizing business costs is therefore essayed in this article. The objective is to develop a means whereby general qualitative properties of economic theorems on cost behavior may be used to provide quantitative answers as a guide to business conduct. It is assumed that total costs are unknown but have the general form theoretically ascribed to total cost functions when output occurs beyond the inflection point (or point of minimum marginal cost) on the total cost function. Certain easily ascertained variables (direct manhours) are then used as surrogates in place of the unknown total costs in order to determine optimal production patterns over a finite time horizon, in which sales are assumed to be known or given. The model includes multi-product and factor considerations but not inventory carrying costs, liquidity considerations, etc. In short, it is a production model intended for application to production problems.</p> </abstract>
<abstract> <p>Conventionally labor supply modeling has been dichotomized from consumption expenditure allocation. We estimate a model unifying both aspects of the consumer's decision problem, and we test for the two-stage decision implied by the conventional dichotomy. We investigate the gains from joint modeling. We use a version of the Rotterdam model recently shown by Barnett [6] to be derivable at the aggregate level under weaker assumptions than those needed to acquire empirically usable theoretical results at the aggregate level with other models; our results are not subject to the restrictiveness imputed to earlier uses of versions of the Rotterdam model.</p> </abstract>
<abstract> <p>This paper concerns individual choice among "opportunity sets," from which the individual will later choose a single object. In particular, it concerns preference relations on opportunity sets which satisfy "preference for flexibility," a set is at least as good as all of its subsets, but which may not satisfy "revealed preference," the union of two sets may be strictly preferred to each one taken separately. A representation theorem is given which "rationalizes" such choice behavior as being as if the individual is "uncertain about future tastes."</p> </abstract>
<abstract> <p>This paper provides a complete set of local duality results for a utility maximizing consumer (or single output cost minimizing firm). Given a continuous local expenditure function defined on a compact, convex set of positive prices we establish the existence of continuous local direct, indirect utility and distance functions. This procedure avoids troublesome continuity problems at the boundary of R&lt;sub&gt;+&lt;/sub&gt;&lt;sup&gt;N&lt;/sup&gt;. In addition it is shown that if two utility functions are second order approximations at some point, then their respective expenditure, distance, and indirect utility functions are also second-order approximations to each other at some point. This latter result provides additional impetus for using duality theory and substantial justification for the use of "flexible" functional forms which can provide second-order differential approximations to any twice continuously differentiable function at a point.</p> </abstract>
<abstract> <p>We show that a "no-cycle" condition, of a continuous type introduced by Ville, is equivalent to the Antonelli (or Slutsky) symmetry conditions, which together with other axioms is known to be a basis for constructing a utility function from expenditure data. The "no-cycle" condition is attractive because--unlike the symmetry condition--it has an evident behavioral interpretation, through which relate it to the strong axiom of revealed preference. We show, nevertheless, that the "no-cycle" condition does not imply even the weak axiom of revealed preference.</p> </abstract>
<abstract> <p> Consider a society with n individuals who must choose an alternative from a given non-empty set X. For an integer d ≤ n, a d-majority equilibrium is an alternative x* ∈ X such that no alternative in X is preferred to x* by at least d individuals. It is proved that when X is a compact and convex set of dimension m, a necessary and sufficient condition that, for every profile of individuals' convex and continuous preferences, there exists a d-majority equilibrium, is that d be greater than (m/(m + 1))n. Using this result for the case when X consists of a finite number, T, of alternatives, it is shown that a necessary and sufficient condition that for every individuals' preference orderings there exists a d-majority equilibrium is that d exceeds ((T - 1)/T)n. </p> </abstract>
<abstract> <p>This paper completely characterizes the solution to the problem of searching for the best outcome from alternative sources with different properties. The optimal strategy is an elementary reservation price rule, where the reservation prices are easy to calculate and have an intuitive economic interpretation.</p> </abstract>
<abstract> <p>When traders come to a market with different information about the items to be traded, the resulting market prices may reveal to some traders information originally available only to others. The possibility for such inferences rests upon traders having "models" or "expectations" of how equilibrium prices are related to initial information. This relationship is endogenous, which motivates the term "rational expectations equilibrium." This paper shows that, in a particular model of asset trading, if the number of alternative states of initial information is finite then, generically, rational expectations equilibria exist that reveal to all traders all of their initial information.</p> </abstract>
<abstract> <p>This paper investigates the behavior of the winning bid in a sealed bid tender auction where each bidder has private information. With an appropriate concept of value, the winning bid will converge in probability to the value of the object auction (as the number of bidders grow large) if and only if a certain information condition is satisfied. In particular, it is not necessary for any bidder to know the value at the time the bids are submitted. These results bear on the relationship between price and value and on the aggregation of private information by the auction mechanism.</p> </abstract>
<abstract> <p>The Arrow-Debreu model is extended to include a sequential market model with financial markets. This is done by dropping the contingent contracts from the Arrow-Debreu model, leaving only a sequence of spot markets for commodities. The resulting market structure is inefficient. Efficiency is restored with a sequence of stock markets and option markets. In addition, consumers are shown to be unanimous in wanting each firm to maximize the price of its common stock.</p> </abstract>
<abstract> <p>This paper examines various theorems of trade and general equilibrium in a generalized framework involving arbitrary numbers of goods and factors. It develops structural relations among the changes in outputs, commodity prices, factor rewards, and factor endowments. By finding a way of inverting a bordered matrix with a singular Hessian, the paper derives explicit expressions for the following matrices: the Stolper-Samuelson matrix; the Rybczynski matrix; the matrix which measures the effect of a change in factor endowments upon factor rewards at constant commodity prices; and the matrix which measures the effect of a change in commodity prices upon outputs at constant factor endowments. Various properties of these matrices are used to obtain, among other results, the reciprocity relations and general results on factor-price equalization. The paper also examines the problem of indeterminancy in production when the number of commodities exceeds the rank of the input-coefficient matrix and presents the correct specifications of the supply functions of outputs. Finally a new theorem on the degree of flatness of the production transformation surface is derived.</p> </abstract>
<abstract> <p> The notion of stability in the sense of Lyapunov is applied to economic dynamic processes of the Champsaur-Drèze-Henry type. </p> </abstract>
<abstract> <p>The concepts of aggregation and disaggregation appearing in some recent literature are compared.</p> </abstract>
<abstract> <p>This paper proposes the Gini coefficient of the censored income distribution truncated from above by the poverty line as an index of poverty. An ordinalist axiomatic approach, which was introduced by Professor Sen, is used to justify this measure. In comparison with Sen's index, our alternative measure is simplerand more concerned with relative deprivation; it can be regarded as a more natural translation of the Gini coefficient from the measurement of inequality into that of poverty.</p> </abstract>
<abstract> <p>The paper presents a model of wealth distribution that makes use of Gibrat's lawof proportionate effect to explain the way wealth is distributed and how the distribution changes over time. The stochastic factor in each period is shown tobe the result of deliberate choices by individual decision makers regarding their savings, investment, and bequests, given their inherited wealth and natural ability. The source of randomness is two-fold: uncertainty about the rates of return and randomness of the distribution of natural skills. Also studied is the impact of government tax parameters on the inequality of wealth. Two categories of taxes are distinguished: taxes on flows, such as those on income, portfolio returns, and earnings, and those on the stock, such as wealth or estate taxes.</p> </abstract>
<abstract> <p>This paper discusses recent work on the existence of aggregate production functions in models in which capital goods are specific to firms and cannot be used interchangeably. It is found that this raises problems not only for capital aggregation but also for the existence of labor and output aggregates. Recent work on the question of using aggregate production functions as approximations is also discussed.</p> </abstract>
<abstract> <p>Existence of competitive equilibrium in a market model introduced by Robert J. Aumann is proved. This paper generalized Aumann's result and simplifies his proof.</p> </abstract>
<abstract> <p>This paper is concerned with the interaction of saving and portfolio decisions of a single consumer. Its building blocks are the classical theory of optimal allocation over time and Arrow's recent formulation of the theory of portfolio selection. The concept of a risk aversion function is extended to a two-period context and the implications of declining risk aversion are explored. Also discussed are the problems of the effect of changes in the rates of return and in the degree of risk, as well as the question of taxation and risk taking.</p> </abstract>
<abstract> <p>In this paper the general mathematical structure of most long term economic policy problems is indicated. For such a general policy problem the existence of optimal policy is established under certain conditions. In the proof of the existence theorem presented in this paper, compactness of the set of feasible policies is established using Helly's selection principle.</p> </abstract>
<abstract> <p>In this paper we estimate a complete system of demand equations making full use of the restrictions implied by economic theory. Our theoretical model is based on the Klein-Rubin linear expenditure system which was first estimated by Stone. We place primary emphasis on maximum likelihood estimates obtained using annual time series observations of prices and per capita consumption for the U.S. economy in the period 1948-1965. The plan of the paper is as follows: Section 1 begins with a discussion of the problems involved in making systematic use of economic theory to estimate demand functions; this is followed by a brief description of the linear expenditure system and discussion of the specification of its dynamic and stochastic structure. In Section 2 we describe three methods of estimating the linear expenditure system, including the maximum likelihood procedure which we believe is most appropriate. We report our results in Section 3 and our conclusions in Section 4.</p> </abstract>
<abstract> <p>Three econometric specifications for the consumer demand model are examined. Although all three rest on classical demand theory, they involve different sets of parameters to be estimated, and the parameters are restricted in different ways by the theory. The paper is concerned with providing efficient estimates for the three models and comparing parameter estimates implied by the different models.</p> </abstract>
<abstract> <p>Several existing single equation estimators, designed for the case where the structural disturbances are generated by a first order autoregressive process, are generalized for situations where the autoregressive process is of higher order than one. These generalized estimators are shown to be consistent, both when the parameters of the generating process are known and unknown; and their asymptotic relative efficiencies are compared. Three full information estimators are also examined. They are shown to be consistent with asymptotic efficiencies ranked like their single equation counterparts. Among the results derived are the consistency of 2SLS and 3SLS in this situation and the overall relative efficiencies of the "Theil-Fisher" type estimators compared with the 2SLS-3SLS and "Madansky" type estimators.</p> </abstract>
<abstract> <p>This paper investigates the restrictions on the indifference map that are implied by alternative assumptions about consumer behavior under uncertainty and, conversely, the restrictions on consumer behavior under uncertainty that are implied by alternative assumptions about the indifference map. It is shown, for instance, that if the individual is to be risk neutral at all incomes and price ratios in a given neighborhood, the income-consumption curves must be linear, and if all income-consumption curves are linear, there exists a cardinal representation of utility which is linear in income. The cases of constant relative risk aversion, constant absolute risk aversion, and quadratic utility functions are also investigated.</p> </abstract>
<abstract> <p>In this paper the question of induced factor augmenting technical change in the context of the profit maximizing firm is addressed. The Kennedy innovation possibility frontier is employed to describe the opportunities available to the firm for factor augmentation and from it the direction of factor augmentation can be chosen. In addition, this opportunity curve can be shifted inwards or outwards according to the expenditure on research and development. The selection of the direction and extent of technical change is first determined via a myopic decision rule and then by maximization of the present value of the stream of net revenue from production and sale of output less the cost of technical advance. In the latter problem the maximum principle of Pontryagin is employed. Questions regarding the existence of stationary states and stability are resolved, and the optimal solutions are compared with the myopic decision rules.</p> </abstract>
<abstract> <p>Stock option prices result from the interaction of many investors of many persuasions. Previous theories of option price have been micronormative, thus having tenuous connections with observed option prices. This paper makes no assumptions about individual expectations or utilities; instead a model is specified for actual prices and tested against twenty years of data. Inferences are then made concerning the aggregate change in investors' expectations and risk attitudes through time.</p> </abstract>
<abstract> <p>The range of admissible price and income elasticity estimates is materially reduced in this investigation of a variety of procedures, including new methods for analyzing deflation bias. Mostly small effects are found for choice of index formula, base year, logarithmic versus linear form, definition of food consumption (e.g., physical versus expenditure measure), procedures related to time, and aggregation. However, the supply elasticity of food is found to be identifiable and to have a value greater than many have believed, providing a basis for deciding whether to favor demand estimates based on consumption-dependent or price-dependent regressions. The most frequently used deflators are inconsistent with the Slutsky-Schultz relation. Estimates are developed of three types of bias due to improper deflation (weight of food in deflator, correlation of deflator with residual, and formula nonconformities).</p> </abstract>
<abstract> <p>The classical certainty equivalence theorem states that the optimal decision in a risky situation is the same as in some associated riskless situation. It holds true under rather specific conditions: quadratic payoff, linear relations between instruments and results.... When these conditions are not met but the various functions involved are differentiable, an approximate property can still be stated. The property is here proved and discussed for the following well known dynamic problem: when information accumulates over time how should one choose the sequence of values to be given to some instruments?</p> </abstract>
<abstract> <p>The paper extends the results of the Solow-Samuelson article [12], reinterpreting their balanced growth model in terms of prices rather than output and weakening their monotonicity assumption to consider cases in which the production system is decomposable, completely decomposable, or approximates one of these. It is shown, among other things, that results similar to those obtained by Simon and Ando [11] and Ando and Fisher [1] for linear systems hold for the asymptotic behavior of this sort of nonlinear system of difference equations.</p> </abstract>
<abstract> <p>Cet article presente les resultats de trois methodes d'ajustement pour les equations d'un systeme de relations inter-industrielles, dit systeme Leontief. Ces ajustements se font en fonction des erreurs de prevision pour les annees anterieures. Le systeme en question est celui du Canada publie en 1960 par le Bureau Federal de la Statistique. Il contient 16 secteurs et est etabli sur la base de l'annee 1949. Les resultats chiffres suggerent que l'ajustement des relations inter-in-dustrielles peut prolonger la periode d'utilisation d'un systeme Leontief, comme instrument de prevision, jusqu'a 8 ou 9 ans.</p> </abstract>
<abstract> <p>The properties of several simple forecasting decision rules which incorporate a priori knowledge are studied in this paper. Moments of these decision rule forecasts are derived, tabled, and discussed. In particular, comparisons are made with moments and mean-squared error of forecasts which incorporate no a priori information. It is found that use of prior information can lead to forecasts with mean-squared errors smaller than those associated with usual forecasting techniques provided that the prior information incorporated in the decision rule forecasts is at least reasonably accurate. Also the relative properties of alternative decision rule forecasts are assessed.</p> </abstract>
<abstract> <p>In this study dynamic programming is applied to the problem of efficient accumulation of capital for the firm. Considering a firm which has both a production decision and a capital decision each period, a variety of conditions are stated which yield the optimality of (S, s) or constant stock capital policies. Other conditions are given which require more complex capital policies, and a numerical example of a complex policy is presented as well.</p> </abstract>
<abstract> <p>This report utilizes decision theory to investigate the possible benefits of better weather information to California raisin growers. A supply curve is fitted to the raisin industry and used to evaluate the importance of various factors: weather is of overwhelming importance. Subsequent analysis focuses on the most vulnerable aspect of raisin production: its dependence on early forecasts of rain in September and October when the grapes are being dried and in danger of being damaged by rain. Section 1, in the tradition of micro-economic, partial equilibrium analysis, is concerned with the value of weather information to a single grower. Only the grower focused upon is assumed to receive the better information. This assumption implies there is high value for forecasting rain three weeks in advance. A figure of 90.95 per acre is the value of perfect three week forecasts. Were this value to be realized for each bearing acre in 1960, the total value of better weather information to the raisin industry would be 20,300,000. But merely summing across firms to get industry totals may lead to a fallacy of composition: the analysis must be extended to a wider partial equilibrium framework. Section 2 examines the value of better weather information to the raisin industry as a whole. Since costs are not greatly affected by improved weather information, profit differences are closely approximated by changes in revenue. The elasticity of demand for raisins is calculated from a demand curve fitted to industry data. The inelasticity of demand causes profit to fall under the impact of better information, at least in the short run. The scope of the analysis is then extended to other industries. Using this wider viewpoint, the Conclusion examines the possibility of a simple tax that would reallocate land and labor. The presence of released resources seems to imply a clear gain for better information. However, the basis of such a gain is positive value for these resources in other uses. But the short run inelasticity of other possible products makes it clear that, at least in the short run, better weather information results in net loss. However, some relief is offered through the possibility of regulation. An Appendix examines the question of the value of inaccurate forecasts.</p> </abstract>
<abstract> <p>Theorems are proved on the convergence of efficient paths in certain closed linear models of production to the Neumann ray (of maximal proportional expansion). Radner's theorem is generalized, in the first theorem, to convergence to a facet of the production cone which contains the Neumann ray. Then the weak convergence is extended in the second theorem from convergence to the facet to convergence to the ray itself.</p> </abstract>
<abstract> <p>In this paper we propose a stochastic programming model which considers the distribution of an objective function and probabilistic constraints. Applying it to a transportation type problem, we derive a nonlinear programming problem constrained by linear inequalities and show that it can be solved by iteration of linear programming.</p> </abstract>
<abstract> <p>Though a vocabulary relating to import-export data is used in this paper, the methods presented are applicable to other kinds of flow data as well. For data on the value (in dollars) of the imports and exports between specific countries, methods are developed that take into account the "effects" of the total size of trade of these countries on the trade between countries, adjusting the total size of trade to compensate for the fact that only data on trade between countries are used in the analysis. These methods are related to those given by Savage and Deutsch [3]. We note here that the methods in [3] require modification, we present the necessary modifications, and suggest alternative methods that are preferable in some respects to those in [3]. In addition, we present a generalization of the model and methods in [3] appropriate when trade between certain countries may be restricted.</p> </abstract>
<abstract> <p>In a recent issue of Econometrica,^2 Leif Johansen suggested a synthesis of models in which fixed production coefficients are assumed and models in which substitution of factors is allowed. It is claimed in what follows that the solution of the problem by Johansen is not always efficient; more specifically, if labor productivity rises through time, Johansen's solution represents an inefficient allocation of the labor force. Furthermore, in the two examples which he presented in Sections (3) and (4) (pages 165 and 168) labor productivity does rise. An alternative formulation free of this difficulty is presented.</p> </abstract>
<abstract> <p>From the simple model used here, it is obvious that the effect of price instability upon the firm's expected profit depends on the degree to which prices can be predicted and the cost at which production adjustment can take place. This point can be made apparent by comparing the results of this simple model with the results of Oi's model. The conclusions from this model suggest that a previous price instability conclusion of Oi's should be qualified.</p> </abstract>
<abstract> <p>The lecture investigates some consequences of a frequently observed phenomenon: There are once and for all costs of switching from one good to one of its substitutes. The decision to substitute then is an investment decision. Such substitution costs, in conjunction with problems of oppportunism, have frequently been seen as a reason for vertical integration. Reputation for a fair treatment of customers may enable suppliers to maintain market relations for goods involving substitution costs. A model looks at "competitive distance" between two goods with substitution costs. If future tastes are uncertain the model shows that with low rates of discount or high rates of market growth competitive distance declines as substitution costs rise. It is also shown that competitive distance rises with a rising rate of discount. Given the effectiveness of the reputation mechanism, numerical analysis shows that competitive distance is smaller in most cases with substitution costs than without substitution costs.</p> </abstract>
<abstract> <p>This paper considers price competition among firms when there are capacity constraints and buyers have limited ability to visit firms. A natural method of allocating buyers among firms arises in the equilibrium of the buyers' search game. Sufficient conditions are given under which the buyers' equilibrium varies continuously with the prices charged by firms. Capacity constraints are used to guarantee that this ensures existence of (mixed strategy) equilibria for the pricing game played by sellers. We show that natural pure strategy equilibria arise when the game is made large in appropriate ways.</p> </abstract>
<abstract> <p>A game form is acceptable if for every preference profile, a Nash equilibrium exists and the outcomes corresponding to Nash equilibria are Pareto efficient. A game form is strongly consistent if the set of strong Nash equilibria is always nonempty. The paper shows that no game form can be both acceptable and strongly consistent. The set of game forms which are both acceptable and dominance-solvable is also characterized in terms of the effectivity functions of game forms.</p> </abstract>
<abstract> <p>This paper proposes a procedure for implementing efficient egalitarian equivalent allocations in an exchange economy, using the perfect equilibrium concept. This procedure is an extension of the "divide and choose" method in two ways: it is defined for more than two agents and the divider's advantage is removed by auctioning the role of divider among the agents (as in Crawford [1]).Thus, in contrast with other equilibrium concepts (Nash, dominant), the perfect one solves the efficiency-justice dilemma.</p> </abstract>
<abstract> <p>This paper addresses the question of nonmyopic strategic behavior in an MDP planning procedure which is terminated when the rate of adjustment in the quantity of the public good is below some prespecified threshold. The problem is formulated as a dynamic game in which utility functions are additively separable. It is shown that the game possesses perfect Nash equilibria whose outcomes are Pareto optima. Moreover, any individually rational Pareto optimum can be attained through one of these Nash equilibria. Strategies in these equilibria involve a rate of revision in the quantity of the public good that is equal to the threshold level and insures montonic convergence of the procedure in finite time.</p> </abstract>
<abstract> <p>The implications for efficient allocation of parents' inability to force transfers among siblings are explored. When there are differences in abilities of children within families, such transfers may be necessary to achieve a first-best solution. In the absence of such transfers, a tax on earned income and a subsidy to inheritance are useful second-best tools, whereas subsidies to investments in human capital or physical capital are not desirable.</p> </abstract>
<abstract> <p>The essence of selection bias is that we do not observe nonoptimal choices. This applies whether the choice variable is discrete or continuous. This paper extends the selection bias methodology to the case where the choice variable is continuous and the choice set is ordered. The leading practical application of this analysis is the schooling choice problem. Schooling is treated as a continuous choice variable and selectivity corrected rates of return are estimated. The findings suggest selectivity is of considerable importance and support the comparative advantage hypothesis of Willis and Rosen [18].</p> </abstract>
<abstract> <p>Discrete choice models are now used in a variety of situations in applied econometrics. By far the model specification which is used most often is the multinomial logit model. Yet it is widely known that a potentially important drawback of the multinomial logit model is the independence from irrelevant alternatives property. While most analysts recognize the implications of the independence of irrelevant alternatives property, it has remained basically a maintained assumption in applications. In the paper we provide two sets of computationally convenient specification tests for the multinomial logit model. The first test is an application of the Hausman [10] specification test procedure. The basic idea for the test here is to test the reverse implication of the independence from irrelevant alternatives property. The test statistic is easy to compute since it only requires computation of a quadratic form which involves the difference of the parameter estimates and the differences of the estimated covariance matrices. The second set of specification tests that we propose is based on more classical test procedures. We consider a generalization of the multinomial logit model which is called the nested logit model. Since the multinomial logit model is a special case of the more general model when a given parameter equals one, classical test procedures such as the Wald, likelihood ratio, and Lagrange multiplier tests can be used. The two sets of specification test procedures care then compared for an example where exact and approximate comparisons are possible.</p> </abstract>
<abstract> <p> This paper investigates the exact sampling distribution of the least squares estimator of β in the model y"t = @m + @by"t"-"1 + @u"t where the @u"t are independently N(0, @s^2). The distribution is calculated for the case where y"0 is a known constant and where y"0 is a random variable. Given y"0 is a constant we prove a small @s asymptotic result and compute the exact powers of nonsimilar tests of the random-walk hypothesis β = 1 and of the stability hypothesis β = 0.9. The exact powers of a test of the stability hypothesis are calculated for the case where y"0 is random. The accuracy of the standard normal approximation is examined for both start-up regimes. </p> </abstract>
<abstract> <p> This paper considers approximations to the distribution of the least squares estimator of α in the model @y"t = @a@y"t"-"1 + @u"t where the @u"t are independently distributed N(0, @s^2) and @y"0 is fixed. An Edgeworth approximation for this case is calculated, and compared with results for the stationary case. For|@a| &gt; 1 and fixed @y"0, the asymptotic distribution is found in closed form; when|@a| &gt; 1 and @y"0 = 0, an Edgeworth-type approximation is again calculated; this is compared with exact results. </p> </abstract>
<abstract> <p>The main objective of this paper is to develop a criterion for model selection when there is minimal prior information. In particular, we will derive an expression for posterior odds to compare models for the minimal prior information case. In this framework, models are compared according to their relative posterior probabilities, termed the posterior odds (conditioned on prior and data-sample information). We will show that the criterion obtained here reflects a tradeoff between parsimony (i.e. parameter space dimensionality) and data fit, has desirable invariance properties, and applies to nested and nonnested model comparisons. Furthermore, in nested model comparisons, this criterion can be interpreted in terms of an adjusted likelihood ratio test in which for large data samples the significance level is a declining function of the sample size. Finally, as a practical matter the criterion obtained here is computationally no more difficult to compute than the classical F ratio, and can be calculated easily from the output of standard regression computer programs.</p> </abstract>
<abstract> <p>This paper develops an example in which persistent deterministic business cycles appear in a purely endogenous fashion under laissez-faire. These cycles are not attributable to exogenous "shocks" nor to any variation of policy since there are none in the model. Markets clear in the Walrasian sense at every date, and traders have perfect foresight along the cycles. The origin of these cycles is the potential conflict between the wealth effect and the intertemporal substitution effect that are associated with real interest rate movements. Business cycles appear in particular when the degree of concavity of a trader's utility function is sufficiently higher for old agents than for younger ones. The techniques employed to study the occurrence and the stability of such business cycles are borrowed partly from recent mathematical theories that have been constructed by using the notion of the "bifurcation" of a dynamical system in order to explain the emergence of cycles and the transition to turbulent ("chaotic") behavior in physical, biological, or ecological systems. The equilibrium level of output is shown to be negatively related to the equilibrium level of the real interest rate. A similar relation exists (but in the opposite direction) between equilibrium real money balances and real interest rates. These relations hold both in the long run, i.e. along business cycles, and in the short run, i.e. on the transition path, and whether movements of the real interest rate are anticipated or not. The basic ingredient there is the condition that older agents have a higher marginal propensity to consume leisure. Monetary policy by means of nominal interests payments is shown to be extremely effective. A permanent change of the rate of growth of the money supply by these means is superneutral. Yet, there exists a very simple deterministic countercyclical policy that enables monetary authorities to stabilize completely business cycles and to force the economy back to the unique (Golden rule) stationary state. Due to the nonlinearity of the model such a policy affects not only the variances of real equilibrium magnitudes but also their means.</p> </abstract>
<abstract> <p>This paper examines the detection of misspecification in the context of maximum likelihood models. The power properties of specification tests based on moment conditions are explicitly considered. Tests of conditional moment restrictions are also discussed and are shown to be particularly useful when exogenous variables are present. The form of optimal conditional moment tests is presented. The general results are then applied to specification tests for probit.</p> </abstract>
<abstract> <p>The first part of this paper considers the interaction between productive and nonproductive savings in a growing economy. It employs an overlapping generations model with capital accumulation and various types of rents, and gives necessary and sufficient conditions for the existence of an aggregate bubble. The second part is a series of thoughts on the definition, nature, and consequences of asset bubbles. First, it derives some implications of bubbles for tests of asset pricing. Second, it demonstrates the specificity of money as an asset and shows that there is a fundamental dichtotomy in its formalization. Third, it discusses inefficiencies of price bubbles. Fourth, it shows that the financial definition of a bubble is not satisfactory for some assets.</p> </abstract>
<abstract> <p>For a specified class of economic environments, a double auction in which numerous buyers and sellers submit sealed bids and offers is incentive efficient, in the sense that there is no other trading rule that is sure to be preferred by each agent, whatever his preferences.</p> </abstract>
<abstract> <p>It is shown that if an iterative price mechanism depends only upon a finite amount of information from the market as given by the aggregate excess demand function, then this mechanism cannot always be effective. That is, there are pure exchange economies where this mechanism will not find a price equilibrium. This statement already holds in the case of two commodities. The approach used to reach this conclusion extends to other iterative systems used to determine the zeros of a function.</p> </abstract>
<abstract> <p>This paper considers a market where pairs of agents who are interested in carrying out a transaction are brought together by a stochastic process and, upon meeting, initiate a bargaining process over the terms of the transaction. The basic bargaining problem is treated with the strategic approach. The paper derives the steady state equilibrium agreements; analyzes their dependence on market conditions such as the relative numbers of agents of different types; and discusses their relations with the competitive equilibrium outcome and other results in the search equilibrium literature.</p> </abstract>
<abstract> <p>The paper studies a strategic sequential bargaining game with incomplete information: Two players have to reach an agreement on the partition of a pie. Each player, in turn, has to make a proposal on how the pie should be divided. After one player has made an offer, the other must decide either to accept it or to reject it and continue the bargaining. Player 2 is one of two types, and player 1 does not know that type player 2 actually is. A class of sequential equilibria (called bargaining sequential equilibria) is characterized for this game. The main theorem proves the (typical) uniqueness of the bargaining sequential equilibrium. It specifies a clear connection between the equilibrium and player 1's initial belief about his opponent's type.</p> </abstract>
<abstract> <p>In a repeated principal-agent game (supergame) in which each player's criterion is his long-run average expected utility, efficient behavior can be sustained by a Nash equilibrium if it is pareto-superior to a one-period Nash equilibrium. Furthermore, if the players discount future expected utilities, then for every positive epsilon, and every pair of discount factors sufficiently close to unity (given epsilon), there exists a supergame equilibrium that is within epsilon (in normalized discounted expected utility) of the target efficient behavior. These supergame equilibria are explicitly constructed with simple "review strategies."</p> </abstract>
<abstract> <p>Expected value maximizing sequential search rules can be expressed in terms of reservation values. In search with learning the reservation value at any stage of the search is unknown until that stage is reached. Thus calculating ex ante (and subsequent) probabilities of search duration and the offer accepted is difficult if these probabilities are expressed in terms of reservation values. This paper shows, for a wide class of learning procedures, how re-expressing these probabilities in terms of fixed points allows their direct calculation and, thereby, calculation of the expected value of adaptive search. Examples and comparative statics results are presented.</p> </abstract>
<abstract> <p>This paper adopts a new approach to the problem of generalizing the properties of the two-by-two Heckscher-Ohlin model, asking whether we can obtain results which hold in very general models for dichotomous categories of commodities and factors. Using duality theory, some results of this kind are derived, which generalize certain properties of the Heckscher-Ohlin model to models which allow for any number of goods and factors, joint production, international factor mobility, and substitution between primary factors and intermediate inputs. The consequences of assuming sector-specific factors and equal numbers of goods and factors' are also examined.</p> </abstract>
<abstract> <p>Official statistics in the United States and the United Kingdom show a rise in poverty between the 1970's and the 1980's but scepticism has been expressed with regard to these findings. In particular, the methods employed in the measurement of poverty have been the subject of criticism. This paper re-examines three basic issues in measuring poverty: the choice of the poverty line, the index of poverty, and the relation between poverty and inequality. One general theme running through the paper is that there is a diversity of judgments which enter the measurement of poverty and that it is necessary to recognize these explicitly in the procedures adopted. There is likely to be disagreement about the choice of poverty line, affecting both its level and its structure. In this situation, we may only be able to make comparisons and not to measure differences, and the comparisons may lead only to a partial rather than a complete ordering. The first section of the paper discusses the stochastic dominance conditions which allow such comparisons, illustrating their application by reference to data for the United States. The choice of poverty measure has been the subject of an extensive literature and a variety of measures have been proposed. In the second section of the paper a different approach is suggested, considering a class of measures satisfying certain general properties and seeking conditions under which all members of the class (which includes many of those proposed) give the same ranking. Those sceptical about measures of poverty often assert that poverty and inequality are being confounded. The third section of the paper distinguishes four different viewpoints and relates them to theories of justice and views of social welfare.</p> </abstract>
<abstract> <p>This study undertakes a systematic analysis of several theoretic and statistical assumptions used in many empirical models of female labor supply. Using a single data set (PSID 1975 labor supply data) we are able to replicate most of the range of estimated income and substitution effects found in previous studies in this field. We undertake extensive specification tests and find that most of this range should be rejected due to statistical and model misspecifications. The two most important assumptions appear to be (i) the Tobit assumption used to control for self-selection into the labor force and (ii) exogeneity assumptions on the wife's wage rate and her labor market experience. The Tobit models exaggerate both the income and wage effects. The exogeneity assumptions induce an upwards bias in the estimated wage effect; the bias due to the exogeneity assumption on the wife's labor market experience, however, substantially diminishes when one controls for self-selection into the labor force through the use of unrestricted generalized Tobit procedures. An examination of the maintained assumptions in previous studies further supports these results. These inferences suggest that the small responses to variations in wage rates and nonwife income found here provide a more accurate description of the behavioral responses of working married women than those found in most previous studies.</p> </abstract>
<abstract> <p>This paper presents a finite horizon job search model that is econometrically implemented using all of the restrictions implied by the theory. Following a sample of male high school graduates from the youth cohort of the National Longitudinal Surveys from graduation to employment, search parameters such as the cost of search, the probability of receiving an offer, the discount factor, and those from the wage offer distribution are estimated. Reservation wages and offer probabilities are estimated to be quite low. Simulations are performed of the impact of the parameters on the expected duration of unemployment. For example, it is estimated that an offer probability of unity, as opposed to the estimate of approximately only one per cent per week, would reduce the expected duration of unemployment from 46 weeks to 20 weeks.</p> </abstract>
<abstract> <p>This paper considers estimation and hypothesis tests for coefficients of linear regression models, where the coefficient estimates are based on location measures defined by an asymmetric least squares criterion function. These asymmetric least squares estimators have properties which are analogous to regression quantile estimators, but are much simpler to calculate, as are the corresponding test statistics. The coefficient estimators can be used to construct test statistics for homoskedasticity and conditional symmetry of the error distribution, and we find these tests compare quite favorably with other commonly-used tests of these null hypotheses in terms of local relative efficiency. Consequently, asymmetric least squares estimation provides a convenient and relatively efficient method of summarizing the conditional distribution of a dependent variable given the regressors, and a means of testing whether a linear model is an adequate characterization of the "typical value" for this conditional distribution.</p> </abstract>
<abstract> <p>In this paper we consider estimation of simultaneous equations models with covariance restrictions. We first consider FIML estimation and extend Hausman's (1975) instrumental variables interpretation of the FIML estimator to the covariance restrictions case. We show that, in addition to the predetermined variables from the reduced form, FIML also uses estimated residuals as instruments for the equations with which they are uncorrelated. A slight variation on the instrumental variables theme yields a simple, efficient alternative to FIML. Here we augment the original equation system by additional equations that are implied by the covariance restrictions. We show that when these additional equations are linearized around an initial consistent estimator and three-stage least squares is performed on the original equation system together with the linearized equations implied by the covariance restrictions, an asymptotically efficient estimator is obtained. We also present a relatively simple method of obtaining an initial consistent estimator when the covariance restrictions are needed for identification. This estimator also makes use of additional equations that are implied by the covariance restrictions. In the final section of the paper we consider identification from the point of view of the moment restrictions that are implied by instrument-residual orthogonality and the covariance restrictions. We show that the assignment condition of Hausman and Taylor (1983) provides necessary conditions for the identification of the structural parameters.</p> </abstract>
<abstract> <p>In a multiple regression model the residual variance is an unknown function of the explanatory variables, and estimated by nearest neighbor nonparametric regression. The resulting weighted least squares estimator of the regression coefficients is shown to be adaptive, in the sense of having the same asymptotic distribution, to first order, as estimators based on knowledge of the actual variance function or a finite parameterization of it. A similar result was established by Carroll (1982) using kernel estimation and under substantially more restrictive conditions on the data generating process than ours. Extensions to various other models seem to be possible.</p> </abstract>
<abstract> <p>This paper extends the simple errors-in-variable bound to the setting of systems of equations. Both diagonal and nondiagonal measurement error covariance matrices are considered. In the nondiagonal case, the analogue of the simple errors-in-variable interval of estimates is an ellipsoid with diagonal equal to the line segment connecting the direct least squares with a two-stage least squares estimate. For the diagonal case, the set of estimates under some conditions must lie within the convex hull of 2^k points.</p> </abstract>
<abstract> <p>We study Hotelling's two-stage model of spatial competition, in which two firms first simultaneously choose locations in the unit interval, then simultaneously choose prices. Under Hotelling's assumptions (uniform distribution of consumers, travel cost proportional to distance, inelastic demand of one unit by each consumer) the price-setting subgames possess equilibria in pure strategies for only a limited set of location pairs. Because of this problem (pointed out independently by Vickrey (1964) and d'Aspremont et al. (1979)), Hotelling's claim that there is an equilibrium of the two-stage game in which the firms locate close to each other is incorrect. A result of Dasgupta and Maskin (1986) guarantees that each price-setting subgame has an equilibrium in mixed strategies. We first study these mixed strategy equilibria. We are unable to provide a complete characterization of them, although we show that for a subset of location pairs all equilibria are of a certain type. We reduce the problem of finding an equilibrium of this type to that of solving three or fewer highly nonlinear equations. At each of a large number of location pairs we have computed approximate solutions to the system of equations. Next, we use our analytical results and computations to study the equilibrium location choices of the firms. There is a unique (up to symmetry) subgame perfect equilibrium in which the location choices of the firms are pure; in it, the firms locate 0.27 from the ends of the market. At this equilibrium, the support of the subgame equilibrium price strategy is the union of two short intervals. Most of the probability weight is in the upper interval, so that this strategy is reminiscent of occasional "sales" by the firms. We also find a subgame perfect equilibrium in which each firm uses a mixed strategy in locations. In fact, in the class of strategy pairs in which the firms use the same mixed strategy over locations, and this strategy is symmetric about .5, there is a single equilibrium. In this equilibrium most of the probability weight of the common strategy is between 0.2 and 0.4, and between 0.6 and 0.8. There is a wide range of pure Nash (as opposed to subgame perfect) equilibrium location pairs: the subgame strategies in which each firm threatens to charge a price of zero in response to a deviation support all but those location pairs in which the firms are very close.</p> </abstract>
<abstract> <p>Previous analyses have shown that if a point is to be a core of a majority rule voting game in Euclidean space, when preferences are smooth, then the utility gradients at the point must satisfy certain restrictive symmetry conditions. In this paper, these results are generalized to the case of an arbitrary voting rule, and necessary and sufficient conditions, expressed in terms of the utility gradients of "pivotal" coalitions, are obtained.</p> </abstract>
<abstract> <p>A collective choice problem involves a set of agents and a set of feasible utility vectors. A solution is a (collective) choice function that assigns to each element in a family of admissible collective choice problems a unique feasible utility allocation. Our concern here is with solutions that are collectively rational, i.e. for which there exists an ordering of utility space such that the solution outcome to each choice problem is obtained as the maximal element of that ordering on the set of feasible utility allocations. An axiom due to Harsanyi, called bilateral stability, is used to obtain the following integrability result: Any solution satisfying Pareto optimality, continuity and bilateral stability can be represented by an additively separable Bergson-Samuelson social welfare function.</p> </abstract>
<abstract> <p>In an economy with one public and one private good, egalitarian-equivalent cost sharing consists of finding the highest public good level x* such that consuming x* for free yields a feasible utility distribution. The corresponding feasible allocation (typically unique), called egalitarian-equivalent, is in the core of the economy. Conversely, any cost sharing method satisfying: (i) Pareto optimality, (ii) cost monotonicity (nobody suffers a utility loss if the production technology improves upon, ceteris paribus), and (iii) individual rationality (no single agent coalition objects) or (iii') no private transfers (no agent receives a positive amount of private good), must select an egalitarian-equivalent allocation in every economy.</p> </abstract>
<abstract> <p>This paper analyzes durable goods monopoly in an infinite-horizon, discrete-time game. We prove that, as the time interval between successive offers approaches zero, all seller payoffs between zero and static monopoly profits are supported by subgame perfect equilibria. This reverses a well-known conjecture of Coase. Alternatively, one can interpret the model as a sequential bargaining game with one-sided incomplete information in which an uniformed seller makes all the offers. Our folk theorem for seller payoffs equally applies to the set of sequential equilibria of this bargaining game.</p> </abstract>
<abstract> <p>We study a differentiated industry in which two firms compete by offering intervals of qualities to heterogenous consumers, in the spirit of the monopolist of Mussa and Rosen (1978). We establish conditions which, for perfect competition and monopoly, preclude the possibility that a given quality level is bought by more than one type of consumer (there is no "bunching"). Under these assumptions we show the existence of a unique price equilibrium in the duopoly case where firms must offer intervals of qualities. At all price equilibria in which both firms make a positive profit, discrimination of consumers is incomplete. When the firms choose their product lines they are influenced by two opposite effects. Discrimination among heterogeneous buyers requires a broad quality range. On the contrary, price competition lowers profit margins on neighboring qualities sold by different firms, creating the Chamberlinian incentive for a firm to differentiate its products from those of its competitors. We show that the second effect dominates the first for intermediate qualities: at a Nash equilibrium of the quality game where each firm makes a positive profit, there is always a gap between the two product lines.</p> </abstract>
<abstract> <p>The college admissions problem is perhaps the simplest model of many-to-one matching in two-sided markets such as labor markets. We show that the set of stable outcomes (which is equal to the core defined by weak domination) has some surprising properties not found in models of one-to-one matching. These properties may help to explain the success that this kind of model has had in explaining empirical observations.</p> </abstract>
<abstract> <p>An act maps states of nature to outcomes; deterministic outcomes as well as random outcomes are included. Two acts f and g are comonotonic, by definition, if it never happens that f(s)&gt;f(t) and g(t)&gt;g(s) for some states of nature s and t. An axiom of comonotonic independence is introduced here. It weakens the von Neumann-Morgenstern axiom of independence as follows: If f&gt;g and if f, g, and h are comonotonic, then @af+(1-@a)h&gt;@ag+(1-@a)h. If a nondegenerate, cOntinuous, and monotonic (state independent) weak order over acts satisfies comonotonic independence, then it induces a unique non-(necessarily-)additive probability and a von Neumann-Morgenstern utility. Furthermore, one can compute the expected utility of an act with respect to the nonadditive probability, using the Choquet integral. This extension of the expected utility theory covers situations, as the Ellsbergparadox, which are inconsistent with additive expected utility. The concept of uncertainty aversion and interpretation of comonotonic independence in the context of social welfare functions are included.</p> </abstract>
<abstract> <p>Let P be a real-valued function defined on the space of cooperative games with transferable utility, satisfying the following condition: In every game, the marginal contributions of all players (according to P) are efficient (i.e., add up to the worth of the grand coalition). It is proved that there exist just one such function P--called the potential--and moreover that the resulting payoff vector coincides with the Shapley value. The potential approach is also shown to yield other characterizations for the Shapley value, in particular, in terms of a new internal consistency property. Further results deal with weighted Shapley values (which emerge from the above consistency) and with the nontransferable utility case (where the egalitarian solutions and the Harsanyi value are obtained).</p> </abstract>
<abstract> <p>We define a new solution concept for transferable utility cooperative games in characteristic function form, in a framework where individuals believe in equality as a desirable social goal, although private preferences dictate selfish behavior. This latter aspect implies that the solution outcome(s) must satisfy core-like participation constraints while the concern for equality entails choice of Lorenz maximal elements from within the set of payoffs satisfying the participation constraints. Despite the Lorenz domination relation being a partial ranking, we show that the egalitarian solution is unique whenever it exists. Moreover, for convex games, the solution is in the core and Lorenz dominates every other core allocation.</p> </abstract>
<abstract> <p>This paper compares the equilibrium behavior and outcomes in a model of two-party competition for legislative seats, under two different assumptions about the parties' goals: (i) parties maximize the expected number of seats won, and (ii) parties maximize the probability of winning a majority of the seats. The two goals may lead to qualitatively different behavior, and studying the differences yields insights into the relationship between the goals, and the role of assymetries between the parties.</p> </abstract>
<abstract> <p>Rothenberg's (1984) Edgeworth size correction is applied to tests of linear hypotheses in the linear regression model with AR(1) errors. Previous simulation findings on the effect of autocorrelation in the regressors on over-rejection (e.g. Park and Mitchell (1980)) are supported when the correction is examined analytically in a special case. A simulation study shows that the correction adjusts the size in the right direction, but still leaves substantial over-rejection when the sample size is small and the original amount of over-rejection is large.</p> </abstract>
<abstract> <p>This paper empirically tests and rejects classical competitive theories of wage determination by examining differences in wages for equally skilled workers across industries. Human capital earnings functions are estimated using cross-sectional and longitudinal data from the CPS and QES. The major finding is that the dispersion in wages across industries as measured by the standard deviation in industry wage differentials is substantial. Furthermore, F tests of the joint significance of industry dummy variables are decisively rejected. These differences are very difficult to link to unobserved differences in ability or to compensating differentials for working conditions. Fixed effects models are estimated using two longitudinal data sets to control for constant, unmeasured worker characteristics that might bias cross-sectional estimates. Because measurement error is a serious problem in looking at workers who report changing industries, we use estimates of industry classification error rates to adjust the longitudinal results. In the fixed effects analysis, the industry wage differentials are sizable and are very similar to the cross-sectional estimates. In addition, the fixed effects estimates are robust under a variety of assumptions about classification errors and are similar using both data sets. These findings cast doubt on explanations of industry wage differentials based on unmeasured ability. Additional analysis finds that the industry wage structure is highly correlated for workers in small and large firms, in different regions of the U.S., and with varying job tenures. Finally, evidence is presented demonstrating that turnover has a negative relationship with industry wage differentials. These findings suggest that workers in high wage industries receive noncompetitive rents.</p> </abstract>
<abstract> <p>Vehicles on uncongested roads damage the pavement and advance the date at which repairs are necessary. Recent empirical work has identified another potentially more important social cost, for the damage to the pavement raises the operating costs of subsequent vehicles, and these operating costs may be an order of magnitude larger than the road maintenance costs. The paper develops a theory to handle this newly identified externality and establishes a remarkable result. If roads are repaired when they reach a predetermined critical condition (not necessarily optimally set) and if road damage is fully attributable to traffic, then in steady state with zero traffic growth the average road damage externality is zero, and the average marginal social cost of road use is equal to the average road maintenance cost. The result holds for arbitrary road damage and vehicle damage functions. Recent empirical studies reveal that weather accounts for a significant fraction of road deterioration, and in this case the road damage externality is no longer exactly zero, but is quantitatively negligible. The appropriate road user charge now only recovers the fraction of damage attributable to traffic.</p> </abstract>
<abstract> <p>The paper contains a simple equilibrium model of the labor market. The supply function allows for intertemporal substitution in response to real wage fluctuations, and the demand function allows for adjustment costs on employment. The supply and demand functions both solve quadratic partial adjustment problems, driven by rational expectations about future real wages. The market equilibrium also solves a partial adjustment problem, driven by rational expectations about future preference and technology shocks. Two related issues are emphasized: the identification of dynamic labor supply and demand functions, and the theoretical interpretation of serial correlation in employment and real wages. When the supply and demand shocks are assumed to be AR(1) processes which are not causally related, the model delivers a simple structure for the equilibrium process for employment and real wages, which is a restricted VAR(2). This simple structure facilitates the interpretation of summary statistics on the time-series properties of observed employment and real wage series. The supply and demand functions can both be (locally) identified using only employment and real wage data. An illustrative application is presented, using U.S. manufacturing data for the period 1948-1971.</p> </abstract>
<abstract> <p>Recently, several authors have argued for the use for the use of dynamic preference structures for leisure which incorporate forms of intertemporally nonseparable utility in the analysis of intertemporal labor supply decisions. In this paper, we examine whether such nonseparable utility functions are important in characterizing microdata on life-cycle labor supply. Using longitudinal data on males from the Panel Study of Income Dynamics, we estimate a model of life-cycle labor supply and consumption under uncertainty in which the structure of intertemporal leisure preferences is allowed to be nonseparable in leisure. Our model nests as special cases a number of alternative specifications considered in the literature. We investigate the robustness of our findings to certain forms of population heterogeneity and to some types of model misspecification. Across a number of alternative specifications, we find evidence that the standard assumption of intertemporally separable preferences for leisure is not consistent with data for prime-age males.</p> </abstract>
<abstract> <p>We prove several versions of the second theorem of welfare economics for exchange economies with nonconvex preferences.</p> </abstract>
<abstract> <p>This paper presents a systematic framework for studying infinitely repeated games with discounting, focussing on pure strategy (subgame) perfect equilibria. It introduces a number of concepts which organize the theory in a natural way. These include the idea of an optimal penal code, and the related notions of simple penal codes and simple strategy profiles. I view a strategy profile as a rule specifying an initial path (i.e., an infinite stream of one-period action profiles), and punishments (also paths, and hence infinite streams) for any deviations from the initial path, or from a previously prescribed punishment. An arbitrary strategy profile may involve an infinity of punishments and complex history-dependent prescriptions. The main result of this paper is that much of this potential strategic complexity is redundant: every perfect equilibrium path is the outcome of some perfect simple strategy profile. A simple strategy profile is independent of history in the following strong sense: it specifies the same player specific punishment after any deviation by a particular player. Thus simple strategy profiles have a parsimonious description in terms of (n+1) paths where n is the number of players. Unlike the undiscounted case there is no need to "make the punishment fit the crime." In particular, a player who has a "myopic" incentive to deviate from his own punishment may be deterred from doing so simply by restarting the punishment already in effect. The key to the above result is that, with discounting, worst perfect equilibria exist for each player. These define an optimal penal code. The notion of a simple penal code yields an elementary proof of the existence of an optimal penal code and leads directly to the theorem on the "sufficiency" of simple strategy profiles.</p> </abstract>
<abstract> <p>A measure of complexity for repeated games strategies is studied. This measure facilitates the investigation of some issues regarding finite rationality and the structure of subgame perfect equilibria of repeated games with discounting. Specifically, the complexity of a strategy in a given repeated game is defined to be the cardinality of the induced strategy set, i.e., the number of distinct strategies induced by the original strategy in all possible subgames. We observe that this cardinality is equal to the size (cardinality of the state set) of the smallest automaton which can implement the strategy. Thus, in a sense, complexity is measured on the basis of the amount of computing power inherent in the strategy. A measure of strategic memory is also studied. The following results are obtained: (1) combining tow notions of "bounded rationality" (epsilon equilibrium and finite complexity), we find that every subgame perfect equilibrium of the repeated game can be approximated (with regard to payoffs) by a subgame perfect epsilon equilibrium of finite complexity. (2) For a generic class of normal form stage games, at every discount robust subgame perfect (DRSP) equilibrium, there are necessary relationships among the complexities and memories of the players' strategies. In the two player case, strategies must be equally complex and must have equal memories. (3) For a second class of two player stage games, we show that the payoff vectors for all DRSP equilibria are obtainable via equilibria in which the players' strategies are equally complex and have equal memories.</p> </abstract>
<abstract> <p>This paper analyzes aspects of optimal fiscal policy for economies with capital accumulation and finitely-lived, heterogeneous agents. For a particular utilitarian social welfare function, the problem faced by a central planner can be broken down into two subproblems, a standard problem of optimally allocating aggregate consumption over time and a problem of distributing aggregate consumption optimally at each moment among those alive. If it can use a sufficiently rich set of lump-sum taxes and transfers, the government can replicate the command optimum as a market equilibrium outcome. No issue of government debt is needed to achieve this decentralization. The discussion emphasizes time-inconsistency problems arising from two sources. First, some social welfare functions give rise to time-inconsistent command optimums, for reasons first discussed by Strotz (1956). Second, in a market setting, optimal fiscal policy can be time-inconsistent when the government's fiscal tools are too limited to allow it to decentralize a time-consistent command optimum. This second possibility is illustrated by an sample in which the government is constrained to levy the same lump-sum tax or transfer on everyone alive on a given date. Unless fiscal policy has no influence over factor prices, optimal fiscal policy can be time-inconsistent if the economy's horizon is finite.</p> </abstract>
<abstract> <p>It is argued that most econometric models are nonparametric and that parameterizations should typically be viewed as approximations made for purposes of estimation. These parameterizations should not be used to judge identification when they are not a precise representation of prior information. Identification in a nonparametric context is defined and theorems are developed that aid in judging the identifiability of both nonparametric and parametric models. The application of parametric models extends previous research as it applies to a broad range of models including those that are nonlinear in variables and/or parameters. The application to nonparametric models results in a simple necessary condition for identifiability. Furthermore, when this condition is met and when the structure is nonlinear (as defined in the paper), all functions are identifiable. Examples are given that illustrate the usefulness of these results for determining sources of identification.</p> </abstract>
<abstract> <p>A two period model of managerial task assignment is developed, where the current employer has the advantage of observing the actual performance of the manager, not observable by outside employers who can observe only the assignments. The manager is assumed to have all the bargaining power and there are many risk-neutral firms competing for his expertise. It is shown that the optimal contrasts are rigid but the market value of the managers is below actual productivity and that old managers are promoted less than is efficient. We introduce the idea of managers exploiting their information to separate themselves out in the market place. As a consequence, our model has the appealing property of small ability-based wage differentials within a task, as well as large ones between tasks.</p> </abstract>
<abstract> <p>Recent papers which use the framework of temporary equilibrium with rationing to explain unemployment phenomena (sometimes termed "disequilibrium" theory, in which prices do not clear spot markets) are surveyed and evaluated critically. Models which posit fixed prices and then explore the effects of price rigidity are studied. I then consider attempts to explain why prices don't clear markets by considering models in which prices and quantity constraints are simultaneously determined. Many such models are based on perceived demand curves, leading one to consider the viability of unemployment equilibria under various restrictions about correctness or rationality of perceptions. I also consider in detail the role of a medium of exchange and assets in general and show that it is incorrect to link liquidity constraints and quantity rationing.</p> </abstract>
<abstract> <p>Using duality theory I show that the Tobin-Houthakker conjecture that a reduction in the ration of one good will increase the consumption of unrationed substitutes and diminish the demand for unrationed complements may not hold in disequilibrium situations.</p> </abstract>
<abstract> <p> We solve the problem of the restrictions imposed on the Jacobian A at prices p̄ of the aggregate excess demand function x(p) of m agents in an exchange economy with l commodities, under the assumption of individual rationality. Given an arbitrary differentiable function x(p) satisfying homogeneity and Walras' law, we attribute rational individual excess demand functions x&lt;sup&gt;1&lt;/sup&gt; (p), ..., x&lt;sup&gt;m&lt;/sup&gt; (p) to the m agents such that at any arbitrarily specified vector p̄ aggregate excess demand is equal to x(p̄) and the following condition is satisfied: There exists a subspace M of dimension m such that the Jacobian at p̄ of x(p) and the Jacobian at p̄ of the aggregate excess demand function define the same linear function on M. If x(p̄) ≠ 0, M can be taken to have dimension (m+1). As an immediate consequence of our proof for m=1 we show that even if p̄, x(p̄), and Dx(p̄) are known for the excess demand function of a single agent, the substitution effect and the income effect cannot be unambiguously determined without knowledge of the utility function. We extend the results proved at a point to large open neighborhoods. We show that if x(p) is an arbitrary function which bounded from below and satisfies homogeneity and Walras' law, and if x(p̄) ≠ 0, then we can find an open neighborhood G of p̄ and (l-1) individually rational excess demand functions x&lt;sup&gt;1&lt;/sup&gt;(p), ..., x&lt;sup&gt;l-l&lt;/sup&gt; (p), such that Σ&lt;sub&gt;k=1&lt;/sub&gt;&lt;sup&gt;l-1&lt;/sup&gt; x&lt;sup&gt;k&lt;/sup&gt; (p) = x(p) everywhere on G. </p> </abstract>
<abstract> <p>We investigate the structure of optimal policies in general multiperiod multiasset consumption-investment problems in the presence of transfer costs. A number of objectives such as utility of a consumption stream, utility of terminal wealth, and multi-attribute utility are encompassed by the formulation. The general problem is first formulated as a stochastic dynamic program. The one-period subproblems are then analyzed using convex duality theory. The principal result is the characterization of a not necessarily convex "region of no transactions" for each period. If in any period the entering asset position is in this set, no transactions are made. Each point of the set side is the vertex of a cone such that if the entering asset position is outside the set, the optimal policy is to move to the vertex of the cone in which the entering asset position lies. It is shown that the region of no transactions is a connected set and that it is a cone when the utility function is assumed to be positively homogeneous. In the latter case, the optimal decision policy and induced utility functions are also positively homogeneous.</p> </abstract>
<abstract> <p>The subject is retiming of lags in a system of linear difference equations. The issues are (i) the effect of lag retiming on stability and (ii) the effect of lag retiming on speed of adjustment. Regarding (i), a theorem due to Bear [1] is sharpened; and new conditions for stability to survive under a retiming of lags are obtained. Regarding (ii), the main results concern the effects of "lag perturbations" and "spread perturbations" on speed of adjustment in models with nonnegative coefficients.</p> </abstract>
<abstract> <p>As a non-Walrasian system tracks through the phase space, the differential equations which govern its motion will typically change as the system crosses certain borders. This increases the complexity of the stability problem considerably. In the present paper we find that some straightforward modifications to Lyapunov's method render the problem tractable. These methods are derived, and their use is illustrated in the case of two different systems which have trading out of equilibrium.</p> </abstract>
<abstract> <p>The purpose here is to make explicit the sense in which two dynamic processes, due to Malinvaud and others (whose solutions determine an efficient allocation for a given economy), are related to the gradient projection method known in the nonlinear optimization literature. The connections we establish derive from simple observations on first order characterizations of efficient allocations; they also lead to the formulation of another process, that applies to a classical welfare maximization problem; finally they provide a common basis for an a priori justification of each of the three processes involved, which supplements the intrinsic properties that they can be shown to have.</p> </abstract>
<abstract> <p>It was conjectured by Pigou that an increase in real national income, as reckoned in the prices of either the initial or the terminal period, would always correctly indicate an improvement in national welfare provided the increase referred to the aggregate income of a given group of persons with fixed preferences and a fixed proportional distribution of income among them. We show that if the individual preferences are assumed to be homothetic, and if by a welfare improvement one means respectively a potential improvement (in which losers can be compensated by gainers) or an actual improvement (in which all are gainers), then on either of these respective criteria Pigou's conjecture holds true under these conditions if and only if individual preferences are identical.</p> </abstract>
<abstract> <p>This paper presents necessary and sufficient conditions for the expected value of consumer surplus to correctly represent a consumer's preferences. A theorem characterizing utility functions which represent preferences over conditional probabilities is used to derive this. An application to price stabilization policy is presented.</p> </abstract>
<abstract> <p>This paper provides a generalization of Sen's poverty measure. The generalization is motivated by the failure of Sen's poverty measure to satisfy some transfer-sensitivity axioms proposed in this paper. A numerical method of computing the alternative poverty measures is provided along with an illustration based on the data from the Australian household expenditure survey carried out during 1974.</p> </abstract>
<abstract> <p> Two solution concepts for games without sidepayments are considered: the stable bargaining solution proposed by Harsanyi[6, 7], and the λ-transfer value first proposed by Shapley [19]. Some examples of games are considered for which both solution concepts yield results which are highly counter-intuitive, and which seem to be inconsistent with the hypothesis that the games are played by rational players. </p> </abstract>
<abstract> <p>In his paper,^1 Roth has used a three-person game example to illustrate certain difficulties connected with generalizations of the Shapley value for games without side payment. This note argues that cooperative solution concepts in general often give rise to similar difficulties, and that the best way of avoiding them is to analyze cooperative games by means of noncooperative bargaining models.</p> </abstract>
<abstract> <p>The effects of entry into an oligopolistic industry are studied, generalizing the usual Cournot model to allow for the possibility of collusion by firms and deriving stronger results than had previously been obtained. Necessary and sufficient conditions for output per firm y to rise or fall as entry occur are given and discussed, and the "perverse" effect (entry increasing y) is shown to be consistent with stable equilibria and not empirically implausible. In contrast, it is shown that industry output unambiguously expands and profits per firm fall as entry into stable equilibria takes place. Total industry profits are also considered and some results obtained.</p> </abstract>
<abstract> <p>The paper discusses the two-stage estimation method for switching simultaneous equations models where the criterion function determining the switching is of the probit type and the tobit type. It derives the asymptotic covariance matrices of these estimators and shows that when the criterion function is of the probit type the correct covariance matrix is underestimated when the heteroscedasticity introduced in the first step is ignored, whereas the same is not necessarily the case for one of the regimes when the criterion function is of the tobit type.</p> </abstract>
<abstract> <p>In the first part of the paper, a model is proposed which places the Marxian and Sraffian conceptions of a capitalist economy in a general equilibrium framework. A central concern of these writers is that the economy be reproducible; this is incorporated formally into the equilibrium definition. Capitalists maximize profits subject to a capital constraint and workers are paid a subsistence wage. Equilibrium existence theorems are proved. In the second part, the welfare properties of the equilibria are examined--which, in the Marxian tradition, involve the notion of exploitation. It is shown that the possibility of exploitation is necessary and sufficient for all equilibria to sustain positive profits, if a certain technological condition holds. Finally, the notion of a subsistence bundle is dispensed with, and a Marxian determination of workers' consumption is proposed. In addition to placing the formal Marxian model into a general equilibrium context, the specification of production here is more general than the usual Leontief or von Neumann technologies: production sets are assumed to be only convex.</p> </abstract>
<abstract> <p>This paper and its sequel present a new approach to the study of production sets with indivisibilities and to the programming problems which arise when a factor endowment is specified. The absence of convexity precludes the use of prices to support efficient production plans and to guide the search for optimal solutions. Instead, we describe the unique minimal system of neighborhoods for which a local maximum is global, and discuss a related algorithm. The definition of this neighborhood system is based on techniques used in the computation of fixed points of a continuous mapping. In Part II of the paper this neighborhood system is investigated in the special case of two activities and it is shown that the algorithm may be accelerated so as to terminate in polynominal time.</p> </abstract>
<abstract> <p>The purpose of this paper is to provide a method for characterizing efficient allocation processes and efficient allocations for a large class of environments in which asymmetric information is an important factor. This method is based on a rigorous application of statistical decision theory and makes explicit both the information available to agents ex ante and the way in which information is transmitted during any multistage allocation process.</p> </abstract>
<abstract> <p>A simple scheme for making governmental decisions about the production and financing of public goods is presented. The "competitive" equilibria under the scheme are Pareto optimal; more importantly, they are Lindahl equilibria. Thus, it is never in any individual's interest to refuse to participate (no one will be worse off at the equilibrium than at his initial holding); moreover, the existence of equilibria is assured in the usual classical public-goods economies.</p> </abstract>
<abstract> <p>Agents are assumed to have smooth preferences with natural boundary conditions. For large regular economies, satisfying an indeconposability conditions, it is shown that core allocations and competitive allocations converge to each other with a rate inversely proportional to the number of agents m. To the extent that the indecomposability condition is harmless, 1/m can be regarded as the normal rate of convergence. However, if indifference surfaces are allowed to have kinks, 1/m cannot be regarded as normal. This is treated in Part II [6].</p> </abstract>
<abstract> <p>The purpose of this paper is to study the effect of uncertainty in the arrival date of a new technology on the rate of depletion of an exhaustible natural resource. It is shown that under a large class of circumstances uncertainty leads to a faster initial depletion rate if the initial resource stock is small and to greater conservation if it is large. A particular kind of certainty equivalence result is proved and the results of the paper are used to comment on possible interpretations of certain historical episodes of resource exhaustion.</p> </abstract>
<abstract> <p>Economists have been paying increasing attention to the study of situations in which consumers face a discrete rather than a continuous set of choices. Such models are potentially very important in evaluating the impact of government programs upon consumer welfare. But little has been said in general regarding the tools of applied welfare economics in discrete choice situations. This paper shows how the conventional methods of applied welfare economics can be modified to handle such cases. It focuses on the computation of the excess burden of taxation, and the evaluation of quality change. The results are applied to stochastic utility models, including the popular cases of probit and logit analysis. Throughout, the emphasis is on providing rigorous guidelines for carrying out applied work.</p> </abstract>
<abstract> <p>This paper emphasizes the importance of endogenous (price) uncertainty as distinct from the standard exogenous uncertainty about the state of the world. Markets where agents can enter into forward contracts contingent upon future spot prices are studied with respect to existence of equilibrium, occurrence of speculation, and efficiency.</p> </abstract>
<abstract> <p>This paper deals with rules suitable for evaluating risky consequences of economic projects when various forms of stochastic dependence have to be taken into account. The Validity of various certainty-equivalence results, including the Arrow-Lind theorem, are reexamined in this framework.</p> </abstract>
<abstract> <p>Linear time series models have come to dominate the macroeconomic literature on rational expectations and equilibrium business cycle theory. But the explicit solution of such models has generally required strong restrictions upon the exogenous process of stochastic shocks (e.g., temporal independence) as well as upon the values of various demand and supply elasticities. This paper exhibits a solution technique, the method of z-transforms, which does not require one to impose such restrictions. The value of this method is illustrated by applying it to completely characterize the symmetric, stationary, rational expectations equilibria of a naive linear model of land speculation. This approach also permits systematic study of the informationally asymmetric equilibria of the model.</p> </abstract>
<abstract> <p> If independent observations x are drawn from the distribution located at @m, f (x; @m)=c"3 exp[-g(x -@m)], and if g is symmetric and strictly convex, then the maximum likelihood estimate of μ lies between the smallest and largest folded sample observations. If the distribution has fatter tails than a normal distribution, then the maximum likelihood estimate lies between the smallest and largest means of trimmed subsamples. If the distribution is assumed to be symmetric and unimodal, the centers of tight clusters of observations can be maximum likelihood estimates. If observations are not independent, then there is no bound: given any example any number is a maximum likelihood estimate for some sampling distribution. Stationary is not sufficient to bound the estimate between the minimum and maximum observations. </p> </abstract>
<abstract> <p>Correlated equilibrium is formulated in a manner that does away with the dichotomy usually perceived between the "Bayesian" and the "game-theoretic" view of the world. From the Bayesian viewpoint, probabilities should be assignable to everything, including the prospect of a player choosing a certain strategy in a certain game. The so-called "game-theoretic" viewpoint holds that probabilities can only be assigned to events not governed by rational decision makers; for the latter, one must substitute an equilibrium (or other game-theoretic) notion. The current formulation synthesizes the two viewpoints: Correlated equilibrium is viewed as the result of Bayesian rationality; the equilibrium condition appears as a simple maximization of utility on the part of each player, given his information. A feature of this approach is that it does not require explicit randomization on the part of the players. Each player always chooses a definite pure strategy,with no attempt to randomize; the probabilistic nature of the strategies reflects the uncertainty of other players about his choice. Examples are given.</p> </abstract>
<abstract> <p>This paper specifies and estimates a four-equation disequilibrium model of the consumption goods market in a centrally planned economy (CPE). The data are from Poland for the period 1955-1980, but the analysis is more general and will be applied to other CPEs as soon as the appropriate data sets are complete. This work is based on previous papers of Portes and Winter (P-W) and Charemza and Quandt (C-Q). P-W applied to each of four CPEs a discrete-switching disequilibrium model with a household demand equation for consumption goods, a planners' supply equation, and a "min" condition stating that the observed quantity transacted is the lesser of the quantities demanded and supplied. C-Q considered how an equation for the adjustment of planned quantities could be integrated into a CPE model with fixed prices and without the usual price adjustment equation. They made plan formation endogenous and permitted the resulting plan variables to enter the equations determining demand and supply. This paper implements the C-Q proposal in the P-W context. It uses a unique new data set of time series for plans for the major macroeconomics variables in Poland and other CPEs. The overall framework is applicable to any large organization which plans economic variables.</p> </abstract>
<abstract> <p>Empirically estimated flexible functional forms frequently fail to satisfy the appropriate theoretical curvature conditions. Lau and Gallant and Golub have worked out methods for imposing the appropriate curvature conditions locally, but those local techniques frequently fail to yield satisfactory results. We develop two new methods for imposing curvature conditions globally in the context of cost function estimation. The first method adopts Lau's technique to a generalization of a functional form first proposed by McFadden. Using this generalized McFadden functional form, it turns out that imposing the appropriate curvature conditions at one data point imposes the conditions globally. The second method adopts a technique used by Diewert, McFadden, and Barnett, who is based on the fact that a nonnegative sum of concave functions will be concave. Our various suggested techniques are illustrated using the U.S. manufacturing data utilized by Berndt and Khaled.</p> </abstract>
<abstract> <p>When a decision rule is implemented using a Bayesian incentive compatible mechanism in which the messages are publicly observable, the players' information is augmented by their observation of each others' strategies. In this paper we study the set of Bayesian implementable decision rules which have the further property that the information conveyed in the process of their implementation does not invalidate the optimality of the players' strategies. We call such rules posterior implementable. We concentrate on a two-person problem with two possible decisions, and, for this problem, we obtain a complete characterization of the set of posterior implementable decision rules. Our main result entails that a posterior implementable social decision rule can take essentially only two values throughout the range of observations of the two players. The domains over which each of these two outcomes is realized can be characterized by the fact that the boundary between them is a step function satisfying a certain set of equations. The motivation for studying posterior implementable rules is that they represent the outcomes of a two-stage cooperative process. In the first stage, communication takes place but no binding commitments can be made. The second stage consists of ratifying ("signing") the agreement obtained at the first stage. At this state, no further information is conveyed. Both parties must be satisfied with the nonbinding commitments obtained at the first stage, so that these actions are actually carried out. Possible applications of this theory are given. A constructive method for finding the posterior implementable rules is presented and the set of such rules is contrasted, in an example, with the full set of Bayesian implementable rules.</p> </abstract>
<abstract> <p>This paper investigates the consequences of the following modification of expected utility theory: Instead of requiring independence with respect to probability mixtures of risky prospects, require independence with respect to direct mixing of payments of risky prospects. A new theory of choice under risk--a so-called dual theory--is obtained. Within this new theory, the following questions are considered: (i) numerical representation of preferences; (ii) properties of the utility function; (iii) the possibility for resolving the "paradoxes" of expected utility theory; (iv) the characterization of risk aversion; (v) comparative statics. The paper ends with a discussion of other non-expected-utility theories proposed recently.</p> </abstract>
<abstract> <p>This paper provides sufficient conditions for the equilibrium price system and a vector of exogenously specified state variable processes to form a diffusion process is a pure exchange economy. The conditions involve smoothness of agents' utility functions and certain nice properties of the aggregate endowments process and the dividend processes of traded assets. In place of the dynamic programming, a martingale representation technique is utilized to characterize equilibrium portfolio policies. This technique is useful even when there does not exist a finite dimensional Markov structure in the economy and thus the Markovian stochastic dynamic programming is not applicable. Agents are shown to hold certain hedging mutual funds and the riskless asset. In contrast to earlier results, the market portfolio does not have a role in hedging. When there exists a finite dimensional Markov system in the economy, the dimension of the hedging demand identified through the Markovian dynamic programming may be much larger than that identified by the martingale method.</p> </abstract>
<abstract> <p>We introduce and investigate a significant behavioral condition on utility functions for wealth, which we call proper risk aversion, namely that an undesirable lottery can never be made desirable by the presence of an independent, undesirable lottery. (Independent random background wealth is allowed.) One consequence is that offering insurance and other forms of hedging to proper investors can only encourage them to accept other, independent risks. Properness implies decreasing risk aversion and, much less immediately, several stronger intuitive conditions on certainty equivalents and risk premiums. We prove properness for all mixtures of risk-averse exponential functions and hence (by complete monotonicity) of all risk-averse power and logarithmic functions. We derive analystical necessary and sufficient conditions, which seem to be unavoidably complicated, and local necessary conditions, alas not sufficient, which are tractable.</p> </abstract>
<abstract> <p>When the error covariance matrix in a linear model depends on a few unknown parameters, the regression coefficients can be estimated by a two-step procedure. Consistent estimates of the covariance parameters are first obtained and then used in a generalized least squares regression. Under the assumption that the errors are normal and the covariance parameter estimates are well behaved, an asymptotic expansion is developed for the distribution function of the two-step GLS estimate. the error in treating the estimate as normal is found to be of order n^-^2 as the sample size n tends to infinity.</p> </abstract>
<abstract> <p>Stochastic expansions are developed for the Lagrange multiplier, likelihood ratio, and Wald statistics for testing regression coefficients in the normal linear model with unknown error covariance matrix. Under suitable regularity conditions, the likelihood ratio statistic is found to be approximately the average of the other two. Critical values are calculated so that the three tests have approximately the same size. The second-order approximate local power functions indicate that, when the null hypothesis is one dimensional, all three tests are equally powerful. When the hypothesis is multidimensional, the power functions differ; no one of the tests is uniformly more powerful than the others.</p> </abstract>
<abstract> <p>The model considered is a two-equations model consisting of a binary choice equation and a regression equation. Tests for the bivariate normal distribution are derived for the truncated samples case and the censored samples case. The tests are Lagrangean multiplier tests for testing the bivariate normal distribution within the bivariate Edgeworth series of distributions. Simple intuitive interpretations for the statistics are provided.For the truncated case, the test compares with the estimated differences between some sample moments of order (r,s) for which r + s &gt; 2 and the corresponding hypothesized moments of the disturbances. For the censored case, the test is equivalent to the testing of some sample semi-invariants for which r + s &gt; 2 are zeros.</p> </abstract>
<abstract> <p>Neglecting across individual heterogeneity can lead to inconsistent or inefficient estimators, and to misleading predictions. This paper develops a specification error test sensitive to neglected heterogeneity, which is viewed as causing parameter variation, by deriving a score test of the hypothesis that parameters have zero variance. The test, whose form is insensitive to the specification of the distribution of varying parameters, turns out to be the Information Matrix test introduced by White [8]. This suggests that the Information Matrix test is a useful diagnostic for researchers using cross-sectional or longitudinal data to estimate models of individual economic agents behavior.</p> </abstract>
<abstract> <p>An approximation to the inconsistency introduced by imposing an incorrect restriction on a parametric model is given. The approximation can be applied to estimators generated by optimizing any objective function satisfying certain regularity conditions. Examples given include analysis of misspecification in discrete choice and time-series models estimated by maximum likelihood, and in a nonlinear regression model.</p> </abstract>
<abstract> <p>This paper presents the problem of aggregation over individual agents as a basic identification problem inherent to interpreting relationships between averaged economic variables. The concept of a complete aggregation structure is introduced, which embodies the correct condition for identification. Several examples of complete aggregation structures are provided by previous work on the aggregation problem in economics. Examples are also provided by work on complete distribution families in statistics, which in turn provide the correct conceptual framework for developing a theory of parameter estimation and tests of specific aggregation assumptions. The potential lack of correspondence between microeconomic behavior and estimated relations between averaged data is illustrated by distribution families obeying linear probability movement, which induce an extreme failure of the completeness property. Certain topics regarding empirical applications of aggregation results are reviewed.</p> </abstract>
<abstract> <p>This paper focuses on developing and adapting statistical models of counts (nonnegative integers) in the context of panel data and using them to analyze the relationship between patents and R &amp; D expenditures. Since a variety of other economic data come in the form of repeated counts of some individual actions or events, the methodology should have wide applications. The statistical models we develop are applications and generalizations of Poisson distribution. Two important issues are (i) Given the panel nature of our data, how can we allow for separate persistent individual (fixed or random) effects? (ii) How does one introduce the equivalent of disturbances-in-the-equation into the analysis of Poisson and other discrete probability functions? The first problem is solved by conditioning on the total sum of outcomes over the observed years, while the second problem is solved by introducing an additional source of randomness, allowing the Poisson parameter to be itself randomly distributed, and compounding the two distributions. Lastly, we develop a test statistic for the presence of serial correlation when fixed effects estimators are used in nonlinear conditional models.</p> </abstract>
<abstract> <p>This paper discusses two approaches that economists have taken in analyzing the timing of births. It formulates an empirical model appropriate for one of these approaches and demonstrates its usefulness using household survey data from Costa Rica. The hazard rate technique employed in this paper is natural way of modeling a broad class of problems where the occurrence of an event is uncertain. It is finding widespread use in economics.</p> </abstract>
<abstract> <p> Results associated with Neumann-type economic growth models are described. We present a new generalization of the Neumann model with the idea of explaining and unifying certain basic results obtained by M. Morishima and J. L̵os̄ in proving general existence theorems. The idea of replacing constant input and output matrices in the Neumann model by those which depend continuously on the growth rate and on the price vector suggested and developed by Morishima, is coupled with an asymmetric-type generalization of the original Neumann model, developed by J. L̵os̄. Consequently, many of our results resemble and partly replace those obtained in both model generalizations. </p> </abstract>
<abstract> <p>The anonymous interaction of large numbers of economic agents is a kind of noncooperative situation which is markedly different from small-numbers strategic conflict. The nonatomic game has been introduced as a model for these many-agent situations. This paper contains a precise definition of what it means for a nonatomic game to be the limit of a sequence of finite-player games, and a theorem which states when the limit of equilibria of finite-player games will be an equilibrium of the nonatomic limit game. This is analogous to theorems prompted by Edgeworth's conjecture in core theory.</p> </abstract>
<abstract> <p>This paper investigates two-person bargaining under incomplete information where one player has strictly better information about the potential value of the transaction than the other. The implications of informational barriers to trade are explored, and optimal bargaining mechanisms are characterized.</p> </abstract>
<abstract> <p>This paper examines the nature of rational choice in strategic games. Although there are many reasons why an agent might select a Nash equilibrium strategy in a particular game, rationality alone does not require him to do so. A natural extension of widely accepted axioms for rational choice under uncertainty to strategic environments generates an alternative class of strategies, labelled "rationalizable." It is argued that no rationalizable strategy can be discarded on the basis of rationality alone, and that all rationally justifiable strategies are members of the rationalizable set. The properties of rationalizable strategies are studied, and refinements are considered.</p> </abstract>
<abstract> <p>This paper explores the fundamental problem of what can be inferred about the outcome of a noncooperative game, from the rationality of the players and from the information they possess. The answer is summarized in a solution concept called rationalizability. Strategy profiles that are rationalizable are not always Nash equilibria; conversely, the information in an extensive form game often allows certain "unreasonable" Nash equilibria to be excluded from the set of rationalizable profiles. A stronger form of rationalizability is appropriate if players are known to be not merely "rational" but also "cautious."</p> </abstract>
<abstract> <p>In recent years, stability analysis has been extended in two directions, which are useful for economic applications. The first direction concerns differential equations with discontinuous right-hand sides. The second direction concerns difference equations with multivalued right-hand sides. The present paper reviews some of these contributions (in Section 5, 6, and the Appendix), brings out their similarities, and illustrates their applications to economic problems. The illustration concerns an economy with both private and public goods (Section 2). It is shown (Section 3) how an efficient allocation for that economy can be reached through a globally stable process, combining a price-guided market allocation of private goods and a quantitative planning procedure for public goods. A discrete version of the planning procedure, using an internally defined variable speed of adjustment, is also shown (Section 4) to be quasi-stable.</p> </abstract>
<abstract> <p>This paper analyzes a one-commodity model in which alternative investment projects are characterized by return functions indicating the output intensities over time resulting from an initial unit investment. Saving is generated partly by households as a constant fraction of net income, and partly by business firms in accordance with a depreciation (or replacement) policy. It is shown that when a "declining value depreciation policy" is adopted, the ordering of consumption streams in terms of their present values, at any fixed interest rate for which these converge, induces an ordering of investment projects in terms of their internal rates of return. The same ordering of projects is also induced by applying the overtaking criterion to the consumption steams.</p> </abstract>
<abstract> <p>The Rawlsian maximin is defined for situations involving risk and applied to growth with an uncertain technology. Existence and uniqueness are discussed and it is shown to unambiguously lead to at least the same savings as under certainty.</p> </abstract>
<abstract> <p>It is shown that, in a wide class of economies with Marshallian externalities, a Pareto optimum can be sustained by a competitive equilibrium with the aid of a tax-subsidy system. The tax-subsidy system consists of commodity taxes, commodity subsidies, lump-sum taxes, and lump-sum subsidies. The model can be interpreted as describing an economy with various kinds of public goods.</p> </abstract>
<abstract> <p>The purpose of this paper is to present a model which explains the formation of firms in a market economy. This is done by unifying two recent developments in economic theory: the labor-managed market economy and the coalition production economy. D. Sondermann's model [30] of a coalition production economy with "increasing returns to coalition" is generalized. Specifically, the concept of coalition structures is introduced, so that the model can be naturally interpreted to represent a labor-managed market economy. Two kinds of specific interpretation are considered: one is a type of socialistic economy and the other a capitalistic economy.</p> </abstract>
<abstract> <p>In analyzing the city as an economic institution, it seems reasonable to ask if the advantages of proximity are sufficient to assure that traders will form and maintain a market place. This process is called agglomeration. A general theorem concerning iterative spatial games is developed first. A spatial general equilibrium model comprised of a sequence of pure trade economies is proffered. Restrictions on transport technologies sufficient to assure agglomeration are determined. The possibility of a policy maker speeding the process of agglomeration is demonstrated. In conclusion, the optimality properties of the model are discussed. The research draws heavily on the works of A. Weber [8] and G. Debreu [2]. The model includes a dynamic adjustment process which is developed from individuals' maximizing behavior.</p> </abstract>
<abstract> <p>The problem of maximizing the gross national product of a country subject to domestic resource constraints, given exogenous international prices, is studied. The primal problem allows for a general technology, including joint and intermediate products and any number of inputs, outputs, and industries. Dantzig's variable coefficients simplex algorithm in conjunction with the duality between production possibility sets and profit functions is suggested as a method for solving the primal problem. The dual problem is used to prove comparative statics theorems, which generalize several theorems of international trade theory. An appendix characterizes the properties of the inverse of a bordered Hessian matrix.</p> </abstract>
<abstract> <p>This paper presents a general proof of a fundamental proposition of rationing theory and demonstrates that it applies to some basic postulates of macroeconomic theory, including the consumption function. The approach used is to study individual consumer behavior under conditions of quantity constraints. In so doing, the choice-theoretic foundations of the household sector's excess demand functions, as well as the functions themselves, in a general disequilibrium model are developed. These functions include quantities as well as prices as arguments. The response to a change in an effective quantity constraint is shown to depend on the substitutability between the goods involved. Responses to price changes are also determined. The results are then related to the literature on macroeconomic disequilibrium.</p> </abstract>
<abstract> <p>The first part of this article integrates the concept of (relative) risk aversion with respect to income (r) with the static analysis of demand for many commodities. Alternative representations of preferences and demand functions, using duality, give rise to many alternative representations of r, and to theorems regarding attitudes towards risk in bundles of quantities and in prices. In the second part, a previous analysis by Deschamps is corrected and completed by specifying the general form of preferences and demands such that r is a function of the utility level only, independent of relative prices. Finally, preferences and demand functions associated with constant r (previously analyzed by Stiglitz and Deschamps) are specified more explicitly and completely. A general conclusion emerging is that demand behavior under uncertainty can hardly throw any light on the nature of attitudes towards risk.</p> </abstract>
<abstract> <p>Social decision mechanisms that admit dominant strategies and result in Pareto optima are characterized by the class of mechanisms proposed by Groves. The concept of decision mechanisms is generalized and the characterization is shown to extend to these cases.</p> </abstract>
<abstract> <p>The low estimates of the income elasticity of housing demand obtained when individual households are the unit of observation are theoretically reconciled with the high estimates obtained when metropolitan-wide averages are used. The omission of the housing price term biases the ungrouped (whether stratified by metropolitan areas or not) estimate(s) downward and the grouped estimate upward. The inclusion of a metropolitan-wide average housing price term worsens the downward bias of the unstratified ungrouped estimate. The corresponding price elasticity estimate is biased upward (toward zero). These results are interpreted in terms of the theory of residential location and used to explain the empirical evidence. For the evidence considered, the true income and price elasticities are approximately .75 and -.75, respectively.</p> </abstract>
<abstract> <p>Edgeworth series expansions are obtained of the finite sample distributions of the least squares estimator and the associated t ratio test statistic in the context of a first-order noncircular stochastic difference equation. General formulae are given for these expansions up to 0(T^-1) where T is the sample size and explicit representations of these in terms of the true parameters are derived up to 0(T^-1/2). Some numerical comparisons of the approximations and the exact distributions are made in the case of the least squares estimator.</p> </abstract>
<abstract> <p>Under classical assumptions, characterizations are given for two classes of instrumental variable estimators of an equation in a simultaneous system. IV estimators where all instruments are nonstochastic are expressed in terms of multinormal random vectors in exactly the same way as the 2SLS estimator of a just-identified equation. These estimators have no finite moments of positive integral order. The second class, consisting of IV estimators based on certain stochastic instruments, includes the OLS, 2SLS, and modified 2SLS estimators. The inadmissibility (under squared-error loss) of some estimators in this class is considered when the equation being estimated contains two endogenous variables.</p> </abstract>
<abstract> <p>Conditions under which a single iteration approximation to the maximum likelihood estimator dominates ordinary least squares are approximated analytically for the class of linear models for which the eigenvectors of the error covariance matrix are known.</p> </abstract>
<abstract> <p> The distributions of the LIML and TSLS estimates of the coefficient of an endogenous variable in a single equation can be approximated by asymptotic expansions. This paper relates the expansions in terms of the noncentrality parameter and the sample size going to infinity, the noncentrality parameter going to infinity with the sample size held fixed, and the standard deviation of the disturbance going to zero (``small-σ''). </p> </abstract>
<abstract> <p>The widespread use of prior information in formulating, estimating, and using econometric models is reviewed. Attempts to avoid the use of prior information by formulating multivariate statistical VAR and ARMA time series models for economic time series data have resulted in heavily over-parametrized models. A simple demand, supply, and entry model is presented to contrast models utilizing prior information provided by economic theory and other sources with multivariate statistical time series models. Formal Bayesian methods for incorporating prior information in econometric estimation, testing, and prediction are presented. A number of published applied Bayesian studies are cited in which Bayesian methods have proved to be effective. It is concluded that wise use of the Bayesian approach will produce improved econometric results.</p> </abstract>
<abstract> <p>When charitable contributions are tax deductible, the marginal price of charitable giving in other consumption foregone per dollar of contributions is generally less than unity. Further, if the income tax schedule is a progressive step function, the marginal price of contributions is generally a rising step function of the level of contributions. The problem of estimating a contributions demand function for individuals is therefore complicated by the spurious correlation between the level of contributions and the observed marginal price. We take this econometric problem into account in estimating a contributions demand function using data from the 1972-73 Consumer Expenditure Survey. After comparing our results with those of estimation techniques used by other authors, we provide evidence on the impacts of alternative tax policies on charitable giving using our estimates of the model parameters.</p> </abstract>
<abstract> <p>Before 1979, unemployment insurance (UI) benefits were not treated as taxable income in the United States. Several economists criticized this policy on the ground that not taxing UI benefits while taxing earned income allegedly encourages unemployed persons to conduct longer than socially optimal job searches. Since 1979, however, UI benefits received by persons in higher-income families have been subject to income tax. This paper investigates whether the introduction of benefit taxation has had the predicted effect of reducing unemployment duration. The study uses data on a sample of persons that filed for UI in 1978 or 1979 to examine whether high-income claimants collected benefits for shorter periods after the tax change than they did before benefits became taxable. As part of the empirical analysis, the paper develops a generalization of the Weibull distribution and applies a limited-dependent-variable technique for this distribution similar to the Tobit technique for the normal distribution. Despite some variation in the results from different model specifications, the analysis presents persuasive evidence of a tax effect on unemployment duration. The 1979 policy change is estimated to have reduced average compensated unemployment duration among the sampled high-income claimants by about one week.</p> </abstract>
<abstract> <p>A family of monotonic solutions to general cooperative games (coalitional form games where utility is not assumed to be transferable) is introduced under the name of egalitarian solutions. These solutions generalize the notion that cooperating players within a coalition should have equal compensation for this cooperation where equal compensation is done in interpersonally compared utilities. The egalitarian solutions generalize the weighted Shapley values defined on the subclass of cooperative games with transferable utility and Kalai's proportional solutions defined on the subclass of bargaining games. It is shown that in the presence of other weak axioms the egalitarian solutions are the only monotonic ones. The monotonicity condition is shown to be necessary and sufficient to bring about full cooperation if we assume that the players are individual utility maximizers and can control their levels of cooperation.</p> </abstract>
<abstract> <p>We consider an oligopolistic market where firms face an uncertain demand for their product. Each firm observes a private signal for the state of demand and decides whether to reveal it to other firms and how complete this revelation will be. After the stage of information transmission the firm chooses its level of output. We derive pure strategy equilibria that are symmetric and subgame perfect, and demonstrate that no information sharing is the unique Nash equilibrium of the game regardless of the degree of correlation among the private signals.</p> </abstract>
<abstract> <p>This paper deals with the optimal design of resource allocation mechanisms in the presence of asymmetric information. A buyer's valuation function is allowed to depend on the characteristics of other buyers as well as his own and sufficient conditions are provided under which the seller can extract the full surplus from the buyers in an "ex post Nash" equilibrium. The result is then applied to the important problem of optimal auction design.</p> </abstract>
<abstract> <p>This paper develops a continuous time general equilibrium model of a simple but complete economy and uses it to examine the behavior of asset prices. In this model, asset prices and their stochastic properties are determined endogenously. One principal result is a partial differential equation which asset prices must satisfy. The solution of this equation gives the equilibrium price of any asset in terms of the underlying real variables in the economy.</p> </abstract>
<abstract> <p>This paper uses an intertemporal general equilibrium asset pricing model to study the term structure of interest rates. In this model, anticipations, risk aversion, investment alternatives, and preferences about the timing of consumption all play a role in determining bond prices. Many of the factors traditionally mentioned as influencing the term structure are thus included in a way which is fully consistent with maximizing behavior and rational expectations. The model leads to specific formulas for bond prices which are well suited for empirical testing.</p> </abstract>
<abstract> <p>We model a search economy where individual inventories accumulate smoothly, but trades of discrete bundles occur at discrete times determined by a Poisson meeting process and the equilibrium probability that a randomly selected trading partner has sufficient inventory. We distinguish between buying for consumption and selling to increase inventory. Higher (uniform) trading prices imply greater inventory stocks. With optimal individual price setting, we elicit conditions for the existence of steady state equilibrium, show that equilibrium is unique when it exists, and that price is higher the greater the flow of endowment relative to the capacity to distribute goods to consumers.</p> </abstract>
<abstract> <p>Does a pure exchange economy with an infinite time horizon have determinate perfect foresight equilibria? When there is a finite number of infinitely lived agents equilibria are generically determinate. This is not true with overlapping generations of finitely lived agents. We ask whether the initial conditions together with the requirement of convergence to a steady state locally determine an equilibrium price path. In this framework there are many economies with isolated equilibria, many with continua of equilibria, and many with no equilibria at all. With two or more goods in every period not only can the price level be indeterminate but relative prices as well. Furthermore, such indeterminacy can occur whether or not there is fiat money and whether or not the equilibria are Pareto efficient.</p> </abstract>
<abstract> <p>The existence of equilibrium and Pareto optimal allocations in economies with an infinite number of commodities is studied. It is shown that any topology stronger than the Mackey topology might lead to the nonexistence of nontrivial Pareto optimal allocations. I.e., there exists a well behaved economy with preferences that are continuous in this topology and without individually rational Pareto optimal allocations. A converse of this theorem, a slight modification of Bewley's [2] existence of equilibrium theorem, is also proved. Using a characterization of the Mackey topology in terms of impatience of consumers, due to Brown and Lewis [3], an interpretation of the theorem above is given: a topology is such that continuity with respect to it implies existence of nontrivial Pareto optimal allocations if and only if it also implies impatience on the part of the consumers.</p> </abstract>
<abstract> <p>This paper formulates a simple regenerative optimal stopping model of bus engine replacement to describe the behavior of Harold Zurcher, superintendent of maintenance at the Madison (Wisconsin) Metropolitan Bus Company. The null hypothesis is that Zurcher's decisions on bus engine replacement coincide with an optimal stopping rule: a strategy which specifies whether or not to replace the current bus engine each period as a function of observed and unobserved state variables. The optimal stopping rule is the solution to a stochastic dynamic programming problem that formalizes the trade-off between the conflicting objectives of minimizing maintenance costs versus minimizing unexpected engine failures. The model depends on unknown "primitive parameters" which specify Zurcher's expectations of the future values of the state variables, the expected costs of regular bus maintenance, and his perceptions of the customer goodwill costs of unexpected failures. Using ten years of monthly data on bus mileage and engine replacements for a subsample of 104 buses in the company fleet, I estimate these primitive parameters and test whether Zurcher's behavior is consistent with the model. Admittedly, few people are likely to take particular interest in Harold Zurcher and bus engine replacement per se. I focus on a specific individual and capital good because it provides a simple, concrete framework to illustrate two ideas: (i) a "bottom-up" approach for modelling replacement investment, and (ii) a "nested fixed point" algorithm for estimating dynamic programming models of discrete choice.</p> </abstract>
<abstract> <p> Time series variables that stochastically trend together form a cointegrated system. In such systems, certain linear combinations of contemporaneous values of these variables have a lower order of integration than does each variable considered individually. These linear combinations are given by cointegrating vectors. OLS and NLS estimators of the parameters of a cointegrating vector are shown to converge in probability to their true values at the rate T^1^-^@d for any positive δ. These estimators can be written asymptotically in terms of relatively simple nonnormal random matrices which do not depend on the parameters of the system. These asymptotic representations form the basis for simple and fast Monte Carlo calculations of the limiting distributions of these estimators. Asymptotic distributions thus computed are tabulated for several cointegrated processes. </p> </abstract>
<abstract> <p>An effectivity function describes the blocking power of coalitions on subsets of alternatives. Given a preference profile, if any coalition blocks an alternative whenever it can, using its own power, make all of its members better off, only alternatives in the core can be reached. In this paper we study the incentives of the coalitions to use this power truthfully, i.e. to not manipulate. Some well known cores, among them the core of an exchange economy, are manipulable. We give sufficient conditions on an effectivity function that assure its core is nonmanipulable.</p> </abstract>
<abstract> <p>This paper establishes the existence of competitive equilibria for economies with production and infinite-dimensional space of commodities. The commodity spaces treated are normed lattices, but no interiority assumptions are made on the positive cone; such infinite-dimensional spaces include most of those which have been found useful in economic analysis. The crucial assumptions are on compactness of the set of feasible allocations, continuity of preferences, boundedness of the marginal rates of substitution in consumption, and boundedness of the marginal efficiency of production. Examples are presented which show that without such assumptions, competitive equilibria may fail to exist.</p> </abstract>
<abstract> <p> The paper studies pure exchange economies with infinite-dimensional commodity spaces in the setting of Riesz dual systems. Several new concepts of equilibrium are introduced. An allocation (x"1,..., x"m) is said to be (a) an Edgeworth equilibrium whenever it belongs to the core of every n-fold replication of the economy; (b) an approximate quasiequilibrium whenever for every ε &gt; 0 there exists some price p ≠ 0 with p · ω = 1 (where @w = @S@w"i is the total endowment) and with x @&gt;"ix"i implying p.x @&gt;p.@w"i -@?; and (c) an extended Walrasian equilibrium whenever it is a Walrasian equilibrium with respect to an extended price system (i.e., with respect to a price system whose values are extended real numbers). The major results of the paper are the following. (i) If [O, ω] is weakly compact, then Edgeworth equilibria exist. (ii) An allocation is an Edgeworth equilibrium if and only if it is an approximate quasiequilibrium (and also if and only if it is an extended Walrasian equilibrium). (iii) If preferences are uniformly proper, then every Edgeworth equilibrium is a quasiequilibrium. (iv) There exists a two person exchange economy with empty core on C[0, 1] such that preferences are norm continuous, strongly monotone, strictly convex, and uniformly proper, and each agent's endowment is strictly positive. </p> </abstract>
<abstract> <p>This paper offers an interpretive comparison of the Arrow/Pratt and Ross characterization of comparative risk aversion for expected utility maximizers. The tools used in this comparison are then applied to obtain a strengthening of the Ross characterization. This strengthened result is then extended to the case of general smooth non-expected utility preferences over probability distributions.</p> </abstract>
<abstract> <p>We study duopolistic competition in a homogeneous good through time under the assumption that its current desirability is an exponentially weighted function of accumulated past consumption. This implies that the current price of the good does not decline by as much to accommodate any given level of current consumption as it would if its desirability were a function solely of present consumption. We analyze the consequences of this "stickness" of the goods's current price through a speed of adjustment parameter that yields instantaneous adjustment as a limiting case. Our analysis is conducted in terms of a differential game. The open-loop and closed-loop Nash equilibrium strategies are derived, from which the corresponding asymptotically stable steady-state equilibrium prices for the good are obtained. These equilibrium prices are then analyzed as the speed of price adjustment becomes instantaneous. It is found that the equilibrium price corresponding to the open-loop Nash equilibrium strategies approaches the static Cournot equilibrium price while the equilibrium price corresponding to the close-loop Nash equilibrium strategies, which are subgame perfect, approaches a price below it.</p> </abstract>
<abstract> <p>Comparative static properties of optimal nonlinear income taxes are obtained for a finite population version of the Mirrlees income tax model with a weighted utilitarian social welfare function and quasilinear preferences. The parameters which are varied are the weights in the welfare function, the slope of the production constraint, and a parameter in the utility function. The endogenous variables are the consumers' consumption levels, pretax incomes (labor supplies in efficiency units), utility levels, and marginal tax rates. For the variations in the parameters considered, it is always possible to sign the directions of change in the levels of consumption and, in some cases, it is possible to sign the directions of change in all of the endogenous variables.</p> </abstract>
<abstract> <p>The paper deals with infinite horizon optimization problems. The existence of optimal solutions is obtained as a consequence of an asymptotic growth condition. We also construct finite horizon approximates that yield upper and lower bounds for the optimal values and whose optimal solutions converge to the long-term optimal trajectories. One type of approximate results from a process which involves smoothening the tail of any trajectory. The other type emerges when we restrain ourselves to stay put after a finite lapse of time.</p> </abstract>
<abstract> <p>This paper deals with the subject of brand loyalty. It contains statistical tests for the presence of a learning function that leads buyers to purchase in a manner reflecting greater reliance on personal experience and less reliance on seller advertising efforts.</p> </abstract>
<abstract> <p>This paper is concerned with the estimation of Cobb-Douglas production function parameters by the analysis of variance, using combined time-series and cross-section data. Some theoretical development is followed by empirical results for a sample of farm firms over a period of years. Estimated marginal returns per dollar of expenditure are obtained for inputs of the "average" firm; returns to labor are below one dollar, and returns to other inputs are above one dollar. Firm constants and time constants are obtained and interpreted. There is a drop in the sum of elasticities as one moves from the usual least squares estimates to analysis of covariance estimates. Alternative explanations for this decrease are considered.</p> </abstract>
<abstract> <p>Problems of simultaneous equations were first examined in connection with errors-in-variables models, but formal results on identification were published only for the errors-in-equations model. This paper shows that very similar results hold for the errors-in-variables cases. It also examines limited information, maximum likelihood estimation for this case.</p> </abstract>
<abstract> <p>Two problems are posed within a matrix multiplier-accelerator framework. The first is to determine the conditions under which positive changes in levels of income accrue to all countries given an autonomous change in the income of one country. The second enquires into the effects of a once-over change in the structure of the trading system, such as war or industrialization programs, on the growth paths of all the trading countries.</p> </abstract>
<abstract> <p>The paper presents a comprehensive method for analyzing structural change in a developing economy. The total change in output in each sector is expressed by means of an interindustry model as a function of four factors: (i) the change in the composition of domestic demand; (ii) the change in the volume of exports; (iii) the change in the volume of imports; and (iv) changes in technology and organization. The model is then applied to explain the process of industrialization in Japan from 1914 to 1954, during which period its economy was transformed from that of a typical underdeveloped country to that of an advanced society. The results show that less than a quarter of the increase in the share of industry in total output is due to increases in exports and changes in domestic demand, while more than three quarters is traceable to changes in supply conditions, including substitution of domestic manufactures for both imported manufactures and primary products. Changes in technology and in labor productivity are also examined briefly.</p> </abstract>
<abstract> <p>This article examines some economic problems concerned with profit sharing in a Socialist economy. Two alternative systems of incentives have been made the subject of parallel investigations, comparing the effects which each of these systems have on the firm's behavior. We also examine some problems of price regulation. In the investigation both linear and nonlinear programming methods have been used.</p> </abstract>
<abstract> <p>If correlations are calculated between ratios containing a common variable the presence of observation errors for the common variable may lead to spurious inferences about the correlations obtaining in the population. By making assumptions regarding the distribution of the observation errors it is possible to adjust for the influence of the errors on the correlations of the ratios. The adjustments are derived on various assumptions and are applied to correlations in which this problem originally arose [4].</p> </abstract>
<abstract> <p>A production set with indivisibilities is described by an activity analysis matrix with activity levels which can assume arbitrary integral values. A neighborhood system is an association with each integral vector of activity levels of a finite set of neighboring vectors. The neighborhood relation is assumed to be symmetric and translation invariant. Each such neighborhood system can be used to define a local maximum for the associated integer programs obtained by selecting a single commodity whose level is to be maximized subject to specified factor endowments of the remaining commodities. It is shown that each technology matrix (subject to mild regularity assumptions) has a unique, minimal neighborhood system for which a local maximum is global. The complexity of such minimal neighborhood systems is examined for several examples.</p> </abstract>
<abstract> <p>When either there are only two players or a "full dimensionality" condition holds, any individually rational payoff vector of a one-shot game of complete information can arise in a perfect equilibrium of the infinitely-repeated game if players are sufficiently patient. In contrast to earlier work, mixed strategies are allowed in determining the individually rational payoffs (even when only realized actions are observable). Any individually rational payoffs of a one-shot game can be approximated by sequential equilibrium payoffs of a long but finite game of incomplete information, where players' payoffs are almost certainly as in the one-shot game.</p> </abstract>
<abstract> <p>The life cycle model analyzed here constrains most work on the main job to be full-time. Partial retirement entails a wage reduction and frequently a job change. Maximum likelihood estimates of utility function parameters are designed to incorporate information on the age of leaving full-time work and the age of full retirement. The model closely tracks retirement behavior, including peaks in retirement activity at 62 and 65. The paper explores the relation of these peaks to nonlinearities in the budget constraint created by various retirement programs. Policy analyses based on this and on earlier models with simpler structures are compared.</p> </abstract>
<abstract> <p>This paper studies the pricing of common stocks in the context of a monetary, rational expectations equilibrium. Special attention is given to the relationship between inflation and asset returns.</p> </abstract>
<abstract> <p>This paper analyzes the optimal tax on capital income in general equilibrium models of the second best. Agents have infinite lives and utility functions which are extensions from the Koopmans form. The population is heterogeneous. The important property of the models is the equality between the social and the private discount rates in the long run. I find that the optimal tax rate is zero in the long run. For a special case of additively separable utility functions, I then determine the tax rates along the dynamic path and conditions that are sufficient for the local stability of the steady state.</p> </abstract>
<abstract> <p>Two costlessly mobile firms are to be located in a market region, a subset of the plane. The firms compete by setting locations and delivered price schedules. To study this competitive situation an appropriate extensive form game is defined, along with an appropriate noncooperative solution concept. Existence and general properties of the equilibrium are demonstrated. Among the results are: Each firm increases its profit by locating so as to decrease total cost to both firms of serving the market. Firms will never locate coincidentally if they have identical production costs and transport cost rates, or if these are different and the firms are located in a circular market region having a uniform demand distribution.</p> </abstract>
<abstract> <p>We consider a regression in which one of the observed variables is a proxy for some unobserved "true" variable. Given a lower bound for the correlation between the proxy and the unobserved true variable for which it substitutes, we derive intervals in which the coefficients of the unobserved true regression must lie, regardless of any other correlations involving unobserved variables or disturbances. We present a simple solution for the important special case in which only the signs of the coefficients are of concern and one seeks the smallest correlation between the proxy and the true variable that guarantees the correctness of the signs of the coefficients in the proxy regression. We also present an algorithm for extending these results to the multiple-proxy problem.</p> </abstract>
<abstract> <p>The encompassing principle is used to develop a testing framework which unifies the literature on non-nested testing, allowing analysis of the relationship between alternative tests and in particular enabling asymptotic and finite sample equivalences and identities to be established easily when they exist, as well as embracing nested hypothesis testing. The concept of the implicit null hypothesis of a test is introduced to show that the effective acceptance region for some tests extends beyond the acceptance region corresponding to the null of interest, and so such tests can be inconsistent against fixed alternatives closely related to the nominal null and alternative. The analysis is illustrated by an application to two non-nested linear regression models, and it is shown that the conventional F test, as well as all the one degree of freedom non-nested tests, has an encompassing interpretation, and that the F test is a "complete" parametric encompassing test.</p> </abstract>
<abstract> <p>This paper considers the impact of certain types of policy related voting patterns on the existence and location of equilibrium strategies in the spatial model of two-candidate competition. In contrast to much of the previous literature, this paper makes a distinction between the aggregate level patterns of voting and the individual level variables which bring them about. By so doing the assumptions can focus on objects which have a more direct empirical referent, namely, the aggregate level support function and the distribution of ideal points. Using this approach, sufficient conditions are found for the existence of equilibrium which, although themselves strong, make considerably weaker demands on individuals than have been generally assumed in the literature. Thus, it is not necessary that each voter vote strictly with regard to policy but, rather, it is sufficient that in the electorate as a whole there is a moderate amount of policy related voting. Other results of the analysis are that policy related voting of any type seems to encourage candidates to converge towards the center, with support from extremists only accentuating this tendency. It is the candidate's most loyal supporters who seem to have the least influence over his policy position.</p> </abstract>
<abstract> <p>The problems associated with interpersonal comparisons are particularly intractable. This paper presents a procedure whereby the relative importance of any particular individual varies over the set of social states. In one sense, the stronger (relative to some norm) a person feels about any particular pairwise decision, the larger his say in that outcome. This procedure leads to a nested sequence of aggregate partial orderings which reflects this strength of preference. Under the assumptions presented it is also possible, given any two social states, to characterize the minimal amount of interpersonal comparison which is necessary in order to arrive at an aggregate ordering.</p> </abstract>
<abstract> <p>Under the assumption of single peaked preferences, the majority rule equilibrium considered as a correspondence from the voters' preference peaks is shown to be continuous. We also complement the work of Fishburn [6], who first presented a general location theorem for majority rule equilibriums, by dropping the assumptions that the alternative set is finite and that voters' preferences are strict partial orders.</p> </abstract>
<abstract> <p>A one-way expected utility representation has the expected utility of one probability measure greater than the expected utility of another probability measure whenever the first is preferred to the second. It requires preferences to be acyclic but not necessarily transitive, and does not require indifference to be transitive. Preference axioms which are sufficient for one-way expected utility for sets of simple probability measures have been presented before (see [8]). This paper uses additional axioms to extend the one-way representation to sets of discrete and more general probability measures.</p> </abstract>
<abstract> <p>The standard two-period consumer choice problem, where current consumption must be decided upon subject to uncertainty about future income and prices is considered in this paper. Previous analyses have been limited to an economy where either there is only one commodity or consumer preferences have a restrictive form. The primary objective of this paper is to generalize these analyses so that these limitations are eliminated. This is accomplished by applying the theory of duality.</p> </abstract>
<abstract> <p>This paper is concerned with showing topological properties of the graph of the Walras correspondence such as connectedness. A useful result is the bundle structure of the graph.</p> </abstract>
<abstract> <p>The empirical content of the paper is the relationship between personal attributes and job performance, as measured by rate of promotion, in a large corporation. It focuses, however, on a behavioral model and method of estimation which provide a natural way of dealing with this and similar phenomena.</p> </abstract>
<abstract> <p>If two linear models have different sets of explanatory variables and the same variable to be explained, the residual variance of the correct model (S^2"n) has a smaller mean value than that of the incorrect one (t^2"n). This note shows under fairly general conditions that S^2"n &lt; t^2"n will hold with probability arbitrarily close to 1 provided that the sample size n is large enough.</p> </abstract>
<abstract> <p>In this paper we analyze implications of a singular contemporaneous disturbance covariance matrix for the estimation and hypothesis testing of systems of equations with autoregressive disturbances. We find that this singularity imposes restrictions on the parameters of the autoregressive process. When these restrictions are not imposed, the specification, maximum likelihood estimates, and likelihood ratio test statistics are conditional on the equation deleted. Furthermore, singularity of the contemporaneous disturbance covariance matrix raises issues concerning identification of parameters of the autoregressive process. This identification problem complicates the interpretation of likelihood ratio statistics. The above results are illustrated with an empirical example.</p> </abstract>
<abstract> <p>The power function of the Durbin-Watson test for first-order serial correlation is examined. The power function depends upon the regression vectors, but useful upper and lower bounds for the power are established. The bounds are obtained from inequalities on the characteristic roots of real non-definite symmetric matrices developed in this article.</p> </abstract>
<abstract> <p>Asymptotically valid procedures are available for testing hypotheses on the slope coefficients in a linear regression model where the error covariance matrix @s depends on an unknown parameter vector @Q. Two situations can be distinguished. In one case, @Q has low dimension and can be well estimated from the data; tests on the slope coefficients are based on generalized least squares using the estimated @Q. In the second case, the parameter vector @Q has high dimension and cannot be well estimated; tests are based on the ordinary least squares slope coefficients using a robust estimate of their covariance matrix. When the null hypothesis consists of a single constraint, the test statistics in both cases are ratios of estimated slope coefficients to their estimated standard errors and are asymptotically normal as the sample size n tends to infinity. In the present paper, Edgeworth approximations with error @?(n^-^1) are developed for the distribution functions of these test statistics under the assumption that the errors are normal. In both cases, adjustments to the asymptotic critical values are found to insure that the tests have correct size to a second order of approximation. Approximate local power functions are derived for the size-adjusted tests and the power loss due to the estimation of @s is calculated. The second-order adjustments to the asymptotic distributions are relatively simple, but depend on the particular functional form for @s(@Q). Two examples, one involving heteroscedasticity and the other autocorrelation, are worked out in detail. These examples suggest that the size and power correction terms can be large even in very simple models where asymptotic theory might be expected to work well. It appears that the null rejection probabilities of the commonly proposed robust regression tests are often considerably greater than their nominal level. Moreover, the cost of not k@s can sometimes be substantial.</p> </abstract>
<abstract> <p>The concept of a near-integrated vector random process is introduced. Such processes help us to work towards a general asymptotic theory of regression for multiple time series in which some series may be integrated processes of the ARIMA type, others may be stable ARMA processes with near unit roots, and yet others may be mildly explosive. A limit theory for the sample moments of such time series is developed using weak convergence and is shown to involve simple functionals of a vector diffusion. The results suggest finite sample approximations which in the stationary case correspond to conventional central limit theory. The theory is applied to the study of vector autoregressions and cointegrating regressions of the type recently advanced by Engle and Granger (1987). A noncentral limiting distribution theory is derived for some recently proposed multivariate unit root tests. This yields some interesting insights into the asymptotic power properties of the various tests. Models with drift and near-integration are also studied. The asymptotic theory in this case helps to bridge the gap between the nonnormal asymptotics obtained by Phillips and Durlauf (1986) for regressions with integrated regressors and the normal asymptotics that usually apply in regressions with deterministic regressors.</p> </abstract>
<abstract> <p>The problem of controlling a stochastic process with unknown parameters over an infinite horizon with discounting is considered. The possibility of sacrificing current period expected reward for information leading to possible increases in future reward is examined. Agents express beliefs about unknown parameters in terms of distributions. Under general conditions the sequence of beliefs converges to a limit distribution. The limit distribution may or may not be concentrated at the true parameter value. In some cases complete learning is optimal; in others the optimal strategy does not imply complete learning. The paper concludes with examination of some special cases including high and low discount rates, discrete parameter and action spaces (the n-armed bandit with correlated arms), and a class of examples in which incomplete learning is optimal.</p> </abstract>
<abstract> <p>This paper utilizes asymptotic expansions of the Edgeworth type to investigate alternative forms of the Wald test of nonlinear restrictions. Some formulae for the asymptotic expansion of the distribution of the Wald statistic are provided for a general case that should include most econometric applications. When specialized to the simple cases that have been studied recently in the literature, these formulae are found to explain rather well the discrepancies in sampling behavior that have been observed by other authors. It is further shown how the corrections delivered by Edgeworth expansions may be used to find transformations of the restrictions which accelerate convergence to the asymptotic distribution.</p> </abstract>
<abstract> <p>The idea that markets might aggregate and disseminate information and also resolve conflicts is central to the literature on decentralization (Hurwicz, 1972) and rational expectations (Lucas, 1972). We report on three series of experiments all of which were predicted to have performed identically by the theory of rational expectations. In two of the three series (one in which participants trade a complete set of Arrow-Debreu securities and a second in which all participants have identical preferences), double auction trading leads to efficient aggregation of diverse information and rational expectations equilibrium. Failure of the third series to exhibit such convergence demonstrates the importance of market institutions and trading instruments in achievement of equilibrium.</p> </abstract>
<abstract> <p>Spot asset trading is studied in an environment in which all investors receive the same dividend from a known probability distribution at the end of each of T = 15 (or 30) trading periods. Fourteen of twenty-two experiments exhibit price bubbles followed by crashes relative to intrinsic dividend value. When traders are experienced this reduces, but does not eliminate, the probability of a bubble. The regression of changes in mean price on lagged excess bids (number of bids minus the number of offers in the previous period), P"t - P"t-1 = @a = @b(B"t"-"1 - O"t"-"1), supports the hypothesis that -@a = E(d), the one-period expected value of the dividend, and that @b &gt; O, where excess bids is a surrogate measure of excess demand arising from homegrown capital gains (losses) expectations. Thus when (B"t"-"1 - O"t"_"1) goes to zero we have convergence to rational expectations in the sense of Fama (1970), that arbitrage becomes unprofitable. The observed bubble phenomenon can also be interpreted as a form of temporary myopia (Tirole, 1982) from which agents learn that capital gains expectations are only temporarily sustainable, ultimately inducing common expectations, or "priors" (Tirole, 1982). Four of twenty-six experiments, all using experienced subjects, yield outcomes that appear to the "chart's eye" to converge "early" to rational expectations, although even in these cases we get @b &gt; O, and small price fluctuations of a few cents that invite "scalping."</p> </abstract>
<abstract> <p>The paper studies a simple two-period principal/agent model in which the principal updates the incentive scheme after observing the agent's first-period performance. The agent has superior information about his ability. The principal offers a first period incentive scheme and observes some measure of the agent's first-period performance (cost or profit), which depends on the agent's ability and (unobservable) first-period effort. The relationship is entirely run by short-term contracts. In the second period the principal updates the incentive scheme and the agent is free to accept the new incentive scheme or to quit. The strategies are required to be perfect, and updating of the principal's beliefs about the agent's ability follows Bayes' rule. The central theme of the paper is that the ratchet effect leads to much pooling in the first period. First, for any first-period incentive scheme, there exists no separating equilibrium. Second, when the uncertainty about the agent's ability is small, the optimal scheme must involve a large amount of pooling. The paper also gives necessary and sufficient conditions for the existence of partition equilibria and looks at the effect of cost uncertainty.</p> </abstract>
<abstract> <p>The Mirrlees-Rogerson conditions for the first-order approach to principal-agent problems to be valid do not work if the principal can observe more than one relevant statistic. This is a problem since much of the literature since Holmstrom (1979) deals with precisely such a case. We also argue that the Mirrlees-Rogerson assumption that the distribution function of output is convex in the agents action is unsatisfactory even in the context of the basic model; it is too restrictive, being satisfied by a very few of the distributions occurring in statistics textbooks. The present paper replaces this assumption and provides conditions which justify the first-order approach in the multi-statistic case.</p> </abstract>
<abstract> <p>We examine the use of stage mechanisms in implementation problems, and we partially characterize the set of choice rules that are implementable in subgame perfect equilibria. The conditions we derive will usually be satisfied if there is at least one private good. Our conclusion is that in these "economic" environments, almost any choice rule can be implemented--even if the rule is nonmonotonic, even if it is single-valued, and, under quite reasonable circumstances, even if there are just two agents. Moreover, the mechanism need have no more than three stages. To illustrate the power of this approach, we discuss a number of models in which it is possible to implement the first-best (although it would have been impossible to do so without using stage mechanisms). The diversity of these models suggests that subgame perfect implementation may find wide application: public goods, contracts, agency theory, constitution design. In several cases, the mechanisms we present have the added attraction that agents' moves are sequential, not simultaneous. Finally, we show that the Walras correspondence is implementable in subgame perfect equilibria.</p> </abstract>
<abstract> <p>The main purpose of this paper is to explore implications for one-stage and two-stage decision processes of a theory of choice that accommodates intransitivities in preferences. To motivate our analysis, we argue that intransitivities and preference cycles deserve serious consideration in normative theories of preference and choice. Examples from decision making under certainty, risk, and uncertainty are used to suggest that there are simply too many settings in which intelligent people and groups can have good reasons for cyclic preferences to exclude intransitivities from normative theory. Our analysis of decision processes enriches the basic alternative set by probabilistic convexification, as in the formation of mixed strategies in game theory. It is noted (and has been known for 30 years) for one-stage processes that if the convex set is finitely generated then it can have an enriched alternative that is maximally preferred regardless of intransitivities among basic alternatives. The core of the paper focuses on two-stage processes, typified by the Bayesian formulation of a choice of experiment followed by selection of a terminal act once the experiment's outcome is observed. We suppress consideration of Bayes's theorem and other facets of specialized formulations of two-stage processes to concentrate on a general treatment in which the "holistic," single-stage choice procedure is compared with two sequential procedures that first choose among different subsets and then choose something from the selected subset. The sequential procedures, referred to as "naive" and "sophisticated," base the choice of subset on a natural extension of holistic preferences. Many combinations of these three procedures are available in settings involving at least three stages of choice. A main result of the comparative analysis is that, when convex sets are finitely generated, the three choice procedures all have unambiguous solutions and these solutions can differ radically when preferences are intransitive. On the other hand, they coincide when preferences are fully transitive, and this can be taken as an argument in favor of transitivity. Moreover, since the imposition of transitivity on our model still allows nonlinearities that violate the von Neumann-Morgenstern utility model, the procedural equivalence obtains for a theory that is more general than their expected utility theory.</p> </abstract>
<abstract> <p>Using cross-section data collected from a panel of institutional investors in 1969, 1970, 1972, this paper focuses on how knowledgeable individuals formulate forecasts of future rates of price inflation. We estimate return-to-normality and error-learning forecasting models and inquire whether such equations can be interpreted simply as reduced forms of an autoregressive forecasting structure. Assuming that respondents' expectations were formed rationally in the sense of Muth, a series of tests lead us to reject the hypothesis of a purely autoregressive forecasting structure. Placed in the context of the return-to-normality model, the decisive evidence consists both of significantly nonzero intercepts and of important variations in survey respondents' anticipated normal rates of inflation that cannot be explained as a reduced-form reflection of past variation in observed inflation rates. These findings indicate that information not collinear with past realizations of price-level change plays an important role in the forecasting process, important enough to allow expected near-term rates of inflation to follow observed inflation rates more closely than autoregressive time series models would suggest.</p> </abstract>
<abstract> <p>This paper reports an experimental study of expectation formation and revision in a time series context. In an adaptive expectations framework, it is shown that the speed of adjustment seems to fall in turning point periods. Expectations are considered as probability density functions, and a scoring system is devised and employed that gives subjects an incentive to report a measure of the dispersion of these functions. This measure, which is inversely related to the confidence with which expectations are held, seems to be inversely related to past forecasting performance.</p> </abstract>
<abstract> <p>Proceeding under the assumption that expectations are formed rationally, this paper describes a procedure for consistent estimation of equations involving unobservable expectational variables and then, using this procedure, develops empirical results bearing on the validity of the natural rate hypothesis. Estimates assuming partial rationality are also reported.</p> </abstract>
<abstract> <p>When the policymaker's utility function is asymmetric, structural shifts in uncertainty have quite different implications for optimal policymaker behavior under uncertainty depending on the source of those shifts. Changes in uncertainty about exogenous variables do not lead to changes in the policymaker's bias. However, this is not necessarily the case when there is a change in uncertainty about policy response parameters. By contrast, symmetric policymaker utility functions do not imply these distinctions.</p> </abstract>
<abstract> <p>The impatience implications of continuous time utility indicators are interesting to the extent that they differ from the discrete time results. The class of traditional integral utility indicators are considered and impatience implications are shown to depend on the different convergence implications of the continuous time case. The stronger separability assumptions of continuous time utility indicators allow a weakening of compactness assumptions often required to demonstrate impatience.</p> </abstract>
<abstract> <p> Two probabilistic theories of choice behavior (the model of independent random utility and the model of elimination by aspects) imply a testable property, the multiplicative inequality, according to which the probability of selecting an alternative x from an offered set A ◡ B is at least as large as the product of the probabilities of selecting x from A and from B. </p> </abstract>
<abstract> <p>This paper seeks to prove that under a large class of group decision rules some sincere voting situations will be unstable because of strategic manipulation by single individuals. The concept of stability used is weaker than the stability concepts figuring in many earlier contributions in this area, insofar as under the concept used here any individual, while disrupting a given voting situation, considers the possibility of retaliation by other individuals.</p> </abstract>
<abstract> <p>A certain set of weak rationality conditions is shown to be necessary and sufficient for a social decision function to be a cooperative game according to the formulation of von Neumann and Morgenstern. In exhibiting this broad connection between game theory and the theory of social choice, attention is focused on the critical role played by the blocking coalitions in such games.</p> </abstract>
<abstract> <p>This paper investigates the justification for the competitive assumption that consumers will act as price takers by considering the utility gain an individual can achieve by manipulating price formation through the use of non-competitive behavior. Although announcing one's competitive demand is generally not a best replay against the excess demand of the rest of the economy, we show that, as the number of consumers becomes large, the gain any one can achieve acting monopolistically goes to zero if the increase in numbers comes through replication or if the sequence of economies converges to an economy at which the equilibrium price correspondence is continuous.</p> </abstract>
<abstract> <p>If the liquidity trap is viewed as a property of the aggregate demand for money (or liquid assets), it can be generated from the agents' microeconomic behavior only in special cases, even in the presence of the Keynesian assumption of inelastic expectations. On the other hand, in an economy where a central bank intervenes by open market operations, short run equilibrium interest rates on long term bonds tending to zero are associated with short run equilibrium money stocks which tend to infinity, once the Keynesian assumption of inelastic expectations is made.</p> </abstract>
<abstract> <p>This paper introduces a new coordinate system for the Lorenz curve. Particular attention is paid to a special case of wide empirical validity. Four alternative methods have been used to estimate the proposed Lorenz curve from the grouped data. The well known inequality measures are obtained as the function of the estimated parameters of the Lorenz curve. In addition the frequency distribution is derived from the equation of the Lorenz curve. An empirical illustration is presented using the data from the Australian Survey of Consumer Expenditure and Finances 1967-68.</p> </abstract>
<abstract> <p>Any misspecification of the disturbance error process in a linear regression may lead to an inefficient estimator. Although spectral methods proposed by Hannan will always be asymptotically efficient, they are frequently used because they are computationally demanding and very large samples are presumably required. This paper presents Monte Carlo evidence from a variety of typical econometric situations which indicates that the estimators perform quite well for moderate-sized samples (100) when the error process is highly dependent, and even for small samples when the error process is simple. The results are used to estimate a second order term in the asymptotic expansion for the variance.</p> </abstract>
<abstract> <p>In this paper we consider the estimation of a model with time varying structure. The parameters of the model are assumed to be subject to permanent and transitory changes over time. Estimation methods are developed, and the asymptotic properties of the estimates are derived.</p> </abstract>
<abstract> <p>This paper analyzes three quarterly investment models for the detection of certain specification errors. The models are those of Anderson [1 and 2], Eisner [4], and Meyer-Glauber [10]. The models are applied to thirteen manufacturing industries. A set of specification error tests developed by Ramsey [12,13, and 14] are applied to the above models so as to detect the specification errors of omission of variables, incorrect functional form, simultaneous equation problems, and heteroskedasticity. The models are ranked in order of the number of times they failed to be rejected by the specification error tests and the rank scheme is compared to that found in a previous study by Jorgenson, Hunter, and Nadiri [6], where more conventional criteria are used for ranking the models.</p> </abstract>
<abstract> <p>An important purpose in combining time-series and cross-section data is to control for individual-specific unobservable effects which may be correlated with other explanatory variables. Using exogeneity restrictions and the time-invariant characteristic of the latent variable, we derive (i) a test for the presence of this effect and for the over-identifying restrictions we use, (ii) necessary and sufficient conditions for identification, and (iii) the asymptotically efficient instrumental variables estimator and conditions under which it differs from the within-groups estimator. We calculate efficient estimates of a wage equation from the Michigan income dynamics data which indicate substantial differences from within-groups or Balestra-Nerlove estimates--particularly, a significantly higher estimate of the returns to schooling.</p> </abstract>
<abstract> <p>A conceptual framework is suggested for integrating fixed effects and random effects models into one framework. In that framework, the pertinent distribution is a convolution of two distributions; one is a degenerate distribution. A method is suggested and analyzed for separating between the two distributions when the second distribution is normal.</p> </abstract>
<abstract> <p>It is well known from the Monte-Carlo work of Nerlove that using the standard within-group estimator for dynamic models with fixed individual effects generates estimates which are inconsistent as the number of "individuals" tends to infinity if the number of time periods is kept fixed. In this paper we present analytical expressions for these inconsistencies for the first order autoregressive case.</p> </abstract>
<abstract> <p>In this paper the concept of approximate slope, introduced by R. R. Bahadur, is used to make asymptotic global power comparisons of econometric tests. The approximate slope of a test is the rate at which the logarithm of the asymptotic marginal significance level of the test decreases as sample size increases, under a given alternative. A test with greater approximate slope may therefore be expected to reject the null hypothesis more frequently under that alternative than one with smaller approximate slope. Two theorems, which facilitate the computation and interpretation of the approximate slopes of most econometric tests, are established. These results are used to undertake some illustrative comparisons. Sampling experiments and an empirical illustration suggest that the comparison of approximate slopes may provide an adequate basis for evaluating the actual performance of alternative tests of the same hypothesis.</p> </abstract>
<abstract> <p>This paper examines some implications of the observation that the same Lagrange multiplier test is sometimes appropriate for quite different alternative hypotheses. A characterization of the class of such alternatives is developed which suggests a simple approach to testing for misspecification, and the consequences for finite sample power properties are examined by Monte Carlo experiments.</p> </abstract>
<abstract> <p>We consider economies with preferences drawn from a very general class of strongly convex preferences, closely related to the class of convex (but intransitive and incomplete) preferences for which Mas-Colell proved the existence of competitive equilibria [13]. We prove a strong core limit theorem for sequences of such economies with a mild assumption on endowments (the largest endowment is small compared to the total endowment) and a uniform convexity condition. The results extend corresponding results in Hildenbrand's book [8]. The proof, which is based on our earlier results for economies with more general preferences [2], is elementary.</p> </abstract>
<abstract> <p>This paper extends the conclusions obtained by Stiglitz and others about the asymptotic wealth distribution in the neo-classical growth model when the saving function is convex. It is shown not only that locally stable two-class unegalitarian equilibria may exist along with the egalitarian equilibrium, but also that they necessarily are Pareto superior to it. More generally, the paper also analyzes the class of Pareto optimal unegalitarian equilibria.</p> </abstract>
<abstract> <p>Recent advances in optimal mechanism design are used to show that a certain type of auction, similar to the "open English auction," is an optimal mechanism in a certain class of environments.</p> </abstract>
<abstract> <p>The comparative statics of optimization models which have nonlinear constraints are examined. It is shown that most of the standard results of "linear" comparative statics still apply. However, it is also shown that individual substitution and income effects are systematically affected by the nature and degree of nonlinearity of the constraint. A model of quantity/quality trade-offs, previously examined in the literature, is analyzed, and several new results are developed.</p> </abstract>
<abstract> <p>One of the most important current questions in economic analysis is whether or not labor markets clear in the short run. To answer this, it is necessary to be able to distinguish between restricted and unrestricted behavior by consumers supplying labor. This paper investigates the forms of preferences which lie behind linear models of labor supply, and derives the functional forms for commodity demands which accompany them, both with and without quantity restrictions in the labor market. Simple linkages between restricted and unrestricted demands are also considered as is the question of perfect aggregation over consumers in the presence of quantity restrictions.</p> </abstract>
<abstract> <p> In this paper it is demonstrated that Wald's price index is the true cost-of-living index corresponding to a utility function of the form (x - γ)′ A (x - γ). Within this framework Frisch's double expenditure method and a related proposal of Samuelson are discussed. </p> </abstract>
<abstract> <p>For an individual with a time independent flow of real income who maximizes the discounted flow of utilities over an infinite horizon, the paper provides an exact characterization of the expenditure and money demand functions. Utility is derived only from consumption while all payments are made in the form of money. Money, which is the only asset, is used both for transaction purposes and as a store of value.</p> </abstract>
<abstract> <p>This paper proposes an econometric methodology to deal with life cycle earnings and mobility among discrete earnings classes. First, we use panel data on male log earnings to estimate an earnings function with permanent and serially correlated transitory components due to both measured and unmeasured variables. Assuming that the error components are normally distributed, we develop statements for the probability that an individual's earnings will fall into a particular but arbitrary time sequence of poverty states. Using these statements, we illustrate the implications of our earnings model for poverty dynamics and compare our approach to Markov chain models of income mobility.</p> </abstract>
<abstract> <p>The paper explores some of the issues involved in constructing measures of mobility when the data are provided in the form of a transition matrix. An initial set of axioms is proposed which is inconsistent. They can, however, be reconciled if empirically unlikely transition matrices are eliminated from consideration. The paper then discusses the problem of comparing matrices not defined over the same interval. An index based on the convergence speed in a Markvov chain process is able to compensate for differing time periods.</p> </abstract>
<abstract> <p>The paper constructs and estimates an augmented system of dynamic demand equations in which the demand for leisure and the demand for transactions balances are imbedded. The Budget constraint, redefined in terms of full income, appears as an argument in the wealth constraint, which in turn provides a way of introducing real money balances in the short run utility function without explicitly introducing prices in it. In the long run, prices do appear in the utility function, but their impact turns out to be negligible in practice. The generalized Stone-Geary utility function, on which the system is based, allows for both habit formation and stock effects. The demand for leisure, in the U.S., appears to be strongly habit forming, in the sense that the minimum number of leisure hours increases steadily over time in the short run and with the implication that adjustment to long run equilibrium is slow. In both runs, the supply of labor is slightly backward bending. The demand for transaction balances, on the contrary, does not display any dynamic behavior and is rather insensitive to changes in the rate of interest.</p> </abstract>
<abstract> <p>For problems involving choices over "certain x uncertain" consumption pairs, it is almost universally assumed that the decision maker's preferences can be represented by an expected TPC (two-period cardinal) utility function. In this paper, we present an alternative representation of preferences, referred to as the "ordinal certainty equivalent" hypothesis, which we argue (i) is at least as intuitive as the expected utility hypothesis, (ii) includes the corresponding TPC representation as a special case with the set of cases not expressible in the latter format being both large and important, and (iii) is based on a more sensible hypothesis concerning the connection between "risk" and "time" preferences.</p> </abstract>
<abstract> <p>For a competitive commodity market with uncertain future prices, the rational behavior of a speculator is described and is used as a basis for the discussion of a self-fulfilling expectations equilibrium.</p> </abstract>
<abstract> <p>It is known that the price mechanism whereby the rate of change of a price is proportional to the excess demand of the corresponding commodity need not converge to a competitive equilibrium for a pure exchange economy with more than two commodities. On the other hand, there exist convergent price mechanisms, similar to the Newton iterative process, where the rate of change of the prices is determined by the excess demand and the marginal excess demands of all the commodities. This is a considerable informational requirement. It is shown that this requirement cannot be substantially reduced for any convergent price mechanisms, that is for price mechanisms expressed in terms of a difference or differential equation where the solutions converge to a competitive equilibrium.</p> </abstract>
<abstract> <p> A number of recent works have addressed the problem of describing the allocation of resources in an economy where prices are fixed at a value that does not achieve equilibrium of supply and demand in the classical sense. In the context of a model developed by Drèze, the purpose of this paper is to describe more precisely the different states of the markets (excess supply or excess demand) that may occur near a competitive equilibrium. A general analytical picture is obtained, which associates with each state of the markets the region of the price domain where it prevails. This allows us to point out an important difficulty which arises in the local comparative statics of this class of model: local unicity is not warrantied, that is, there may exist price systems very close to some competitive price, for which all the fixed price allocations are far from the competitive allocation. Examples are shown in the macroeconomic model (two aggregate agents: households and firms, and three goods: money, output, and labor); necessary and sufficient conditions for local unicity are given in this context. They require that all commodities be Hicks-substitute for the consumer, which is another way of saying that both the marginal propensities for consumption and leisure are positive and smaller than one. On the other hand, assuming local unicity, one can look for the implications of the foregoing results for the long-run determination of prices. It is easy to show that, if the economy always reaches a fixed price allocation, an increase of the price of one commodity near the competitive equilibrium is always to the advantage of the sellers and to the disadvantage of the buyers. This suggests that the determination of prices should take the form of a struggle between buyers and sellers in each market. </p> </abstract>
<abstract> <p>Using nonstandard analysis, we provide a simple proof of existence in an infinite productive economy. Special care has been taken to make the proof accessible to readers familiar with Debreu's Theory of Value, and the elementary procedure for deriving asymptotic results is also described.</p> </abstract>
<abstract> <p>The purpose of this paper is to examine within the context of a particular U.S. econometric model the sensitivity of fiscal policy effects to alternative assumptions about the behavior of the Federal Reserve. Five cases are considered, four in which Fed behavior is exogenous and one in which Fed behavior is endogenous. In each of the four exogenous cases the Fed is assumed to control a particular variable, which is then taken to be exogenous for purposes of the fiscal-policy experiments. For the endogenous case an estimated equation explaining Fed behavior is added to the model, and the expanded model is used to perform the experiments. The results of some optimal control experiments are also reported in this paper. These latter experiments are designed to examine the sensitivity of optimal fiscal policies to alternative assumptions about Fed behavior. The main conclusion of this paper is that fiscal policy effects and optimal fiscal policies are quite sensitive to assumptions about the behavior of the Fed.</p> </abstract>
<abstract> <p> For a finite linear distribution lag model the problem of estimating the coefficients when the loss function is quadratic is considered. The explanatory variable is assumed to be generated by an autoregressive or moving average process. A result due to Szegö is used to approximate the latent roots of the sample moment matrix. The approximation is then used in a comparison of the risk of four estimators--unrestricted least squares, restricted least squares, the James-Stein estimator, and the pre-test estimator. The effect of increasing autocorrelation in the explanatory variable on the relative performance of the four estimators is also studied. </p> </abstract>
<abstract> <p>This article considers a two-equation simultaneous equation model in which one of the dependent variables is completely observed and the other is observed only to the extent of whether or not it is positive. A class of generalized least squares estimators are proposed and their asymptotic variance-covariance matrices are obtained. The estimators are based on the principle which is applicable whenever the structural parameters need to be determined from the estimates of the reduced form parameters.</p> </abstract>
<abstract> <p>This paper explores the extent to which standard, general equilibrium analysis of Pareto optima and of competitive equilibria can be applied to environments with moral hazard and adverse selection problems. Allowing for lotteries, contracts with random components, we first establish that an adverse-selection insurance economy, a moral-hazard insurance economy, a signaling economy, and a private-information labor market economy are all special cases of a simple, general structure. We then show that techniques for characterizing Pareto optimal contracts as solutions to concave programming problems are useful and nice and appear to be broadly applicable; allowing for lotteries, we show how to characterize the optimal allocations for the adverse-selection insurance and labor market economies. We then show that standard existence and optimality theorems for competitive equilibria apply in the linear space containing lotteries if agents with characteristics which are distinct and privately observed at the time of initial trading enter the economy-wide resource constraints in a homogeneous way (other kinds of diversity are not critical). For economies with moral hazard which satisfy the homogeneity condition, competitive contract markets single out a subset of the optima and thus can be consistent with apparent unemployment and with a random allocation of labor supplied though all households are averse to risk. The adverse-selection insurance and signaling economies, however, do not satisfy the homogeneity condition and are difficult to decentralize efficiently with a price system.</p> </abstract>
<abstract> <p>A model of job-matching is considered, in which the set of employees hired by each firm, and the set of jobs accepted by each worker, are endogenously determined, as are the job descriptions settled on by each worker-firm pair. The set of outcomes that are in equilibrium, in the sense of being stable with respect to recontracting, is shown to be nonempty. It is shown that the interests of the firms and workers are polarized over the set of stable outcomes: There exists a firm-optimal stable outcome that is the best stable outcome for every firm and the worst for every worker, and a corresponding worker-optimal stable outcome that is best for every worker and worst for very firm. These results generalize and extend previous results for models of this type, and raise questions about the nature and underlying causes of such polarization of interests.</p> </abstract>
<abstract> <p>This article focuses on the desirability of quantity controls in an economy when the first best optimum is not attainable. Based on the analysis of constrained consumer demand, a formula for the desirability of small personalized compensated quotas is first established. Conclusions for the desirability of anonymous quotas: redistribution in kind, rationing, are derived. Actions on prices through taxes and quantity controls are compared. The analysis is also shown to have implications for the theory of optimum income taxation.</p> </abstract>
<abstract> <p>Recent work in game theory has shown that, in principle, it may be possible for firms in an industry to form a self-policing cartel to maximize their joint profits. This paper examines the nature of cartel self-enforcement in the presence of demand uncertainty. A model of a noncooperatively supported cartel is presented, and the aspects of industry structure which would make such a cartel viable are discussed.</p> </abstract>
<abstract> <p>This paper investigates the performances of dynamic econometric models in relationship to their size. More precisely, the central issue addressed in this paper is whether there exists a procedure that systematically associates with every large-scale model a small-scale model that constitutes a reasonably good approximation of the large-scale model. Such a procedure is shown to exist for both endogenous and exogenous variables. This result is applied to show that a model with approximately twenty endogenous and four hundred exogenous variables can do almost as well as the current models with thousands of variables. This result also implies that a necessary condition for small models of twenty to thirty variables to perform satisfactorily is that only regular patterns of variations for the exogenous variables be considered.</p> </abstract>
<abstract> <p>This paper provides general conditions which ensure consistency and asymptotic normality for the nonlinear least squares estimator. These conditions apply to time-series, cross-section, panel, or experimental data for single equations as well as systems of equations. The regression errors may be serially correlated and/or heteroscedastic. For an important special case, we propose a new covariance matrix estimator which is consistent regardless of the presence of heteroscedasticity or serial correlation of unknown form. We also give some new tests for model misspecification, based on the information matrix testing principle.</p> </abstract>
<abstract> <p>We consider the nature of the inferences that can be made when all variables in a linear regression are measured with error. Assuming that the measurement errors are orthogonal to each other and the unobserved correctly measured regressors, we demonstrate that the true regression coefficient vector can be restricted to the convex hull of all possible regressions iff all these regressions yield coefficient vectors lying in the same orthant. Otherwise, the set of feasible coefficient vectors is unbounded. For the unbounded case, we demonstrate that prior information concerning the "seriousness" of the measurement errors in the variables can bound the feasible region. Two diagnostics are proposed to indicate the sensitivity of conventional inferences to measurement error in the regressors, and an illustrative example is presented.</p> </abstract>
<abstract> <p>Exact expressions are given for the first two moments of a linear combination of the elements of an instrumental variables estimator for the coefficients of the endogenous variables in a general structural equation. These results generalize previous exact results for equations containing just two or three endogenous variables. In addition, we provide bounds on the moments that can easily be estimated, and generalize some earlier qualitative results for these estimators.</p> </abstract>
<abstract> <p>Conventional analyses of single spell duration models control for unobservables using a random effect estimator which the distribution of unobservables selected by ad hoc criteria. Both theoretical and empirical examples indicate that estimates of structural parameters obtained from conventional procedures are very sensitive to the choice of mixing distribution. Conventional procedures overparameterize duration models. We develop a consistent nonparametric maximum likelihood estimator for the distribution of unobservables and a computational strategy for implementing it. For a sample of unemployed workers our estimator produces estimates in concordance with standard search theory while conventional estimators do not.</p> </abstract>
<abstract> <p>This paper proposes the residual-based stochastic predictor as an alternative procedure for obtaining forecasts with a static nonlinear econometric model. This procedure modifies the usual Monte Carlo approach to stochastic simulations of the model in that calculated residuals over the sample period are used as proxies for disturbances instead of random draws from some assumed parametric distribution. In comparison with the Monte Carlo predictor, the residual-based should be less sensitive to distributional assumptions concerning disturbances in the system. It is also less demanding computationally. The large-sample asymptotic moments of the residual-based predictor are derived in this paper and compared with those of the Monte Carlo predictor. Both procedures are asymptotically unbiased. In terms of asymptotic mean squared prediction error (AMSPE), the Monte Carlo is efficient relative to the residual-based when the number of replications in the Monte Carlo simulations is large relative to sample size. This order of relative efficiency is reversed, however, when replication and sample sizes are similar. In any event, the amount by which the AMSPE of either predictor exceeds the low bound for AMSPE is small as a percentage of the lower bound AMSPE when sample and replication sizes are at least of moderate magnitude. The paper also discusses the extension of the residual-based and Monte Carlo procedures to the estimation of higher order moments and cumulative distribution functions of endogenous variables in the system.</p> </abstract>
<abstract> <p>Recent micro-simulation studies of the demand for electricity by residences have attempted to model jointly the demand for appliance and the demand for electricity by appliance. Within this context it becomes important to test the statistical exogeneity of appliance dummy variables typically included in demand for electricity equations. If, as the theory would suggest, the demand for durables and their use are related decisions by the consumer, specifications which ignore this fact will lead to biased and inconsistent estimates of price and income elasticities. The present paper attempts to test this bias using a subsample of the 1975 survey of 3249 households carried out by the Washington Center for Metropolitan Studies (WCMS) for the Federal Energy Administration. We discuss and derive a unified model of the demand for consumer durables and the derived demand for electricity. To determine the magnitude of the bias resulting from estimating a unit electricity consumption (UEC) equation by ordinary least squares when unobserved factors influence both choice of appliances and intensity of use, we introduce and estimate a joint water-heat space-heat choice model, and conclude with the consistent estimation and specification of demand for electricity equations.</p> </abstract>
<abstract> <p>Hours of work equations are derived from the constrained maximization of a utility function that is not separable over time and that varies with a household's demographic structure. these equations are fitted to observations on wage rates, nonlabor income, and other variables for husbands, wives, and unmarried women. The dependence of each household's current work and consumption behavior upon its work and consumption behavior in the past is examined closely and different interpretations of the relationship are confronted with the data.</p> </abstract>
<abstract> <p>An exogenous switching regression model with imperfect regime classification information is specified and applied to a study of cartel stability. An efficient estimation method is proposed which takes this imperfect information into account. The consequences of misclassification are analyzed. The direction of the least squares bias is derived. An optimal regime classification rule is obtained and compared theoretically and empirically with other classification rules. We then examine the Joint Executive Committee, a railroad cartel in the 1880's. The econometric evidence indicates that reversions to noncooperative behavior did occur for the firms in our sample, and these reversions involve a significant decrease in market price.</p> </abstract>
<abstract> <p>This paper considers the interpolation of percentiles when the value of the c.d.f. F(x) is specified at n points. Upper and lower bounds on the percentile are obtained assuming that data is generated by a unimodal density. Sharper bounds are derived when the average of the observations in each interval is given. In addition to improving the standard linear interpolation, our results indicate that much information can be gained by reporting group means as well as frequency counts.</p> </abstract>
<abstract> <p>We often see that the F test is applied to testing significance of a subset of coefficients even in a structural equation. This is obviously a doubtful method because the sum of squared errors is not distributed as χ &lt;sup&gt;2&lt;/sup&gt; in a simultaneous equation system. It is known, however, that the likelihood ratio test is asymptotically distributed as χ &lt;sup&gt;2&lt;/sup&gt; with proper degrees of freedom. We analyze the asymptotic properties of these two kinds of test statistics. We find the likelihood ratio method associated with limited information maximum likelihood estimation is reliable in practice.</p> </abstract>
<abstract> <p>The paper uses a generalized Nash bargain to analyze input levels, profits, and wages in the absence of binding contracts, and compares these with the conventional binding contracts model. It is shown that if the union has any power, investment is lower in the absence of binding contracts. The associated input levels and shareholders' profits are identical to those that emerge if contracts are binding and the firm acts as if it faces a cost of capital which is a linear combination of the purchase price of capital and the resale value. This implicit cost is greater than the purchase price, is an increasing function of union power, and is independent of the profit function and the alternative wage. Increases in union power reduce shareholders' profits but may increase wages at some points and decrease wages at others. In the absence of binding contracts, shareholders' profits are lower but there is a critical level of union power (depending on the profit function) such that the union is worse off if its power is higher than this level and better off if it is lower.</p> </abstract>
<abstract> <p>A generalization of the Nash bargaining solution is defined for two-person bargaining problems with incomplete information. These solutions form the smallest set satisfying three axioms: a probability-invariance axiom, an extension (or independence of irrelevant alternatives) axiom, and a random-dictatorship axiom. A bargaining solution can also be characterized as an incentive-compatible mechanism that is both equitable and efficient in terms of some virtual utility scales for the two players.</p> </abstract>
<abstract> <p>How potential entrants to an open-access fishery form their expectations determines the fishery's adjustment path to a steady state but not the steady state values themselves. It is well known that, in the standard model with myopic expectations (those based on current values), boats enter the fishery only when the fish stock is greater than its steady state stock. We show that, with rational expectations (perfect foresight), however, boats may enter when the fish stock is much lower than its steady state value if the boat fleet is sufficiently small. This paper contrasts myopic and rational expectations within a general dynamic model of an open-access fishery.</p> </abstract>
<abstract> <p>This paper develops a general, competitive model of commodity differentiation. The structure analyzed is sufficiently rich to admit the basic structures of many of the common models of commodity differentiation as special cases. Thus, the model provides a unifying framework within which alternative formulations of strategic product choice can be compared. It is shown that competitive equilibria exist under only mild restrictions on the underlying economic structure and vary continuously with endowments. Finally, some results relevant to all models of commodity differentiation featuring price taking consumers are presented. The results are shown to point to some potentially important methodological restrictions.</p> </abstract>
<abstract> <p>This paper investigates the implications for asset price dispersion of conventional security valuation models. Successively sharper variance bounds on asset prices are derived. Large-sample tests of the bounds are determined and applied to aggregated and disaggregated price and earnings data of U.S. corporations.</p> </abstract>
<abstract> <p>This paper analyzes a model of a futures market in which both pure speculators and producers participate. Traders form rational expectations about the return on holding futures (the spot price) and the amount they will produce, on the basis of diverse private information and the futures price. Constant absolute risk aversion utility functions and normal distributions are assumed in the model. A set of necessary and sufficient conditions is established for the informational efficiency of the futures market, which is taken to mean that the futures price is a sufficient statistic for information about the spot price. In this model the futures price is not in general a sufficient statistic unless there is information available about only one side of the spot market, in which case the sufficient statistic equilibrium is shown to be the only rational expectations equilibrium in which price is a linear function of the information.</p> </abstract>
<abstract> <p>Given two agents with von Neumann-Morgenstern utilities who wish to divide n commodities, consider the two-person noncooperative game with strategies consisting of concave, increasing von Neumann-Morgenstern utility functions as well as rules to break ties and whose outcomes are some solution to the bargaining game determined by the strategies used. It is shown that, for a class of bargaining solutions which includes those of Nash and Raiffa, Kalai and Smorodinsky, any constrained equal-income competitive equilibrium allocation for the true utilities is a Nash equilibrium outcome for the noncooperative game.</p> </abstract>
<abstract> <p>This paper argues that the traditional Arrow-Pratt measures of risk aversion are generally too weak for making comparisons between risky situations. A new stronger ordering is proposed, and it is applied to some canonical problems in insurance and finance, for which the Arrow-Pratt measures give ambiguous results.</p> </abstract>
<abstract> <p>The paper introduces the concept of a conditionally dispersed endowment distribution (given the consumption set) in the study of regularizing effect of aggregation and existence of equilibria in an exchange economy with an atomless measure space of consumers. It is shown that there is a regularizing effect of aggregation whenever the endowment distribution of an economy is conditionally dispersed. On the basis of this regularizing effect, the existence of a competitive equilibrium is proved in a very general setting where some of the commodities are indivisible and where different consumers may have different consumption sets.</p> </abstract>
<abstract> <p>The theory of duality has been an extremely useful tool in the analysis of the standard models of consumer and producer behavior. This paper describes an extension of the theory to a wider class of problems of static optimization. The generalized duality theory is then applied to the integrability question in fairly general optimization models. A major gap in the comparative statics of optimization models is partially closed.</p> </abstract>
<abstract> <p>This paper presents a general framework for the analysis of programs over an infinite horizon in continuous time. Sufficient conditions for the existence of an optimal program are derived and are shown to reduce to the condition that the underlying preference ordering exhibit impatience in a topology determined by the underlying technology.</p> </abstract>
<abstract> <p>The last section of this paper presents a rigorous version of Tiebout's theory of local public goods. It is shown that equilibria exist and are Pareto optimal. This rigorous theory follows closely the more rigorous part of Tiebout's work. This rigorous theory makes a number of very special assumptions which make local public goods essentially private. The body of this paper presents a series of examples, which show that if one tries to generalize the rigorous version of Tiebout's theory in a number of interesting directions, then equilibria may no longer exist or may not be Pareto optimal. The conclusion is that Tiebout's idea does not lead to a satisfactory general theory of local public goods.</p> </abstract>
<abstract> <p>The expected sample autocorrelation function of residuals from regression of a random walk on time is shown to imply strongly pseudo-periodic behavior in the "detrended" series. The shape of the autocorrelation function is effectively independent of sample size and the corresponding spectral density function has a single peak at a period equal to .83 of sample size; thus the apparent dynamic properties of the residuals are artifactual. Sampling experiments are used to describe the distribution of the spectral peak and further suggest that nonzero autocorrelation in first differences of the raw data will have little effect on the spurious periodicity phenomenon.</p> </abstract>
<abstract> <p> This paper investigates the distribution of the least squares estimator of the coefficient α in the model @c"t = @a@c"t -"1 + @?"t where the @?"t where the @?"t are independently distributed N (O, @s^2). The exact finite sample and limiting distributions are calculated when α ≥ 1 and finite sample distributions when α &lt; 1. These distributions are used to compute the power functions of tests of the random walk hypothesis α = 1 as well as the hypotheses. </p> </abstract>
<abstract> <p>Several procedures are proposed for testing the specification of an econometric model in the presence of one or more other models which purport to explain the same phenomenon. These procedures are shown to be closely related, but not identical, to the nonnested hypothesis tests recently proposed by Pesaran and Deaton [7], and to have similar asymptotic properties. They are remarkably simple both conceptually and computationally, and, unlike earlier techniques, they may be used to test against several alternative models simultaneously. Some empirical results are presented which suggest that the ability of the tests to reject false hypotheses is likely to be rather good in practice.</p> </abstract>
<abstract> <p>Support prices are derived for weakly maximal paths in an optimal growth model which is time dependent but without uncertainty. The notion of "reachable" stocks and paths is defined and used to derive turnpike theorems by the value loss method. The proofs do not depend on the presence of optimal balanced paths nor on the usual transversality conditions. The theorems are extended to the classical model which has a non-trivial von Neumann facet.</p> </abstract>
<abstract> <p>Global necessary conditions are obtained for a discrete capital version of the putty-clay model first introduced by Johansen [7]. Convergence of the optimal solution to a steady state is discussed for concave utilities. Also, the role of obsolescence is analyzed when utility is linear</p> </abstract>
<abstract> <p>A logically consistent specification of the adaptive expectations hypothesis in continuous time is derived from an underlying discrete time model. We distinguish between (i) the time interval between predictions and (ii) the time horizon over which predictions are made. Taking limits of the expectation equation as these time intervals approach zero, we derive a mixed difference-differential equation and a mixed total-partial differential equation, respectively, describing actual changes. When these are combined with other equations in an economic model, the expectation mechanism provides a simultaneous determination of both expected and actual dynamics. New results are obtained in three separate applications: (i) the short-run stability of multi-asset markets, (ii) a heterogeneous capital goods model, and (iii) a Phillips curve model of wage-price inflation.</p> </abstract>
<abstract> <p> Previous work on non-tâtonnement processes has allowed only trading of titles to commodities or promises to produce to take place out of equilibrium. The present work allows production and consumption to take place. The basic model used is that of the Hahn Process, since the making of irreversible commitments in production and consumption seems especially suited to a model whose basic feature is the decline of target profits and utilities. The attempt to introduce production and consumption out of equilibrium brings to the fore a number of problems implicit in most stability analysis which must now be explicitly faced. </p> </abstract>
<abstract> <p>An established seller's pre-entry price policy is studied under the assumption that this policy affects the probability of entry and that rivals need an entry lag to make their entries effective. The analysis is conducted using a modified Kamien-Schwartz limit pricing model.</p> </abstract>
<abstract> <p>This is an analytical paper generalizing the standard input-output (IO) model on the one hand and the equally standard method of solving the model by means of power series expansion, on the other. The IO coefficients here are allowed to vary with the endogenous variables in a general fashion, and the model analysis is approached via the iterative scheme, referred to above. The analysis includes (i) detailed and rigorous examination of the properties of the scheme, (ii) clarification of the notion of viability, and (iii) derivation of suitable viability conditions in the present generalized context and their comparison with the corresponding results in the linear model. Finally, the economic interpretation of the computational procedure in terms of a planning process of the "material balance" type is extended to the generalized context. It is shown that this results in a considerable "informational economy" on the part of the central planning authority, which is lacking in the case of the linear model.</p> </abstract>
<abstract> <p>The paper derives a function that describes the size distribution of incomes. The two functions most often used are the Pareto and the lognormal. The Pareto function fits the data fairly well towards the higher levels but the fit is poor towards the lower income levels. The lognormal fits the lower income levels better but its fit towards the upper end is far from satisfactory. There have been other distributions suggested by Champernowne, Rutherford, and others, but even these do not result in any considerable improvement. The present paper derives a distribution that is a generalization of the Pareto distribution and the Weibull distribution used in analyses of equipment failures. The distribution fits actual data remarkably well compared with the Pareto and the lognormal.</p> </abstract>
<abstract> <p>In this paper we provide a statement of the relationship between the weak axiom of revealed preference (WA) and the negative semidefiniteness of the matrix of substitution terms (NSD). As a corollary we determine the relation between WA and the strong axiom of revealed preference (SA). The latter is equivalent to NSD and the symmetry of the matrix of substitution terms. The former, WA, implies NSD but is not implied by NSD. Also, WA is implied by the condition that the matrix of substitution terms is negative definite (ND), but it does not imply ND. Application of these results yield an infinity of demand functions which satisfy WA but not SA.</p> </abstract>
<abstract> <p>A representative consumer exists if market behavior corresponds to a representative income or utility level which is a function of the income distribution. Necessary and sufficient conditions are given on micro behavior and macro behavior (whether maximizing or not) for a representative consumer to exist. Nonlinear Engel curves and taste differences are permitted. If the representative income level is restricted to be mean income, we obtain the traditional linear Engel curves solution. A striking result on economy of information in the representation of a social welfare function is given.</p> </abstract>
<abstract> <p>Independence of irrelevant alternatives imposes three conditions on the social ordering of any subset U of social states: By individualism the ordering depends only on individual tastes, by ordinalism it depends only on preference orderings, and by localism it does not depend on states outside U. We keep individualism and replace ordinalism by the assumption that individuals have interval utility functions. If we also keep localism we still get an impossibility theorem (the main result). If we relax localism as well we can get a social welfare function. But even a minimal relaxation of localism makes the social ordering of each two states (x, y) depend on utilities for n other states, where there are n individuals.</p> </abstract>
<abstract> <p>Recent papers by Houthakker [3 and 4], Samuelson [8 and 9], Sir John Hicks [2], and others deal with the question of the existence of a nontrivial preference ordering which exhibits the same mathematical properties in terms of both the direct and indirect utility functions. It is shown that homogeneity and separability are compatible with both the direct and indirect utility functions,but that direct and indirect additivity is consistent with only limited classes of utility functions. Samuelson has raised the question of whether there exists a nontrivial self-dual preference ordering which requires more stringent conditions than homogeneity and separability. By a self-dual preference is meant a preference ordering such that the direct utility function is exactly identical with the corresponding indirect utility function The purpose of this paper is to present a complete solution to the problem of self-duality. First, we elaborate on the necessary and sufficient conditions for an "exactly" or "strongly" self-dual utility function (in Samuelson's sense). Then, using some well-known concepts of the continuous group theory of transformations, we study the case of "weakly" self-dual preference orderings and give a precise formulation of the concept of the "same" mathematical form. Some special classes of self-dual preferences are subjected to detailed analysis.</p> </abstract>
<abstract> <p>A choice function, which maps each set of alternatives in a domain of feasible sets into a non-empty subset of itself (called the choice set), is said to be representable by a weak order if some weak order on the alternatives has maximum elements within each feasible set, all of which are in the choice set of that feasible set. A Partial Congruence Axiom ("every non-empty finite collection of feasible sets has an alternative which is in the choice set of every feasible set in the collection which contains that alternative") is shown to be necessary and sufficient for weak order representability when all choice sets are finite. A stronger form of partial congruence is proved to be necessary and sufficient for weak order representability when the number of feasible sets is countable, regardless of the cardinalities of the choice sets. The general case of arbitrary cardinalities for the domain and the choice sets is presently unsettled.</p> </abstract>
<abstract> <p>This paper presents a Bayesian analysis of a single equation from a simultaneous equations system. The analysis is carried out under "limited information" because no prior information (other than a list of endogenous and exogenous variables) is introduced on the parameters of the remaining equations in the in the system. These parameters are integrated out analytically. The equation of interest may or may not be identified by means of exact a priori information; probabilistic prior information is equally acceptable. The prior density is either of the non-informative or the natural conjugate type. The kernel of the posterior density for the regression coefficients is a ratio of t kernels. The existence of posterior moments is ascertained. This approach is applied for illustrative purposes to Tintner's model of the meat market.</p> </abstract>
<abstract> <p>The parameters of dynamic simultaneous equation models are often estimated using methods which are appropriate only when the errors of the equations are serially independent. The purpose of this paper is to propose a large sample test for serial correlation to replace the invalid Durbin-Watson test. The test requires only simple calculations and can be easily added to standard two-stage least squares/instrumental variables programs. The treatment of serial correlation is discussed. An example is given to illustrate the test procedure.</p> </abstract>
<abstract> <p>Two economies with different propensities to save but the same overall capital coefficients are considered under a number of alternative degrees of dependence upon foreign sources for the supply of producers' goods. Based upon a closed dynamic linear model in 52 variables, the exercise is intended to show that, where the assumptions made permit of a well-defined equilibrium path,the two economies will eventually grow at the same proportionate rate despite their different potentialities. Under assumptions which make this common rate of growth rather close to the one prevailing in the high-saving economy considered in isolation, the gross national product of the low-saving economy will settle down to a small fraction of the gross national product of the high-saving economy.</p> </abstract>
<abstract> <p>The present paper gives an experimentally oriented theory of decision making. It is shown that the axioms lead to the separate measurement of subjective probability and utility.</p> </abstract>
<abstract> <p>The relationship between extrapolative expectations and dynamic stability is studied in the context of a multiple market system. Metzler's theorem on the Hicksian stability conditions is extended to include a simple expectations function. It is shown that a stable dynamic system can absorb the effects of some extrapolation of price movements and remain stable.</p> </abstract>
<abstract> <p>The paper presents a general theoretical framework for the analysis of integrated life-cycle models of consumption and family labor supply under uncertainty. Profit functions are used to represent intertemporally additive preferences and to yield convenient characterizations of "constant marginal utility of wealth" or "Frisch" demand functions. Conditions on preferences derived that allow additive fixed-effect specifications for the Frisch demands. Data from the British Family Expenditure Surveys from 1970-77 are used to derive panel-like information on male labor supply and consumption for several age cohorts over time. These data reproduce standard life-cycle patterns of hours and wages, but more detailed analysis shows that the theory is incapable of offering a satisfactory common explanation of the behavior of hours and wages over both the business cycle and the life cycle. Similarly, although the theory can explain the life-cycle behavior of hours and consumption separately, the same model cannot explain both, essentially because of a failure in symmetry.</p> </abstract>
<abstract> <p>We study the problem of optimal pricing for a bundle of services characterized by two attributes (e.g., quantity and quality) and subject to capacity limitations or peakloading. An application is to services that take the form of a load-duration curve. Using separability assumptions on the demand and cost functions, we derive the optimal pricing policy for a monopolist seller. An example is solved completely.</p> </abstract>
<abstract> <p>A tractable dynamic general equilibrium model of continuous product innovation is developed. Patents, or any imitation lag, of infinite duration may achieve too much, too little, or the socially optimum level of innovation. Most surprising, finite-life patents may induce undamped oscillations in innovation.</p> </abstract>
<abstract> <p>This paper examines the existence conditions of an oligopolistic equilibrium. We use the Cournot concept (the strategies are the production levels) in a partial equilibrium framework. We allow for U-shaped average cost curves and nonidentical firms. We show that an equilibrium exists if the demand is large. The main part of the proof is a new fixed point theorem for a class of noncontinuous mappings.</p> </abstract>
<abstract> <p>Shapley's Non-transferable Utility Value correspondence is characterized by a set of axioms, which combine the features of the axioms for the value of transferable utility games, and those for Nash's solution to the bargaining problem. The axioms refer to values as payoff vectors only--the comparison weights associated with a value make no explicit appearance. A key axiom is conditional additivity, which may be stated as follows: If the same payoff vector is a value of each of two games, then it is also a value of the half-half probability combination of these two games, unless it is Pareto-dominated there. For the axioms to work, the boundary of the set of feasible outcomes must be smooth.</p> </abstract>
<abstract> <p>The existence of perfect equilibrium is demonstrated for a class of games with compact space of histories and continuous payoffs, and in which the set of actions feasible at any given period is a lower hemicontinuous correspondence of the previous history of the game. The proof is by construction. A set of histories is constructed, each of which is the equilibrium path of some perfect equilibrium point of the game. Also, any equilibrium path is a member of this set. The construction therefore provides a characterization of perfect equilibrium.</p> </abstract>
<abstract> <p>This paper finds in closed form a noisy rational expectations equilibrium for a class of economies with many risky assets and analyzes the properties of such equilibria. Because of the various interactions between the assets, phenomena appear that do not arise in models with a single risky asset. For example, an asset's equilibrium price might be decreasing in its own payoff and/or increasing in its own supply; an asset might be a Giffen good; a higher price for an asset (holding other prices fixed) might be "bad news" for the asset's payoff; and even for assets in fixed supply, uncertainty about other assets' supplies may prevent their prices from being fully revealing.</p> </abstract>
<abstract> <p>In this paper we consider the computation of an equilibrium in a pure exchange economy with a block diagonal pattern. Such a structure appears in, e.g., international trade models. Utilizing the block diagonal pattern, the full equilibrium problem is reformulated as a problem on the product space of several lower-dimensional price-simplices. Some numerical results show the usefulness of this method.</p> </abstract>
<abstract> <p>This paper surveys some recent studies of economies where trading takes place sequentially over time, and where each agent makes decisions at every date in the light of his expectations about his future environment, which are functions of his information on the present and past states of the economy. The paper reviews particularly the issues raised by arbitrage in capital markets, by the consideration of money and banking activities, and by the introduction of production in temporary competitive equilibrium models. A thorough investigation of the logic of temporary equilibrium models with quantity rationing is also offered, as well as a quick review of the study of stochastic processes of temporary equilibria.</p> </abstract>
<abstract> <p>This paper presents a general equilibrium model which exhibits non-Walrasian equilibria. In Walrasian models economic agents act only on the basis of relative prices. In this model agents take quantity signals into account as well: firms maximize profits subject to a constraint on expected sales, and consumers maximize utility subject to a constraint on realized income. With such a specification it is shown that a non-Walrasian equilibrium will generally exist. Furthermore, the non-Walrasian equilibrium will be a stable equilibrium of an appropriate dynamical system while the Walrasian equilibrium will be unstable.</p> </abstract>
<abstract> <p>The fact that preference maximizing consumers generate aggregate excess demand is utilized to prove (i) a statement on the values of the excess demand correspondence and (ii) that the economies having an excess demand function are dense in the set of all economies. This is applied to get a straightforward proof for the existence of an equilibrium distribution.</p> </abstract>
<abstract> <p>This paper treats dynamic optimization problems from the point of view of programming in infinite dimensional spaces. Sufficient conditions (of a dominant diagonal nature) for smooth dependence of optimal paths on initial conditions and other parameters are given. It is also shown that the smooth dependence and the so called "turnpike property" are very closely related. This relationship is used to prove a turnpike theorem for a model that may be interpreted as a multisectoral model of optimal growth with positive discounting and to show that the turnpike property is kept under "small" perturbations.</p> </abstract>
<abstract> <p>This study introduces ordinally additive, ordinally linear, and ordinally Cobb-Douglas utility functions for the analysis of risky decisions when the uncertainty affects several attributes. Practical algorithms for the determination of utility functions with these forms are provided. Further, the study offers several risk invariance axioms on choice behavior under multidimensional risk. These axioms, for the first time, extend to the multidimensional context the heuristic correspondence between risk aversion and subjective wealth, heretofore familiar in only one dimension. In addition, the consequences of these new risk invariance axioms for utility functional forms in the multi-dimensional context are investigated. The result is a sequence of theorems which show that ordinally linear, ordinally Cobb-Douglas, and ordinally additive von Neumann-Morgenstern utility functions are characterized by the risk invariance axioms.</p> </abstract>
<abstract> <p>We consider a theoretical model of a consumer who faces a price that varies with the number of units bought, and who faces random future changes in his demand for the good. an example is cumulative deductibles in health insurance policies. The problem is treated as a dynamic program involving medical demand under an insurance policy with a deductible. In this model, the perceived price of care falls (following a nonlinear path) as the consumer approaches the deductible. The model suggests: (i) Because demand and administrative costs are likely to be insensitive to the size of the deductible above a certain range, deductibles above that range will not be optimal; they add risk with no return. (ii) Demand estimates will be biased if insurance policies in the sample contain deductibles and if the dependent variable is annual medical demand. (iii) Demand analysis by episode of illness is the appropriate framework in such circumstances.</p> </abstract>
<abstract> <p>The Shapley value is shown to be a von Neumann-Morgenstern utility function. The concept of strategic risk is introduced, and it is shown that the Shapley value of a game equals its utility if and only if the underlying preferences are neutral to both ordinary and strategic risk.</p> </abstract>
<abstract> <p>A decision scheme makes the probabilities of alternatives depend on individual strong orderings of them. It is strategy-proof if it logically precludes anyone's advantageously misrepresenting his preferences. It is unilateral if only one individual can affect the outcome, and duple if it restricts the final outcome to a fixed pair of alternatives. Any strategy-proof decision scheme, it is shown, is a probability mixture of schemes each of which is unilateral or duple. If it guarantees Pareto optimal outcomes, it is a probability mixture of dictatorial schemes. If it guarantees ex ante Pareto optimal lotteries, it is dictatorial.</p> </abstract>
<abstract> <p>This paper reports the testing of hypotheses concerning: (i) whether the household is better viewed as planning over a single-period versus a multiperiod horizon; (ii) whether the household is better viewed as planning in a single-asset or a multiasset framework; (iii) the relative importance of substitution and wealth effects as sources of change in the stock demand for automobiles. The findings are that a multiperiod, multiasset model best describes stock demand, that the separation theorem which implies a zero wealth effect is rejected, and that substitution effects are seven times more important than wealth effects.</p> </abstract>
<abstract> <p>This paper examines the durability of capital goods produced under different market structures when tax considerations are included. Since investment tax credit and depreciation allowances are realized by the owner of the durable good, the durability of products produced by an industry which sells its output differs from that of an industry which rents. For each of these two commercial forms we consider both monopolistic and competitive market structure. Potential gains from different forms of regulation are discussed.</p> </abstract>
<abstract> <p>The Lorenz curve is widely used as a convenient graphical device to represent the size distribution of income and wealth. In the present paper the concept of the Lorenz curve has been extended and generalized to study the relationships among the distributions of different economic variables. Some theorems have been proved which have a large number of economic applications.</p> </abstract>
<abstract> <p>This paper is concerned with the class of homogeneous programming problems. It is shown, to assure the existence of a saddlepoint for the associated Lagrangian, that some restrictions must be imposed on the degrees of homogeneity of the functions. Some properties of the perturbation function are also demonstrated.</p> </abstract>
<abstract> <p>This paper deals with the theoretical development of some aspects of the trend removal problem. The objective is to show the difference between the two most popular trend removal methods: first differences and linear least squares regression. On the one hand, we show that if first differences are used to eliminate a linear trend, the series of residuals would be stationary but would not be white noises as they contain a first lag autocorrelation of -0.50. Furthermore, the spectral density function (SDF) of these residuals relative to that of a white noise series would be exaggerated at the high frequency portion and attenuated at the low frequency portion. On the other hand, we show that the regression residuals from the linear detrending of a random walk series would contain large positive autocorrelations in the first few lags. Relative to that of white noises, the SDF of the regression residuals would be exaggerated at the low frequency portion and attenuated at the high frequency portion.</p> </abstract>
<abstract> <p>This paper considers (i) the robustness of the @t and Durbin-Watson bounds tests for first-order autocorrelation when disturbances in the linear regression model are heteroskedastic and (ii) the robustness of the Goldfeld-Quandt and Glejser tests for heteroskedasticity when the disturbances follow a first-order autoregressive scheme.</p> </abstract>
<abstract> <p>Despite over two decades of U.S. experience in operating large scale subsidized training programs for low income and unemployed workers, the effects of these programs are still highly controversial. The controversy arises from the difficulty of specifying the model of participant outcomes in the absence of training that is necessary in any nonexperimental program evaluation. In this paper we suggest that some of these difficulties may be overcome by focusing on a very simple measure of outcomes: namely, the probability of employment. We present a variety of estimates of the effect of training on the probability of employment for the 1976 cohort of adult male participants in the Comprehensive Employment and Training Act (CETA) program. Our methods range from a simple comparison of pre- and post-training employment probabilities between trainees and nonparticipants, to a fully specified first-order Markov model of employment probabilities with individual heterogeneity. There is consistent evidence across methods that CETA participation increased the probability of employment in the three years after training by from 2 to 5 percentage points. Classroom training programs appear to have had significantly larger effects than on-the-job programs, although the estimated effects of both kinds of programs are positive. We also find that movements in and out of employment for trainees and nonparticipants are reasonably well described by a first-order process, conditional on individual heterogeneity. In the context of this model, CETA participation appears to have increased both the probability of moving into employment, and the probability of continuing employment.</p> </abstract>
<abstract> <p>In a somewhat general context, and in a variety of special cases, we calculate the order in probability of the difference between consistent roots of rival estimating equations, with application to point estimators in parametric and nonparametric models, interval estimators, and test statistics. The emphasis is on comparison of statistics having the same first-order asymptotic distribution, our results indicating the degree to which their stochastic expansions correspond. Differences in the statistical performances of various commonly-used iterative procedures are detected. We discuss implications of our results for higher-order efficiency comparisons, and for matching the first-order efficiency of an implicitly defined target statistic in finitely many iterative steps, commenced by an estimator that is consistent but not T^1^/^2 -consistent, justifying as suitable initial estimators ones obtained via a search of the objective function, and nonparametric methods. Some of our results are applied to the linear-in-variables simultaneous equations system.</p> </abstract>
<abstract> <p>The paper introduces a class of alternating-move infinite-horizon models of duopoly. The timing is meant to capture the presence of short-run commitments. Markov perfect equilibrium (MPE) in this context requires strategies to depend only on the action to which one's opponent is currently committed. The dynamic programming equations for an MPE are derived. The first application of the model is to a natural monopoly, in which fixed costs are so large that at most one firm can make a profit. The firms install short-run capacity. In the unique symmetric MPE, only one firm is active and practices the quantity analogue of limit pricing. For commitments of brief duration, the market is almost contestable. We conclude with a discussion of more general models in which the alternating timing is derived rather than imposed. Our companion paper applies the model to price competition and provides equilibrium foundations for kinked demand curves and Edgeworth cycles.</p> </abstract>
<abstract> <p>We provide game theoretic foundations for the classic kinked demand curve equilibrium and Edgeworth cycle. We analyze a model in which firms take turns choosing prices; the model is intended to capture the idea of reactions based on short-run commitment. In a Markov perfect equilibrium (MPE), a firm's move in any period depends only on the other firm's current price. There are multiple MPE's, consisting of both kinked demand curve equilibria and Edgeworth cycles. In any MPE, profit is bounded away from the Bertrand equilibrium level. We show that a kinked demand curve at the monopoly price is the unique symmetric "renegotiation proof" equilibrium when there is little discounting. We then endogenize the timing by allowing firms to move at any time subject to we short-run commitments. We find that firms end up alternating, thus vindicating the ad hoc timing assumption of our simpler model. We also discuss how the model can be enriched to provide explanations for excess capacity and market sharing.</p> </abstract>
<abstract> <p>Recently, attention has been given to a model of two-person bargaining in which the parties alternate making offers and there is uncertainty about the valuation of one party. The purpose of the analysis has been to identify delay to agreement with a screening process, where agents with relatively lower valuations distinguish themselves by waiting longer to settle. We point out a fundamental difficulty with this program by demonstrating that the assumptions used in the literature allow for delay only in so far as the time between offers is significant.</p> </abstract>
<abstract> <p>It is well-known that capital can be aggregated in an economy only under very restrictive conditions. What is not so widely understood is that these same problems exist when aggregating goods that are efficiently allocated in an economy. For example, suppose there are many types of labor in an economy and each type of labor is efficiently allocated among firms. To form a labor aggregate in the economy requires that certain aggregation restrictions be satisfied; these are the subject of this paper. In this paper, we make two contributions to the literature on aggregating inputs and outputs in economy production functions. First, we present necessary and sufficient conditions for aggregating efficiently allocated goods under general assumptions on the technology. Second, we present a general theorem on the conditions for the existence of aggregates of both efficiently allocated goods and arbitrarily allocated goods. We use this theorem to elucidate the role of efficiency in determining the existence of aggregates.</p> </abstract>
<abstract> <p>The Helpman-Razin model of international trade under uncertainty is extended to allow for country specific productivity shocks. It is shown that for the multiple-sector multiple-factor case there exists a set of interesting sufficient conditions under which it is possible to predict the pattern of international trade in securities, goods, and factor content, on the basis of cross-country differences in factor endowments. It is shown that under these circumstances some well known empirical tests have to be modified.</p> </abstract>
<abstract> <p>This paper is concerned with the amount of communication that must be provided to implement a performance standard by a mechanism whose stationary messages have the Nash property. We study the question whether a given message space is large enough to implement a given performance standard. In general, an implementing mechanism with the Nash property in messages requires a larger message space than sufficies for decentralized realization without regard to individual incentives. In particular, we study implementation of Walrasian allocations in exchange environments. We show that the smallest message space that implements Walrasian allocations is one of dimension, roughly, n. (l - 1) + l/(n - 1), where l is the number of commodities and n the number of agents We exhibit an implementing mechanism whose message space has that dimension.</p> </abstract>
<abstract> <p>Contributions by Maskin (1977) and Williams (1984b) have established that any social choice correspondence satisfying monotonicity, no veto power, and having at least three participants is Nash implementable, under the proviso that the number of social alternatives is finite and that the social choice correspondence satisfies citizen sovereignty. For an alternative set of arbitrary size, however, Williams' proof requires an additional restriction on the nature of the alternative set. In this paper, I make improvements in two important aspects of their Nash implementation theorems: a significant reduction in the strategy space and a proof for an arbitrary alternative set. In Maskin's and Williams' game forms, each participant announces a preference profile of all participants and also a socially optimal alternative with respect to the announced preference profile. I prove Maskin's theorem using a much smaller strategy space with respect to the preference announcements. Namely, each participant announces only two participants' preferences (i.e., his own preferences and his successor's preferences), an alternative that is not necessarily optimal, and a positive integer not exceeding the number of participants. With this specification of the strategy space, I confirm the theorem for an arbitrary size of the alternative set.</p> </abstract>
<abstract> <p>Rational expectations equilibria are analyzed as incentive compatible mechanisms. The sense in which they can be implemented is made precise. Then, a welfare analysis of these equilibria is carried out within the class of mechanisms having the same incentive compatibility properties and the same communication constraints.</p> </abstract>
<abstract> <p>The rational expectations hypothesis in a life cycle context asserts that agents seek to keep the marginal utility of discounted expenditure constant across time. In this paper we present estimates of a demand system that takes explicit account of this hypothesis. We show how the restrictions from demand theory enable us to identify the model despite the presence of an unobservable variable that can be interpreted as revisions to the marginal utility of discounted expenditure. This allows us to take explicit account of the different effects of anticipated and unanticipated price changes. An important check on our model is the derivation of expenditure elasticities despite the fact that the demand system does not have expenditure on the right hand side. Our estimates are "sensible" and we find that we can reject the hypothesis that revisions to the marginal utility of discounted money are orthogonal to the past values of our variables. We interpret this to be a rejection of the rational expectations hypothesis.</p> </abstract>
<abstract> <p>This paper considers a repeated principal agent relationship where the principal is risk neutral, the agent is risk averse, the principal can borrow or save at a fixed interest rate, and the agent discounts future consumption. It is shown that memory plays a very strong role in every Pareto-optimal contract. Sufficient conditions for Pareto-optimal contracts to exhibit rising or falling wages are identified. Finally, it is shown that the restriction of the agent's access to credit is necessary to achieve a Pareto-optimal outcome. In particular, under every Pareto-optimal contract for every outcome of every period the agent would choose to save some of his wage if he could.</p> </abstract>
<abstract> <p>Much attention has been devoted recently to the problem of implementing an optimal provision of public goods with imperfect information about preferences. The literature studies mechanisms with individual agents directly revealing information about their preferences, and focuses on two types of truthful equilibria: dominant strategy and Bayesian-Nash. We introduce "Stackelberg" mechanisms with truth-telling a dominant strategy for all agents but the first. The first agent plays "before" the other maximizing his expected utility on the assumption that others will reveal their true preferences. We present sufficient conditions for the construction of Stacekelberg mechanisms which yield an efficient provision of public goods, balance the budget, and induce every participant to reveal their true preferences. These results strengthen and extend the known results of the Bayesian-Nash approach.</p> </abstract>
<abstract> <p>Revolution is viewed as a two person game, between Lenin and the Tsar, who compete for support of coalitions of the population. The payoff is the probability of revolution, which Lenin seeks to maximize and the Tsar to minimize. Lenin's strategies are income distribution proposals; the Tsar's strategies are lists of penalties which members of the population will pay should they join Lenin and their bid for revolution fail. The probabilities of revolution depend on the strategies which the two revolutionary entrepreneurs propose. There is an equilibrium pair of strategies; the task is to study with properties it has. In particular, it is shown that various "tyrannical" aspects of the Tsar's strategy, and "progressive" aspects of Lenin's strategy need not flow from ideological precommitments, but are simply good optimizing behavior, given their respective goals in this game. Thus apparently ideological positions of Lenin and the Tsar are provided with microfoundations of a sort. The paper thus aims to: (i) study revolutions as strategic games, and more generally to (ii) be a case study of the rational evolution of apparently ideological behavior.</p> </abstract>
<abstract> <p>The paper proves sufficient conditions for aggregate demand curves to be decreasing in an economy with identical consumers. The restrictions affect the functional form of Engel curves and the shape of expenditures distribution within the economy. It is shown that most of the empirical literature about Engel curves uses functional forms of the type studied here. Eventually, conditions about expenditures distribution are empirically tested.</p> </abstract>
<abstract> <p>This paper reexamines U.S. postwar data to investigate if the observed comovements between money, interest rates, inflation, and output are compatible with the money to real interest to output links suggested by existing monetary theories of the business cycle, which include both Keynesian and equilibrium models. We find these theories are incompatible with the data, and in light of these results, we propose an alternative structural model which an account for the major dynamic interactions among the variables. This model has two central features: (i) output is unaffected by the money supply, and (ii) the money supply process is influenced by policies designed to achieve short-run price stability.</p> </abstract>
<abstract> <p>Different definitions of noncausality (according to Granger, Sims, Pierce and Haugh), are analyzed in terms of orthogonality in the Hilbert space of square integrable variables. Conditions, when necessary, are given for their respective equivalence. Some problems of testability are mentioned. Finally noncausality is also analyzed in terms of "rational expectations," extending previous results of Sims.</p> </abstract>
<abstract> <p>We apply the third-order efficient method of estimation to the estimation problem of a system of structural equations in econometrics. The maximum likelihood estimator (hereafter m.l.e.) of structural equations is proved to give uniformly higher probability of concentration about true values than any regular best asymptotically normal estimator, if its asymptotic bias is properly adjusted. For instance, the full-information or limited-information m.l.e give asymptotically uniformly higher probability of concentration than the three-stage or two-stage least-squares estimators, given that these estimators are adjusted to have the same biases. The same result holds for he subsystem m.l.e. We prove the asymptotic completeness of Fuller's modified estimator. Asymptotic expansions of the distributions of the full-information m.l., subsystem m.l., and limited-information m.l. estimators are derived to terms of order O(T^-1). Our general theorem is also applied to the multi-equation seemingly unrelated regression (SUR) model.</p> </abstract>
<abstract> <p>A constrained n-person game is considered in which the constraints for each player, as well as his payoff function, may depend on the strategy of every player. The existence of an equilibrium point for such a game is shown. By requiring appropriate concavity in the payoff functions a concave game is defined. It is proved that there is a unique equilibrium point for every strictly concave game. A dynamic model for nonequilibrium situations is proposed. This model consists of a system of differential equations which specify the rate of change of each player's strategy. It is shown that for a strictly concave game the system is globally asymptotically stable with respect to the unique equilibrium point of the game. Finally, it is shown how a gradient method suitable for a concave mathematical programming problem can be used to find the equilibrium point for a concave game.</p> </abstract>
<abstract> <p>This article is an econometric study of the relationship between the level of schooling in rural farm areas and income of farm workers. The following shows that more schooling in rural farm areas will accelerate the process of farm outmigration, thus relieving farmers of their poverty.</p> </abstract>
<abstract> <p>The existence of a competitive equilibrium is shown, without using the well-known fixed point technique, under weak gross substitutability. The price domain is limited to the strictly positive orthant in order that the excess demand functions may not reduce to the trivally vanishing one under Walras's law and the gross substitutability conditions. We use instead a boundary condition such that a zero price will result in infinite demand. The case of an open subcone price domain for which the ordinary fixed point theorem is not applicable is also treated.</p> </abstract>
<abstract> <p>The role of price in international trade continues to be a favorite field of investigation by researchers. The debate on the appropriateness of the use of the single-stage least-squares regression method also continues to draw attention. The present paper, however, makes use of this method and reports some results with respect to certain merchandise items of the Indian foreign trade. It thus claims to minimize errors due to aggregative studies relating to total exports and imports of an economy.</p> </abstract>
<abstract> <p>This note gives a brief account of the derivation of a Tchebychev inequality employed elsewhere in a discussion of statistical inference involving economic simultaneous equations models [6, Sec. 4].</p> </abstract>
<abstract> <p>The unsatisfactory state of index number theory is demonstrated in this paper by showing that the theory lacks a consistent set of rules of choice. It is further proved, by an example, that at least two standard rules conflict with the requirements of economic theory. An error in the literature of index numbers is also corrected.</p> </abstract>
<abstract> <p>In this paper the relationship between transitive preference and the structure of the space on which preference is defined is studied in detail. The main results give conditions under which the transivity of indifference is sufficient to prove the transitivity of preference.</p> </abstract>
<abstract> <p>This paper develops an economic theory of replacement investment that can provide a basis for specifying an econometric model of investment behavior. The long-run and short-run effects of changes in the interest rate and in tax laws are examined. The paper also investigates several reasons why the common assumption of a technologically constant rate of replacement is incorrect even as an asymptotic limit.</p> </abstract>
<abstract> <p>A set of independent axioms are exhibited which characterize the price mechanism. The Debreu-Scarf limit theorem on the core of an economy and a category theory viewpoint are employed in the analysis.</p> </abstract>
<abstract> <p>We study an economic model where one group of agents is guided by different prices from those of another group. This situation arises, e.g., in the case of excise taxes, subsidies, and import and export tariffs in international trade. It is established that a decrease in the specified divergence between the equilibrium price vectors implies an increase in welfare in a certain natural sense. The result broadens a conclusion of a classical theorem of welfare economics and answers a question of Foster and Sonnenschein [6]. It also has some bearing on the theory of second best.</p> </abstract>
<abstract> <p>The economic environment (social output) is treated explicitly as a social choice variable. Individual orderings over the set of environments are derived from maximizing behavior. The influence of each individual on the choice made by society is expressed in terms of a power indice which is itself a function of the prevailing environment. These individual power indices give rise to a power rule which provides a social ordering over the set of environments, with the social choice set under any environment being non-empty. The existence of equilibrium in terms of the power rule is established.</p> </abstract>
<abstract> <p> The earlier results of Hahn and Negishi [4] and Arrow and Hahn [1] on non-tâtonnement in pure exchange are extended to the production case. However, just as in pure exchange, consumption does not take place until equilibrium is reached; in the present paper production is also delayed until that date. The result that the sum of target utilities of households can be taken to be a Lyapounov function is preserved; reduction in target utilities comes about partly as a consequence of reduction in target profits expected by firms out of equilibrium. </p> </abstract>
<abstract> <p>Decentralized decision making is consistent if it is executed without cost (i.e., without a loss of output or utility). Consistency requires that the objective function be appropriately structured. In this paper, a hierarchical decision making structure is rationalized by an objective function which combines some of the properties of homothetic separability and asymmetric separability.</p> </abstract>
<abstract> <p>Identification and estimation problems concerning simultaneous equation models which have random parameters are considered, and some results are derived. For instance, a reducibility condition is derived under which the conditions for identification of such a system are identical to those that would be relevant if the parameters were not random. This condition is then weakened to one which relates to the identification and estimation problems of a particular equation in the model. Examples are given. Further generalizations are also considered but only conditional results are given. Further work is suggested.</p> </abstract>
<abstract> <p>This paper looks at some aspects of the three stage least squares approach to estimating simultaneous econometric models. Three stage least squares is derived along the lines of best linear unbiased estimators in classical regression, whereby it is indicated that the usual assumption of non-singularity of the disturbance covariance matrix is unnecessary. Consistency of the estimator is shown as is the irrelevance of exactly identified equations to the estimation of other equations in a model when three stage least squares is used. Also included are an easily computed test for the validity of all specified overidentifying restrictions, and a method of efficiently estimating the reduced form using only the information contained in structural equations that are thought to be well specified.</p> </abstract>
<abstract> <p>An eight equation dynamic model of aggregate demand in the United Kingdom is estimated by a variety of methods which make different assumptions about, and provide different treatments of, the problems of simultaneity and serial correlation, the latter being both within and between equations. Although the system appears to perform quite well on conventional criteria, the alternative estimators reveal a number of misspecifications and demonstrate the sensitivity of the results to estimators choice. The paper also considers the methodological problems involved in estimating dynamic simultaneous equation models with possibly auto-correlated errors using seasonally unadjusted quarterly data.</p> </abstract>
<abstract> <p>This paper describes an iterative procedure for obtaining maximum likelihood estimates of the parameters of a generalized regression model when direct maximization with respect to all parameters is difficult. A proof of convergence and some interesting applications are provided.</p> </abstract>
<abstract> <p>The notion of "neutral" want association is introduced. The relationship between this and strictly additive utility, or "want independence," is discussed. It is argued that the expression of neutral want association always in terms of want independence obscures the generality of the former concept and inhibits further progress. A simple empirical test for neutral want association is proposed. It is shown that the number of parameters to be estimated in a complete set of demand equations can be drastically reduced by grouping in accordance with want relationships. By following the initial grouping with further groupings of successively higher orders, all parameters may be estimated from as few as two sets of observations of price and quantity changes. It is shown that in general no index number problem is involved in this process.</p> </abstract>
<abstract> <p>In this paper the problem of constructing a generalized error of forecast for a set of dependent variables in a multi variate regression model (which may be the set of reduced-form equations of an econometric model) is considered. The distribution of this multivariate error of forecast is derived and its use in the construction of probabilistic forecast regions is presented.</p> </abstract>
<abstract> <p>For a structural econometric model, we obtain formulas for the covariance matrix of the coefficients of the derived reduced-form system and for the covariance matrix of forecasts. A numerical illustration is provided.</p> </abstract>
<abstract> <p>This paper considers the problem of criteria for the identifiability of a structural equation which is one of a set of equations linear in the parameters but not in the variables. The criteria are developed in terms of parameter restrictions of the rank condition type. By expansion in Taylor's series and combination with the results of Fisher [2], these results can be easily extended to far more general nonlinear systems.</p> </abstract>
<abstract> <p>A unified statistical approach to index number problems is suggested in this note. Through this approach, it is possible to derive index number formulae evolved on ad hoc considerations on the one hand and, on the other, to work out a condition of equivalence giving the "true index" of cost of living. Besides facilitating studies in the problem of index numbers in general, this approach would afford a practicable generalisation for working out multi-dimensional index number formulae.</p> </abstract>
<abstract> <p>The method of best linear index numbers is mathematically related to the principal-component technique. This article consists of an application and an extension of Theil's article on this subject (Econometrica, Vol. 28, April 1960).</p> </abstract>
<abstract> <p>In this paper the stability problem of general dynamic processes is investigated by the classical Lyapunov second method. The stability theorem will be applied to handle the price adjustment process in a competitive economy and generalize some of the results recently obtained by K. J. Arrow, H. K. Block, and L. Hurwicz.</p> </abstract>
<abstract> <p>This paper is concerned with the optimal degree of excess capacity to be built into a new facility such as a pipeline, a steel plant, or a superhighway. The problem is complicated by: (1) the presence of substantial economies of scale in plant construction; (2) the penalties involved in accumulating backlogs of unsatisfied demand; and (3) the use of a random-walk pattern rather than a deterministic upward trend in demand.</p> </abstract>
<abstract> <p>In this paper the theory of cost and production in the multi-product firm is formulated as a problem of minimizing a convex function subject to convex inequality constraints. The Kuhn-Tucker theorem is used to solve the problem. Differences between cost and production theories for the single-product firm and for the multi-product firm are noted briefly.</p> </abstract>
<abstract> <p>This paper attempts to apply the theory of marginal cost pricing to the services of the highways of the U.S.A. Empirical evidence suggests that "efficient prices" are generally much higher than present levels. This Implies that gasoline taxes should be increased and Special tolls charged in congested areas.</p> </abstract>
<abstract> <p>An iterative method for solving nonlinear programming problems is proposed. Its convergence is proved for the case in which the minimand is strictly convex. An upper bound for the error at each step is given.</p> </abstract>
<abstract> <p>This paper investigates the factors determining how rapidly the use of a new technique spreads from one firm to another. A simple model is presented to help explain differences among innovations in the rate of imitation. Deterministic and stochastic versions of this model are tested against data showing how rapidly firms in four industries came to use twelve important innovations. The empirical results seem quite consistent with both versions of the model.</p> </abstract>
<abstract> <p>A procedure is presented for the efficient computational solution of linear programs having a certain structural property characteristic of a large class of problems of practical interest. The property makes possible the decomposition of the problem into a sequence of small linear programs whose iterated solutions solve the given problem through a generalization of the simplex method for linear programming.</p> </abstract>
<abstract> <p>We extend present theorems on conditions for a constrained maximum to the case where the maxim and the constraint functions are quasi-concave (e.g., utility functions). Economic applications are briefly discussed.</p> </abstract>
<abstract> <p>We test whether a model of reputation formation in an incomplete information game, using sequential equilibrium, predicts behavior of players in an experiment. Subjects play an abstracted lending game: a B player lends or does not lend; then if B lends, and E player can pay back or renege. The game is played 8 times, and there is a small controlled probability that the E player's induced preferences make him prefer to pay back (but usually he prefers to renege). In sequential equilibrium, even E players who prefer to renege should pay back in early periods of the game, and renege with increasing frequency in later periods, to establish reputations for preferring to pay back. After many repetitions of the 8-period game, actual play is roughly like the sequential equilibrium, except that E players pay back later in the game and more often than they should. This behavior is rational if B players have a "homemade" prior probability of .17 (in addition to the controlled probability) that E players will prefer to pay back. We conclude that sequential equilibrium with homemade incomplete information describes actual behavior well enough that it is plausible to apply it to theoretical settings where individuals make choices (e.g., product markets, labor markets, bargaining).</p> </abstract>
<abstract> <p>A standard efficient markets model states that a stock price equals the expected present discounted value of its dividends, with a constant discount rate. This is shown to imply that the variance of the innovation in the stock price is smaller than that of a stock price forecast made from a subset of the market's information set. The implication follows even if prices and dividends require differencing to induce stationarity. The relation between the variances appears not to hold for some annual U.S. stock market data. The rejection of the model is both quantitatively and statistically significant.</p> </abstract>
<abstract> <p>Traditionally, household behavior is derived from the maximization of a unique utility function. In this paper, we propose an alternative approach, in which the household is modeled as a two-member collectivity taking Pareto-efficient decisions. The consequences of this assumption are analyzed in a three-good model, in which only total consumption and each member's labor supply are observable. If the agents are assumed egoistic (i.e., they are only concerned with their own leisure and consumption), it is possible to derive falsifiable conditions upon household labor supplies from both a parametric and nonparametric viewpoint. If, alternatively, agents are altruistic, restrictions obtain in the nonparametric context; useful interpretation stems from the comparison with the characterization of aggregate demand for a private-good economy.</p> </abstract>
<abstract> <p>This paper examines household fertility and female labor supply over the life cycle. We investigate how maternal time inputs, market expenditures on offspring, as well as the benefits they yield their parents, vary with ages offspring, and influence female labor supply and contraceptive behavior. Our econometric framework combines a female labor supply model and a contraceptive choice index function. It also accounts for the fact that conceptions are not perfectly controllable events. Using longitudinal data on married couples from the Panel Study of Income Dynamics, we estimate these equations and test alternative specifications of the technologies governing child care. Our findings suggest that while parents cannot perfectly control conceptions, variations in child care costs do affect the life cycle spacing of births. Furthermore, our results demonstrate the gains of modelling the linkages between female labor supply and fertility behavior at the household level.</p> </abstract>
<abstract> <p>The basic model is a Gaussian AR(1) model with an intercept, but no additional exogenous variables. The paper studies the distribution of the t statistic for testing the value of the autoregressive parameter when the model is estimated by least squares. The Monte Carlo estimates of the quantiles of the t statistic show that Student's t is not a satisfactory approximation for sample sizes typical in economic applications. The main problem is not the shape of the distribution of the t statistic, but its location. Adjusting the t statistic so that it has the same mean and standard deviation as Student's t, the distribution of this adjusted t statistic is accurately approximated by Student's t. Techniques are presented for accurately approximating the mean and standard deviation of the t statistic such that the adjusted t statistic can be readily calculated in practice. The analysis is extended in two directions. The first is to examine the effect of introducing an exogenous variable into the basic model. In the expanded model Student's t also accurately approximates the distribution of the t statistic for testing the autoregressive parameter and for testing the coefficient of the exogenous variable after these t statistics are adjusted for mean and standard deviation. The problem of obtaining a feasible adjustment procedure is that the moments of these t statistics now depend on nuisance parameters. The second is to examine the robustness of the results in the basic model to several nonnormal error distributions. For each nonnormal distribution the t statistic is adjusted using the mean and standard deviation appropriate for the case of normal errors and then the distribution of the modified t statistic is compared with that of Student's t.</p> </abstract>
<abstract> <p>The paper solves the stochastic inverse optimal problem. Dynamic programming is used to transform the original problem into a differential equation. Such an equation is well-defined (with probability one) if the production function is sufficiently concave at infinity. When the production function has a finite slope at the origin, we show that a solution to the aforementioned problem exists for a twice continuously differentiable, strictly increasing consumption function provided the savings function, starting from the origin, is steep initially and flat eventually. Three well-known consumption functions, linear (in the capital-labor ratio), Keynesian, and Cantabrigian, are also studied within the stochastic framework. A well-known result in discrete time models--that a logarithmic utility function and a Cobb-Douglas production function imply a Keynesian consumption function--does not carry through to the continuous time case.</p> </abstract>
<abstract> <p>A model of competitive bidding is developed in which the winning bidder's payoff may depend upon his personal preferences, the preferences of others, and the intrinsic qualities of the object being sold. In this model, the English (ascending) auction generates higher average prices than does the second-price auction. Also, when bidders are risk-neutral, the second-price auction generates higher average prices than the Dutch and first-price auctions. In all of these auctions, the seller can raise the expected price by adopting a policy of providing expert appraisals of the quality of the objects he sells.</p> </abstract>
<abstract> <p>A fundamental assumption in much of game theory and economics is that all the relevant information for determining the rational play of a game is contained in its structural description. Recent experimental studies of bargaining have demonstrated effects due to information not included in the classical models of games of complete information. The goal of the experiment reported here is to separate these effects into components that can be attributed to the possession of specific information by specific bargainers, and to assess the extent to which the observed behavior can be characterized as equilibrium behavior. The results of the experiment permit us to identify such component effects, in equilibrium, including effects that depend on whether certain information is common knowledge or not. The paper closes with some speculation on the causes of these effects.</p> </abstract>
<abstract> <p>The process through which individuals accumulate information on their productive traits has been analyzed extensively. The manner in which firms utilize this information has received little attention. This paper examines the latter problem in a simple optimal assignment framework. The optimal assignment is characterized, and the impact of improved information quality on the equilibrium level of output, wages, and degree of specialization is investigated.</p> </abstract>
<abstract> <p>This paper considers the possibility of static and dynamic speculation when traders have rational expectations. Its central theme is that, unless traders have different priors or are able to obtain insurance in the market, speculation relies on inconsistent plans, and thus is ruled out by rational expectations. Its main contribution lies in the integration of the rational expectations equilibrium concept into a model of dynamic asset trading and in the study of the speculation created by potential capital gains. Price bubbles and their martingale properties are examined. It is argued that price bubbles rely on the myopia of traders and that they disappear if traders adopt a truly dynamic maximizing behavior.</p> </abstract>
<abstract> <p>We devise and apply a new method for estimating demand for local public goods from survey data. Individuals' responses to questions about whether they want more or less of various public goods are combined with observations of their incomes, tax rates, and of actual spending in their home communities to obtain estimates of demand functions. This estimation technique requires no "median voter" assumptions. Functions estimated in this way can be much richer in detail than estimates obtained from aggregate cross-section studies and allow one to distinguish between the effects of individual characteristics and the effects of the character of one's home jurisdiction on demand. Estimates of the effects of income and price turn out to be quite similar to those found in aggregate studies.</p> </abstract>
<abstract> <p>This paper studies the fiscal (benefit and tax) incidence of a local budget by comparing the distributional impacts of Lindahl and Bowen solutions. The median voter methodology is used to estimate net benefits for different income groups from the St. Louis city 1972 budget. The results indicate a progressive or pro-poor distributional impact. The distribution of fiscal incidence for population grouping other than income is also explored.</p> </abstract>
<abstract> <p>This paper studies the effects of monetary policy in a small, open economy with a floating exchange rate, sticky wages, and rational expectations in both the asset and labor markets. The model developed emphasizes the link between exchange-rate depreciation and nominal wage inflation, embodying it in an expectations-augmented Phillips curve. The economy studied produces both traded and non-traded goods, and thus provides a framework in which to explore the connection between the dynamic behavior of the exchange rate and the supply structure and degree of openness of the economy. In addition, the paper examines the "vicious circle" hypothesis, showing how an explosive cycle of exchange-rate depreciation and wage-price inflation may arise in response to an expected monetary expansion.</p> </abstract>
<abstract> <p>Over half a century ago Frank Graham argued that decreasing costs could justify protection. Although this contention stimulated a huge literature, a correct analysis has never been made. The present paper attempts to fill this gap. It is shown that Graham's case applies to trade between approximately equally-sized economies and that a greater degree of increasing returns actually reduces its likelihood. Furthermore, increasing returns yield a positive analysis nearly completely symmetric to that of Ricardian constant costs. A new analytical tool, the allocation curve, is introduced, with which Marshallian stability is fully analogous to Walrasian instability with offer curves.</p> </abstract>
<abstract> <p>This paper describes a method for estimating and testing nonlinear rational expectations models directly from stochastic Euler equations. The estimation procedure makes sample counterparts to the population orthogonality conditions implied by the economic model close to zero. An attractive feature of this method is that the parameters of the dynamic objective functions of economic agents can be estimated without explicitly solving for the stochastic equilibrium.</p> </abstract>
<abstract> <p>The paper derives and compares the local power of three different methods of testing non-nested regression models that are available in the literature. It shows that the asymptotic power of the orthodox F test against local alternatives is strictly less than that of Cox's non-nested test or the J test recently proposed by Davidson and MacKinnon [5], unless the number of non-overlapping variables of the alternative hypothesis over the null hypothesis is unity, in which case all three tests are shown to be asymptotically equivalent. The final section of the paper gives results of Monte Carlo experiments designed to check the validity of the theoretical findings of the paper and also to shed light on the small sample properties of the Cox test and the orthodox test.</p> </abstract>
<abstract> <p>Examples are given which show that normality is not necessary for the consistency of the quasi-maximum likelihood estimator in the nonlinear simultaneous equations model (nonlinear FIML) even when there are major departures from linearity. A possibility theorem is proved which demonstrates that when nonlinear FIML is consistent under normality it is always possible to find a nonnormal error distribution for which the consistency of nonlinear FIML is maintained. The procedure that is developed for finding a class of error distributions which preserve the consistency of nonlinear FIML can be applied more generally and may be useful in other contexts.</p> </abstract>
<abstract> <p>The small sample properties of five standard simultaneous equation estimating methods are examined by means of the distribution sampling technique. The methods are appraised on the basis of the sampling properties of three different kinds of estimates generated by them: (1) conditional predictions, (2) structural coefficient estimates, and (3) Studentized estimates of structural coefficients. Though one basic model is employed in the investigation, variants used in particular sampling experiments make it possible to compare the methods with respect to bias and efficiency in estimation in the presence of substantial multicollinearity in the redetermined variables and under certain conditions of misspecification. Careful attention is paid to the stochastic variation which necessarily is present in distribution sampling applications.</p> </abstract>
<abstract> <p>The properties of the optimal growth path are investigated in a two-sector economy model. The objective is the attainment of the von Neumann path in minimum time, and the optimal strategy is given. In order to characterize and establish the optimality of the path, an extensive use is made of Pontrayagin's Maximum Principle.</p> </abstract>
<abstract> <p>This article approaches demand analysis in a probabilistic manner. The main ingredient is the value share (the proportion of total expenditure spent on a particular commodity), which can be regarded as a probability in view of the fact that it is nonnegative and adds up to one when summed over all commodities.This ingredient is used for the formulation of price and quantity index numbers and demand equations, partly in the light of information theory.</p> </abstract>
<abstract> <p> The New York Stock Exchange is characterized as a black box with an input of orders and an output of executed prices. Feedback, or coupling between the output and input, is determined by the standard types of orders, and the time delay between the receipt of information on prices, the output, and the decision to enter an order as input. From this picture plus the knowledge that the distribution in time of orders has the characteristic of concentrated bursts, the price output is described by a random sequence of starting transients of the form &lt;tex-math&gt;$A\ e^{\lambda t}$&lt;/tex-math&gt; with λ real, complex, or imaginary. It is quite essential to the description of the trading process to take account of its discrete, rather than continuous, nature in both price and time. The conclusions from the theory are in qualitative agreement with the "folklore" of stock trading. </p> </abstract>
<abstract> <p>This article deals with proposals for the efficient organisation of the calculations to be made in the context of planning models in order to establish what instruments should be used and with what intensity to meet certain targets of macroeconomic policy and what the effects are of target revision and instrument-diversification. It suggests a basically new approach to these matters.</p> </abstract>
<abstract> <p>The planning task may originally be formulated as a single linear programming problem of the maximizing type. This overall central information (OCI) problem may be decomposed into subproblems that can be solved by mutually independent "sectors," coordinated by the "centre" through having the latter allocate the resources to the various sectors. The original OCI problem is then transformed into a two-level problem, in which the "central problem" is to evolve an allocation pattern where the sum of the maximal yields of the "sector problems" will be the greatest. The solution of the two-level problem is achieved by setting up a game-theoretical model. The players are on the one hand the centre, on the other the team of sectors. The strategies of the centre are the feasible allocation patterns, those of the sectors are the high feasible shadow price systems in the duals of the sector problems. The payoff function is the sum of the dual sector objective functions. It is shown that if certain regularity conditions are satisfied, then the value of the polyhedral game which has thus been defined is the maximal yield of the OCI problem. In place of a direct solution of the polyhedral game, a fictitious play of the game is undertaken. The first part of the paper discusses a general model, within whose scope the symbols and definitions are presented and the mathematical theorems are proved. In the second part, the results of the first part are applied to a long-term macroeconomic planning model.</p> </abstract>
<abstract> <p>The impatience implications of several axiom sets assumed for preferences over an infinite future are explored. Sufficient conditions for the existence of a continuous utility function are presented. This analysis is done both for the product topology and the metric function equating the distance between two infinite streams to the maximal one-period difference between them.</p> </abstract>
<abstract> <p>The estimation of the influence of an (infinite) weighted sum of all lagged values of one economic variable upon another is considered. The asymptotic distributional properties of a simple estimation procedure are derived under very general conditions. Efficient estimates, using spectral methods, are derived and their asymptotic distributions obtained. Tests for validity of the model and estimation procedures for more general models are discussed. Vector models, small sample situations, and the relevance of the models used are discussed.</p> </abstract>
<abstract> <p>The order in which consumer durable goods are acquired is of considerable importance, chiefly in market research. In the following paper the possibility of an econometric approach to this problem is demonstrated, as well as the way in which psychometric tools can be used in an economic investigation. Although the order of acquisition is conceptually linked with the passage of time, the econometric analysis is here based on simple cross-section data.</p> </abstract>
<abstract> <p>This note provides a rigorous description of a game in a strategic form whose Nash equilibria are all strong equilibriaccoinciding with the Walras equilibria of the underlying Arrow-Breau pure exchange economy.</p> </abstract>
<abstract> <p>This paper reports both theoretical results and also computational experience with a method for approximating a competitive equilibrium in a piecewise linear economy. The algorithm consists of solving a sequence of linear programs, alternating between: (a) a "master" problem which ensures a balancing bundle of choices and generates a price vector; and (b) a "sub" problem which indicates the maximum level of utility attainable by each household--given the initial resource endowments--and also given the prices generated at the current iteration of the master problem. Each subproblem provides a "price-consistent" utility vector. The master problem determines a convex combination of the utility vectors generated at previous iterations. This convex combination mimimize the distance between the quantity-consistent and the price-consistent set. For the sequence of sub and master problems to approach a competitive equilibrium, this distance must approach zero. Thus far, the algorithm has failed whenever all equilibria are "unstable." and it has converged rapidly when there are "suitable" equilibria. It will be shown that the algorithm does not cycle. It will also be shown that if the sequence of solutions (obtained from the algorithm) converges, then it converges to a Walrasian equilibrium.</p> </abstract>
<abstract> <p>A new way of looking at repeated games is introduced which incorporates a bounded memory and rationality. In these terms, a resolution of the prisoner's dilemma is given.</p> </abstract>
<abstract> <p>This paper proposes a concept of the core for games with differential information, using Aumann's notion of common knowledge. The concept is applied to solve the syndicate problem, for cases in which members have different private information on the uncertain prospects of each syndicate action and the contract (the risk sharing rule and the decision rule) is to be determined before they exchange their information.</p> </abstract>
<abstract> <p>In this paper the Le Chatelier principle for a Leontief model due to Samuelson and Morishima is extended to a more general system of nonlinear equations. The global strong version of the principle as well as the weak one is presented without using any determinant theory or calculus. The strong version is concerned with comparative statics when some constraints are relaxed together with changes in parameters. It is also shown that our results are applicable to the system of excess demand functions which satisfy the gross substitute condition.</p> </abstract>
<abstract> <p>A continuum of goods is introduced into the general Ricardian model of international trade. By looking at the derived demand for labor, it is demonstrated that the analysis of the model can be reduced to the analysis of anequivalent model of pure exchange in which each country essentially trades its own labor of other countries. Furthermore unlike the case where the number of goods is finite, the derived demand for labor becomes a differentiable function of the relative wages of the different countries. How this facilitates the analysis of comparative statics exercises is illustrated by establishing number of propositions in the theory of growth, technical change, and tariffs.</p> </abstract>
<abstract> <p>This paper discusses the properties of parellel preference structures and their potential usefulness in empirical research. The parallel structure can accommodate arbitrarily flexible substitution properties and linear or nonlinear income-consumption curves, with the linear form representing a special case of the Gorman polar form. The global properties and distinctive in that indifference loci are parallel surfaces, identical in shape and scale at all utility levels. These properties are potentially appropriate for models of family labor supply but are less suitable for applications to production over a wide range of output. Alternative parameterizations and estimation forms are discussed and the model is shown to provide a basis for interpretation of models of labor supply estimated by Professors Ashenfelter and Heckman.</p> </abstract>
<abstract> <p>This paper presents a decomposition of the total effect of a change in a quantity constraint on the money income constant conditional demand function into an income compensated substitution effect and a pure income effect. The substitution effect is shown to be equivalent to the Hicks substitute-complementrelation while the income effect is shown to depend directly on the extent of over-or under-supply of the constrained good. The Tobin-Houthakker results on rationing constraints can be generalized in an intuitive manner to cases of non-optimal values of the constraint when the two goods involved are: (1) Hicks substitutes, the constrained good is over-supplied and the unconstrained good is a normal good; or, (2) Hicks complements, the constrained good is under-supplied and the unconstained good is a normal good. In the case of substitutes and under-supply or complements and over-supply, the sing of the Hicks substitute-Complement relation is not sufficient to determine the sign of the partial derivative of interest. Several aplications of the theoretical results are also presented.</p> </abstract>
<abstract> <p>Three forms of congestion are defined and characterized. The strongest form is shown to be closely related to the law of variable proportions.</p> </abstract>
<abstract> <p>This paper analyzes an intertemporal production-investment model of the firm, which includes as a special case the adjustment cost model. Properties of the optimal value function are related to properties of the quasi-profit function. An intertemporal analogue of Hotelling's Lemma is derived, which allows derivation of optimal investment demand equations from knowledge of the optimal value function alone. A simple example illustrates the main results.</p> </abstract>
<abstract> <p>General concern about the scarcity of natural resources, including the present main sources of energy, leads to the following question: what should be the behavior of our societies if they are to use optimally over time both renewable and depletable resources? This is the main question we address in this paper.</p> </abstract>
<abstract> <p>The purpose of this study is to introduce a measure of inter-income inequality that complements the traditional ones proposed by Gini, Theil, and others. The latter are measures that account for the degree of income inequality within a given population of economic units (called here intra-income inequality ratios) while the former is intended to measure the degree of inequality between income distributions, which is called here economic distance ratio. The generalized mathematical form of this ratio is provided and two particular forms of economic distances ratios are identified. They are presented under both the discrete form, which is distribution-free, for a direct application to observed income distributions, and the parametric form corresponding to a given model of income distribution. Applications are made to the five economic regions of Canada and to white and black family income distributions of the U.S.A.</p> </abstract>
<abstract> <p>This article presents the first and second moments of an estimator which might be used when two subsamples are characterized by the same regression coefficients, but possibly different error variances. The estimator is the OLS estimator if the hypothesis of equal variances is accepted and the two-step Aitken estimator otherwise. The estimator is similar to one suggested by Goldfeld and Quandt and is applicable to a reparameterized version of the error components model.</p> </abstract>
<abstract> <p>In an economy with complete markets, the owners of a firm will unanimously desire the firm to maximize profits if it is a perfect competitor. We generalize this result to an economy with incomplete markets. We show that if competitive conditions prevail--that is, if each firm is negligible relative to the aggregate economy--a firm's shareholders will want the firm to maximize the (net) market value of its shares. This result holds whether or not the so-called spanning condition is satisfied. However, while there may be agreementabout what goal the firm should pursue, there may be disagreement among shareholders about how best to pursue this goal.</p> </abstract>
<abstract> <p>This paper proves that for majority voting over multidimensional alternative spaces, the majority rule intransitivities can generally be expected to extend to the whole alternative space in such a way that virtually all points are in the same cycle set. In other words, given almost any two points in the alternative space, it is possible to construct a majority path which starts at the first, and ends at the second. It is shown that for the intransitivities not to extend to the whole space in this manner, extremely restrictive conditions must be met on the frontier (or boundary) of the cycle set. Similar results are shown to hold for any social choice rule derived from a strong simple game. These results hold under fairly weak assumptions on individual preferences: individuals need only have continuous utility representations of their preferences such that no two individuals' preferences coincide locally. The results seem to rule out the possibility, at least in models of interest to economists, of using the transitive closure of the majority relation as a useful social choice function. They also imply that under any social choice rule meeting the conditions assumed here, it is generally possible to design agendas based on binary procedures which will arrive at virtually any point in the alternative space, even Pareto dominated points.</p> </abstract>
<abstract> <p> In a well known paper, Plott has given a sufficient and a necessary condition onthe set of gradients of individual preferences at a point in a multidimensional space, for the point to be an equilibrium under simple majority voting. This paper defines a class of α-majority voting rules under which, given some α, 0 &lt; α &lt; 1, an alternative x is socially at least as good as y iff the number of individuals who prefer x to y is at least α/(1 - α) times the number who prefer y to x. Simple majority rule is α = 1/2 while setting α near 0 and 1 gives two types of unanimity rule. For all elements in this class, this paper generalizes Plott by giving necessary and sufficient conditions on the set of gradients for a point in a multidimensional space to be a voting equilibrium. </p> </abstract>
<abstract> <p>Suppose that social choice is based on interpersonal comparisons of welfare levels. Suppose too that, whenever all but two persons are indifferent between two options, a choice is made between these options which is equitable, in some sense. Then provided that individual welfare functions are unrestricted, and social choice is independent of irrelevant alternatives, it follows that social choice is always equitable, in the same sense. This applies when equity means satisfying Suppes' indifference rule, or Suppes' original justice criterion, or the lexicographic extension of Rawls' difference principle.</p> </abstract>
<abstract> <p>It is proved that Groves' scheme is unique on restricted domains which are smoothly connected, in particular convex domains. This generalizes earlier uniqueness results by Green and Laffont and Walker. An example shows that uniqueness may be lost if the domain is not smoothly connected.</p> </abstract>
<abstract> <p>The purpose of this note is to show that the so-called Principle of Minimum Differentiation, as based on Hotelling's 1929 paper "Stability in Competition," is invalid.</p> </abstract>
<abstract> <p>A simultaneous equations model of bid and offer functions for housing attributes(dwelling quality, dwelling size, and lot size) is estimated in order to accountfor the heterogeneity of the housing good. Estimation of a traditional, nonlinear hedonic price equation in the first stage provides a basis for calculating implicit prices for housing attributes which are used in the second stage simultaneous equations model. Empirical results confirm the theoretically expected negative coefficient for each attribute in its own bid function and theexpected positive or zero coefficient for each attribute in its own offer function. Cross price relationships reveal a general pattern of complementarity in consumption of housing attributes.</p> </abstract>
<abstract> <p>This paper investigates the role of inheritances in the determination of the national distribution of wealth. Assuming that households care both about their own consumption and that of their descendants, we construct a model of family bequest behavior. Then, embedding the model in a simple consumption-loan framework, we study the evolution of the national distribution of wealth. A key generalization of previous work is that we allow bequests to both male and female children so that each family receives two inheritances (one from the husband's parents and one from the wife's). This means the national distribution of wealth develops in a complicated manner depending on mating patterns. Using a fixed-point argument we prove the existence of a stationary distributionof wealth consistent with accurate expectations on the part of all households. We then present a test for uniqueness and several economic characterizations of all possible equilibria.</p> </abstract>
<abstract> <p>Opportunities for individual learning in multi-period insurance contexts introduce fundamental economic aspects not present in conventional static models. Using a two-period model in which there are two states (accident and no accident), it is shown that more precise prior probability assessments lead to increased insurance coverage and reduced self-protection. These dynamic adverse incentive problems can be diminished by merit rating, which has a backwards influence on earlier actions. Self-protection and insurance purchases in the initial period respond in opposite fashion to changes in insurance pricesin the second period, the interest rate, and parameters of the prior probability assessment.</p> </abstract>
<abstract> <p>If preferences reflect a certain separability among commodities, the set of demand equations can be represented by a two-level demand system. This fact is used to formulate an approach to the estimation of a large set of possibly nonlinear demand equations. A new functional form is introduced which allows the two-level representation and exhibits nonlinear Engel curves. This functional form is used in an experiment which compares the proposed approach with familiar approaches which do not aggregate commodity prices consistently. The results of the experiment suggest that the proposed approach may be worthwhile even if the relative price changes reflected in the data set are small.</p> </abstract>
<abstract> <p>This paper proposes an econometric methodology for estimating the conditional probability of an individual leaving unemployment in any particular week of his spell out of work. We use this method with cross-section data on uncompleted unemployment spells to investigate a number of questions concerning unemploymentduration. In particular we look at the impact of unemployment benefits and how this changes over the course of an unemployment spell. We also consider how our results are affected by unobserved sample heterogeneity and present estimates which, to some extent, take account of this problem.</p> </abstract>
<abstract> <p>The paper investigates an econometric method for selecting macroeconomic policy rules when expectations are formed rationally. A simple econometric model of the U.S. is estimated subject to a set of rational expectations restrictions using a minimum distance estimation technique. The estimated model is then used to calculate optimal monetary policy rules to stabilize fluctuations in output and inflation, and to derive a long run tradeoff between price stability and output stability which incorporates the rationally formed expectations. The optimal tradeoff curve is compared with actual U.S. price and output stability and with the results of a monetary policy rule with a constant growth rate of the money supply.</p> </abstract>
<abstract> <p>A simple test for heteroscedastic disturbances in a linear regression model is developed using the framework of the Lagrangian multiplier test. For a wide range of heteroscedastic and random coefficient specifications, the criterion is given as a readily computed function of the OLS residuals. Some finite sampleevidence is presented to supplement the general asymptotic properties of Lagrangian multiplier tests.</p> </abstract>
<abstract> <p>The purpose of this paper is to amend some of Hannan's [6] identification results for ARMAX models and to discuss identification in ARMAX models when identities are present. In addition, we briefly discuss how the ARMAX results can be adapted to obtain identification conditions for dynamic simultaneous equations models with autoregressive disturbances.</p> </abstract>
<abstract> <p>This paper is concerned with "supergames" in which the action taken in a given time period by a player will affect the payoff to any other player in the subsequent period. A supergame consists of a set of players and a countable sequence of "ordinary" games. To illustrate "time-dependence," consider an economic market in discrete time. Say each firm must choose a price in each time period. This market has time-dependence if the amount demanded of a firm today is a function of the prices chosen today and of the prices chosen in the preceding period. Conditions are given for the existence of non-cooperative equilibria of two types: (i) steady state, in which the individual moves of the players converge over time to some s^0 and (ii) balance temptation equilibria of the sort developed by Friedman [6] for games lacking time dependence.</p> </abstract>
<abstract> <p>Assuming each assignment of strong preference orderings to individuals is equally likely, we examine how the probability of social intransitivity (under asimple majority vote decision rule) changes with changes in the number of alternatives and the number of voters. A similar study is made of violation of quasi-transitivity and failure of existence of a maximal alternative.</p> </abstract>
<abstract> <p>An infinite horizon consumption model is considered where the labor part of income is random. An upper bound on optimal consumption is obtained by considering the expected value of the optimal return function in the deterministic labor income case. This upper bound on consumption is easily shown to be lower than the value of optimal consumption in the case where the random labor income is replaced by its mean.</p> </abstract>
<abstract> <p> In a classical optimal control problem the terminal time, either prescribed a priori or not, is always a real number. In many dynamic optimization problems in economics one is led to consider optimal control problems in which the terminal time is the extended real number + ∞. These are the so called optimal control problems with infinite horizon. In this paper we give a precise formulation for a standard problem of that type, and we establish a necessary condition for that problem. </p> </abstract>
<abstract> <p> In long-run planning, targets may be set further in the future than economic technologies can be estimated. How costly in hindsight is limited foresight? Following Łos, we say k is an ε-horizon if knowledge of economic possibilities for the next K time periods permits decisions with results at most ε worse than optimal. For a dynamic Leontief model, we show that any T-period efficient path is almost as big as all feasible T-period paths during the first T - K periods. Thus, given K periods notice, we can plan an ε-optimal growth path no matter how technologies change after T. </p> </abstract>
<abstract> <p>This paper develops a dynamic model of oligopoly and discusses the existence andcharacteristics of optimal policies for firms in such a model. The firms are assumed to face a random demand so they hold inventories which fluctuate from one period to the next. This necessitates a dynamic model rather than a static one. Our extension of the equilibrium concept to the oligopoly model is founded on recent generalizations of Shapley's stochastic game. We show the existence of equilibrium price-quantity strategies for the firms and also (i) an equilibrium strategy may be found by solving an appropriate static game and (ii) the quantity part of the strategy is often a constant (time invariant).</p> </abstract>
<abstract> <p>The durabilities of a consumption good produced in a perfectly competitive market or by a monopoly are compared. The analysis is conducted in terms of firm profit maximization in a Cournot industry. Conclusions are based on the properties of the entire optimal path rather than on the steady state alone.</p> </abstract>
<abstract> <p>This paper considers the asymptotic distribution of forecasts made several time periods in the "future." Such forecasts typically arise in the dynamic simulation of an econometric model.</p> </abstract>
<abstract> <p>In this paper we consider a number of estimators for the linear structural simultaneous equations model containing lagged endogenous variables and autocorrelated errors. The special case is considered in which the matrix of autocorrelation coefficients of the (vector) structural error process is diagonal. We consider the two stage least squares analogue (C2SLA) in this case, its relation to the estimators proposed earlier by Fair, the estimator obtained when the autocorrelation matrix is known, and a number of instrumental variables estimators, as well as a modification of the method of scoring which yields an estimator that is asymptotically equivalent to the C2SLA estimator. The asymptotic distributions of such estimators are obtained and we determine their relative asymptotic efficiencies.</p> </abstract>
<abstract> <p> In applications of linear regression analysis, the unknown error covariance matrix has to be somehow estimated. This can lead to biased estimates of the covariance matrix of the regression coefficients. Since such bias is difficult to eliminate completely, its sensitivity to alternatives estimates of error covariances is studies by Watson, Theil, Malinvaud, and others with the help of bounds on the bias derived under certain assumptions. This paper gives similar bounds under less restrictive assumptions, and illustrates them context of heteroscedasticity and autocorrelation problems. In particular, for the first order error autocorrelation coefficient of ρ the upper bound on proportionate bias is shown to be reasonably approximated by (1 + ρ)/(1 - ρ) - 1. </p> </abstract>
<abstract> <p>This paper considers the application of various models of consumer demand to United Kingdom time series from 1900 to 1970. As well as testing the various forms of the "Rotterdam" model, reparametrization of that system is carried out in order to test the linear expenditure system and the direct addilog system on an exactly comparable basis. A further variant of the Rotterdam model is also introduced; this is intermediate between symmetry and additivity and allows for the calculation of all cross price elasticities from information on own price and income elasticities alone. The results of testing these models on a nine commodity model using maximum likelihood estimation are presented and discussed. Unlike most previous work, and in spite of some anomalous results, the United Kingdom experience seems broadly consistent with neoclassical demand theory. However, all restrictions more stringent than those directly implied by the theory are rejected, though it is maintained that these may still be of considerable practical significance in particular instances.</p> </abstract>
<abstract> <p>In this paper we study the properties of the ordinary least squares estimator ofthe coefficients of linear macor models when the coefficients of linear micro relations are random with identical mean and variance. It is shown that the ordinary least squares estimator is equal to a minimum variance linear unbiased estimator of the coefficients of a linear macro equation obtained by aggregating over all micro relations. Futher, some problems associated with estimating the covariance matrix of the ordinary least squares estimator, using only aggregate data, are indicated.</p> </abstract>
<abstract> <p>A model of interstate migration is estimated for three groups of Venezuelan migrants disaggregated by their level of educational achievement. Inclusion of education-specific wage rates and regional educational opportunities among the explantory variables clarifies the sometimes contradictory effects of education on migration noted in previous studies. Zellner's seemingly unrelated regressiontechnique is employed, and appropriate F statistics are generated to test the null hypothesis of equal response of migrants to each of the explanatory variables across education levels.</p> </abstract>
<abstract> <p>Definitions are proposed for weak and strong exogeneity in terms of the distribution of observable variables. The objectives of the paper are to clarify the concepts involved, isolate the essential requirements for a variable to be exogenous, and relate them to notions of predeterminedness, strict exogeneity and causality in order to facilitate econometric modelling. Worlds of parameter change are considered and exogeneity is related to structural invariance leading to a definition of super exogeneity. Throughout the paper, illustrative models are used to exposit the analysis.</p> </abstract>
<abstract> <p>In most statistical estimation problems, the distribution of errors is unknown, and the traditional assumption of normality is used for convenience. We investigate here the fragility of the inferences based on normality by hypothesizing a neighborhood of distributions around the normal distribution, and by identifying the set of alternative maximum likelihood estimates corresponding to the set of error distributions.</p> </abstract>
<abstract> <p>The Wald approach to testing direct explicit restrictions on a parameter vector is generalized to the case of nonlinear implicit constraints. When applied to subsystems of simultaneous equations models, the generalization enables the symmetric joint testing of nonlinear overidentifying structural restrictions under very wide conditions. By varying the choices of certain matrices used to construct the generalized Wald statistic, one produces a whole class of tests which have equal asymptotic power yet whose associated structural coefficient estimators have different asymptotic efficiencies for any given reduced-form estimator from which they are derived.</p> </abstract>
<abstract> <p>Differing opinions about the specification of econometric relationships often lead to a situation in which there are competing non-nested models. This paper is concerned with the problem of testing such models. It is first assumed that tests are based upon instrumental variable estimates (so that the models can be alternative versions of an equation in a system). The tests so derived are then specialized to the case in which ordinary least squares is an appropriate estimator.</p> </abstract>
<abstract> <p>For a general parametric model we consider the problem of consistently estimating that permissible subset of the parameter space that contains the true parameter point and has smallest dimension. The subset selection is done by means of a model selection criterion of the form used by Akaike [2, 3] and Hannan [5]. Properties of the parameter estimates are discussed and a two step estimation technique is given. The theory is then applied to the logistic regression model.</p> </abstract>
<abstract> <p>This paper reconsiders the aliasing problem of identifying the parameters of a continuous time stochastic process from discrete time data. It analyzes the extent to which restricting attention to processes with rational spectral density matrices reduces the number of observationally equivalent models. It focuses on rational specifications of spectral density matrices since rational parameterizations are commonly employed in the analysis of time series data.</p> </abstract>
<abstract> <p>In the axiomatic theory of n-person bargaining, we show that there is a unique solution possessing properties: efficiency, symmetry, invariance under utility transformations, independence of irrelevant alternatives other than ideal point, and individual monotonicity on comparable domains. The solution obtained both extends and modifies the solution characterized by Kalai and Smorodinsky which is also known as Raiffa's solution, and is given a characterization as the lexicographic maxmin solution up to a normalization of utilities. The result can be translated to provide an axiomatic derivation of the Rawlsian lexmin choice function with a suitable redressing.</p> </abstract>
<abstract> <p>The paper considers a world in which one public good is financed by commodity taxes. It concentrates on the global study of the set of tax equilibria (corresponding to any possible taxation scheme). In particular conditions for "regularity" and connectedness of this set are exhibited. Consequences obtain both for the positive theory of tax incidence in general equilibrium (existence, continuity, uniqueness) and for normative theory ("regularity" of optimal solutions, algorithms of tax reform...).</p> </abstract>
<abstract> <p>In contrast to current counterfactual linear programming definitions of labor values this paper introduces an at least equivalent definition of such values for the general case of joint production which is exclusively based on actual data. This task is accomplished by extending Marx's concept of "individual values" from multiple activities to joint production by means of certain price ratios at those points where production data are insufficient to ensure the positiveness of "embodied labor time." Our approach generalizes the simple formula that relates labor values to monetary input-output tables, and it reformulates the labor theory of value in such a way that the "theoretical priority" of the case of a uniform composition of capital is reaffirmed.</p> </abstract>
<abstract> <p>An example is given of a sequence of labor managed economies with decreasing efficiency sizes for which a free entry Cournot equilibrium fails to exist.</p> </abstract>
<abstract> <p>This paper analyzes bundling decisions of a multiproduct monopolist facing uncertain demand. The monopolist sells his products using an auction mechanism and the market is analyzed as a game with incomplete information in which the buyers as well as the seller are strategic agents. With a small number of buyers, a profit maximizing seller will bundle all his output. This makes buyers uniformly worse off compared to the case where the same monopolist does not bundle, in the sense that any buyer is worse off regardless of his demand for the monopolist's outputs. With a larger number of buyers, the seller will have a tendency to unbundle his output and "high-demand" buyers are worse off than they would be if the monopolist bundled his output. "Low-demand" buyers, on the other hand, are always better off when the monopolist unbundles his output, regardless of the number of competing buyers. Despite the fact that "high demand" buyers are the typical purchasers of the monopolist's output, the net effect of increasing the number of buyers is greater market efficiency since bundling creates market inefficiencies both ex post and ex ante.</p> </abstract>
<abstract> <p>This paper concerns the relationship between the variability of the daily price change and the daily volume of trading on the speculative markets. Our work extends the theory of speculative markets in two ways. First, we derive from economic theory the joint probability distribution of the price change and the trading volume over any interval of time within the trading day. And second, we determine how this joint distribution changes as more traders enter (or exit from) the market. The model's parameters are estimated by FIML using daily data from the 90-day T-bills futures market. The results of the estimation can reconcile a conflict between the price variability-volume elationship for this market and the relationship obtained by previous investigators for other speculative markets.</p> </abstract>
<abstract> <p>Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented.</p> </abstract>
<abstract> <p>The implications for applied econometrics of the assumption that unobservable expectations are formed rationally in Muth's sense are examined. The statistical properties of the resulting models and their distributed lag and time series representations are described. Purely extrapolative forecasts of endogenous variables can be constructed, as alternatives to rational expectations, but are less efficient. Identification and estimation are considered: an order condition is that no more expectations variables than exogenous variables enter the model. Estimation is based on algorithms for nonlinear-in-parameters systems; other approaches are surveyed. Implications for economic policy and econometric policy evaluation are described.</p> </abstract>
<abstract> <p>This paper considers the econometric problems raised by multi-market disequilibrium models. It uses a specification which is derived from general disequilibrium theory and, therefore, provides a first bridge between the economic theory approach and the econometric theory approach of disequilibrium. The model is piecewise linear; the problem of the existence of a reduced form, which is a crucial issue in nonlinear models, is solved. Limited information estimators as well as full information estimators are proposed; a simple numerical algorithm is given for the computation of a FIML estimator.</p> </abstract>
<abstract> <p>Methods of estimation for markets in disequilibrium have been limited to the single-market case. However, the spill-over effects of the unsatisfied demand or supply in other markets are considered to be an essential feature of disequilibrium analysis. This paper (i) develops a two-market disequilibrium model that is amenable to estimation; (ii) provides the maximum likelihood method and the two-stage least squares method for estimation; and (iii) generalizes this disequilibrium model to the n-market case, showing sufficient conditions for the existence and uniqueness of a quantity-constrained equilibrium--i.e., for its solvability as an econometric model.</p> </abstract>
<abstract> <p> In this paper, Pareto efficiency properties of a non-Walrasian equilibrium for an exchange economy are analyzed. The equilibrium considered is a generalized version of Drèze's equilibrium with price rigidities and rationing. </p> </abstract>
<abstract> <p>This paper evaluates the benefits to consumers from price stabilization in terms of the convexity-concavity properties of the consumer's indirect utility function. It is shown that in the case where only a single commodity price is stabilized, the consumer's preference for price instability depends upon four parameters: the income elasticity of demand for the commodity, the price elasticity of demand, the share of the budget spent on the commodity, and the coefficient of relative risk aversion. All of these parameters enter in an intuitive way and the analysis includes the conventional consumer's surplus approach as a special case. The analysis is extended to consider the benefits of stabilizing an arbitrary number of commodity prices. Finally, some issues related to the choice of numeraire and certainty price in this context are discussed.</p> </abstract>
<abstract> <p>Given a preference basis for the model, a household's general equivalence scale is seen to be a cost of living index relative to the reference household type defined at constant prices. A maximum likelihood estimation procedure which can be applied both to cross-section data and pooled time-series/cross-sections is derived. The lack of identification of the model is established theoretically and checked empirically on British family expenditure survey data. With prior information, e.g. a nutrition based food scale, identification can be reached but ultimately the model is rendered implausible because of its zero substitution implication and the empirical results bear this out.</p> </abstract>
<abstract> <p>We consider the intertemporal problem of optimally consuming a natural resource and exploring for new sources of supply of that resource. Resource consumption yields social utility while the exploration effort controls the uncertainty in the timings of discoveries as well as their magnitudes. The objective is to choose an optimal consumption and exploration policy so as to maximize the expected discounted utility of consumption net of the exploration cost over an infinite planning horizon. We present a controlled storage process model of the problem and under reasonable conditions we characterize the existence and the properties of optimal policies and prices.</p> </abstract>
<abstract> <p>The "new urban economics" is rather explicit with respect to demand for housing but the supply of housing is usually treated in a very cursory fashion. Housing is normally regarded as a one-dimensional good and equilibrium in a city is mostly concerned with the land market rather than the housing market. This paper introduces, on the supply side, the concept of design parameters of a building and, on the demand side, the concept of housing attributes, i.e. multi-dimensional housing. By means of structural analysis an engineering cost function is obtained in terms of design parameters. Due to housing attributes, the demand for housing and the supply of housing, the latter being based on the engineering cost function, should be derived simultaneously. Equilibrium is defined on the housing market rather than on the land market. It is shown how in equilibrium housing rent, land rent design parameters on buildings, and population density all depend on distance from the center of the city and on parameters of the problem: income, city population, diameter of the city, and technical coefficients.</p> </abstract>
<abstract> <p>The predictive content of the quantity-quality model of fertility is analyzed and the empirical information required for verification under a minimal set of restrictions on the utility function is described. It is demonstrated that commodity-independent compensated price effects must be known to infer the existence of the unobservable interdependent shadow prices of the model with a relatively weak structure imposed on preference orderings. A method of using multiple birth events to substitute for these exogenous prices is proposed and applied to household data from India.</p> </abstract>
<abstract> <p> For the regression equation C = Aβ + ε where A is a p × q stochastic matrix whose elements are independently distributed and contemporaneously correlated with the elements of ε, the lth moment of the least squares estimator of β exists if and only if l &lt; p - q + 1. In particular, this implies that the lth moment of the k-class estimator of the coefficients of the &lt;tex-math&gt;$G_{1}-1$&lt;/tex-math&gt; non-normalizing endogenous variables of an equation with K&lt;sub&gt;1&lt;/sub&gt; included and K&lt;sub&gt;2&lt;/sub&gt; excluded exogenous variables in a simultaneous system with N observations exists if and only if l &lt; M where M = &lt;tex-math&gt;$N-K_{1}-G_{1}+2$&lt;/tex-math&gt; for 0 ≤ k &lt; 1, M = &lt;tex-math&gt;$K_{2}-G_{1}+2$&lt;/tex-math&gt; for k = 1. For k = 1 this condition is known as Basmann's conjecture for two-stage least squares estimators where &lt;tex-math&gt;$K_{2}-G_{1}+1$&lt;/tex-math&gt; is the number of overidentifying restrictions on the equation. </p> </abstract>
<abstract> <p>Using smooth profit functions to characterize production possibilities, we extend the concepts of regularity and fixed point index to economies with very general technologies, involving both constant and decreasing returns. To prove the genericity of regular economies we rely on an approach taken by Mas-Colell that utilizes the topological concept of transversality. We also generalize the index theorem given by Kehoe. Our results shed new light on the question of when an economy has a unique equilibrium.</p> </abstract>
<abstract> <p>This paper develops a framework of analysis for studying the informational properties of a certain class of "parametric" resource allocation processes. It is shown that the Taylor process (related to certain ideas for planning in the so-called socialist economies as put forward by Taylor [126] is informationally efficient in the sense that any informationally decentralized resource allocation process which has similar (static) properties (Pareto optimality) must use a message space which is dimensionally at least as large as that of the Taylor process. We also show that in general greater informational decentralization can be achieved through parametric than through "nonparametric" processes.</p> </abstract>
<abstract> <p>An exchange economy is considered in which there are a finite number of individuals, the same number of indivisibles, and a fixed amount of a divisible good. Each individual consumes exactly one of the indivisibles and a certain quantity of the divisible good. The existence of prices corresponding to Pareto-efficient allocations is proved. It is also shown that this economy possesses fair allocations, income-fair allocations, coalition-fair allocations, and Pareto-efficient egalitarian-equivalent allocations.</p> </abstract>
<abstract> <p>It is shown that wquilibria with dispersed prices exist in environments with identical and rational agents on both sides of the market. In particular, the original Stigler model of nonsequential search often has many equilibria, some with price dispersion. Also, price dispersion holds in equilibrium in general if search is "noisy," i.e., there is some chance of learning two or more prices when an agent is looking for one price.</p> </abstract>
<abstract> <p> This paper develops a systematic relationship between the price responsiveness of an optimizing agent and the conditions prevailing in its relevant markets. The result rests on a formal comparative statics phenomenon that has a striking similarity to the classical strong LeChâtelier principle as well as to the interesting new comparative statics phenomenon established by Edlefsen. </p> </abstract>
<abstract> <p>A model is constructed in which a potential entrant uses prices to make inferences about industry conditions. Stochastic demand shocks occur after the incumbent firm's action, so that prices reveal only statistical information about the incumbent's private information. The equilibrium differs from standard signalling equilibria in that it can be unique, it depends on prior beliefs, and it is rich in comparative statics. Conditions are obtained for entry threats to result in limit pricing, lower entry probabilities, and lower expected profits for potential entrants.</p> </abstract>
<abstract> <p> If all individuals of a given group have the same consumption behavior described by a common demand function f(p,w), which is assumed to satisfy the weak axion of revealed preference, and if the distribution of individual total expenditure w is given by a decreasing density ρ, with ρ(O) &gt; 0, then we show that the market demand function F(p) = ∫f(p, w)ρ(w)dw is monotone, i.e., for any two price vectors p and q one has (q - p). (F(q) - F(p)) ≤ 0. Thus, all partial market demand curves are decreasing. Furthermore, if the expansion paths of f for two different price vectors are different then we show that F is strictly monotone, which implies that the market demand function F satisfies the weak axiom of revealed preference. The result is applied to prove uniqueness and global stability in distribution economies and special exchange economies. </p> </abstract>
<abstract> <p>This paper presents an expected utility theory for state-dependent preferences. It proposes axioms that permit the joint derivation of subjective probabilities and utilities when the decision maker's preferences are not independent of the prevailing state of nature. In addition to the usual von Neumann-Morgenstern axioms, these axioms also include the requirement that the decision-maker's actual preferences are consistent with his preferences contingent on an hypothetical probability distribution over the states of nature. Two versions of the consistency axiom are introduced and their significance in the context of Bayesian decision theory is discussed.</p> </abstract>
<abstract> <p>This paper explores the possibilities opened up by combining preference aggregation and randomization in passing from individual to collective judgements about alternatives. We study the distribution of power under functions which assign to each profile of individual preferences a probabilistic judgement on each pair of alternatives x, y, that is, a probability of x being considered socially at least as good as y. Conditions are described under which the power of coalitions to guarantee that an alternative is not declared worse than another is subadditive, while their power to guarantee that one alternative is not declared better than another is superadditive. These results extend some of the main findings on the structure of decisive coalitions under deterministic social choice rules.</p> </abstract>
<abstract> <p>In order to develop a theory of coalition formation and maintenance, we first establish a valuation criterion for each individual player in a given coalition structure. Various stability concepts based on it are then developed and studied.</p> </abstract>
<abstract> <p> The main result of this paper is a generalization of the quasilinear mean of Nagumo [29], Kolmogorov [26], and de Finetti [17]. We prove that the most general class of mean values, denoted by &lt;tex-math&gt;$M_{\alpha \phi}$&lt;/tex-math&gt;, satisfying Consistency with Certainty, Betweenness, Substitution-independence, Continuity, and Extension, is characterized by a continuous, nonvanishing weight function α and a continuous, strictly monotone value-like function φ. The quasilinear mean &lt;tex-math&gt;$M_{\phi}$&lt;/tex-math&gt; results whenever the weight function α is constant. Existence conditions and consistency conditions with first and higher degree stochastic dominance are derived and an extension of a well known inequality among quasilinear means, which is related to Pratt's [31] condition for comparative risk aversion, is obtained. Under the interpretation of mean value as a certainty equivalent for a lottery, the &lt;tex-math&gt;$M_{\alpha \phi}$&lt;/tex-math&gt; mean gives rise to a generalization of the expected utility hypothesis which has testable implications, one of which is the resolution of the Allais "paradox." The &lt;tex-math&gt;$M_{\alpha \phi}$&lt;/tex-math&gt; mean can also be used to model the equally-distributed-equivalent or representative income corresponding to an income distribution. This generates a family of relative and absolute inequality measures and a related family of weighted utilitarian social welfare functions. </p> </abstract>
<abstract> <p>Income inequality as observed tends to overstate the true inequality. This is due to the fact that income observations are contaminated by measurement errors. On the other hand, the usual income classification tends to reduce observed income inequality. Without appropriate correction for both phenomena the value of comparing income inequality over various populations is dubious. In this paper the impact of both factors is assessed and a correction procedure suggested. Empirical evidence is provided on the basis of a large survey of Dutch families.</p> </abstract>
<abstract> <p>Humans exhibit a fundamental propensity to interact for social, cultural, technological, and other reasons. In this paper we seek to determine when the propensity to interact, expressed in the form of a spatial externality, becomes strong enough to induce agglomeration. We design an abstract world where the only possible reason for agglomeration is the spatial externality. In this world the uniform population distribution is a steady-state. Then to ask under what circumstances does a spatial externality induce agglomeration is to ask under what circumstances does it induce an instability of the steady-state: anything other than a uniform steady-state implies some agglomeration.</p> </abstract>
<abstract> <p>How should corporate capital be allocated and which, if any, rules of behavior ought to guide business investment decisions? Much of the debate in the 1950's literature (see Solomon [18]) which set out to answer these questions focused upon choosing between the top two contending criteria for correct investment selection: present value and internal-rate-of-return. By invoking the assumption of perfect capital markets (see Hirshleifer [10]), the debate was essentially resolved in favor of present value. However, by dropping the perfect capital markets assumption and imposing a borrowing constraint, we characterize the relationship between an investment project's asymptotic (internal) growth rate and its set of internal-rates-of-return; this characterization resolves the debate in favor of the latter criterion.</p> </abstract>
<abstract> <p>This article is devoted to computable techniques for solving comparative static problems when only the sign of the partial derivatives of the model is considered. We first show how to extract unambiguously signed multipliers, or more generally qualitatively linked multipliers. This information then helps to reduce the size of the original system by means of a qualitative aggregation principle which we establish. As to the computation of solutions, a branch-and-bound algorithm is presented which considerably increases the efficiency of the Samuelson-Lancaster elimination principle. Finally we derive an efficient algorithm to check for signed determinants. The techniques are then applied to the analysis of an actual 20 equation model.</p> </abstract>
<abstract> <p>A solution method and an estimation method for nonlinear rational expectations models are presented in this paper. The solution method can be used in forecasting and policy applications and can handle models with serial correlation and multiple viewpoint dates. When applied to linear models, the solution method yields the same results as those obtained from currently available methods that are designed specifically for linear models. It is, however, more flexible and general than these methods. The estimation method is based on the maximum likelihood principal. It is, as far as we know, the only method available for obtaining maximum likelihood estimates for nonlinear rational expectations models. The method has the advantage of being applicable to a wide range of models, including, as a special case, linear models. The method can also handle different assumptions about the expectations of the exogenous variables, something which is not true of currently available approaches to linear models.</p> </abstract>
<abstract> <p>ARMAX systems in structural specification are considered. Topological and geometric properties of the parameter space and of the parameterization which are important for the properties of the estimators, regardless of their special form, are investigated and the specification of the maximum lag lengths is discussed.</p> </abstract>
<abstract> <p>In the context of regression models with stochastic explanatory variables, the exact sampling distribution of the omitted variable (OV) estimator is derive. We show that stochastic regressors present the problem of larger variance for the OV estimator than in the nonstochastic case, and derive conditions under which omission may be better under a minimum mean square error criterion. Since the errors-in-variables problem can be interpreted as a specification error problem with stochastic regressors, we also consider the issue of MSE dominance of the proxy-variable estimator over the OV estimator.</p> </abstract>
<abstract> <p>An attempt is made to set out the implications of the log transformation on the stochastic properties of the model, which are postulated in the original multiplicative relationship. The estimation of the mean of the dependent variable, given some vector of explanatory variables, is accomplished by minimizing the mean square error within a certain class of estimators allowing for biased estimators, assuming known variance. The resulting estimator is modified in order to face the problem of unknown variance. This modified estimator turns out to dominate (in MSE) the least-squares, the ML, and the MVU Bradu and Mundlak estimator.</p> </abstract>
<abstract> <p>This empirical study presents an analysis of mode choice for selected urban trips in the San Francisco Bay area. The economic model is a restricted consumer choice model, where the mutually exclusive collectively exhaustive choice is between auto driver and transit passenger. The main testable hypothesis is that, in the absence of knowledge about the value a traveler attaches to his time, if a choice exists and if a mode is cheaper than the alternative in terms of both time and money, it should be chosen. The hypothesis was subjected to empirical analysis and, within the limits of the data, appeared to be a good approximation to reality. The statistical model used to further investigate the data was discrimination-classification analysis. The discriminant function can be interpreted as an indifference hypersurface of the indirect or constrained utility function. The statistical theory underlying the three versions of the model used is presented along with a derivation of the elasticity of choice. The data were a subset of the Bay Area Transportation Study Commission origin and destination home interview data merged with interzonal travel times and costs of both modes. Trips were stratified by purpose. Elasticities were calculated and compared both within a purpose by different variables and between purposes for each variable. Some potential policy changes were treated in the context of the model and compared with results of other studies.</p> </abstract>
<abstract> <p>In this paper bounds on the values of parameters of simultaneous equation models, implied by usual specifying assumptions, are derived and discussed. It is pointed out that such bounds can be estimated consistently and are formally analogous to those encountered in the classical errors-in-the-variables model.</p> </abstract>
<abstract> <p>It has been shown that the assumptions usually adopted in simultaneous equation models imply constraints on the structural parameters. Here, these constraints are investigated in two practical examples. Some general properties of the constraints are also derived.</p> </abstract>
<abstract> <p>We shall present an alternative proof of G. Debreu's theorem on the finiteness of the set of equilibria [1], and some related new results. The set of economies with finitely many equilibria is dense with respect to a very weak topology on the set of exchange economies. The equilibrium correspondence and the number of equilibria are continuous with respect to a strong topology on the set of regular economies where "regular" is defined as in [1]. Perhaps the methods used in this paper are more interesting than the results. The key concept we use is that of transversality. The essential regularity property of regular economies is that their excess demand functions are transversal to zero.</p> </abstract>
<abstract> <p>A decomposition technique for linear programs is presented, in which the master program distributes the common resources and aims directly among the subprograms, rather than using price setting as is done in the Dantzig-Wolfe method. The technique is essentially a dual formulation of the Dantzig-Wolfe method. Consequently the optimum is reached in a finite number of steps. This is in contrast with the Kornai-Liptak method.</p> </abstract>
<abstract> <p>The paper discusses cases where an econometric model linear in the variables is identified, but where the estimators are not asymptotically normally distributed. Maximum likelihood estimators and instrumental variable estimators are considered in some detail and the results are illustrated by means of a Monte Carlo simulation of a particularly simple case.</p> </abstract>
<abstract> <p>This paper advocates the use of simultaneous equations estimators (especially LIML) to estimate dynamic random effects models from panel data. The methods are found to perform quite satisfactorily in Monte Carlo experiments. The LIML procedures are also extended to the case where some of the regressors are correlated with the effects and a theorem on identification is proved. Finally, the Michigan Panel is used for some illustrations.</p> </abstract>
<abstract> <p>This paper describes an analysis of survey data on unemployed people who reported their asking wage and the wage they expected to earn. We show that if agents' behavior is described by the standard optimal job search model, one can use these data to deduce structural parameters rather than estimate them, and we report our deductions from two surveys^1 of British unemployed people.</p> </abstract>
<abstract> <p>The employment and earnings effects of the minimum wage on youth are analyzed by parameterizing the relationship between the distribution of market wage ratesindividuals would receive in the absence of the minimum and the observed distribution with the minimum. Estimates are based on individual data.</p> </abstract>
<abstract> <p>Price elasticities are estimated for local telephone calls and minutes of conversation using data from a pricing experiment in central Illinois conducted by General Telephone and Electronics. The experiment charges separately for calls and for minutes. Using a model that is consistent with the theory of telephone demand, the authors estimate the effects of both prices. The nonlinear generalized least squares estimates of the elasticities are fairly small--about 0.1 or less in absolute value at experimental price levels--but they are estimated with high precision. The report briefly considers the application of these results to predict the effects of introducing measured service telephone rates in other cities.</p> </abstract>
<abstract> <p>We consider an open question in applied price theory: Without a priori knowledge of a firm's cost function or a consumer's indirect utility function, it is possible to estimate price and substitution elasticities consistently by observing a demand system? As the work of White [30], Guilkey, Lovell, and Sickles [19], and others has shown, ordinary flexible functional forms such as the translog cannot achieve this objective. We find that if one is prepared to assume that elasticities of substitution cannot oscillate wildly over the region of interest then consistent estimation is possible using the Fourier flexible form provided the number of fitted parameters increases as the number of observations increases. This result obtains with any of the commonly used statistical methods as, for example, multivariate least squares, maximum likelihood, and three-stage least squares. It obtains if the number of fitted parameters is chosen adaptively by observing the data or chosen deterministically according to some fixed rule. We approach the problem along the classical lines of estimability considerations as used in the study of less than full rank linear statistical models and thereby discover that the problem has a fascinating structure which we explore in detail.</p> </abstract>
<abstract> <p>When is the Pareto optimal amount of public goods independent of income distribution? Subject to certain simple regularity conditions, the answer is "when preferences of each individual i can be represented by a utility function of the form U"i(X"i, Y) = A(Y) X"i + B"i(Y) where X"i is the amount of the (one)private good consumed by i and Y is the vector of public goods." Besides proving necessity and sufficiency conditions for utility to be of this special form, we show implications of this form for Lindahl equilibrium, majority voting, and the Groves--Clarke mechanism for preference revelation.</p> </abstract>
<abstract> <p>When a principal with private information designs a mechanism to coordinate his subordinates, he faces a dilemma: to conceal his information, his selection of mechanism must not depend on his information; but his information may influence which mechanism he prefers. To resolve this dilemma, this paper develops a theory of inscrutable mechanism selection. The principal's neutral optima are defined as the smallest possible set of unblocked mechanisms. They are shown to exist and are characterized using parametric linear programs. Any safe and undominated mechanism is a neutral optimum. Any neutral optimum is an expectional equilibrium and a core mechanism.</p> </abstract>
<abstract> <p>We compare six concepts of efficiency for economies with incomplete information, depending on the stage at which individuals' welfare is evaluated and on whether incentive constraints are recognized. An example is shown in which an incentive-efficient decision rule may be unanimously rejected by the individuals in the economy. We define durable decision rules, which can resist such unanimous rejection, and show that efficient durable decision rules exist.</p> </abstract>
<abstract> <p>We consider optimal capital accumulation in a nonlinear activity analysis model in which production and primary resource supplies are affected by a stationary stochastic process of exogenous shocks; the optimality criterion is the sum of discounted expected future social utilities. Under various "neoclassical" conditions on technology and preferences, (i) there exists an optimal policy of investment and consumption expressible as a continuous time-invariant function of the capital stocks and the history of stochastic shocks, and (ii) there is a stationary stochastic process of capital stocks that is consistent with the optimal policy.</p> </abstract>
<abstract> <p>In this paper we develop a rational expectations exchange-rate model which is capable of confronting explicitly agents' beliefs about a future switch in exogenous driving processes. In our set-up the agents know with certainty both the initial exogenous process and the new process to be adopted when the switch occurs. However, they do not know with certainty the timing of future switch as it depends on the path followed by the (stochastic) exchange rate. The model is discussed in terms of the British return to pre-war parity, in 1925. However, our results are applicable to a variety of situations where process switching depends on the motion of a key endogenous variable.</p> </abstract>
<abstract> <p>Forward and spot exchange rates are modelled as an unrestricted bivariate autoregression from weekly data on the New York foreign exchange market for June, 1973 to April, 1980. The null hypothesis that the forward exchange rate is an unbiased estimate of the corresponding future spot exchange rate is tested by means of a nonlinear Wald test and is rejected for all six currencies considered. The results cast doubt on a central assumption in many current models of exchange rate behavior.</p> </abstract>
<abstract> <p>Under the assumption that demand behavior depends on intertemporal preferences as well as (point) expectations concerning future prices, it is demonstrated that under plausible conditions rationality imposes no observable restrictions on the demand function and expectations and preferences are observationally indistinguishable.</p> </abstract>
<abstract> <p>The notion of a club efficient allocation, introduced in this paper, is shown to allow for a unifying treatment of rather disparate matters. The main result deals with the dual characterization of such allocations. The fundamental theorems of welfare economics, the limit theorem on the core, and a general version of a Henry George Theorem on the relationship between aggregate land rent and local public expenditures, all can be recovered as special cases of this paper's main result.</p> </abstract>
<abstract> <p>We discuss long-run properties of Leontief input-output systems that are subject to technical change. We derive a necessary and sufficient condition on changes in input coefficients for the economy to remain productive for all time. We define Harrod-neutral change in Leontief systems, and show that, under specified conditions, Harrod-neutral change increases output possibilities asymptotically as quickly as any alternative form of technical change.</p> </abstract>
<abstract> <p>A firm maximizing expected discounted profits, taking account of the actions of its competitors, choosing price and output before (stochastic) demand is known, and holding inventories or unfilled orders to accommodate discrepancies between output and demand, is shown to respond to a change in demand by changing its output whether the demand change is transitory or permanent, but by changing the price it charges only if the demand change is permanent. This proposition is shown to be consistent with German data. The data are qualitative. The empirical analysis uses the multivariate conditional logit model; the empirical results are summarized by gamma coefficients.</p> </abstract>
<abstract> <p>This paper examines the effect of the capital gains tax on investors' optimal consumption and investment behavior and on equilibrium asset prices in an intertemporal economy. It explictly considers the fact that capital gains and losses on stock are taxed only when the investor sells the stock. Ownership of stock then confers upon the investor a timing option which enables him to realize capital losses immediately and defer capital gains. This option is a large fraction of the total benefit which accrues to the stockholder, and is the prime reason for the novel implications of capital gains taxation, discussed in this paper.</p> </abstract>
<abstract> <p>This paper analyzes the optimal capital policy of an entrepreneurial firm whose cost of borrowing depends on its debt-equity ratio. The firm chooses the investment plan that maximizes the entrepreneur's intertemporal utility, given static expectations. It is shown that the rate of investment is closely related to the rate of profit retention. It is also demonstrated that the optimal plan can be approximated by a flexible accelerator model of investment. If expectations prove wrong, the investment behavior of the firm could involve instantaneous debt and capital stock adjustment prior to the operation of the flexible accelerator.</p> </abstract>
<abstract> <p>This paper derives an exhaustive set of restrictions implied by the multistock flexible accelerator specification of the adjustment-cost model for the firm. The flexible accelerator specification of the demand for capital and labor is estimated using U.S. manufacturing annual data for the 1947-1976 period. The consistency of the data with the model is examined by testing the validity of the derived restrictions. Finally, the conditions which permit aggregation over firms are determined and tested empirically.</p> </abstract>
<abstract> <p>This paper characterizes a market economy with infinitely long-lived consumers, and value-maximizing firms which face costs of adjustment for capital. The temporary equilibrium of this economy is similar to the short-run equilibrium of standard macroeconomic models. Consumption is a function of wealth, investment is related to the value of firms; equilibrium between aggregate demand and aggregate supply is achieved by the endogenous adjustment of the sequence of current and future interest rates. The dynamic behavior of output, consumption, and investment in this economy is the same as in an optimal growth model with adjustment costs. The paper shows this equivalence and then uses it, together with the equivalence of taxes to technological shocks, to study the dynamic effects of fiscal policy.</p> </abstract>
<abstract> <p>In this paper we develop and implement an econometric methodology estimating a family-specific exogenous component of life-expectancy in order to determine the responsiveness of fertility to exogenous changes in child mortality. We use a generalized waiting time regression model applied to length of life which is viewed as the output of a production process. We allow for family-specific heterogeneity in duration of life and for time-varying explanatory variables. The heterogeneity component retrieved from the production function estimation is used to estimate the impact of exogenous child mortality on a measure of fertility.</p> </abstract>
<abstract> <p>Recent work on inference in regressions with heteroscedastic disturbances leads to the possibility of more efficient estimators. The conditions needed to exploit the possibility are discussed. A sampling experiment indicates that the gains can be achieved even in small samples.</p> </abstract>
<abstract> <p> When the binary choice probability model is derived from a random utility maximization model, the choice probability for one alternative has the form F[V(z, θ)]. Here V(z, θ) is a given function of the exogenous variables z and unknown parameters θ, representing the systematic component of the utility difference, and F is the distribution function of the random component of the utility difference. This paper describes a method of estimating the parameters θ without assuming any functional form for the distribution function F, and proves that this estimator is consistent. F is also consistently estimated. The method uses maximum likelihood estimation in which the likelihood is maximized not only over the parameter θ but also over a space which contains all distribution functions. </p> </abstract>
<abstract> <p>Particularly under the assumption of rational expectations, a model may have serially correlated errors and those errors may be uncorrelated with contemporaneous and lagged values of a predetermined instrument, yet the instruments may not be strictly exogenous. This paper proposes a method for transforming such a model to one without serial correlation, while keeping the instrument predetermined. Standard theory of instrumental variables estimation then applies. Furthermore, it turns out that for transformations of the class proposed, asymptotic distribution theory is the same whether the serial correlation properties of the errors are known a priori or estimated. As the number of lagged values of the predetermined variables used as instruments increases, the asymptotic variance of the standard instrumental variables estimator applied to the transformed model approaches that of the optimal estimator proposed by Hansen and Sargent [8].</p> </abstract>
<abstract> <p>This paper considers the maximum likelihood estimator of the first order moving average process when the true value of the coefficient is one. The results are also extended to regression analysis. It is shown that there is a local maximum of the likelihood function within an interval of O(T^-^1) of the true value and also that the probability that the maximum occurs exactly at the true value and also that the probability that the maximum occurs exactly at the true value can be calculated in finite samples.</p> </abstract>
<abstract> <p>In the estimation of structural coefficients it is well-known that both two-stage least squares (TSLS) and limited information maximum likelihood (LIML) estimators are consistent and asymptotically efficient, and that the exact mean of the LIML estimator does not exist. Then the TSLS estimator, which is computationally simpler, has appeared a proper choice to empirical researchers. In this article asymptotic properties of the k-class and related estimators are sorted out according to the ratio between the total number of exogenous variables and the number of observations. It is found that the TSLS distribution deviates far from its traditional asymptotic distribution; the LIML distribution stays stable about its traditional asymptotic distribution. The LIML estimator now seems more attractive than the TSLS estimator except for the fact that its exact moments do not exist. A modified estimator is proposed which is asymptotically better than the LIML estimator and whose exact moments exist.</p> </abstract>
<abstract> <p>This paper is concerned with linear models. In this context it gives a necessary and sufficient condition for equality of two generalized least squares estimators of any subset of the parameters. The result is applied to many kinds of problems: the Frisch-Waugh problem, equality of OLS and GLS estimators, equality of the single equation GLS and the overall GLS estimators in seemingly unrelated regressions, equality of partial and overall 3SLS, lineartransformation of a linear model, superfluous observations, and mixed estimation.</p> </abstract>
<abstract> <p>A simple method for approximating the maximum likelihood solution to the truncated normal regression model is suggested. This approximation uses the results from a linear regression and a table of conversion factors which is produced here. The approximation is quite accurate and compares favorably with Amemiya's estimator.</p> </abstract>
<abstract> <p>This paper is concerned with giving general conditions for the validity of a approximation to a distribution function by means of the weighted sum of a set of Chi-squared distribution functions. The conditions are very similar to those of [3]. The approximation is illustrated by an example of its use.</p> </abstract>
<abstract> <p>This paper discusses inferential procedures for the family of stable distributions, when the data are tabulated in the form of interval frequencies. The estimation criteria used are minimum chi-square and multinomial maximum likelihood. In evaluating the theoretical probabilities corresponding to the intervals, use is made of the inversion theorem for characteristic functions. Chi-square tail probabilities for independent samples are pooled by means of theKolmogorov statistic. As an illustration, the methods are applied to Dutch and Australian income data.</p> </abstract>
<abstract> <p>This paper is concerned with testing for causation, using the Granger definition, in a bivariate time-series context. It is argued that a sound and natural approach to such tests must rely primarily on the out-of-sample forecasting performance of models relating the original (non-prewhitened) series of interest. A specific technique of this sort is presented and employed to investigate the relation between aggregate advertising and aggregate consumption spending. The null hypothesis that advertising does not cause consumption cannot be rejected, but some evidence suggesting that consumption may cause advertising is presented.</p> </abstract>
<abstract> <p>Production problems in which profit is to be maximized subject to constraints onresources and outputs are frequently modeled as linear programs. While in practice the technology coefficients are often treated as constants, they are frequently better regarded as random variables. Stochastic programming models, first developed in the 1950's, recognized that explicit representation of randomness in factors of production is sometimes necessary to derive program solutions [3]. In 1970 Resh [12] noted that a certain class of implied chance-constrained models with zero-order decision rules were utilizing inappropriately conservative constraints. We have extended his results from integer to continuous decision variables and present experimental evidence from dental services production that substantial undercalculation of "optimal" output does occur when Resh's model variant is not used. We show that solutions for continuous variable models can be closely approximated by linear programming models derived by Resh in an integer case.</p> </abstract>
<abstract> <p>A dynamic noncooperative game in which firms choose output and cost-reducing investment sequences is developed. The sequences exhibit several properties of manufacturing industries. Several steady states exist. Under some reasonable conditions only industry structures in which firms have different market shares can be locally stable steady states. So the model presents one explanation of the source of differences among firms in homogeneous good oligopolies.</p> </abstract>
<abstract> <p>In this paper we prove a global index theorem for general equilibrium models with activity analysis production technologies. We begin by constructing a single-valued function whose fixed points are equivalent to the equilibria of such a model. We then associate each fixed point with an index that is an integer determined by the local properties of this function at that point. The global index theorem makes a statement about the sum of all the indices of equilibria that implies conditions sufficient for uniqueness of equilibrium.</p> </abstract>
<abstract> <p>In this paper we discuss the (Pareto) optimal provision of a public good (desirable or undesirable) in an intergenerational model of an economy, where regeneration is an endogenous decision variable of the households in the economy. We show that in addition to providing desirable public goods and/or levying the well known Pigouvian taxes on polluting industries, a government must subsidize households who are consuming a desirable public good and tax consumers who are consuming an undesirable public good (pollution). In addition, we show that there is public rate of return, distinct from the market rate of return, that should be used by the government for the evaluation of investments in public goods. This public rate of return is smaller than the market rate of return in a growing economy and larger than the market rate of return in a declining economy. We investigate biases in both the public and market rates of return, as well as in other parameters which characterize the economy, as a result of nonoptimal governmental behavior. We discuss the question of how to aggregate biases due to different public goods.</p> </abstract>
<abstract> <p>Normally the objective functions used in optimal growth theory either treat present generations more favorable than future ones or give an incomplete ranking of possible welfare distributions. The problem is the infinite horizon. In this paper we analyze the conditions under which it is possible to treat a countably infinite number of generations equally. The existence of equitable objective functions or preferences, giving a complete ranking of possible welfare distributions, is proved.</p> </abstract>
<abstract> <p>The purpose of this paper is to reconsider the theory of Pareto optimal redistribution from a game-theoretic point of view. We define an income redistribution game in strategic form, which may allow many varieties of utilityinterdependencies. The strategy of each individual is a vector that describes a plan of transfers from him to every other individual. The results state that while a Pareto optimal redistribution is always achieved by a Nash equilibrium in the two-person case, it is not so when there are more than two individuals. The sufficient condition we derived seems unlikely to be satisfied in general.</p> </abstract>
<abstract> <p>Rational behavior under complete ignorance is described by means of requirements such as invariance of choice with respect to modifications of states of nature. Possible criteria necessarily involve intransitivities of indifference, and are incompatible with the ascribing of personal probabilities to events. Characterization of criteria shows that in first order approximation they take into account only the extremal possible outcomes of each choice; effects linked to events also come into play, although only in the second order, whereas an axiom system like that of Arrow and Hurwicz, which requires transitivity of indifference, excludes their being taken into account at all.</p> </abstract>
<abstract> <p>This paper develops a sequential model of the individual's economic decision problem under risk. On the basis of this model, optimal consumption, investment, and borrowing-lending strategies are obtained in closed form for a class of utility functions. For a subset of this class the optimal consumption strategy satisfies the permanent income hypothesis precisely. The optimal investment strategies have the property that the optimal mix of risky investments is independent of wealth, noncapital income, age, and impatience to consume. Necessary and sufficient conditions for long-run capital growth are also given.</p> </abstract>
<abstract> <p>This paper is concerned with the estimation of an econometric model for the Israeli economy as it existed through 1965. The model is disaggregated to seven sectors and contains substantial detail for import and export equations. Multiplier analysis suggests that the economy is stable for unemployment rates around eight percent but is unstable at full employment unless discretionary fiscal and monetary policies are applied. A new version of the IS-LM curve is developed to explain this result. The model is used to "forecast" the recession of 1966 and it is determined that the ex post record is more successful than official government predictions issued at that time.</p> </abstract>
<abstract> <p>This paper introduces the concept of size-of-risk aversion, discusses its mathematical properties, and relates it to other measures of risk aversion. Decision makers whose utility functions display size-of-risk aversion will pay greater percentage premiums over fair actuarial value to insure against risks of greater magnitude.</p> </abstract>
<abstract> <p>In this paper recursive decision systems are structured so that topological concepts can be applied to formulate and help solve existence problems. Existence of stationary states and compact orbits is established. The analysis is then applied to recursive programs, a special class of recursive decision systems in which the decision operator is a mathematical program. The theory is extended to models of many decision makers with rolling schedules of future actions.</p> </abstract>
<abstract> <p>In recent years spectral techniques have been used to assess the effects of applying various types of seasonal adjustment procedures to economic time series. Similar analyses using artificially generated time series have also been attempted. The effects, desirable or undesirable, of a particular method of seasonal adjustment can, however, only be assessed properly in the time domain and only in relation to the objectives of such adjustment. Despite the fact that such objectives have not been clearly formulated nor any definitive conception of the nature of seasonality developed, in this paper we do adopt a general approach consistent with what has been written on the subject since the time of Jevons. In terms of a simple three component model of an economic time series having properties similar to those found in many actual time series, we devise several "methods" of seasonal adjustments based on a minimum mean-square-error criterion of optimality. We show that such methods of seasonal adjustment produce seasonally adjusted series bearing the same relationship to the unadjusted series in spectral terms as that found by Nerlove and others in their studies of BLS and Census methods of adjustment. Our conclusion is not that spectral methods are useless, but rather that comparisons in the frequency domain must be interpreted with great care. Further research must emphasize objectives and models. Whether these are formulated in frequency terms or in the time domain is of secondary importance.</p> </abstract>
<abstract> <p>The relationship between efficiency and competitiveness of production programs is explored in a general disaggregated model.</p> </abstract>
<abstract> <p>Two stage least squares methods are used to estimate a postwar quarterly model of U.S. labor demand, supply, and wage adjustment. Analytical techniques are used to derive the long-run equilibrium properties of the estimated model. Short run properties are obtained by approximating the model in the form of two simultaneous difference equations. Simulation methods show the response of the model to an increase in the size of the armed forces.</p> </abstract>
<abstract> <p>In this paper a distribution sampling study consisting of four major experiments is described. The L1 norm is employed in two new estimating techniques, direct least absolute (DLA) and two-stage least absolute (TSLA), and these two are compared to direct least squares (DLS) and two-stage least squares (TSLS). Four experiments testing the normal distribution case, a multicollinearity problem, a hetoroskedastic variance problem, and a misspecified model were conducted. Two small sample sizes were used in each experiment, one with N = 20 and one with N = 10. In addition, conditional predictions were made using the reduced form of the four estimators plus two direct methods, least squares no restrictions (LSNR) and another new method known as least absolute no restrictions (LANR). The general conclusion was that the L1 norm estimators should prove equal to or superior to the L2 norm estimators for models using a structure similar to the overidentified one specified for this study, with randomly distributed error terms and very small sample sizes.</p> </abstract>
<abstract> <p>Production correspondences are defined and their properties explored. Properties of the distance function of a production structure are studied and are related to properties of the correspondence. Homotheticity is generalized to allow discontinuities in returns to scale and to take cognizance of several outputs. The factorization of the distance function into the product of two functions, one depending only on output and the other only on input, is demonstrated. The cost function is then shown to give rise to a production structure. A Shephard-type duality theorem is proven which implies, among other things, that the cost function and the distance function are dually related; that is, given one, a minimization proglem yields the other. The duality theorem is then used to deduce necessary and sufficient conditions, in terms of the cost function, for a production correspondence to be homothetic. A few simple applications of the duality theorem are given which relate properties of the cost function to geometric properties of the correspondence.</p> </abstract>
<abstract> <p>Most analyses of the principal-agent problem assume that the principal chooses an incentive scheme to maximize expected utility subject to the agent's utility being at a stationary point. An important paper of Mirrlees has shown that this approach is generally invalid. We present an alternative procedure. If the agent's preferences over income lotteries are independent of action, we show that the optimal way of implementing an action by the agent can be found by solving a convex programming problem. We use this to characterize the optimal incentive scheme and to analyze the determinants of the seriousness of an incentive problem.</p> </abstract>
<abstract> <p> Fix-price equilibria à la Benassy are shown to be unique under a condition, closely related to the macroeconomic stability condition, at any price system p &gt;&gt; O. </p> </abstract>
<abstract> <p>This paper presents an implicit-contract model in which workers are allowed to differ in both their productive abilities and their preferences. It is shown that if firms are able to vary hours costlessly among their workers, workers are risk averse, and there are no outside payments to laid-off workers, then efficient contracts between firms and their workers will never provide for layoff unemployment. If, however, such hours variations are not costless, layoffs are no longer generally inefficient since they are an alternative means by which firms can adjust the labor inputs of selected groups of workers.</p> </abstract>
<abstract> <p>A utility maximizing consumer with a completely known system of ordinary demand functions q = h(p,C) is considered. Let (p^o, q^o) and (p^1,q^1) to two arbitrary equilibrium situations; the problem is to evaluate in which of the situations the utility is higher without knowing the utility function. Revealed preference theory tells that the ordinary demand functions (which are in principle observable) contain enough information to solve the problem. Remaining difficulties are therefore mainly computational. We present how the how the compensated income C^1 = C(p^1,q^o) and the compensated demand q^1 = h(p^1,C^1) are calculated with arbitrary accuracy using only the ordinary demand system. Our two efficient algorithms also have interesting interpretations in terms of index numbers and consumer surplus measures.</p> </abstract>
<abstract> <p>Horizontal equity and social mobility are discussed in terms of a non-utilitarian social welfare function which takes account of the process by which an ex ante distribution is mapped into an ex post distribution (by the tax system, for example). An index of inequality is proposed which decomposes into two components, corresponding to vertical and horizontal equity respectively. A functional form for the social welfare function is derived for the purposes of empirical work, and an application to data for 5895 UK households is presented. The paper contains a theoretical application of the index to a model of optimal taxation.</p> </abstract>
<abstract> <p>This paper is concerned with the efficient estimation of structural parameters in closed linear systems of higher order stochastic differential equations when the data are in discrete form and the model generally includes both stock and flow variables. A general existence and uniqueness theorem for the solution of such a system is proved and used in the rigorous derivation of exact discrete models satisfied by the various types of data. It is shown how these models can be used in the computation of various asymptotically efficient estimates obtained by the maximization of the Gaussian likelihood or approximations to it.</p> </abstract>
<abstract> <p>This paper considers the null hypothesis that the errors on a regression equation form a random walk. By using the standard Durbin-Watson assumptions, we derive three test statistics that are uniformly most powerful against the alternative hypothesis that the errors are being generated by the stationary first order Markoff process. Unfortunately, the tabulated lower and upper bounds are too wide apart and so we compare the powers of the three tests using simulated as well as economic data. It is then recommended that the Imhof routine should be attached to standard regression programs to calculate the exact limit of the Berenblut-Webb statistic.</p> </abstract>
<abstract> <p>This paper examines the identifiability of the coefficients of a single equation in a simultaneous equation model which is nonlinear only in the variables. The concept of identifiability in this model is motivated and developed using the closely related concept of observational equivalence. This framework is then utilized to develop necessary and sufficient conditions for identifiability when the disturbances are required to be independent of the exogenous variables. The approach recommended by Fisher is shown to yield sufficient but not necessary conditions for identifiability. For several relatively common special cases the necessary and sufficient conditions are found to simplify to the familiar rank condition for identifiability in the linear model.</p> </abstract>
<abstract> <p>This paper presents the necessary and sufficient conditions for determining the signs of the solution variables of a system of linear equations based only upon a knowledge of the signs of the coefficient matrix and the signs of the "right hand side" variables. This problem was initially formulated in economics due to the idea that the signs of an equation's derivatives might have a stronger empirical basis than that of a particular functional form. A new interest in qualitative problems has arisen in connection with the need to develop analytic measures in order to better manage the understanding and use of large, computer-based mathematical systems. The conditions for the qualitative determinancy of nonhomogeneous systems are developed in terms of a small number of necessary conditions which are jointly sufficient. Algorithmic approaches are given for testing a given system for qualitative determinancy. For nonhomogeneous systems algorithms are given for constructing all possible qualitatively determinate systems of a given size. For the homogeneous case conditions are also given for the qualitative invertibility of the (irreducible) coefficient matrix. These conditions are then related to the problem of partially qualitatively determinate systems and the signs in the qualitative inverse of a matrix.</p> </abstract>
<abstract> <p>In this paper, I attempt to peek into the "black box" of the firm and explore some very simple models of expectation formation and planning using data on a group of French and German manufacturing firms who report over time on both expectations and their subsequent realizations. Important differences are obtained between the results for French firms and for German firms. For both groups, however, firms' expectations or plans are found to be much more concentrated in the no change category than are the realizations they forecast. Consistent biases in the other categories are found for the German, but not the French firms; for the former the conditional distributions of realizations, given prior expectations or plans, are stable over time, while they are unstable for the latter. Of the simple models dealing with the formation of price and demand expectations, the error-learning model (a form of adaptive expectations) gives the best and most parsimonious explanation of the data. Estimation of a joint error-learning model for price anticipations and production plans suggests that the two processes are nearly independent of one another for both groups. A conditional probability model relating production plans to expectations of future demand, inventory level, or order backlog appraisals and recent changes in demand, explains the data about as well as a mechanical error-learning model, but offers scope for improvement and more economic content. Deviations between prior expectations of demand and realizations in the current period are found to affect the deviations between price expectations and production plans from their respective realizations, except for French firms' price expectations. These and related variables and relationships are further explored in a recursive conditional log-linear probability model which is discussed in a sequel to the paper.</p> </abstract>
<abstract> <p>We examine the implications of arbitrage in a market with many assets. The absence of arbitrage opportunities implies that the linear functionals that give the mean and cost of a portfolio are continuous; hence there exist unique portfolios that represent these functionals. These portfolios span the mean-variance efficient set. We resolve the question of when a market with many assets permits so much diversification that risk-free investment opportunities are available. Ross [12, 14] showed that if there is a factor structure, then the mean returns are approximately linear functions of factor loadings. We define an approximate factor structure and show that this weaker restriction is sufficient for Ross' result. If the covariance matrix of the asset returns has only K unbounded eigenvalues, then there is an approximate factor structure and it is unique. The corresponding K eigenvectors converge and play the role of factor loadings. Hence only a principal component analysis is needed in empirical work.</p> </abstract>
<abstract> <p>We present a definition of factor structure that is less restrictive than the one typically used in arbitrage pricing models. Our factor structure restrictions build on the following intuitive distinctions between factor variance and idiosyncratic variance: (i) A well-diversified portfolio contains only factor variance. (ii) If a portfolio is uncorrelated with the well-diversified portfolios, then it contains only idiosyncratic variance; so if a sequence of such portfolios becomes well-diversified, the limiting variance should be zero. Our factor structure restrictions imply Ross' [5] arbitrage pricing formula. We obtain upper and lower bounds on the approximation error in that formula; these bounds may be useful in empirical work. They imply that arbitrage pricing is exact if and only if there is a risky, well-diversified portfolio on the mean-variance frontier. If all mean-variance efficient portfolios are well-diversified, then the well-diversified portfolios provide mutual fund separation. Our factor structure restrictions are satisfied (with K factors) if and only if the covariance matrix of asset returns has only K unbounded eigenvalues as the number of assets increases.</p> </abstract>
<abstract> <p>Economic theorists have interpreted the "efficient markets hypothesis" to assert that equilibrium asset prices reveal all decision-relevant information in the market. This paper establishes conditions on investors' utility functions of future wealth which are necessary for the efficient markets hypothesis to be satisfied and be robust to slight perturbations of endowments and the joint distribution of current information and future asset values. The main result states that over the relevant range of future wealth values, there are three possible cases: (i) all investors are risk-neutral; (ii) modulo a change in the wealth origin, each investor has constant relative risk aversion with the same constant for all investors; or (iii) all investors have constant absolute risk aversion.</p> </abstract>
<abstract> <p>We consider a portfolio selection problem for an investor who consumes at the end of a finite horizon. With important qualifications on the sufficiency part, we show that convergence of the optimal investment policy as the horizon becomes distant occurs if and only if the corresponding Arrow-Pratt coefficient of relative risk aversion converges as wealth increases. A major step in the proof shows that convergence of the Arrow-Pratt coefficient of relative risk aversion is equivalent to regular variation of the marginal utility function.</p> </abstract>
<abstract> <p>This paper analyzes the effects of futures trading in a market for a storable commodity, in which producers and speculators are assumed to be risk averse and specifications of the aggregate supply and inventory demand functions are derived from explicit optimization. A critical aspect is how the parameters of these functions change with the introduction of the future market as it is through these induced parameter changes that the futures market exerts its influence on the spot price. The effects of the futures market on both the long-run average spot price and its variance are analyzed. While we are unable to draw any definitive conclusions on this issue, we find that in all cases considered the futures market stabilizes the spot price, as well as lowering its long-run mean.</p> </abstract>
<abstract> <p>The well known result that every finite, strictly deterministic game with perfect information has a unique solution unless the utility functions of the players lie in a low dimensional exception space, is generalized to games containing change moves. Two group decision procedures, "voting by successive proposal and veto" and "voting by repeated veto," are analyzed in this context. The first procedure is efficient, anonymous, and neutral for an arbitrary number n of participants and an arbitrary finite set of alternatives, the second only if n @? 3.</p> </abstract>
<abstract> <p>We analyze equilibria in exchange economies following the approach taken by Borch and Wilson. We correct Wilson's main result by showing that linear contracts are sufficient but not necessary for the existence of syndicates under heterogeneous beliefs. We introduce the concept of the decomposability of syndicates and demonstrate that linear contracts are necessary only for arbitrarily decomposable syndicates. This implies that groups that form syndicates under an arbitrary set of beliefs must also employ linear contracts.</p> </abstract>
<abstract> <p>A model of the determination of the union status of workers is developed that incorporates the separate decisions of workers and potential union employers and recognizes the possibility of an excess supply of workers for existing union jobs. This model is estimated using data from the Quality of Employment Survey that have a unique piece of information on worker preferences which allows identification and estimation of the model. The empirical results yield some interesting insights into the process of union status determination which cannot be gained from a simple logit or probit analysis of unionization.</p> </abstract>
<abstract> <p>Strategic considerations may induce a resource importing country to invent a substitute earlier than it intends to put it to use. There are also circumstances in which it would wish to delay an invention date even if it could obtain it at an earlier date at no extra cost. Similar paradoxical results obtain if resource cartels behave strategically. Setting prices high may be a way of deterring invention. If those engaged in R &amp; D are not resource users, and the cartel has access to similar R &amp; D technology, it will pre-empt rivals. This may not be the case if resource users can also engage in R &amp; D.</p> </abstract>
<abstract> <p>The purpose of this article is to provide a deeper empirical insight into the structural change of an industry which is more relevant than that obtained by an analysis based on the traditionally estimated average production function. The main contribution is a long run analysis of technical progress and structural change by means of the short-run industry production function introduced by Johansen [13], and based on micro data for individual production units. For that purpose we have developed Johansen's approach into an operational framework for discrete capacity distributions including a special algorithm for the computation of the short-run industry production function.</p> </abstract>
<abstract> <p>In a market where firms offer products which differ in quality, an upper bound may exist to the number of firms which can coexist at a noncooperative price equilibrium. We fully characterize the conditions under which this possibility arises.</p> </abstract>
<abstract> <p>A general equilibrium model with money and finitely many immortal consumers is studied. Consumers hold money for self-insurance against random fluctuations. Money may earn interest. Equilibria exist if the rate of interest is sufficiently small. Equilibria may not exist if the rate of interest is too close to some consumer's rate of pure time preference. It follows that Pareto optimality may not be guaranteed by paying interest on money.</p> </abstract>
<abstract> <p>This article proposes a new approach to small sample theory that achieves a meaningful integration of earlier directions of research in this field. The approach centers on the constructive technique of approximating distributions developed recently by the author in [10]. This technique utilizes extended rational approximants (ERA's) which build on the strengths of alternative, less flexible approximation methods (such as those based on asymptotic expansions) and which simultaneously blend information from diverse analytic, numerical and experimental sources. The first part of the article explores the general theory of approximation of continuous probability distributions by means of ERA's. Existence, characterization, error bound, and uniqueness theorems for these approximants are given and a new proof is provided for the convergence result obtained earlier in [10]. Some further aspects of finding ERA's by modifications to multiple-point Pade approximants are presented and the new approach is applied to the noncircular serial correlation coefficient. The results of this application demonstrate how ERA's provide systematic improvements over Edgeworth and saddlepoint techniques. These results, taken with those of the earlier article [10], suggest that the approach offers considerable potential for empirical application in terms of its reliability, convenience, and generality.</p> </abstract>
<abstract> <p>Necessary and sufficient conditions for identification with linear coefficient and covariance restrictions are developed in a limited information context. For the limited information case, covariance restrictions aid identification if and only if they imply that a set of endogenous variables is predetermined in the equation of interest (generalizing the idea of recursiveness). Under full information, covariance restrictions imply that residuals from other equations are predetermined in a particular equation and, under certain conditions, can aid in identification. Sufficient conditions for identification are obtained for the hierarchical system in which the identification of a particular equation does not depend upon the identifiability of higher-numbered equations. In the general case, the FIML first order conditions show that if the system of equations is identifiable as a whole, covariance restrictions cause residuals to behave as instruments. In both limited and full information settings, the link between identification and estimation is worked out: restrictions useful for identification yield instruments required for estimation.</p> </abstract>
<abstract> <p>When estimating a single equation with an error generated by an autoregressive process of higher order than one using a sequence of likelihood ratio tests to determine the correct order, the asymptotic size of the tests will be biased because of multiple optima of the likelihood function. A new type is suggested similar to the Durbin test [2] which is not biased in this way.</p> </abstract>
<abstract> <p>A firm minimizes cost or maximizes profit subject to the constraint implied by a production function. Demand equations for inputs, formulated in terms of changes over time, are described in two steps. The first is the total input decision, which describes the Divisia input volume index in terms of the change in output (for cost minimization) or price changes (for profit maximization). The second is the input allocation decision, which describes the changes in the demand for the individual inputs in terms of the Divisia input volume index and the input price changes. It is shown that the input allocation decision allows a simple transformation so that (1) the change in the demand for each transformed input is independent of the changes in the relative prices of all others and (2) the log-change in output is the sum of certain components, each representing the contribution of one transformed input, in such a way that the interaction of these inputs is confined to terms of the third order of smallness.</p> </abstract>
<abstract> <p>Research on the spatial firm has shown that spatial results often differ from non-spatial. This paper considers the relationship between the price of a spatial monopoly and the price of a spatially competitive firm. The conditions under which the competitive price will exceed the monopoly price are outlined.</p> </abstract>
<abstract> <p>This paper describes a decentralized planning procedure which converges to a global optimum--as seen by a central planning board--whether or not the production possibility sets of the firms are convex. All information is exchanged in the form of quantities: the planning board proposes quotas and the firms respond with feasible production programs.</p> </abstract>
<abstract> <p>This paper describes a market in which firms vary their quantities of production according to a new adjustment process. Each firm bases its new production entirely upon a knowledge of its own previous productions and profits. It has no knowledge of the payoff functions of the market. Numerical analysis of the process indicates an approach to equilibrium for all initial states. The set of allowed limit points is rigorously characterized, and determined explicitly in the case of two firms. Some exact solutions are found. The process can be regarded as a way of playing a continuous game with a minimum of information.</p> </abstract>
<abstract> <p>In recent studies of the temporary competitive equilibrium, agents' current decision correspondences are derived using a standard recursion procedure, which is only applicable when the planning horizon is finite. This paper presents a general derivation of the current decision rule without restrictions on the time horizon or the number of states of the world in any period. It is shown that if utility is continuous in the product topology and if, in each period, expectations and the current constraint correspondence are continuous, then the current decision rule is upper semi-continuous. This result is obtained by associating with each current decision a set of feasible future plans. The expected utility of a current decision is then the expected utility of the best feasible future plan. The feasible future plan correspondence is shown to be continuous and the Maximum Theorem completes the proof.</p> </abstract>
<abstract> <p>This paper examines conditions for the uniqueness of an equilibrium price distribution in stochastic macroeconomic models with rational expectations. A model is developed in which many price distributions, each with a finite variance, satisfy the equilibrium requirements of rationality. Hence, the condition that the variance of the equilibrium price distribution be finite, or equivalently, that the conditionally expected price path be stable, does not guarantee uniqueness. In such cases it is shown that an arbitrary random quantity which is widely publicized can become a leading indicator of prices and, consequently, influence the behavior of actual prices. However, by extending the finite variance (stability) condition to a minimum variance condition, these nonuniqueness problems can be avoided. Such stability or minimum variance conditions suggest a kind of collective rationality which, although not unreasonable, has not yet been fully analyzed in rational expectations models.</p> </abstract>
<abstract> <p>Samuelson conjectured that Arrow's impossibility theorem will hold in a cardinal setup where individuals and society express their preferences by von-Neumann Morgenstern utility functions. It is shown that this is true provided that an additional axiom of continuity is imposed and that it is not true when continuity is not required.</p> </abstract>
<abstract> <p>Using the methods of the theory of structural stability it is possible to discuss the existence of stable limit cycles in the set of complete Keynesian systems. Control possibilities are also discussed.</p> </abstract>
<abstract> <p>This paper attempts to determine conditions under which distributed lag analysis is appropriate. Results indicate that lag functions are stable and linear under fairly general (but constant) objective criteria and decision constraints as long as the underlying economic environment is characterized by a stationary Gauss-Markov process, and observed environmental variables as Gaussian perturbations of that process. The results appear particularly useful for specifying the lag distribution inherent in subjective parameters in specific decision problem contexts.</p> </abstract>
<abstract> <p>Univariate autoregressive moving average models for the endogenous variables of a dynamic simultaneous equations system can be interpreted as a form of solution of that system. This paper considers the interrelationships between the various representations of the system, and develops joint estimation and model selection procedures for the multiple time series model which arises as a multivariate representation of the individual autoregressive moving average models. A test of the restriction of common autoregressive parameters is incorporated. Two empirical examples are presented, the first concerned with a model of the hog cycle and the second with a model of the United States economy previously considered by Zellner and Palm.</p> </abstract>
<abstract> <p>The chief difficulty in applying Aitken estimators to large linear systems stems from the dimensionality of the inverse. Here the conjugate gradient algorithm is applied to the problem, leading to substantial savings in storage and some savings in time. Given that the information matrix is not computed, an inference procedure is developed which involves two easily computed statistics which straddle the conventionally estimated standard errors. The paper is intended to open the way for the application of more efficient estimation procedures to large econometric systems.</p> </abstract>
<abstract> <p>A theorem on the validity of Edgeworth type expansions for sample distributions of quite general statistics with limiting normal distributions is established. The result is related to the allied work of Sargan [12] and Chambers [3].</p> </abstract>
<abstract> <p>A basic problem in the theory of noncooperative games is the following: which Nash equilibria are strategically stable, i.e. self-enforcing, and does every game have a strategically stable equilibrium? We list three conditions which seem necessary for strategic stability--backwards induction, iterated dominance, and invariance--and define a set-valued equilibrium concept that satisfies all three of them. We prove that every game has at least one such equilibrium set. Also, we show that the departure from the usual notion of single-valued equilibrium is relatively minor, because the sets reduce to points in all generic games.</p> </abstract>
<abstract> <p>A price equilibrium existence theorem is proved for exchange economies whose consumption sets are the positive orthant of arbitrary topological vector lattices. The motivation comes from economic applications showing the need to bring within the scope of equilibrium theory commodity spaces whose positive orthant has empty interior, a typical situation in infinite dimensional linear spaces.</p> </abstract>
<abstract> <p>The paper develops and estimates a theoretical model of wage determination and union-nonunion wage differentials. In order to overcome the institutional criticisms of the formal bargaining literature, the paper generalizes the Nash-Zeuthen-Harsanyi model by linking the solution to the institutional concepts of bargaining power and fear or cost of disagreement and by making the outcome depend not only on endogenous but also on exogenous factors. An operational specification of bargaining power and fear of disagreement allows the model to be estimated with data covering twelve companies and trade unions during the period from mid-1950's to the late 1970's. While giving limited support to the Nash-Zeuthen-Harsanyi solution, the empirical analysis indicates that the bargaining outcome usually deviates from the Nash-Zeuthen-Harsanyi point and, in accordance with the institutionalist claim, that it varies significantly with exogenous factors. Contrary to the traditional labor economics view, the results do not support the general conclusion that the bargaining solution lies on the marginal revenue product curve of labor. Instead, the relevant coefficients suggest that for many firms and unions the outcome might be better characterized by the efficient contract (vertical contract curve).</p> </abstract>
<abstract> <p>This paper examines the implications of adverse selection in the private annuity market for the pricing of private annuities and the consequest effects on consumption and bequest behavior. With privately known heterogeneous mortality probabilities, adverse selection causes the rate of return on private annuities to be less than the actuarially fair rate based on population average mortality. However, a fully funded social security system with compulsory participation can offer an implied rate of return equal to the actuarially fair rate based on population average mortality. Thus, since social security offers a higher rate of return than private annuities, consumers cannot completely offset the effects of social security by transacting in the private annuity market. Using an overlapping generations model with uncertain lifetimes, we demonstrate that the introduction of actuarially fair social security reduces the steady state rate of return on annuities. In addition, it raises the steady state levels of average bequests and average consumption of the young, if factor prices are fixed exogenously. The steady state national capital stock rises or falls according to the strength of the bequest motive.</p> </abstract>
<abstract> <p>This paper develops a positive theory of credibility, ambiguity, and inflation under discretion and asymmetric information. The monetary policymaker maximizes his own (politically motivated) objective function that is positively related to economic stimulation through monetary suprises and negatively related to monetary growth. The relative importance he assigns to each target shifts stochastically through time. His current preference trade-off is known to him but not to the public. When choosing the (state contingent) path of money growth for the present and the future, the policy aker compares the benefits from current stimulation with the costs associated with higher future inflation expectations. Current monetary growth conveys information to the public about future money growth because there is persistence in the policymaker's objectives. Although expectations are rational, information is imperfect because monetary control procedures are imprecise. As a result the public cannot correctly distinguish persistent changes of emphasis on different policy objectives from transitory monetary control errors. The public becomes aware of changes gradually by observing past monetary growth. Credibility is defined in terms of the speed with which the public recognizes changes in the objectives of the policymaker. Credibility is lower the noisier monetary control and the more stable the objectives of the policymaker. Looser monetary control and a higher degree of time preference on the part of the policymaker induce him to produce higher and more variable monetary growth. When the policymaker is free to determine the accuracy of monetary control he does not always choose the most effective control available in spite of the fact that monetary surprises always have an expected value of zero. The reason is that ambiguous control procedures enable the policymaker to generate positive surprises when he cares more than on average about economic stimulation. He leaves the inevitable negative surprises for periods in which he cares more about inflation prevention. This result provides an explanation for the Fed's preference for ambiguity, recently documented by Goodfriend (1986). The policymaker is more likely to pick more ambiguous control procedures the more uncertain his objectives and the higher his time preference. The paper also provides a theoretical underpinning for the well documented cross-country positive correlation between the level and the variability of inflation.</p> </abstract>
<abstract> <p>This paper investigates whether agents can learn how to form rational expectations using standard econometric techniques in the case of a linear stochastic supply and demand model with a production lag. This model has a unique rational expectations equilibrium in which the expected price is a linear function of an observable exogenous random variable. Outside of rational expectations equilibrium agents predict the price by using a regression of past prices on the exogenous random variable where the regression is estimated by either ordinary least squares or Bayesian methods. If the agents are Bayesians, they may have diverse prior beliefs on the mean of the estimated parameter, but all have the same precision. This estimation procedure would be appropriate for an outside observer estimating the parameters of the model in rational expectations equilibrium the coefficient of the equation relating the mathematical conditional expectation of the price to the exogenous variable is constant through time. Outside rational expectations equilibrium this coefficient, which changes each time new data change the regression coefficient. The data are generated by a time-varying parameter model where the varying parameter is determined by past data and the estimation procedure. Agents fail to take this feedback into account and so are estimating a misspecific model.</p> </abstract>
<abstract> <p>Stochastic equilibria under uncertainty with continuous-time security trading and consumption are demonstrated in a general setting. A common question is whether the current price of a security is an unbiased predictor of the future price of the security plus intermediate dividends. This is the hypothesis of "no expected financial gains from trade." The relevance of this hypothesis in multi-good economies is called into question by the following demonstrated fact. For each set of probability assessments there exists a corresponding equilibrium, one with the original agents, original equilibrium allocations, and no expected financial gains from trade under the given probability assessments. The spanning number is linked directly to agent primitives, in particular the manner in which new information resolves uncertainty over time. The spanning number is shown to be invariant under bounded changes in expectations. Several examples are given in which the spanning number is finite even though the number of potential states of the world is infinite.</p> </abstract>
<abstract> <p>We consider a revenue maximizing server who has the opportunity to suppress information on actual queue length, leaving demanders to decide on joining the queue on the basis of the known distribution of waiting times. We address the following second best problem: If suppression, but not pricing, can be socially controlled, is it socially optimal to prevent suppression? We show that it may be, but is not always, socially optimal to prevent suppression and that it is never optimal to encourage suppression when the revenue maximizer prefers to reveal the queue length.</p> </abstract>
<abstract> <p> This paper considers a generalized mean value m(p) defined implicitly for a probability measure p on the reals as the unique y for which ∫ φ(x,y) dp(x) = 0, where φ is skew-symmetric and strictly increasing in its first argument. Conditions on m that are necessary and sufficient for the implicit characterization are given and its relationship to certainty equivalence is discussed. </p> </abstract>
<abstract> <p>This paper investigates a property of estimators called stability. The stability exponent of an estimator is defined to be a measure of the effect of any single observation in the sample on the realized value of the estimator. High stability often is desirable for robustness against misspecification and against highly variable observations. Stability exponents are determined and compared for a wide variety of estimators and economietric models. They are found to depend on the maximal moment exponent (i.e., the number of finite moments) of the estimator's influence curve. Since it is possible often to construct estimators with specified influence curves, estimators with different stability exponents can be constructed.</p> </abstract>
<abstract> <p>This paper considers the problem of specifying and estimating demand systems for samples which contain a significant proportion of observation with zero consumption of one or more goods. Our approach uses virtual prices, which are dual to the Kuhn-Tucker conditions, to select the set of goods consumed--the demand regime--and to transform binding nonnegativity constraints into nonbinding constraints. It has the advantage of permitting the use of indirect cost and utility functions such as the translog, and the analytic decomposition of demand effects for goods at the nonnegativity limit.</p> </abstract>
<abstract> <p>Considerable research effort has been devoted to the determinants of changes in money wage rates. Most of this research has used aggregate wage index data to form the dependent variable. However, there are substantial difficulties involved with the use of such data. Recently a number of important papers have appeared which systematically examine these difficulties and recommend appropriate estimation and hypothesis testing procedures. The conclusions obtained from the use of these appropriate methods are quite negative. One purpose of this study is to determine whether this negative assessment of economists' knowledge of the determinants of wage changes is in fact appropriate. In order to do this, the study employs individual contract data. The study also examines the expectations hypothesis, the effect of unanticipated inflation on wage changes ("catch-up") and the effect of uncertainty about future inflation on negotiated wage settlements.</p> </abstract>
<abstract> <p>This paper studies the effects of irreversibility of capital investment upon optimal exploitation policies for renewable resource stocks. It is demonstrated that although the long-term optimal sustained yield is not affected by the assumption of irreversibility (except in extreme cases), the short-term dynamic behavior of an optimal policy may depend significantly upon the assumption. It is suggested that the results may have profound implications for problems of rehabilitation of overexploited fisheries and other renewable resource stocks.</p> </abstract>
<abstract> <p>This paper describes a simple, operational procedure that, under reasonable economic assumptions, always generates Pareto-efficient egalitarian-equivalent allocations (PEEEA) when agents know each other's preferences. The procedure constitutes a new, constructive proof of Pazner and Schmeidler's theorems on the existence of PEEEA, and shows that PEEEA, like fair and Pareto-efficient allocations, can be decentralized using less information than is required by the standard market procedure for decentralizing allocations that maximize a neoclassical, individualistic social welfare function.</p> </abstract>
<abstract> <p>Collective choice problems are studied from the Bayesian viewpoint. It is shown that the set of expected utility allocations which are feasible with incentive-compatible mechanisms is compact and convex, and includes the equilibrium allocations for all other mechanisms. The generalized Nash solution proposed by Harsanyi and Selten is then applied to this set to define a bargaining solution for Bayesian collective choice problems.</p> </abstract>
<abstract> <p>Two sets of orderings are characterized. In either case, these orderings are defined on the n-dimensional Euclidean space, interpreted as the utility space. Moreover, they all satisfy the strong Pareto principle, an anonymity axiom, and an axiom of interpersonal comparability of utility levels. In one case, interpersonal comparability of utility gains is also assumed.</p> </abstract>
<abstract> <p>Finite horizon sequential decision problems with a "temporal von Neumann-Morgenstern utility" criterion are analyzed. This criterion, as developed in [7], is a generalization of von Neumann-Morgenstern (expected) utility of the vector of rewards, wherein an individual's preferences concerning the timing of the resolution of uncertainty are taken into account. The preference theory underlying this criterion is reviewed and then extended in natural fashion to yield preferences for strategies in sequential decision problems. The main result is that value functions for sequential decision problems can be defined by a dynamic programming recursion using the functions which represent the original preferences, and these value functions represent the preferences defined on strategies. This permits citation of standard results from the dynamic programming literature, concerning the existence of (memory less) strategies which are optimal with respect to the given preference relation.</p> </abstract>
<abstract> <p>This paper investigates the degree to which a "leading" time series improves the prediction of coincident series. The theory of covariance-stationary processes is used as the theoretical framework and empirical tests dealing with the predictive ability of "leading" series are supplied.</p> </abstract>
<abstract> <p> In the regression model &lt;tex-math&gt;$y_{t}=\alpha +\beta t+\varepsilon _{t}$&lt;/tex-math&gt; it is found that when the residuals ε &lt;sub&gt;t&lt;/sub&gt; follow a first-order stationary Markoff process with zero mean and autocorrelation coefficient ρ, -1 &lt; ρ &lt; 1, the greatest lower bound for the efficiency of the least-squares estimator of β (relative to the Gauss-Markoff estimator) over the interval &lt;latex&gt;$0\leq \rho &lt;1$&lt;/latex&gt; is .753763. This compares with a greatest lower bound of .535898 for the relative efficiency of the Cochrane-Orcutt estimator of β. </p> </abstract>
<abstract> <p>A method is proposed for the estimation of a general class of scalar linear time series models. The model takes the form of a stochastic difference equation for the dependent variable with exogenous variable inputs, and the disturbances are autocorrelated through an autoregressive moving average process. In the present paper an asymptotically efficient yet computationally simple estimation procedure (in the time domain) is derived for this model. The resulting estimator is shown to be asymptotically equivalent to the maximum likelihood estimator and to possess a limiting multivariate normal distribution.</p> </abstract>
<abstract> <p>This paper discusses the bias that results from using nonrandomly selected samples to estimate behavioral relationships as an ordinary specification error or "omitted variables" bias. A simple consistent two stage estimator is considered that enables analysts to utilize simple regression methods to estimate behavioral functions by least squares methods. The asymptotic distribution of the estimator is derived.</p> </abstract>
<abstract> <p>The purpose of this paper is to indicate the problems connected with the intertemporal comparability of Lorenz curves and Gini indices estimated by standard numerical approaches in the case of the classified empirical income distribution and a growth of individual income at a fixed rate. "Collector effects" of the higher class intervals lead to shifts of the Lorenz curve to the right or to the left; accordingly the Gini index may rise or fall. This "class phenomenon" occurs although actually nothing has changed in distribution.</p> </abstract>
<abstract> <p>This paper presents the classical theorem on the existence of equilibrium as it was proved in the 1950's with the various improvements that have been made since then. In particular, the elimination of the survival assumption and of the requirement of transitive preferences are carried through with a proof that uses a mapping of social demand. This approach favors intuitive understanding and generalization of the results. Finally, the role of the firm and the introduction of external economies are critically viewed.</p> </abstract>
<abstract> <p>This paper models the dynamics of the earnings distribution among successive generations of workers as a stochastic process. The process arises from the random assignment of abilities to individuals by nature, together with the utility maximizing bequest decisions of their parents. A salient feature of the model is that parents cannot borrow to make human capital investments in their offspring. Consequently the allocation of training resources among the young people of any generation depends upon the distribution of earnings among their parents. This implies in turn that the often noted conflict between egalitarian redistributive policies and economic efficiency is mitigated. A number of formal results are proven which illustrate this fact.</p> </abstract>
<abstract> <p>When an initial income distribution pattern Y = (Y"1, Y"2,...Y"n) is given, the operation of a balanced budget fiscal program that involves taxation and transfer payments leads to a disposable income pattern D = (D"1, D"2,...D"n). Let I(X) be an index of income inequality. When the total budget (i.e. total taxes collected) is fixed, the problem is to design a "most equitable" fiscal program that minimizes I(D). It will be shown that the solution can be readily calculated from Y and is independent of I(X) as long as the Dalton's "principle of transfer" is satisfied. This condition is met by a family of indices of inequality including the Gini coefficient, Theil index, coefficient of variation and the Atkinson index.</p> </abstract>
<abstract> <p>Two theorems are derived about social choice functions, which are defined on comprehensive convex subsets of utility allocation space. Theorem 1 asserts that a linearity condition, together with Pareto optimality, implies that a social choice function must be utilitarian. Theorem 2 asserts that a concavity condition, together with Pareto optimality and independence of irrelevant alternatives, implies that a social choice function must be either utilitarian or egalitarian. These linearity and concavity conditions have natural interpretations in terms of the timing of social welfare analysis (before or after the resolution of uncertainties) and its impact on social choices.</p> </abstract>
<abstract> <p> This paper considers the possibility of extending the Arrow-Pratt results on risk aversion to cases in which initial wealth is random. Specifically, we consider a situation in which an individual's wealth is the sum of two independent random variables \tilde{x} and ỹ. We define the risk premium π(\tilde{x}, ỹ) which represents the reduction in mean wealth an individual is willing to accept to eliminate the random variable x̃ while retaining the random variable ỹ. It is shown that if u&lt;sub&gt;1&lt;/sub&gt; is uniformly more (Arrow-Pratt) risk averse than u&lt;sub&gt;2&lt;/sub&gt; and if either u&lt;sub&gt;1&lt;/sub&gt; or u&lt;sub&gt;2&lt;/sub&gt; exhibit nonincreasing (Arrow-Pratt) risk aversion, then &lt;tex-math&gt;$\pi _{2}(\tilde{x},\tilde{y})$&lt;/tex-math&gt; is always smaller than &lt;tex-math&gt;$\pi _{1}(\tilde{x},\tilde{y})$&lt;/tex-math&gt;. An example is given in which both u&lt;sub&gt;1&lt;/sub&gt; and u&lt;sub&gt;2&lt;/sub&gt; exhibit increasing risk aversion and in which this result fails. </p> </abstract>
<abstract> <p>Most rational expectations market equilibrium models are not models of price formation, and naive mechanisms leading to such equilibria can be severely manipulable. In this paper, a bidding model is developed which has the market-like features that bidders act as price takers and that prices convey information. Higher equilibrium prices convey more favorable information about the quality of the objects being sold than do lower prices. Bidders can benefit from trading only if they have a transactions motive or if they have access to inside information. Apart from exceptional cases, prices are not fully revealing. A two stage model is developed in which bidders may acquire information at a cost before bidding and for which the equilibrium price is fully revealing, resolving a well-known paradox.</p> </abstract>
<abstract> <p>This paper presents a theoretical and empirical model of labor supply when there are fixed costs associated with entry into the labor market. An implication of the existence of fixed costs is that individuals will not be willing to work below some minimum number of hours, termed reservation hours. A maximum likelihood estimator that allows reservation hours to be nonzero and differ randomly among individuals is developed. The estimator is applied to data on married women to estimate their labor supply functions. The results indicate that fixed costs of work are of prime importance in determining the labor supply behavior of married women. The results also suggest that large own-wage elasticities found in earlier studies of married women's labor supply are, in part, due to ignoring the existence of fixed costs of labor market entry.</p> </abstract>
<abstract> <p>This paper extends the empirical version of a job-search model to permit heterogeneity in the location of wage offer distributions. Population variance in wage offers is decomposed into variance due to heterogeneity and variance facing each individual. Heterogeneity is found to be an important source of offer variance in the population. The amount of "pure wage offer dispersion" facing individuals is found to contribute little to population variance.</p> </abstract>
<abstract> <p>Previous work in the demand for freight transportation has followed an aggregate approach without any consideration of the underlying behavior of the individuals who actually make mode-choice decisions. In this paper, we analyze mode-choice behavior at the level of the individual decision maker with the purpose of applying the results to various issues related to intermodal competition. Based on a theory of shipper/receiver behavior, a random expected utility model suitable for econometric analysis is developed and estimated. Data for the empirical analysis consists of a large number of shipments covering a wide range of commodities, lengths of haul, and origin-destination pairs. The transport modes considered in the analysis are regulated and unregulated motor freight and rail. The central conclusion is that each mode has an opportunity to attract a substantial amount of traffic in particular markets through either service or price competition. In general, however, it appears that the opportunities for attracting traffic are greater through lower rates than improvements in service quality.</p> </abstract>
<abstract> <p>This paper is concerned with characterizing the probability distributions that describe the (stochastic) stationary state of a neoclassical model of optimal growth. In particular, using both theoretical analysis and numerical simulation, we search for systematic relationships between savings (and investment) rates and variability of the economy's aggregates.</p> </abstract>
<abstract> <p>The exact finite-sample density function of the likelihood ratio (LR) test of overidentifying restrictions in linear equations with any number of included endogenous and exogenous variables is presented, along with its derivation. Properties of the density function and the LR test are presented and discussed. A new approximation for the LR test critical region is proposed and studied.</p> </abstract>
<abstract> <p> Let the time series Y&lt;sub&gt;t&lt;/sub&gt; satisfy &lt;tex-math&gt;$Y_{t}=\alpha +\rho Y_{t-1}+e_{t}$&lt;/tex-math&gt;, where Y&lt;sub&gt;1&lt;/sub&gt; is fixed and the e&lt;sub&gt;t&lt;/sub&gt; are normal independent (0, σ &lt;sup&gt;2&lt;/sup&gt;) random variables. The likelihood ratio test of the hypothesis that (α, ρ) = (0, 1) is investigated and a limit representation for the test statistic is presented. Percentage points for the limiting distribution and for finite sample distributions are estimated. The distribution of the least squares estimator of α is also discussed. A similar investigation is conducted for the model containing a time trend. </p> </abstract>
<abstract> <p>This paper examines the consequences and detection of model misspecification when using maximum likelihood techniques for estimation and inference. The quasi-maximum likelihood estimator (OMLE) converges to a well defined limit, and may or may not be consistent for particular parameters of interest. Standard tests (Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in the presence of misspecification, but more general statistics are given which allow inferences to be drawn robustly. The properties of the QMLE and the information matrix are exploited to yield several useful tests for model misspecification.</p> </abstract>
<abstract> <p>For the Tobit model with independent observations, Amemiya [1] has established the strong consistency and asymptotic normality of a stationary point, θ̂, of the log-likelihood. The likelihood for dependent observations may be computationally intractable, so the behavior of θ̂ in the presence of serially correlated observations is of interest. Under a relaxation of Amemiya's assumption of independence, we prove that θ̂ is strongly consistent and asymptotically normal, and give an expression for the limiting covariance matrix.</p> </abstract>
<abstract> <p>A new class of tests for heteroscedasticity in linear models based on the regression quantile statistics of Koenker and Bassett [17] is introduced. In contrast to classical methods based on least-squares residuals, the new tests are robust to departures from Gaussian hypotheses on the underlying error process of the model.</p> </abstract>
<abstract> <p>This paper considers the problem of testing statistical hypotheses in linear regression models with inequality constraints on the regression coefficients. The Kuhn-Tucker multiplier test statistic is defined and its relationships with the likelihood ratio test and the Wald test are examined. It is shown, in particular, that these relationships are the same as in the equality constrained case. It is emphasized, however, that their common asymptotic distribution is a mixture of chi-square distributions under the null hypothesis.</p> </abstract>
<abstract> <p>This paper presents a numerical algorithm for computing full information maximum likelihood (FIML) and nonlinear three-stage least squares (3SLS) coefficient estimates for large nonlinear macroeconometric models. The new algorithm, which is demonstrated by actually computing FIML and 3SLS coefficient estimates for two versions of the 97 equation Fair model, is substantially more effective than other algorithms on large FIML and 3SLS estimation problems.</p> </abstract>
<abstract> <p>Two players have to reach an agreement on the partition of a pie of size 1. Each has to make in turn, a proposal as to how it should be divided. After one player has made an offer, the other must decide either to accept it, or to reject it and continue the bargaining. Several properties which the players' preferences possess are assumed. The Perfect Equilibrium Partitions (P.E.P.) are characterized in all the models satisfying these assumptions. Specially, it is proved that when every player bears a fixed bargaining cost for each period (c&lt;sub&gt;1&lt;/sub&gt; and c&lt;sub&gt;2&lt;/sub&gt;), then: (i) if &lt;latex&gt;$c_{1}&lt;c_{2}$&lt;/latex&gt; the only P.E.P. gives all the pie to 1; (ii) if &lt;latex&gt;$c_{1}&gt;c_{2}$&lt;/latex&gt; the only P.E.P. gives to 1 only c&lt;sub&gt;2&lt;/sub&gt;. In the case where each player has a fixed discounting factor (δ &lt;sub&gt;1&lt;/sub&gt; and δ &lt;sub&gt;2&lt;/sub&gt;) the only P.E.P. is &lt;tex-math&gt;$(1-\delta _{2})/(1-\delta _{1}\delta _{2})$&lt;/tex-math&gt;.</p> </abstract>
<abstract> <p>A strategic model of exchange is presented in which the agents set both prices and quantities. It is shown that the strategic (Nash) equilibria are strong and coincide with the competitive (Walras) equilibria.</p> </abstract>
<abstract> <p>We consider the market value of excess demand as a measure of disequilibrium. We show that, in a fixed exchange economy, there exist approximate equilibria whose measures of disequilibrium depend only on the endowments and not on the preferences. A related bound on the norm of excess demand, depending on the endowments and the approximate equilibrium price, is also obtained. We show the existence of allocations which are nearly competitive, as measured by the largest proportion of demand given up at the allocation by any trader. We use these results to obtain, for very general sequences of exchange economies, allocations giving all traders bundles close in norm to their demands. This result includes a O(1/n) rate of convergence in the case of uniformly bounded endowments.</p> </abstract>
<abstract> <p>We give necessary conditions for a neutral social choice function to be partially implementable by means of a strong equilibrium (i.e., implementable by cooperative agents): the veto power of the various coalitions should be maximally distributed. If moreover the social choice function is veto-anonymous, then the veto power of a coalition must be (roughly) proportional to its size: x per cent of the agents have the power to veto x per cent of the candidates. The procedure of "voting by successive veto" is an example of a neutral and (nearly) veto-anonymous social choice function which is implementable.</p> </abstract>
<abstract> <p>Both the class position of agents and their status as exploiters or exploited is endogenously determined as they optimize against asset constraints which limit their capacity to produce revenue. The Class Exploitation Correspondence Principle (CECP) asserts that class and exploitation status are related in a classical way. It is further shown that the class structure associated with a labor market can be generated isomorphically by a credit market, demonstrating the functional equivalence of these markets. Morever, these results hold in models of precapitalist, subsistence economy, showing that the phenomena of Marxian exploitation and class are applicable in economic mechanisms other than capitalist ones. The possibility for a general theory of exploitation is thereby suggested.</p> </abstract>
<abstract> <p>This paper disaggregates the income of individuals or households into different factor components, such as earnings, investment income, and transfer payments, and considers how to assess the contributions of these sources to total income inequality. In the approach adopted, a number of basic principles of decomposition are proposed and their implications for the assignment of component contributions are examined.</p> </abstract>
<abstract> <p>It is increasingly recognized that Tobin's conjecture that investment is a function of marginal q is equivalent to the firm's optimal capital accumulation problem with adjustment costs. This paper formalizes this idea in a very general fashion and derives the optimal rate of investment as a function of marginal q adjusted for tax parameters. An exact relationship between marginal q and average q is also derived. Marginal q adjusted for tax parameters is then calculated from data on average q assuming the actual U.S. tax system concerning corporate tax rate and depreciation allowances.</p> </abstract>
<abstract> <p>The concept of economic reform is described as a planned shift from one, Pareto inefficient, but quasi-stable, equilibrium (or "trap") to a new Pareto superior equilibrium which is, or is designed to become, stable too. The concept is applied to recent "shock" stabilization programs, with special reference to Israel, where the economy was credibly shifted from a 3-digit inflationary process with considerable inertia, to relative price stability with higher real growth, at only small adjustment costs, by means of a "heterodox" plan. This two-pronged stabilization program consisted of a substantial correction of budget and external account "fundamentals" together with a synchronized, wage-price-exchange rate freeze. The idea is theoretically rationalized within a simple dual equilibrium inflation model, for which some econometric estimates are also given.</p> </abstract>
<abstract> <p>In this paper, we develop a classical approach to model selection. Using the Kullback-Leibler Information Criterion to measure the closeness of a model to the truth, we propose simple likelihood-ratio based statistics for testing the null hypothesis that the competing models are equally close to the true data generating process against the alternative hypothesis that one model is closer. The tests are directional and are derived successively for the cases where the competing models are non-nested, overlapping, or nested and whether both, one, or neither is misspecified. As a prerequisite, we fully characterize the asymptotic distribution of the likelihood ratio statistic under the most general conditions. We show that it is a weighted sum of chi-square distribution or a normal distribution depending on whether the distributions in the competing models closest to the truth are observationally identical. We also propose a test of this latter condition.</p> </abstract>
<abstract> <p>In the context of the classical linear model, the problem of comparing two arbitrary hypotheses on the regression coefficients is considered. Problems involving nonlinear hypotheses, inequality restrictions, or non-nested hypotheses are included as special cases. Exact bounds on the null distribution of likelihood ratio statistics are derived. The bounds are based on the central Fisher distribution and are very easy to use. In an important special case, a bounds test similar to the Durbin-Watson test is proposed. Multiple testing problems are also studied: the bounds obtained for a single pair of hypotheses are shown to enjoy a simultaneity property that allows one to combine any number of tests. This result extends to nonlinear hypotheses a well-known result given by Scheffe for linear hypotheses. A method of building bounds induced tests is also suggested.</p> </abstract>
<abstract> <p>This paper proposes a very tractable approach to modeling changes in regime. The parameters of an autoregression are viewed as the outcome of a discrete-state Markov process. For example, the mean growth rate of a nonstationary series may be subject to occasional, discrete shifts. The econometrician is presumed not to observe these shifts directly, but instead must draw probabilistic inference about whether and when they may have occurred based on the observed behavior of the series. The paper presents an algorithm for drawing such estimation of population parameters by the method of maximum likelihood and provides the foundation for forecasting future values of the series. An empirical application of this technique to postwar U.S. real GNP suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3% permanent drop in the level of GNP.</p> </abstract>
<abstract> <p>That in-kind and cash transfers differ in value in general is well known to economists. However, the magnitude of the value of in-kind transfers has not been well determined in past work. Here the value of one in-kind transfer, Food Stamps, is examined by evaluating the experience of an actual conversion from stamps to cash in Puerto Rico in 1982. Perhaps surprisingly, the evidence clearly indicates that the cashout of the stamps had no detectable influence on food expenditures. The explanation partly lies in the distribution of expenditures, for the stamps were inframarginal for most recipients. However, some evidence also indicates that trafficking in stamps was widespread, including indirect evidence from estimates of the piecewise-linear constraint model.</p> </abstract>
<abstract> <p>This paper presents an empirical analysis of individual earnings and hours data from three different longitudinal surveys. In the first part of the paper we catalog the main features of the covariance structure of earnings and hours changes. We find that this structure is very similar across data sets, and may be adequately summarized by a simple components-of-variance model, consisting of (i) serially uncorrelated measurement error, (ii) a shared component of earnings and hours with a second-order moving average covariance structure, and (iii) a nonstationary component that affects only the variances and contemporaneous covariances of earnings and hours. In the second part of the paper we offer an interpretation of this model in terms of a simple life-cycle labor supply model. On the assumption that we can identify individualproductivity growth with the shared component of earnings and hours variation, we obtain estimates of the intertemporal substitution elasticity. The results are not favorable to the life-cycle model: most of the covariation of earnings and hours occurs at fixed hourly wage rates.</p> </abstract>
<abstract> <p>This paper considers the enforceability of employment contracts when employees' performance cannot be verified in court so that piece-rate contrasts are not legally enforceable. Part I shows that there exists a not legally enforceable. Part I shows that there exists a variety of self-enforcing implicit contracts, modelled as perfect equilibria in a repeated game, and characterizes all the wage and performance outcomes that can be implemented. Implementation requires a strictly positive surplus from employment, the form of the contract depending on how this surplus is divided between firm and employee. Piece-rate contracts, and contracts with an informally agreed bonus, can be made self-enforcing but the use of severance pay and bonding does not extend the set of implementable allocations. The resulting contracts resemble actual labor contracts more than do the contracts in standard principal-agent models. Part II analyses market equilibrium with these contracts, also modelled as perfect equilibria in a repeated game, and shows that many such equilibria exist. Unfilled vacancies and unemployed workers can co-exist despite the existence of contracts that are potentially mutually beneficial. For those jobs that are filled, any division of the potential surplus is possible so that the market can have, at the same time, involuntary unemployment and vacancies that are unfilled depsite filled jobs earning positive profits. As a criterion for selecting equilibria, a notion of renogotiation proofness is is applied. Then either all workers are employed or all jobs filled but any division of the potential surplus is still possible. The paper explores what further restrictions on beliefs give rise to a Walrasian outcome, in which all the potential surplus goes to the short side of the market, and to an efficiency wage type outcome, in which the potential surplus goes to the long side.</p> </abstract>
<abstract> <p>Altruistic parents make choices of family size along with decisions about consumption and intergenerational transfers. We apply this framework to a closed economy, where the determination of interest rates and wage rates is simultaneous with the determination of population growth and the accumulation of capital. Thus, we extend the literature on optimal economic growth to allow for optimizing choices of fertility and intergenerational transfers. We use the model to assess the effects of child-rearing costs, the tax system, the conditions of technology and preferences, and shocks to the initial levels of population and the capital stock.</p> </abstract>
<abstract> <p>Priority service rations available supplies according to contracts that specify each customer's priority or rank order. This alternative market form can achieve most of the efficiency gains attributed to spot markets, which in some industries are expensive to organize. Rationing by priorities is prominent in capital-intensive industries with non-storable outputs, as well as in service-sector and make-to-order industries where service is queued or congested. This paper describes the role of priority service and sketches a basic model. The main topic is efficient implementation by public enterprises and by competitive firms.</p> </abstract>
<abstract> <p>This paper analyzes as a dynamic game the optimal price and storage strategies of, respectively: (a) the seller of a storable good, who must keep pace with inflation but incurs a cost to changing his price; (b) his customers, who speculate on the timing of price adjustments to buy and store just before. A unique (Markov) perfect equilibrium is shown to exist, and is fully characterized. It generally involves a phase of mixed strategies, during which the seller tries to deter speculation by injecting uncertainty into its price dynamics, while speculators store in increasing numbers, with possibly a final "run" on the good. The stochastic price policies of a large number of such firms are shown to aggregate back to a price index growing at the rate of general inflation in response to which they arose. The model thus establishes that: (a) a constant rate of aggregate inflation can at the same time generate and cover up significant uncertainty and social costs at the microeconomic level; (b) speculation can be destabilizing and socially wastful, even in the absence of shocks and imperfect information. Most importantly, they provide a theoretical foundation for the frequently encountered claim that inflation causes price uncertainty.</p> </abstract>
<abstract> <p>A transferable utility economy in which each agent holds a resource which can be used in combination with the resources of other agents to generate value (according to the characteristic function V) is studied using a dynamic model of bargaining. The main theorem establishes that the payoffs associated with efficient equilibira converge to the agent's Shapley values as the time between periods of the dynamic game gose to zero. In addition it is demonstrated that an efficient equilibrium exists and is unique when an additivity condition is satisfied. To demonstrate the sensitivity of the solution to the institutional detail we modify the model to allow for partnerships and show that the Shapley value is no longer achieved.</p> </abstract>
<abstract> <p>The paper presents a bargaining approach to spatial competition. Sellers compete by choosing locations in a market region. Consumers face a cost to moving from one place to another. The price of the good is determined as the perfect equilibrium of a bargaining game between seller and buyer. In this game, the consumer has the outside option to move to another seller so that prices at all stores are interdependent. Existence of a location-price equilibrium is established. The outcome approaches the perfectly competitive one if the consumer's costs of travelling become negligible or if the number of sellers tends to infinity.</p> </abstract>
<abstract> <p>In this paper, we analyze the problem of designing incentive compatible mechanisms in pure exchange economic environments when agents have incomplete information. The equilibrium concept employed is Bayesian Nash equilibrium and the notion of implemantation is full implementation, which is stronger than the more commonly employed notion of truthful implementation. An allocation rule is truthfully implementable if there exists a direct mechanism to which truth telling is an equilibrium and which yields the allocation rule as its truthful equilibrium outcome. An allocation rule is fully implementable if there exists mechanism which yields the allocation rule as its unique equilibrium outcome. More generally, a set of allocation rules, or a social choice set, is fully implementable if there exist a mechanism whose equilibrium outcomes coincide with the set. This stronger notion of implemention avoids the well known problems of multiple equilibria which arise in direct revelation games. We develop a condition, termed Bayesian monotonicity, which we show is necessary for full implementation. An incentive compatibility condition is also necessary. We prove that Bayesian monotonicity and a slightly stronger incentive compatibility condition are sufficient for full implementation when there are at least three agents. We present several examples of allocation rules which do and do not satisfy our condition. One example is that of an allocation rule which is fully inplementable by an indirect mechanism, but for which every equivalent direct mechanism has multiple equilibrium outcomes.</p> </abstract>
<abstract> <p>This paper presents an analysis of the structure of competitive equilibrium in a model whose principle feature is incomplete financial markets formulate in the spirit of Arrow. Specifically, the overall payoffs or returns from financial instruments are assumed to be fixed or predeterimed, independently of the operation of the economy, and these instruments are assumed to be fewer in number than required to span all potential spot markets for commodities. Our main result established that market incompleteness generates a corresponding degree of allocation indeterminateness: Suppose there are N + 1 spot market, but only 0 &lt;M &lt;N (linarly independent) financial instruments, so that the deficiency in financial markets is 0 &lt; n = N - M &lt; N. Then, subject tosome relatively innocuous technical qualifications, the set of equilibrium allocations contains a smooth, n-dimensional submanifold. We also indicate how real indeterminacy may increase when returns (for example, the market price and promise interest on an ordinary bond) are treated as variables rather than parameters.</p> </abstract>
<abstract> <p>An important application of the theory of choice under uncertainty is to asset markets and an important property in these markets is a preference for portfolio diversification. If an investor is an expected utility maximizer, then (s)he is risk averse if and only if (s)he exhibits a preference for diversification. This paper examines the relationship between risk aversion and protfolio diversification when preferences over probability distributions of wealth do not have an expected utility representation. Although risk aversion is not sufficient to guarantee a preference for portfolio diversification, it is necessary. Quasicon-cavity of the preference functional (over probability distributions of wealth) together with risk aversion does imply a preference for portfolio diversification.</p> </abstract>
<abstract> <p>In this paper we propose a procedure for testing commoh roots hypothesis for polynomials in lag operator. Using a generalized Bezout property, we first show that this hypothesis can be written in a "mixed" form, i.e. as a set of equations linking the parameters of interest (the coefficients of the polynomials) and a set of auxiliary parameters. This mixed form is particulary convenient since it is bilinear with respect to these two sets of parameters. This implies, in particular, that for a given null hypothesis a generalized Wald test can be implemented by using O.L.S. and G.L.S. techniques. A sequence of such tests is then proposed and studied.</p> </abstract>
<abstract> <p>The time path of asset prices is studied within a stationary experimental environment. After several replications prices converge to a perfect foresight equilibrium. A sequential market having an "informational trap" and a futures market are also studied.</p> </abstract>
<abstract> <p>Linear predictor definitions of causality are not adequate for discrete data. The paper extends the Granger and Sims definitions by using conditional independence instead of linear predictors. The extended definition of "y does not cause x" is that x is independent of past y conditional on past x. This is stronger than the strict exogeneity condition that y be independent of future x conditional on current and past x. Under a weak regularity condition, however, if y is independent of future x conditional on current and past x and past y, then y does not cause x.</p> </abstract>
<abstract> <p>The relationship between Granger's [8] and Sims' [13] concepts of noncausality is analyzed in terms of conditional independence and Bahadur's [1] concept of transitivity. In particular their equivalence is easily shown to disappear when uncorrelatedness is replaced by independence.</p> </abstract>
<abstract> <p>An allocation is said to be t-wise optimal (for t a positive integer) if for every collection of t traders, there is no reallocation of their current holdings that will make some better off while making none worse off. The allocation is pairwise optimal if it is t-wise optimal for t = 2. A t-wise optimal allocation is the outcome of a trading process more decentralized than that of the Walrasian equilibrium. It represents the result of a variety of separate transactions in small groups without the (centralized) coordination provided by a single Walrasian auctioneer. Necessary conditions and sufficient conditions on allocations for t-wise optimality to imply Pareto optimality are developed. These generally require sufficient overlap in goods holdings among traders to ensure the presence of common support prices. This is formalized as indecomposability of a truncated submatrix of the allocation matrix. A necessary and sufficient condition remains an open question.</p> </abstract>
<abstract> <p>This paper proposes a simple theory to explain bargaining impasses, which is based on Schelling's view of the bargaining process as a struggle between bargainers to commit themselves to favorable bargaining positions. Because bargaining impasses are generally Pareto-inefficient, anything involving a positive probability of impasse is Pareto-inefficient as well. It is demonstrated that in spite of this avoidable inefficiency, when successful commitment is uncertain and irreversible it can still be rational for individuals to attempt commitment and thereby risk an impasse; in a leading special case, the model reduces to a Prisoner's Dilemma game, in which only strategic-dominance arguments are needed to establish this conclusion. Further, making commitment more difficult, or changing the costs of disagreement in a way that makes available a wider range of settlements that are better for both bargainers than disagreement, need not always lower the probability of impasse, in spite of the conventional wisdom to the contrary.</p> </abstract>
<abstract> <p>Recent results have shown that, for bargaining over the distribution of commodities, or other riskless outcomes, Nash's solution predicts that risk aversion is a disadvantage in bargaining. Here we consider bargaining games which may concern risky outcomes as well as riskless outcomes, and we demonstrate that, in such games, risk aversion need not always be a disadvantage in bargaining. Intuitively, for bargaining games in which potential agreements involve lotteries which have a positive probability of leaving one of the players worse off than if a disagreement had occurred, the more risk averse a player, the better the terms of the agreement which had occurred, the more risk averse a player, the better the terms of agreeement which he must be offered in order to induce him to reach an agreement, and to compensate him for the risk involved. For bargaining games whose disagreement outcome involves no uncertainty, we characterize when risk aversion is advantageous, disadvantageous, or irrelevant from the point of view of Nash's solution.</p> </abstract>
<abstract> <p>Recent evidence shows that within an industry, smaller firms grow faster and are more likely to fail than large firms. This paper provides a theory of selection with incomplete information that is consistent with these and other findings. Firms learn about their efficiency as they operate in the industry. The efficient grow and survive; the inefficient decline and fail. A perfect foresight equilibrium is proved by means of showing that it is a unique maximum to discounted net surplus. The maximization problem is not standard, and some mathematical results might be of independent interest.</p> </abstract>
<abstract> <p>A theory of dynamic optimal resource allocation to R and D in an n-firm industry is developed using differential games. This technique represents a synthesis of the analytic methods previously applied to the problem: static game theory and optimal control. The use of particular functional forms allows the computation and detailed discussion of the Nash equilibrium in investment rules.</p> </abstract>
<abstract> <p>In this paper the method of least absolute deviations is applied to the estimation of the parameters of a structural equation in the simultaneous equations model. A class of estimators called two stage least absolute deviations estimators is defined, their asymptotic properties are derived, and the problem of finding the optimal member of the class is considered.</p> </abstract>
<abstract> <p> A class of decompositions is derived for the variance-covariance matrix Ω of a generalized error components model, introduced in [18 and 19]. The spectral decomposition of Ω is a member of this class. For estimation purposes certain other members of the class are preferred, especially those that allow for simplifying transformations of the model not depending on unknown parameters. The transformations suggest simple and asymptotically efficient estimators of both the parameters in Ω and the parameters in the systematic part of the model. </p> </abstract>
<abstract> <p>The matrix weighted average (H = V^-1)^-1Hb, where H and V are symmetric positive definite matrices and b is a vector, is shown to lie in one ellipsoid if V is bounded from below, V "* @&lt; V, another ellipsoid if V is bounded from above, V @&lt; V*, and another ellipsoid if V is bounded from above and below, V "*@&lt; V @&lt; V*. These results are applied to bound the posterior mean vector of the normal linear regression model.</p> </abstract>
<abstract> <p>In the classical linear regression model the conflict between the W, LR, and LM tests is due to the tests not having the correct significance level. This paper shows that the probability of conflict can be substantial when the three tests are based on the asymptotic chi-square critical value. For this model some computable correction factors for the chi-square critical values are examined, including those derived from a second-order Edgeworth approximation to the exact distributions. It is shown that the probability of conflict between the Edgeworth size-corrected tests is of no practical importance over a wide range of conditions.</p> </abstract>
<abstract> <p>The maximum likelihood approach is used to derive Hausman's specification test. Its asymptotic power is compared with the more conventional test procedures.</p> </abstract>
<abstract> <p>Experimental studies have shown that the key behavioral assumption of expected utility theory, the so-called "independence axiom," tends to be systematically violated in practice. Such findings would lead us to question the empirical relevance of the large body of literature on the behavior of economic agents under uncertainty which uses expected utility analysis. The first purpose of this paper is to demonstrate that the basic concepts, tools, and results of expected utility analysis do not depend on the independence axiom, but may be derived from the much weaker assumption of smoothness of preferences over alternative probability distributions. The second purpose of the paper is to show that this approach may be used to construct a simple model of preferences which ties together a wide body of observed behavior toward risk, including the Friedman-Savage and Markowitz observations, and both the Allais and St. Petersburg Paradoxes.</p> </abstract>
<abstract> <p>Noncooperative strategic behaviors are studied in the Malinvaud-Dreze-de la Vallee Poussin decentralized planning procedure. We depart from the assumption of myopic behavior by assuming that every agent takes into account the effect over a given period of time [0, T] of his answers to the Center. One shows that, for T large, every Nash equilibrium of the ensuing game in intertemporal strategies approaches: (i) a competitive equilibrium in an exchange economy, and (ii) a Lindahl equilibrium in an economy with public goods. Thus, the Center loses any significant influence on the income distribution.</p> </abstract>
<abstract> <p>This paper focuses on a theoretical issue involved in modeling the functioning of a centrally planned economy. Results derived in a model of inventory behavior indicate that the uncertain consequences of economic decisions and their cumulative impact on the functioning of the economy need to be explicitly recognized in order to study plan implementation. When this is done the need for institutions or procedures with homeostatic properties, i.e., negative feedback mechanisms, is revealed. Without such mechanisms orders consistent with a feasible plan may be unable to insure implementation of that plan even when all economic agents are perfectly motivated. Three types of feedback mechanisms resolving this difficulty are formulated and discussed with respect to inventory behavior in a centrally planned economy.</p> </abstract>
<abstract> <p>Equilibria in stochastic economic models are often time series which fluctuate in complex ways. But it is sometimes possible to summarize the long run, average characteristics of these fluctuations. For example, if the law of motion determined by economic interactions in Markovian and if the equilibrium time series converges in a specific probabilistic sense then the long run behavior is completely determined by an invariant probability distribution. This paper develops and unifies a number of results found in the probability literature which enable one to prove, under very general conditions, the existence of an invariant distribution and the convergence of the corresponding Markov process.</p> </abstract>
<abstract> <p>In this paper we analyze the solutions of linear econometric models with rational expectations. More precisely, we describe in detail the set of all the solutions; in particular this set is shown to be much larger than the sets previously considered. We also study various criteria of selection in this set of solutions and we examine to what extent these criteria reduce the set of the solutions.</p> </abstract>
<abstract> <p>We consider a model of capital accumulation and prove the existence of a support price path for the optimal path of capital accumulation. The considered model is a continuous time model of infinite horizon. Our problem is the so-called convex problem of optimal control without differentiability. We adopt the overtaking optimality criterion and prove the existence of a dual price path which supports the value function as well as the Hamiltonian function.</p> </abstract>
<abstract> <p>Limit pricing involves charging prices below the monopoly price to make new entry appear unattractive. If the entrant is a rational decision maker with complete information, pre-entry prices will not influence its entry decision, so the established firm has no incentive to practice limit pricing. However, if the established firm has private, payoff relevant information (e.g., about costs), then prices can signal that information, so limit pricing can arise in equilibrium. The probability that entry actually occurs in such an equilibrium, however, can be lower, the same, or even higher than in a regime of complete information (where no limit pricing would occur).</p> </abstract>
<abstract> <p>As yet, the theory of instrumental variables (IV) estimation is not applicable to data from a stratified cross section (e.g., census data) since the moment matrices need not converge. This study provides general conditions for the consistency and asymptotic normality of the IV estimator in this case. Homoskedastic errors are not assumed, and a new, more general asymptotic parameter covariance matrix estimator is given which is consistent regardless of the presence of heteroskedasticity. A new estimator, two-stage instrumental variables, is proposed which is asymptotically efficient relative to two-stage least squares. Tests for model misspecification are also discussed.</p> </abstract>
<abstract> <p>Quandt and Ramsey have suggested an estimator for normal mixtures and switching regressions, which minimizes a sum of squared differences between empirical and theoretical values of the moment generating function. This paper demonstrates how their estimator can be improved by minimizing a generalized sum of squares rather than an ordinary sum of squares. When this is done, more points of evaluation (moments) are unambiguously better than less. Most of the results presented are also applicable to method of moments estimators in general.</p> </abstract>
<abstract> <p>This paper derives the exact finite sample distribution of the two-stage generalized least squares (GLS) estimator in a multivariate linear model with general linear parameter restrictions. This includes the seemingly unrelated regression (SUR) model as a special case and generalizes presently known exact results for the latter system. The usual classical assumptions are made concerning nonrandom exogenous variables and normally distributed errors. The theoretical results of this paper are made possible by the author's development of a matrix fractional calculus. This operator calculus is the main theoretical tool of the paper and may be used to solve a wide range of other unsolved problems in econometric distribution theory.</p> </abstract>
<abstract> <p>A general problem faced by both private firms and public enterprises is how to allocate the costs of common facilities fairly among the different goods and services produced. Any such cost accounting method can create incentives among product managers within the firm for altering the production function to their advantage. It is therefore both reasonable and desirable that a method reward increased efficientl by attributing lower unit costs to products whose marginal cost of production uniformly decreases. It is shown that there is only one "symmetric" method that satisfies this "monotonicity" principle--namely, the Aumann-Shapley price mechanism based on the Aumann-Shapley value for nonatomic games. This provides a new and simple axiomatization of this method without resorting to the usual assumption of additivity.</p> </abstract>
<abstract> <p>The so-called Principle of Minimum Differentiation, stated by Hotelling, has been challenged by many authors. This paper restores the Principle by showing that n firms locate at the center of the market and charge prices higher than the marginal cost of production when heterogeneity in consumers' tastes is "large enough."</p> </abstract>
<abstract> <p>This paper presents a dynamic model of consumer trading on the primary, secondary, and scrap markets for a stochastically deteriorating durable good in a stationary economy with perfect information and no transaction costs. We explicitly model the trading process by tracking each durable from its "birth" in the primary market, through its sequence of owners in the secondary market, until its "death" in the scrap market. We prove that a stationary equilibrium tests, characterize the distribution of consumer holdings of durables, and show that equilibrium asset prices are shadow prices to a particular regenerative optimal stopping problem. We show that each heterogeneous agent equilibrium is observationally equivalent to a homogeneous agent equilibrium. We derive a differential equation for equilibrium rental rates, and a functional equation which links rental rates to asset prices. These equations show precisely how the structure of durable prices and rental rates embody the functional form and population distribution of preferences and the technological characteristics of durable goods.</p> </abstract>
<abstract> <p>Let E be a private ownership economy. Call an allocation w "voluntary for a price vector p" if no agent can benefit by trading less at p. We prove that, under differentiability and interiority, if w is voluntary and Pareto efficient it is a competitive equilibrium for E. Perhaps surprisingly, production economies where some agents receive profit income require a stronger voluntariness condition than the one sufficient for exchange economies. The stronger condition singles out a commodity as a medium of exchange.</p> </abstract>
<abstract> <p>This paper studies the limits of contracting as a method for achieving efficient allocation, with particular attention to how informational asymmetries interact with the timing of commitment to a mechanism. There are arguments to suggest, in the spirit of the Coase "Theorem," that if agents can agree on a mechanism before observing their private information (or, a fortiori, if information is perfect or symmetric), they can realize an incentive-efficient allocation. If, however, agents observe their private information before contracting, there may be further restrictions, due to information leakage during the process of bargaining over mechanisms, on what they can achieve by contract. These restrictions are characterized and compared to those proposed for this setting by Holmstrom and Myerson [6]. It is also shown that there is at least one specification of the rules that govern mechanism design that makes it possible for agents to achieve, contracting after they observe their private information, the same incentive-efficient allocations that are attainable when they can commit themselves to a mechanism before observing their private information.</p> </abstract>
<abstract> <p>In a series of eleven markets, sellers possessed products that were exogenously designated as either grade "regular" or grade "super." Supers were valued more by buyers but grade could not be observed by buyers prior to purchase. Sellers could add costly units of quality to their products that were observable and valued by buyers. The data are analyzed with perfect information models, signaling equilibrium models, and pooling models. A variety of behaviors are observed across the eleven markets. Signaling is observed in most markets with some markets approaching the most efficient signaling equilibrium. Pooling or partial pooling occurs in a few markets. The performance seems to be sensitive to the relative cost of signaling and the market institutional setting.</p> </abstract>
<abstract> <p>We study two-sided markets in which agents are buyers and sellers or firms and workers or men and women. The agents are to form partnerships (which provide them with satisfaction) and at the same time make monetary transfers (e.g. salaries or dowries). The core of this market game is shown to have a particularly nice structure so that precise answers can be given to questions concerning comparative statics and manipulability.</p> </abstract>
<abstract> <p>An action in an extensive game that has a suboptimal payoff in every sequential equilibrium is said to be useless. There are sequential equilibria in which beliefs at each information set assign positive probability only to those nodes reached by the fewest useless actions. An action is second order useless if it is not useless but is strictly suboptimal in every equilibrium satisfying this condition on beliefs, and there exist sequential equilibria in which beliefs assign positive probability only to those nodes reached by the fewest useless actions, and, in this set, only those nodes requiring the fewest second order useless actions. Higher order uselessness is defined inductively, and beliefs satisfying the associated sequence of conditions are said to be justifiable. The existence of sequential equilibria with justifiable beliefs is demonstrated.</p> </abstract>
<abstract> <p>We study subgame perfect equilibria of finitely repeated games. We prove a limit "folk theorem" for these games. Under weak conditions, any feasible and individually rational payoff vector of the one-shot game can be approximated by the average payoff in a perfect equilibrium of a repeated game with a sufficiently long horizon.</p> </abstract>
<abstract> <p>This paper presents general results on the existence and properties of expected-utility-maximizing search rules for problems in which searchers may choose both the number of periods in which samples are taken and the size of the sample taken in each period. These rules include fixed-sample-size rules and sequential rules as special cases. Also presented are conditions sufficient for sequential and fixed-sample-size rules to be optimal.</p> </abstract>
<abstract> <p>This paper formulates a set of decreasing-absolute-risk-aversion postulates and shows that only mean-variance utility functionals can satisfy them. These postulates are used to axiomatize specific classes of mean-variance functionals. Finally, an equivalence is established between these postulates and corresponding comparative statics properties of asset demands in two-asset portfolio problems.</p> </abstract>
<abstract> <p>For many multivariate linear rational expectations models a symmetry condition is satisfied, and when it is, a convenient representation of the solution exists that has many of the desirable features of the solution to the univariate model. This allows for greater insight into the identification, stability, and comparative dynamics properties of the model.</p> </abstract>
<abstract> <p>This paper presents a general equilibrium model in which private commodities are allocated through competitive markets and public commodities according to government allocation and taxing rules that depend on information communicated to the government by consumers regarding their preferences. A wide range of strategic behavior for consumers in their communication with the government is allowed; in particular, consumers may understate their preferences and be "free riders" if they choose. Although several examples of allocation-taxation schemes falling within the general model are discussed, the major contribution of the paper is the formulation of a particular government allocation-taxation scheme for which the behavioral equilibria are Pareto optimal. That is, given the government rules, consumers find it in their self-interest to reveal their true preferences for public goods.</p> </abstract>
<abstract> <p>Necessary and sufficient conditions for Pareto optimality are derived for problems involving convex criteria and convex constraints. For a wide class of convex functions, the characterization of Pareto optimality is given in terms of systems of linear programs, which, under suitable regularization conditions, reduce to a single linear program. The consideration of a system of linear programs and their duals leads naturally to a system of partial prices associated with a Pareto optimum.</p> </abstract>
<abstract> <p>In recent studies of economic organization [9 and 15] it has been found necessary to impose certain restrictions (regularity conditions) on the communication process. Such restrictions limit the class of environments for which the organization is Pareto satisfactory. We show that a class of environments admits a Pareto satisfactory (and regular) resource allocation mechanism if and only if the graph of the Pareto correspondence is a union of continuous functions. We also study the shape of the Pareto set for a given environment and the way that set varies as the environment varies. We present examples showing that near any environment whose Pareto utility frontier is homeomorphic to a simplex there is whose Pareto frontier is badly chopped up. We also give an example of a class of environments for which the Pareto correspondence has no continuous selection through a given point.</p> </abstract>
<abstract> <p>This paper deals with a situation of middle-man behavior between two otherwise separated economies. A two-person noncooperative exchange game is proposed whereby each player's strategy consists of two parts taken in a sequence: a price strategy, to be followed by a demand strategy. The concept of a special kind of mixed strategies is introduced, and a solution is defined in terms of a Nash equilibrium pair of such mixed strategies. It is shown that a trivial class of equilibria always exists, and that under certain conditions, more interesting nontrivial equilibria can exist as well.</p> </abstract>
<abstract> <p>If there exist heterogeneous capital goods, a steady state may be "paradoxical" in the sense that increasing the rate of interest above the Golden Rule level may lead to an increase in consumption or utility, rather than to the decrease which always occurs in one-sector models. It is shown that, in many cases, a path of capital accumulation which maximizes the minimum consumption or utility level is unlikely to converge to a paradoxical steady state of this kind.</p> </abstract>
<abstract> <p>A social decision function operates on individual weak orderings to produce acyclic social preference. The structure of a general neutral monotonic SDF is studied. It is shown to be characterized by the veto, if individual indifference is banned. With such indifference present, the characterization is by a veto structure, a hierarchy embracing all individuals. The reason for the interest in acyclicity [21, 22] is that it averts the voting paradox, permitting a choicefrom each subset of alternatives. This is for the finite case. When that assumption is dropped, an infinite ascending sequence of preferences prevents a choice. It is shown that this last phenomenon need not be prohibited along with cycles; the absence of such social sequences is implied by their absence from individual preferences.</p> </abstract>
<abstract> <p>This paper shows that no nondictatorial voting procedure exists that induces each voter to choose his voting strategy solely on the basis of his preferences and independently of his beliefs concerning other voters' preferences. This necessary dependence between a voter's beliefs and his choice of strategy means that a voter can manipulate another voter's choice of strategy by misleading him into adopting inaccurate beliefs concerning other voters' beliefs.</p> </abstract>
<abstract> <p>A choice function is defined to exist if there is a "best" (under a binary relation R) element in all non-empty compact subsets of S, the set of all possible alternatives, whereas a demand correspondence exists if there is a "best" element in only the budget sets of S. Some basic restrictions on R are considered. First, if the "at least as good as" sets are closed, then none of the standard restrictions on R are shown to be necessary for the existence of a demand correspondence: the "domination" of finite sets is necessary and sufficient. This is shown to imply that acyclicity of R is necessary and sufficient for the existence of choice functions. Second, if either there is a restriction on convergent P monotone sequences or if R satisfies a regularity condition, then a condition on cyclical sets of alternatives is enough to guarantee the existence of demand correspondences. For the existence of rational choice functions, however, reflexivity, completeness, and transitivity of R, together with the above-mentioned condition on P-monotone sequences, are necessary and sufficient. Finally, if the strictly preferred sets are taken to be convex, then under a restriction weaker than the first, a best element in budget sets exists.</p> </abstract>
<abstract> <p>By looking at approximate multivariate risk premiums a matrix measure of multivariate local risk aversion is introduced for a multi-attributed utility function u. This matrix function R(x) = [-u"i"j(x)/u"i(x)] generalizes the univariate measure of Pratt [11] and the conditional measure of Keeney [7]. It has particular advantages in assessing the attitude of a decision-maker toward correlated risks, a concern of Richard [13], and is more informative than the scalar measure proposed by Kihlstrom and Mirman [8]. Simple characteristics of the absolute risk aversion matrix R determine whether a utility function is additive or concave. Assumptions of either constancy or proportionality of R are shown to lead to specific restrictions on the form of u which are more stringent than those of Rothblum [15].</p> </abstract>
<abstract> <p>A new formulation of the linear quadratic control, LQC, problem with known coefficients, called the LAG, is presented. The LAG generally does not generate a recursive Ricatti system. For models with long lags an important issue is whatformulation leads to an efficient algorithm both with respect to storage and speed. At present the most efficient known formulation is the minimum state variable representation, MSV. The LAG requires much less storage than the MSV as the LAG does not require conversion to state space representation. For short time horizons the LAG is computationally faster than the MSV. As the time horizon increases, the efficiency of the LAG relative to the MSV declines. Numerical comparisons of the Theil, Chow, MSV, and LAG formulations are shown.</p> </abstract>
<abstract> <p>Modifications of the limited information estimator and of the fixed k-class estimator that possess finite moments are presented. It is demonstrated that, through terms of O(T^-^2) where T is the sample size, the fixed k-class estimator is dominated by the modification of the limited information estimator.</p> </abstract>
<abstract> <p>The consistency and the asymptotic normality of the maximum likelihood estimator in the general nonlinear simultaneous equation model are proved. It is shown that the proof depends on the assumption of normality, unlike in the linear simultaneous equation model. It is proved that the maximum likelihood estimator is asymptotically more efficient than the nonlinear three-stage least squares estimator if the specification is correct. However, the latter has the advantage of being consistent even when the normality assumption is removed. Hausman's instrumental-variable interpretation of the maximum likelihood estimator is extended to the general nonlinear simultaneous equation model.</p> </abstract>
<abstract> <p>The finite sample behavior in a dynamic, simultaneous system of least squares and instrumental variables estimators which allow for autoregressive errors is studied by control variable (CV) simulation. To increase simulation precision, the CV's are based on asymptotic approximations to the econometric estimators and so have the same asymptotic distributions, but known finite sample moments. The CV formulae also clarify the properties of the econometric techniques and combined with response surfaces, reduce the specificity of simulation findings. The results confirm the value of asymptotic theory and show that the autoregressive instrumental variables estimator provides a reasonable approach to the simultaneity-autocorrelation-dynamics interaction.</p> </abstract>
<abstract> <p> "If one is willing to interpret Q̃ [the Goldberger, Nagar, Odeh reduced form coefficient covariance estimate] as a covariance matrix of the random parameter π around the constant &lt;tex-math&gt;$\tilde{\pi}$&lt;/tex-math&gt;, rather than as a covariance matrix of the random estimates &lt;tex-math&gt;$\tilde{\pi}$&lt;/tex-math&gt;, then using &lt;tex-math&gt;$\tilde{\pi}$&lt;/tex-math&gt; for &lt;tex-math&gt;$\tilde{\pi}$&lt;/tex-math&gt; and Q̃ for Q &lt;tex-math&gt;$[\tilde{\pi}$&lt;/tex-math&gt; and Q are the mean and covariance matrix of the random parameter π] will provide an approximate solution to the evaluation of expectations required in our optimal control problem" [3, p. 641], italics added). </p> </abstract>
<abstract> <p>When drawing up a contract, it is often impracticable for the parties to specify all the relevant contingencies. In particular, they may be unable to describe the states of the world in enough detail that an outsider (the courts) could later verify which state had occurred, and so the contract will be incomplete. The parties can make up for this incompleteness to some extent bybuilding into the contract a mechanism for revising the terms of trade as they each receive information about benefits and costs. One striking conclusion of our analysis is that because the parties can rescind the original contract and write a new one, severe limitations are placed on the form the revisions can take. Moreover, these limitations depend crucially on what means of communication the parties have at their disposal during the revision process. We characterize an optimal contract in two cases. First, when a contract is being used to facilitate trade between two agents who must undertake relationship-specific investments, it is generally not possible to implement the first-best. Fora a particular example, we are able to confirm the idea that the second-best outcome will involve under-investment. Second, when a contract is being used to share risk, and there are no specific investments, we find that it is possible to implement the first-best provided messages sent between the agents can be publicly verified.</p> </abstract>
<abstract> <p>Many electoral rules (such as those governing the U.S. Constitution) require a super-majority vote to change the status quo. It is well known that without some restriction on preferences, super-majority rules have paradoxical properties. For example, electoral cycles are possible with anything other than 100%-majority rule. Can these problems still arise if there is sufficient similarity of attitudes among the voting population? We introduce a definition of social consensus which involves two restrictions ondomain: one on individual preferences, the other on the distribution of preferences. Individuals vote for the proposal closest (in Euclidean distance) to ther most preferred point. The density of voters' ideal points is concave over its support in R''. Under these conditions, there exists an unbeatable proposal according to 64%-majority rule. In addition, no electoral cycles are possible. For n-dimensional decision problems, the precise majority size necessary to avoid cycles is 1 - (n/n + 1))^n which rises monotonically to 1 - (1/e), just below 64%. Our approach is based on the Simpson-Kramer min-max rule. We compare this rule with Condorcet's original proposal for an electoral system immune to his paradox of votiing. We conclude by considering the properties of a voting constitution based on 64%-majority rule.</p> </abstract>
<abstract> <p>This paper presents a theory of equilibrium asset pricing that generalizes the recent work of Connor. The model extends Connor's results to more general sets of asset returns and consumer preferences, introduces production, and provides a framework for analyzing exact and approximate equilibrium asset pricing. The other major contribution of the paper is the introduction of geometric arguments that exploit the properties of induced preferences over assets. This method of analyzing asset pricing provides an intuitively appealing way of analyzing equilibrium asset pricing theories.</p> </abstract>
<abstract> <p>Dasgupta et al. (1983) showed that if a resource importing country can commit to the future development of a backstop technology, such a development program can be used strategically to affect the pricing policy of a resource supplier. The analysis showed that early invention is not always better, and it revealed the interesting possibility that by delaying development of the substitute, the importing country could benefit from a favorable production response on the part of the exporting country. In this paper we demonstrate that (i) this effect can indeed occur, but (ii) the set of parameters for which it does occur is smaller than previously realized. In the course of doing so, we develop further the intuition behind this effect.</p> </abstract>
<abstract> <p>Consider a homogeneous product market where firms have private information about an uncertain demand parameter and compete in quantities. We examine the convergence properties of Bayesian-Cournot equilibria as the economy is replicated and conclude that large Cournot (or almost competitive) markets do not aggregate information efficiently except possibly when the production technology exhibits constant returns to scale. Even in a competitive market there is a welfare loss with respect to the first best outcome due to incomplete information in general. Nevertheless a competitive market is efficient, taking as given the decentralized private information structure of the economy. Endogenous (and costly) information acquisition is examined and seen to imply that the market outcome always falls short of the first best level with decreasing returns to scale. The results are also shown to be robust to the addition of extra rounds of competition which allows firms to use the information revealed by past prices. Explicit closed form solutions yielding comparative static results are obtained for models characterized by quadratic payoffs and affine information structures.</p> </abstract>
<abstract> <p>A great deal of research on the empirical behavior of inventories examines some variant of the production smoothing model of finished goods inventories. The overall assessment of this model that exists in the literature is quite negative: there is little evidence that manufacturers hold inventories of finished goods in order to smooth production patterns. This paper examines whether this negative assessment of the model is due to one or both of two features: costs shocks and seasonal fluctuations. The reasons forconsidering costs shocks is that, if firms are buffered more by cost shocks thandemand shocks, production should optimally be more variable than sales. The reasons for considering seasonal fluctuations are that seasonal fluctuations areprecisely the kinds of fluctuations that producers should most easily smooth, and that seasonally adjusted data are likely to produce spurious rejections of the production smoothing model even when it is correct. We integrate cost shocks and seasonal fluctuations into the analysis of the production smoothing model in three steps. First, we present a general production smoothing model of inventory investment that is consistent with both seasonal and nonseasonal fluctuations in production, sales, and inventories. Themodel allows for both observable and unobservable changes in marginal costs. Second, we estimate this model using both seasonally adjusted and seasonally unadjusted data plus seasonal dummies. The goal here is to determine whether the incorrect use of seasonally adjusted data has been responsible for the rejections of the production smoothing model reported in previous studies. The third part of our approach is to explicitly examine the seasonal movements in the data. We test whether the residual from an Euler equation is uncorrelated with the seasonal component of contemporaneous sales. Even if unobservable seasonal cost shocks make the seasonal variation in output greater than that in sales, the timing of the resulting seasonal movements in output should not necessarily match that of sales. The results of our empirical work provide a strong negative report on the production smoothing model, even when it includes cost shocks and seasonal fluctuations. At both seasonal and nonseasonal frequencies, there appears to be little evidence that firms hold inventories in order to smooth production. A striking piece of evidence is that in most industries the seasonal in productionclosely matches the seasonal in shipments, even after accounting for the movements in interest rates, input prices, and the weather.</p> </abstract>
<abstract> <p>We analyze the substitution bias in Laspeyres type price indexes, such as CPI, using 1959-1985 NIPA consumption data for 101 commodities. We employ two methodological approaches to calculation of the bias. First, we construct the tightest theoretical bounds on the COL by applyiong nonparametric methods, using algorithms developed by Afriat and by Varian. Second, following Diewert, we construct superlative price index formulas, namely Tornqvist and Fisher's Ideal indexes, under a chain as well as a fixed-base specification. Homothetic preferences are found to be consistent with the data, and sensitivitytests indicate that this result is not vacuous. If the hypothesis of homotheticity is maintained, the COL bias has upper and lower limits of 0.22 and0.14 percent per year, respectively. Using the superlative indexes as the measure of the COL, the bias is about 0.18 percent per year for the period 1959-85. This estimate is somewhat larger than found in earlier studies. Our useof more disaggregated data is found to be responsible for part of the difference, with the size of the estimated bias directly related to the extent of the aggregation.</p> </abstract>
<abstract> <p> One type of semiparametric regression on an &lt;tex-math&gt;$\scr{R}^{p}\times \scr{R}^{q}\text{-valued}$&lt;/tex-math&gt; random variable (X, Z) is β′X + θ(Z), where β and θ(Z) are an unknown slope coefficient vector and function, and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of β based on incorrect parameterization of θ are generally inconsistent, whereas consistent nonparametric estimators deviate from β by a larger probability order than N&lt;sup&gt;-1/2&lt;/sup&gt;, where N is sample size. An estimator generalizing the ordinary least squares estimator of β is constructed by inserting nonparametric regression estimators in the nonlinear orthogonal projection on Z. Under regularity conditions β̂ is shown to be &lt;tex-math&gt;$N^{1/2}\text{-consistent}$&lt;/tex-math&gt; for β and asymptotically normal, and a consistent estimator of its limiting covariance matrix is given, affording statistical inference that is not only asymptotically valid but has nonzero asymptotic first-order efficiency relative to estimators based on a correctly parameterized θ. We discuss the identification problem and β̂'s efficiency, and report results of a Monte Carlo study of finite-sample performance. While the paper focuses on the simplest interesting setting of multiple regression with independent observations, extensions to other econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models. </p> </abstract>
<abstract> <p>Social experiments are characterized by their high cost. A tempting alternative to the establishment of a contemporaneous statistical control group is pre-experiment observation of the treatment group. In this paper we analyze the trade-off between these two types of experimental "control" as a function oftheir relative cost and information content in the context of a multi-period error components framework, where the allocation of observations across the two groups is always done in an optimal manner. Solutions for the optimal proportion of the sample to be devoted to a contemporaneous control group are presented and their behavior as a function of relevant parameters is studied.</p> </abstract>
<abstract> <p>This paper derives necessary and sufficient conditions for Arrow-Debreu choices of contingent consumption to be compatible with the maximization of a state independent expected utility function that exhibits increasing or decreasing absolute risk aversion, or increasing or decreasing relative risk aversion. The conditions can be used to bound different measures of risk aversion based on a single observation of Arrow-Debreu portfolio choice.</p> </abstract>
<abstract> <p>Scotchmer has shown that, even if the population is homogeneous, hedonic prices do not correctly measure long-run benefits of a large public project. This paper examines the direction of errors caused by the use of cross-sectional land rent differentials in benefit estimation. The short-run benefits with fixed lot size are also considered.</p> </abstract>
<abstract> <p>The primary aim of this paper is to propose a new measure of poverty, which should avoid some of the shortcomings of the measures currently in use. An axiomatic approach is used to derive the measure. The conception of welfare in the axiom set is ordinal. The information requirement for the new measure is quite limited, permitting practical use.</p> </abstract>
<abstract> <p>We consider how large a committee must be before it is possible to achieve simultaneously any two rankings of m alternatives by two seemingly consistent procedures.</p> </abstract>
<abstract> <p>This paper is concerned with Fisher's tests for index numbers. In particular, uniqueness and inconsistency theorems are proved. Beyond that, Fisher's system of tests is weakened considerably. Without any regularity assumption (such as differentiability or continuity) it is shown that every subset of the system of weakened tests is consistent while the whole system is inconsistent. The question of how far the whole system must be weakened to obtain a consistent set of tests is also considered.</p> </abstract>
<abstract> <p> Le but de ce papier est d'étendre les résultats de W. Hildenbrand relatifs aux "coalition production economy" quand à chaque coalition est associé un vecteur (dans un espace de paramètres--biens non-marchands) qui détermine son ensemble de production. Si cette dépendance est à rendements constants, l'ensemble des équilibres de Walras de l'économie est non vide et (si chaque agent a une influence négligeable) égal au noyau. Nous donnerons aussi une condition nécessaire et suffisante pour que l'ensemble des équilibres de Walras soit non vide mais dans ce cas un exemple montre que nous ne pouvons espérer avoir de théorème d'égalité. </p> </abstract>
<abstract> <p>This paper provides an existence proof of an oligopolistic equilibrium which solves some difficulties in the construction of a consistent descriptive model of imperfect competition.</p> </abstract>
<abstract> <p>This paper investigates a simple economic model for the commercial fishing industry. The results imply the phenomena of non-explosive fishing capital investment and non-extinctive fishery resources. Both investments and resources will always tend to an equilibrium position. A comparison with a more general model is also made.</p> </abstract>
<abstract> <p> A theory of financial markets based on a two-parameter portfolio model is shown to imply stochastic dependence between transaction volume and the change in the logarithm of security price from one transaction to the next. The change in the logarithm of price can therefore be viewed as following a mixture of distributions, with transaction volume as the mixing variable. For common stocks these distributions (of which the distribution of Δ log p is a mixture) appear to have a pronounced excess of frequency near the mean and a deficiency of outliers, relative to the normal. These findings are consistent with the hypothesis that stock price changes over fixed intervals of time follow mixtures of finite-variance distributions. </p> </abstract>
<abstract> <p>The paper develops a simple iterative procedure for deriving linear decision rules which provide the optimal control policy for a stochastic dynamic linear system. The procedure works for a quadratic objective function with any time horizon up to and including infinity, either with or without time discounting. The role of target variables is considered and there is a discussion of the results which ensue if these targets are incompatible, that is, if they do not satisfy the underlying structural model. The paper concludes with some consideration of the convergence and other properties of the controlled system.</p> </abstract>
<abstract> <p>A Bayesian procedure for comparing linear models with non-scalar covariance matrices is developed. For the case of first order auto-regressive disturbances, an approximate expression for the error in the posterior odds due to ignoring the serial correlation is given, and it's accuracy is investigated via sampling experiments.</p> </abstract>
<abstract> <p>One should be very careful in using the "non-informative" priors suggested in the Bayesian econometric literature for the covariance matrix of residuals in simultaneous equations models. To highlight the inadequacies of the prior, this paper shows that the prior leads to sharp posterior distributions even in under identified models. Similar problems also arise with the 2SLS method, but one can apply tests for underidentification. Something similar has to be done in the Bayesian context.</p> </abstract>
<abstract> <p>This paper considers the effect of aggregation on the variance of parameter estimates for a linear regression model with random coefficients and an additive error term. Aggregate and micro variances are compared and measures of relative efficiency are introduced. Necessary conditions for efficient aggregation procedures are obtained from the Theil aggregation weights and from measures of synchronization related to the work of Grunfeld and Griliches.</p> </abstract>
<abstract> <p>In this paper we derive and present optimal critical points for pre-tests in regression using a minimum average relative risk criterion. We use the same type risk functions as Sawa and Hiromatsu [8] who, in a recent paper in this journal, derived pre-test critical values using a minimax regret criterion. Since James-Stein type estimators can be shown to dominate any pre-test estimator for the risk functions used here and in [8], no normative claims are made for the critical values we give. However, the use of pre-testing procedures continues in practice and the results given here, contrasted with other results, add to information about the character of costs and returns to such practices.</p> </abstract>
<abstract> <p>The situation in which a principal-agent relationship is repeated finitely many times (T) is formulated as a sequential game. For any Pareto-optimal cooperative arrangement in the one-period game that dominates a one-period Nash equilibrium, and any positive number epsilon, there exists for every sufficiently large T a (noncooperative) epsilon equilibrium of the T-period game that yields each player an average expected utility that is at least his expected utility in the one-period cooperative arrangement, less epsilon.</p> </abstract>
<abstract> <p>Cournot-Nash models of free entry into industries with large fixed costs yields equilibria with only a few operating firms, and each firm has some monopoly power. I consider a model where each firm's strategy is a function q(P) which specifies how much it will supply at each price. Unlike in Cournot models, the competitive equilibrium (where it exists) is always a Nash equilibrium in supply function strategies, and under weak assumptions it is the only equilibrium. This permits a Nash equilibrium model of the threat of entry as a deterrent to the exercise of monopoly power by operating firms.</p> </abstract>
<abstract> <p>Conditions which imply the existence of strict rational expectations equilibria for "most" static pure exchange economies where prices convey information about the state of the world are analyzed. Information is aggregated and transmitted by prices because agents have different initial information and maximize the conditional expectation of state-dependent utility functions. Major results state that if, with probability one, agents' characteristics do not vary too much as the state of the world varies, then, generically, there exist equilibrium price functions which reveal all initial information. More precisely, if the support of the image measure is contained in a compact C^1 manifold (satisfying some technical conditions) which is of sufficiently low dimension relative to the number of commodities, then existence of completely revealing equilibria is generic. Any economy satisfying these assumptions can be approximated by a sequence of economies which also obey these assumptions and have completely revealing equilibrium price functions. Slight perturbations do not destroy the existence property.</p> </abstract>
<abstract> <p>A study is made of a competitive trading process in which a specific price-maker calls prices periodically under the obligation of supporting these prices by trading for his own account to satisfy excess demand. Specifically, the paper considers price formation in an organized securities exchange using the specialist system to facilitate trading. Following procedures developed in an earlier study, characteristics of the price-maker's optimal behavior are derived. These characteristics are used to obtain properties of stationary distributions of market prices and to compare the operation of the specialist system with an alternative market clearing system.</p> </abstract>
<abstract> <p>The paper studies an economy with H households, N + 1 commodities, and M fixed factors with commodity taxes and government expenditures on goods and services. The first and second order directional derivatives of a certain weighted sum of utility functions with respect to any direction of tax change are calculated. The resulting measure of deadweight loss, due essentially to Boiteux, is contrasted with a measure based on Debreu's coefficient of resource utilization as well as with the more familiar measure of loss due to Hotelling and Harberger. The basic analytical technique used is the usual comparative statistics analysis, except that duality theory is used to simplify the computations.</p> </abstract>
<abstract> <p>If optimal tax theory is to be the basis for calculating tax rates, a close understanding is required of the relationship between the structure of preferences and the configuration of optimal tax rates. Otherwise hypotheses chosen by the econometrician for practical convenience may completely determine the results, independently of measurement. This paper explores the relationship between various types of separability, particularly weak and implicit separability, and optimal tax rates in the various models discussed in the literature. The use of distance functions and the Antonelli matrix provides a significant unification of previously disparate results.</p> </abstract>
<abstract> <p>The sequential nature of activities like research, development, or exploration requires optimal funding criteria to take account of the fact that subsequent funding decisions will be made throughout the future. Thus, there is a continual possibility of reviewing a project's status, based on the latest information. After setting up a model to capture this feature, optimal funding criteria are investigated. In an important special case, an explicit formula is derived. As well as throwing light upon the nature of development activities, the analysis is also relevant to the general theory of information gathering processes.</p> </abstract>
<abstract> <p>A discrete-choice probability model can be estimated from a sample stratified on the choice variable by maximizing the "pseudo-likelihood," a quantity closely related to the log likelihood for a random sample. We investigate the asymptotic properties of the estimator, and show that it is consistent, asymptotically normally distributed, and satisfies a commonly used criterion for asymptotic efficiency.</p> </abstract>
<abstract> <p>Estimates of parameters in Tobit and other models for limited, truncated and censored dependent variables are not robust against misspecification. A test of the standard assumptions against a general misspecified alternative in the univariate censored normal model is derived and extended to the Tobit regression case. Computational ease and freedom from specification of a specific alternative hypothesis are primary attractions of the test.</p> </abstract>
<abstract> <p>The asymptotic distribution of prediction is derived for the general simultaneous equation model with lagged endogenous variables and vector autoregressive errors. The results turn out to be particularly simple when no lagged endogenous variables are present.</p> </abstract>
<abstract> <p>This paper presents econometric evidence on the effect of tax incentives on business investment in the United States in the period from 1953 through 1978. The analysis emphasizes that the interation of inflation and existing tax rules has contributed substantially to the decline of business investment since the late 1960s. Because the investment process is far too complex for any simple econometric model to be convincing, I have estimated three quite different models of investment behavior. The strength of the empirical evidence rests on the fact that all three specifications support the same conclusion. More generally, the analysis and evidence show that the theoretical models of macroeconomic equilibrium should specify explicitly the role of distortionary taxes, especially taxes on capital income. The failure to include such tax rules can have dramatic and misleading effects on the qualitative as well as the quantitative properties of macroeconomic theories.</p> </abstract>
<abstract> <p>We propose a new criterion for equilibria of extensive games, in the spirit of Selten's perfectness criteria. This criterion requires that players' strategies be sequentially rational: Every decision must be part of an optimal strategy for the remainder of the game. This entails specification of players' beliefs concerning how the game has evolved for each information set, including informaiton sets off the equilibrium path. The properties of sequential equilibria are developed; in particular, we study the topological structure of the set of sequential equilibria. The connections with Selten's trembling-hand perfect equilibria are given.</p> </abstract>
<abstract> <p>This paper presents a set of axioms which characterize a family of price mechanisms for consumption goods, including marginal cost prices and Aumann-Shapley prices. By strengthening one of the axioms, marginal cost prices are characterized and by requiring that cost is shared the Aumann-Shapley prices are characterized. A discussion of the economic interpretation of the axioms is also provided.</p> </abstract>
<abstract> <p>We consider the problem of how to regulate a monopolistic firm whose costs are unknown to the regulator. The regulator's objective is to maximize a linear social welfare function of the consumer's surplus and the firm's profit. In the optimal regulatory policy, prices and subsidies are designed as functions of the firm's cost report so that expected social welfare is maximized, subject to the constraints that the firm has nonnegative profit and has no incentive to misrepresent its costs. We explicitly derive the optimal policy and analyze its properties.</p> </abstract>
<abstract> <p>This paper establishes a natural and satisfying characterization of the class of collective choice rules which are acyclic and satisfy the Arrow axioms (unrestricted domain, independence of irrelevant alternatives, and the weak Pareto principle). We show that, when the number of alternatives is larger than the number of individuals, there must exist an individual who can "veto" at least some critical number of pairwise decisions. This critical number of veto pairs depends on the number of alternatives and individuals, and, as the number of alternatives increases without limit, the fraction of all pairs which some individual can veto approaches unity. We also present a global veto theorem and an axiomatic characterization of the Pareto extension rule which utilizes acyclicity rather than quasi-transitivity.</p> </abstract>
<abstract> <p>This paper shows how to test data for consistency with utility maximization, recover the underlying preferences, and forecast demand behavior without making any assumptions concerning the parametric form of the underlying utility or demand functions.</p> </abstract>
<abstract> <p>There are many infinite horizon optimal problems in economic models. In such problems, the transversality condition may not be verified, as shown by Halkin'sexample. But we prove another property: the maximum of the Hamiltonian converges to zero when time goes to infinity. And, if along the optimal trajectory, after some time, changes of speed (by controls) in all directions at a given level are possible, then the transversality condition is verified. Examples show that the additional property proved here (i) allows exclusion of nonoptimal trajectories which verify the usual necessary conditions in an infinite horizon; (ii) is a result directly useful in some economic studies.</p> </abstract>
<abstract> <p>Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autoregressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.</p> </abstract>
<abstract> <p>The distributions of the Limited Information Maximum Likelihood estimator for the coefficient of one endogenous variable are evaluated numerically. Tables are given for enough values of the parameters to cover all cases of interests. Comparisons are made with the Two-Stage Least Squares estimator.</p> </abstract>
<abstract> <p>This paper studies estimators that make sample analogues of population orthogonality conditions close to zero. Strong consistency and asymptotic normality of such estimators is established under the assumption that the observable variables are stationary and ergodic. Since many linear and nonlinear econometric estimators reside within the class of estimators studied in this paper, a convenient summary of the large sample properties of these estimators, including some whose large sample properties have not heretofore been discussed, is provided.</p> </abstract>
<abstract> <p>Econometrics is seen as a vehicle for fundamental innovations in scientific method, above all in the development of operative forecasting procedures in nonexperimental situations. On the one hand there is the evolution relative to experimental situations, for experiments yield reproducible knowledge and new results can therefore be used for forecasting as soon as they are established. Within nonexperimental forecasting, on the other hand, there is the evolution from the largely passive forecasts that are typical in demography over to the operative forecasts in terms of instruments and targets that are the aspiration, difficult to materialize with success, of the multirelational forecasting models of modern econometrics. The econometric developments confirm the general experience that scientific evolution is never a straightforward affair. To solve new types of problems requires a reorientation of the analysis, sometimes a sharp turn from the old tracks, and the sharper the turn the more confused and controversial is the accompanying debate. As briefly reviewed in my Address, econometrics has been pathbreaking in several fundamental developments connected with forecasting, included the transition from exact to stochastic relations, from descriptive to explanatory analysis, and from unirelational to multirelational model building.</p> </abstract>
<abstract> <p>Implications for the distribution of wealth and income of alternative assumptions about savings, reproduction, inheritance policies, and labor homogeneity are investigated in the context of a neoclassical growth model. The paper isolates the different economic forces which tend to make the distribution of wealth in the long run equalitarian and those which tend to make wealth unevenly distributed.</p> </abstract>
<abstract> <p> Least squares type estimates of the coefficients b(τ), τ = 0, ± 1, ± 2,..., in the general multidimensional distributed lag model &lt;tex-math&gt;$Y(t)=\sum_{s=-\infty}^{\infty }b(s)X(t-s)+\varepsilon (t)$&lt;/tex-math&gt; (t =... -1, 0, 1,...) are considered, where {X(t)} and {Y(t)} are observable random processes, ε(t) is an unobservable noise process. The asymptotic joint distribution of the estimates, conditional on the observed spectral density of the input X(t), is given, as well as the unconditional first and second moments, a readily computable confidence ellipsoid, and an approximate expression for the expected covariance in predicting Y(t) from a new realization of X(t), using the estimated coefficients. </p> </abstract>
<abstract> <p>This paper estimates a model specifying the determinants of trade credit in the United States total manufacturing sector for the postwar period. Trade credit is considered as a selling expense, like advertising outlays. Its determinants are derived from a profit maximization model in which the price, volume of output, and the selling costs are all variables to be jointly determined. The opportunity or user cost of accounts receivable and accounts payable are specified and the response of these accounts as well as net trade credit to changes in various monetary decision variables is examined.</p> </abstract>
<abstract> <p>There occurs on some occasions a difficulty in deciding the direction of causality between two related variables and also whether or not feedback is occurring. Testable definitions of causality and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous causality is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible causal variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single causal arm of a feedback situation. Measures of causal lag and causal strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested.</p> </abstract>
<abstract> <p>In performing their distribution function, retailers ordinarily offer certain services related to the buying environment. The geographic layout of retail networks also determines the level of buyers' travel disutility. These services and costs are not subject to explicit pricing, and only indirect welfare evaluations of the system's performance are possible. In this paper, a spatial model of retailing is formulated with the objective of allowing indirect evaluations. The working of the model is illustrated by applying it to a system of retail food stores.</p> </abstract>
<abstract> <p>We examine the question of whether Leontief's [15] conditions for separability and aggregation need be approximately satisfied if only approximate aggregates are to be constructed. It is found that they must be approximately satisfied unless the functions involved exhibit increasingly irregular behavior as the approximation involved in the aggregate gets better. The irregularities involved are related to the violation of Lipschitz Conditions. The stringency of the Leontief conditions for aggregation is therefore not easily evaded.</p> </abstract>
<abstract> <p>This paper discusses the dynamics of disequilibrium in a single market where both price and quantity change in response to disequilibrium. We describe the nature of the adjustment path under a wide variety of assumptions, noting in particular the properties of stability in the large and in the small and the existence of limit cycles.</p> </abstract>
<abstract> <p>This paper examines the asymptotic efficiency of the exact tests for autocorrelation and trend introduced in Krishnaiah and Murthy [10].</p> </abstract>
<abstract> <p>So far we have several conditions for consistency in the simple majority decision rule. These conditions assume that some preference orderings are not in the list of the possible individual orderings and each individual is free to choose any ordering in this list. When the list of the possible individual orderings is too wide, inconsistency may arise. But when the list is selected from a group of narrower ones, inconsistency never arises, no matter how each individual selects his own preference ordering in the list. The purpose of this paper is to give the complete catalogue of such lists. Our Catalogue, of course, includes the conditions so far obtained. But it also includes some new conditions.</p> </abstract>
<abstract> <p>This article gives a reformulation of the simplex method for quadratic programming having the advantage of generating tableaux with certain symmetry properties. It is proved that this method gives the same sequence of iterations as formulation of the simplex method for quadratic programming given earlier by Dantzing and the authors, which may be called the asymmetric formulation. A proof of convergence to the optimal solution is given, which is much simpler than the corresponding proof for the asymmetric formulation. A second symmetric formulation, which is equivalent to the two other formulations, is indicated. For the usual method for quadratic programming as well as for parametric quadratic programming, similar formulations exist. Wolfe's long form of his method for quadratic programming turns out to be the asymmetric variant for parametric quadratic programming.</p> </abstract>
<abstract> <p>This paper gives estimates of the elasticity of substitution between capital and labor in eleven manufacturing industries. The elasticities are calculated from capital-labor ratios and factor prices (wage rates, machinery prices, and interest rates) in two countries--the United States and Peru. The distinguishing feature of this study is that capital is measured directly (starting from book value of fixed assets) instead of being inferred from the nonlabor share of value added. The resulting elasticity estimates are strikingly low.</p> </abstract>
<abstract> <p>This paper addresses the problem of estimating unknown regression coefficients when erroneous data and other violations of the standard assumptions are possible. An estimator which has a limited sensitivity to these departures from the assumptions is presented, and some of its properties are derived. This estimator is shown to have a certain efficiency property relative to other estimators with the same sensitivity to erroneous data.</p> </abstract>
<abstract> <p>This paper develops a test of the rational expectations hypothesis advanced by Muth [18]. The framework considered here allows for multiperiod expectations of several endogenous variables, with or without lagged exogenous variables. In conventional (linear) models, the hypothesis implies that the expectations are linear in certain relevant variables, and restricts the coefficients of these variables to be certain functions of the parameters in the imbedding model. The test is developed as a test of the validity of these restrictions. The paper also treats the estimation problem in some details, under the alternative hypothesis which is taken as simply the negation of the rational expectations hypothesis.</p> </abstract>
<abstract> <p>Recursive equilibrium theory is extended and generalized. Optimality of equilibria and supportability of optima are established in a direct way. Four economic applications are reformulated as recursive competitive equilibria and analyzed.</p> </abstract>
<abstract> <p>A method is described in this paper for estimating, by means of stochastic simulation, the asymptotic variances of multipliers of nonlinear models. It is used to estimate the uncertainty of the results of eight policy experiments for a particular model.</p> </abstract>
<abstract> <p>Previous equilibrium business cycle models are extended by the incorporation of an economy-wide capital market. This extension alters the information structure of these models and modifies the relative price variable that transmits money shocks to real variables. Monetary effects on nominal and real interest rates are a focus of the analysis.</p> </abstract>
<abstract> <p>This paper provides a general proof of the existence of a proportional price with the interest rate strictly greater than the growth rate associated with any efficient proportional program in the polyhedral technology. Proofs for similar theorems are also given for the programs satisfying the regularity condition in other technologies.</p> </abstract>
<abstract> <p>Alternative measures of the rate of diffusion of a new innovation are discussed and applied to study the spread of hybrid corn in the United States, in the light of more recent data and improved estimating techniques. An assessment is made of the importance of "profitability" variables in accounting for variations in the rate of diffusion between states.</p> </abstract>
<abstract> <p>Over two decades ago, Charles Tiebout conjectured that in economies with local public goods, consumers "vote with their feet" and that this "voting" creates an approximate "market-type" equilibrium. He hypothesized that this approximate equilibrium is "nearly" optimal and, the smaller the moving costs, the closer the equilibrium is to an optimum. This paper provides a formal model of an economy with a local public good and endogenous jurisdiction structures (partitions of the set of agents into jurisdictions) which permits proofs of Tiebout's conjectures. Analogues of classical results pertaining to private-good economies, such as existence of equilibrium and convergence of the core to equilibrium states of the economy, are obtained for the approximate equilibrium and approximate cores.</p> </abstract>
<abstract> <p>In our previous paper, "Optimal Allocation of Public Goods...," [5] we presented a mechanism for determining efficient public goods allocations when preferences are unknown and consumers are free to misrepresent their demands for public goods. We proved the basic welfare theorem for this model: If consumers are competitive in markets for private goods and follow Nash behavior in their choice of demands to report to the mechanism, then equilibria will be Pareto optimal. In this paper we show this result is not vacuous by proving that an equilibria will be Pareto optimal. In this paper we show this result is not vacuous by proving that an equilibrium will exist for a wide class of economies. Our conditions are slightly stronger than those required to prove the existence of a Lindahl equilibrium. In order to rule out the possibility of bankruptcy, we assume additionally that at all Pareto optimal allocations, private goods consumption is bounded away from zero.</p> </abstract>
<abstract> <p>This paper shows how a number of questions about dominant strategy mechanisms in models with public goods can be conveniently formulated as systems of partial differential equations. The question of the existence of dominant strategy mechanisms with given desirable properties becomes equivalent to the integrability of these equations.</p> </abstract>
<abstract> <p>In a broad class of situations not covered by the Gibbard-Satterthwaite Theorem it is shown that one cannot design a strategy-proof choice mechanism which attains Pareto optimal outcomes. The results are shown to be genetic in character--i.e., any nonmanipulable mechanism will attain nonoptimal outcomes virtually everywhere--and they cover, in particular, certain problems of allocating public and private goods. The analysis is carried out in transferable utility environments, and makes extensive use of the mechanisms recently introduced by Groves.</p> </abstract>
<abstract> <p>In many countries patentees must pay an annual renewal fee in order to keep their patents in force. This paper presents and then estimates a model which uses observations on the proportion of different cohorts of patents which are renewed at alternative ages, and the relevant renewal fee schedules, to estimate the distribution of the returns earned from holding patents, and the evolution of this distribution function over the lifespan of the patents. Since patents are often applied for at an early exploratory stage of the innovation process, the model allows patentees to be uncertain about the sequence of returns that will be earned if the patent is kept in force. The paper solves the implied optimal stopping problem for the micro units, derives the implications of these solutions on the aggregate proportion renewed, and then estimates the parameters of the model from the aggregate data. Separate estimates are obtained from data on post World War II cohorts of patents in each of France the United Kingdom, and Germany.</p> </abstract>
<abstract> <p>A model of decentralized exchange and price formation is defined using the bargaining theory of A. Rubinstein. Agents meet at random and bargain over the terms of trade. If there are no transaction costs, every perfect equilibrium of the bargaining game implements a Walrasian equilibrium of the underlying exchange economy.</p> </abstract>
<abstract> <p>In Gale (1986) I developed a model of decentralized exchange and price formation. Agents meet at random and bargain over the terms of trade. In the absence of transaction costs every perfect equilibrium of this game implements a Walrasian equilibrium of the underlying exchange economy. In this paper I show, conversely, that every Walrasian equilibrium of the underlying exchange economy can be implemented as a perfect equilibrium of this game. In particular this shows the set of perfect equilibria is nonempty.</p> </abstract>
<abstract> <p>A general model of arbitrator behavior in conventional and final-offer arbitration is developed that is based on an underlying notion of an appropriate award in a particular case. This appropriate award is defined as a function of the facts of the case independently of the offers of the parties. In conventional arbitration the arbitration award is argued to be a function of both the offers of the parties and the appropriate award. The weight that the arbitrator puts on the appropriate award relative to the offers is hypothesized to be a function of the quality of the offers as measured by the difference between the offers. In final-offer arbitration it is argued that the arbitrator chooses the offer that is closest to the appropriate award. The model is implemented empirically using data gathered from practicing arbitrators regarding their decisions in twenty-five hypothetical cases. The estimates of the general model strongly support the characterizations of arbitrator behavior in the two schemes. In addition, no substantial differences were found in the determination of the appropriate award implicit in conventional arbitration decisions and the determination of the appropriate award implicit in the final-offer decisions.</p> </abstract>
<abstract> <p>Macroeconomic models with rational expectations find a new justification if these models appear as limits of some learning procedures. In this paper we consider the case in which, during the learning period, the predictions are obtained by regression. We exhibit the necessary and sufficient condition on the parameter of the model ensuring the convergence of the learning process. The limit is the solution of a rational expectations model in which the information set only includes the exogenous variables used in the auxiliary regression.</p> </abstract>
<abstract> <p>This paper proposes efficient instrumental-variable estimators for an error-components model considering alternative assumptions about the sources of endogeneity and the variance-covariance properties of disturbances. The analysis develops a general result that provides for the construction of asymptotically efficient estimators when there exist variables that are predetermined for only a subset of the equations making up a structural model.</p> </abstract>
<abstract> <p>This paper derives the exact finite sample distribution of the Wald statistic for testing general linear restrictions on the coefficients in the multivariate linear model. This generalizes all previously known results, including those for the standard F statistic in linear regression, for Hotelling's generalized T^2"0 test. The results presented here encompass both the null and the non-null distributions. They also yield in a simple and elegantway the asymptotic distribution theory and related higher order asymptotic expansions. Various specializations of our general result are presented, including a computable formula for the null distribution in the case of a test of single restriction. Conventional classical assumptions of normally distributed errors and nonrandom exogenous variables are employed.</p> </abstract>
<abstract> <p> This paper establishes a simple existence result for solutions to variational problems of the form ∫&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;∞&lt;/sup&gt; G(x, ẋ, t) dt or ∫&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;∞&lt;/sup&gt; G(x, ẋ, ẍ, t) dt. The key assumptions are that G have an integrable upper bound, that it satisfy a growth condition, and that it be concave as a function of the highest order derivative in the problem, other arguments held constant. The discussion illustrates why three well known types of problems fail to have solutions. For two of these—chattering and cake eating—extended solution concepts are contrasted with simple modifications that restore the existence of a conventional solution. In a third case—state variables with jumps—the source of the difficulty is fundamental. For these problems a natural extended solution, analogous to the extension from probability density functions to general distribution functions, is suggested. </p> </abstract>
<abstract> <p>This paper considers stochastic social choice rules which, for every feasible set of alternatives and every profile of individual orderings, specify social choice probabilities for the feasible alternatives. It is shown that if such a stochastic social choice rule satisfies (1900): (i) a probabilistic counterpart of Arrow's independence of irrelevant alternatives, (ii) ex-post Pareto optimality, and (iii) "regularity" (a "rationality" property postulating that given the individual preference orderings, if the feasible set of alternatives is expanded, then the social choice probability for an initially feasible alternative cannot increase), then the power structure under it is almost completely characterized by weighted random dictatorship.</p> </abstract>
<abstract> <p>We extend the principal-agent framework with risk-neutral principals to situations in which several principals simultaneously and independently attempt to influence a common agent. We show that implementation is, in the aggregate, always efficient (cost-minimizing), and that noncooperative behavior induces an efficient (potentially second-best) action choice if and only if collusion among the principals would implement the first-best action at the first-best level of cost. We also investigate the existence of equilibria, the distribution of net rewards among principals, the characteristics of actions chosen in inefficient equilibria, and potential institutional remedies for welfare losses induced by noncooperative behavior.</p> </abstract>
<abstract> <p>We develop a duopoly model in which exit occurs because of the existence of fixed costs or opportunity costs. Each firm enters the market knowing its own cost, but not that of its opponent. As times goes on, each firm becomes increasingly pessimistic about the cost of its remaining rival. The time of exit is the only strategic variable, so that our model is a "war of attrition." In contrast to the classic war of attrition, however, we assume that with positive probability each firm's costs may be low enough that staying in forever is a dominant strategy. Thus our model, unlike the classic one, has a unique equilibrium.</p> </abstract>
<abstract> <p>Seasonally adjusted monetary aggregate data as published by the Federal Reserve,are subject to large revisions, which can be interpreted as error in the preliminary measures. Since short-run monetary policy is set for seasonally adjusted data when only preliminary estimates for recent months are available, an interesting question is: Would policy have been much different if final data had been available? For the period of the seventies, we estimate what would have been the monthly Federal Open Market Committee targets for M1 and the federal funds rate if the preliminary estimate of the rate of growth of seasonally adjusted M1 had been equal to the final one. We find that, despite their large size, revision errors seem to have little impact on the setting of targets. The results suggest that the Fed reacts to a signal in the rate of growth of M1 which is smoother than the seasonally adjusted series and less affected by revisions. Since the error associated with the revision is orthogonal to the preliminary measurement, while the noise extracted is orthogonal to the "true" variable (the signal), the analysis illustrates the different effects of the two alternative error-in-variable specifications.</p> </abstract>
<abstract> <p>This paper presents a parameter covariance matrix estimator which is consistent even when the disturbances of a linear regression model are heteroskedastic. This estimator does not depend on a formal model of the structure of the heteroskedasticity. By comparing the elements of the new estimator to those of the usual covariance estimator, one obtains a direct test for heteroskedasticity, since in the absence of heteroskedasticity, the two estimators will be approximately equal, but will generally diverge otherwise. The test has an appealing least squares interpretation.</p> </abstract>
<abstract> <p>Data sets which contain jointly endogenous discrete and continuous variables often occur in practice. This paper presents a model of the economic and stochastic processes generating such data as well as methods of estimation. A maximum likelihood estimator is examined and found to exhibit the usual optimality but it is computationally burdensome. A simpler estimator, the OREG, which is a simple weighted average of separate probit (or logit) and regression estimates is suggested as an attractive alternative. The QREG is also found to be optimal but only when a certain covariance restriction is found to hole. Thus a test of the restriction based on the joint distribution of separate probit and regression estimates is proposed.</p> </abstract>
<abstract> <p>A number of estimators of parameters in nonlinear models have been proposed in the econometric literature. Various specialized methods have been developed to demonstrate the consistency of the suggested estimators. The first part of this paper presents a general scheme of the consistency proof. This method can be used to prove consistency of the large class of estimators of parameters in nonlinear regression models and nonlinear simultaneous equation models. The second part of this paper utilizes this general method to demonstrate the consistency of maximum likelihood estimators of nonlinear regression models with autocorrelated errors.</p> </abstract>
<abstract> <p>This paper derives the exact probability density function of instrumental variable estimators of the coefficient vector of the endogenous variables in a structural equation containing n + 1 endogenous variables and N degrees of overidentification. This generalizes the presently known results for the special cases where n = 1 or 2 and N = 0. The usual classical assumptions [19] are made of nonrandom exogenous variables and normally distributed disturbances. Some numerical computations are reported for the case n = 2.</p> </abstract>
<abstract> <p>This paper discusses the constraints on a dynamic equation represented by the possibility of factoring out an autoregressive error specification from a general lag structure. A suitable Wald test is defined and applied to a practical case.</p> </abstract>
<abstract> <p>Measures, incorporating the utility stream, serve as generalized plans to continuous allocation processes. The paper establishes the automatic existence of optimal plans, characterization of optimal solutions by a multiplier-type condition, and representation of generalized plans as sums of ordinary ones and singular measures. The latter can, in particular, be chosen as a finite combination of atoms.</p> </abstract>
<abstract> <p>This paper examines the effect that imperfectly informed capital market agents have on the equilibrium paths of output, investment, and asset prices of value maximizing firms. Though information is imperfect, rational expectations is imposed as an equilibrium condition. It is found that both asset prices and corporate decisions are simultaneously affected in such a way that the efficiency-optimality relationship between them is preserved. The effect of a fuller information structure is to move the economy from one efficient markets equilibrium to another.</p> </abstract>
<abstract> <p>Grunberg and Modigliani [3] investigated the existence of a public prediction which, if believed by economic decision makers, would prove correct. The present paper continues this research using a model of general exchange equilibrium under uncertainty. The principal results is that unless the public prediction is based on a very narrow class of data, a statistically correct prediction may fail to exist even for otherwise well-behaved economies.</p> </abstract>
<abstract> <p>The comparative static effects of increased uncertainty in standard two-period models of consumer and producer behavior under uncertainty have been shown in [10 and 11] to be complex. Two principal objectives of this paper are: (i) to describe some assumptions, forms of risk independence, about preferences and technologies, that simplify the behavioral effects of increased variability; and(ii) to characterize the preferences and technologies that are consistent with risk independence. The theory of duality plays an important part in the analysis.</p> </abstract>
<abstract> <p>In many economic situations, individuals carry out activities as coalitions, and have personal preferences for belonging to specific groups (coalitions). These situations are studied in the framework of cooperative games with coalition structures, by defining for each player a utility function with two arguments, namely his consumption bundle and the coalition to which (s)he belongs. The optimality analysis brings out a surprising property of the games with hedonic coalitions, namely that transfers among coalitions may be necessary to attain Pareto optimality. Moreover, quite restrictive assumptions are needed to rule out this property. The stability analysis is concerned with the conditions under which no individual has incentives and opportunities to change coalitions. Two concepts of "individual stability" of a coalition structure are introduced, and their existence properties are analyzed.</p> </abstract>
<abstract> <p>It is well known that the set of feasible net production points is convex if the underlying production functions are of the constant-returns, no-joints-products type and if there are no technological externalities. It is now shown that if there are public intermediate goods and if at least one public intermediate good is essential to the output of each final good, then the set of production possibilities is convex and the transformation surface strictly concave, and this is so whatever the numbers of primary factors, final goods, and public intermediate goods.</p> </abstract>
<abstract> <p>The fine structure of earnings is defined by a theoretically meaningful decomposition of the covariance matrix of earnings (or log earnings) time series. A three-element variance components model is proposed for analyzing earnings of young workers. These components are interpreted as the effects of differential on-the-job training (OJT) and differential economic ability. Several properties of these components and relationships between them are deduced from the OJT model. Background noise generated by a nonstationary first-order autoregressive process, with heteroscedastic innovations and time-varying AR parameters is also assumed present in observed earnings. ML estimates are obtained for all parameters of the model for a sample of Swedish males. The results are consistent with the view that the OJT mechanism is an empirically significant phenomenon in determining individual earnings profiles.</p> </abstract>
<abstract> <p>In this paper we present a continuous-time model of changes in employment status. The model implies that the binary logit model describes the probability of employment in equilibrium; it also has implications for the probability of employment out of equilibrium and for the length of spells of employment and nonemployment. Parameters of the model can be readily estimated from employment history data using the method of maximum likelihood. In an application to data from the Seattle and Denver income maintenance experiments, we find that both the proposed model and the logit model reveal significant decreases in the probability of employment of family heads under NIT programs that decrease the net wage rate and increase nonwage income. Based on the proposed model, the decrease in the employment probability results primarily from a significant increase in the average length of spells of nonemployment.</p> </abstract>
<abstract> <p>This paper generalizes the poverty index introduced by Sen and demonstrates that (i) for every homothetic social evaluation function there is one relative poverty index, (ii) Sen's index is a relative poverty index and corresponds to a Gini social evaluation function, (iii) for every translatable social evaluation function there is one absolute poverty index, and (iv) ethical content in these poverty indices requires that the social evaluation function be structured so that any group of poor people is strictly separable from anyone richer.</p> </abstract>
<abstract> <p> A test for fourth order autocorrelation in the error term of a regression equation estimated from quarterly data is described. The development draws on the finite sample results of Durbin and Watson and illustrates how their procedure for the first order case can be generalized. In the model y = Xβ + u where X is a matrix of fixed regressors and &lt;tex-math&gt;$u_{t}=\rho u_{t-4}+\varepsilon _{t}$&lt;/tex-math&gt;, an appropriate test statistic for &lt;tex-math&gt;${\rm H}_{0}^{-}\colon \rho =0$&lt;/tex-math&gt; is the statistic &lt;tex-math&gt;$d_{4}=\{\Sigma (z_{t}-z_{t-4})^{2}\}/\Sigma z_{t}^{2}$&lt;/tex-math&gt; computed from the least squares regression residuals z = y - Xb. Bounds to the significance points of d&lt;sub&gt;4&lt;/sub&gt; are tabulated. Maximum likelihood estimation methods are described; these are equally appropriate when lagged values of the dependent variable appear among the regressors, and they provide asymptotic tests for general autoregressive error structures, as well as for the special case &lt;tex-math&gt;$u_{t}=\alpha _{1}u_{t-1}+\alpha _{4}u_{t-4}-\alpha _{1}\alpha _{4}u_{t-5}+\varepsilon _{t}$&lt;/tex-math&gt;. Examples from the empirical literature are presented. </p> </abstract>
<abstract> <p>This paper deals with two single-equation estimators in a set of simultaneous linear stochastic equations--namely, ordinary least squares (OLS) and two-stage least squares (2SLS). Under the assumption that all predetermined variables in the model are exogenous, necessary and sufficient conditions are obtained for the existence of even moments of the above estimators. It is shown that for the general case with an arbitrary number of included endogenous variables, even moments of the 2SLS estimator are finite if and only if the order is less than K2 - G1 + 1. Furthermore, even moments of the OLS estimator exist if and only if the order is less than N - K1 - G1 + 1, where N is the sample size, G1 + 1 is the number of included endogenous variables, K1 and K2 respectively are the number of included and excluded exogenous variables in the equation to be estimated.</p> </abstract>
<abstract> <p>This paper is concerned with the so-called k-class estimators of structural parameters in a simultaneous system. The structural equation being estimated is assumed, as is common in other small-sample investigations, to consist of two endogenous variables; and the number of the exogenous variables (included or excluded) as well as the number of equations in the system are arbitrary so long as the identifiability condition of the estimated equation is satisfied. Moreover, we assume that the system contains no lagged endogenous variables and disturbance terms of each period are independently distributed as multivariate normal. The exact finite-sample moments of the k-class estimators are evaluated for 0 @&lt; k &lt; 1. For k &gt; 1 it is proved that the estimator does not possess even the first-order moment. The exact moment functions are expanded in terms of the inverse of the noncentrality (or concentration) parameter. This expansion sheds more light on the comparative study of alternative k-class estimators. Numerical calculations of the mean square error and the bias for some specific cases are also given for illustrative purposes.</p> </abstract>
<abstract> <p>In this paper, the authors develop a simultaneous equation model of birth rates composed of four estimated equations. This work differs from past research in that it considers the simultaneous relationship between birth rates and income and includes the cost of fertility as an explanatory factor. This cost is measured by the female labor participation rate under the assumption that income foregone due to fertility is a significant opportunity cost.</p> </abstract>
<abstract> <p>The standard F test for linear restrictions in regression is relevant as a criterion but fails to capture the notion of tradeoff between bias and variance. Average squared distance criteria yield operational tests that are more appropriate, depending upon objectives. In the present paper two alternative criteria are developed. The first allows testing of the hypothesis that the average squared distance of a restricted estimator from the parameter point in k space is less than the average squared distance of the unrestricted, ordinary least squares estimator from the same parameter point. The second sets up a test of betterness of the restricted estimator over the unrestricted estimator of E(Y/X), where betterness is again defined in average squared distance.</p> </abstract>
<abstract> <p> Tables of critical points for the noncentral F are presented with noncentrality equal to ½ of numerator degrees of freedom for denominator degrees of freedom of 1-30, 40, 60, 120, 200, 400, and 1,000, and numerator degrees of freedom of 1-30, 40, 60, 120, and 200, and type one errors of 0.05, 0.10, 0.25, and 0.50. These critical points can be used to test the second weak MSE criterion discussed in the companion paper [2]. An approximation is suggested for noncentral F(θ), and accuracy checks are given. An appendix provides a Fortran function for the approximation. The approximation is intended for using the first weak MSE test discussed in the companion paper. </p> </abstract>
<abstract> <p>In this paper a mathematical theorem by Samuelson on what he calls the "LeChatelier principle" is generalized, the generalization being closer to the original formulation of the LeChatelier principle than Samuelson's theorem itself. The results obtained are formulated in terms of systems theory and mathematical programming. Examples concerning cost minimization or profit maximization are included.</p> </abstract>
<abstract> <p>This paper presents a new version of the factor-price equalization theorem. The numbers of outputs and of factor inputs are allowed to be unmatched. The domestic factor prices in a country facing the international commodity prices are uniquely determined in the neoclassical framework once the factor endowment is taken as given. This allows the factor-price equalization proposition to maintain the invariance of the derivatives of the social production possibility schedule with respect to input parameter perturbations, considering the possible repercussions in the outputs. A necessary and sufficient condition for this invariance is presented in terms of second derivatives of the social production possibility frontier with its economic interpretation. This condition is applied to the non-joint production case. The paper asserts that the equalization theorem holds true if the constant-returns to scale production functions are strictly concave except along rays and satisfy the full-rank condition on the input-coefficient matrix, and if the number of commodities is no less than that of factors. Several other varieties are also presented including the joint production case.</p> </abstract>
<abstract> <p>A utility tree is specified yielding a complete set of demand equations that subsumes the linear expenditure system as a special case. In contrast to the Stone-Geary system, it is shown that our S-branch model allows for Hicks-Allen complements, and it does not restrict the own-price elasticity. Moreover, it is not costly in terms of the additional parameters required. Maximum-likelihood estimates of the S-branch system are presented; these are derived by means of the Bard version of the Gauss-Newton algorithm. In this application to food expenditure data in the United States the use of the S-branch system avoids a potential misspecification which would have resulted from the application of the linear expenditure system.</p> </abstract>
<abstract> <p>In this paper four alternative quarterly econometric models of investment behavior are fitted to a common set of data for individual manufacturing industries in the United States. Goodness of fit and absence of autocorrelation of errors are used as a basis for comparison of the performance of the alternative models. The econometric models are compared with each other and with alternative explanations of data on investment based on surveys of anticipated investment and on mechanical forecasting schemes. The four econometric models included in our study are those of Anderson [2], Eisner [15], Jorgenson and Stephenson [38], and Meyer and Glauber [46]. On the basis of our comparison, the ranking of the alternative models is as follows: (1) Jorgenson-Stephenson, (2) Eisner, (3) Meyer-Glauber, (4) Anderson. Anticipatory data give a better fit to data on investment expenditures than that provided by any of the econometric models. Mechanical forecasting schemes provide a fit that is superior to the Anderson and Meyer-Glauber models. These schemes are slightly inferior to the Eisner model and clearly inferior to the Jorgenson-Stephenson model. The alternative econometric models included in our comparison differ in specification of the time structure of the investment process and in the role ascribed to specific determinants of investment behavior. Both aspects of an econometric model affect its performance so that it is difficult to discriminate among alternative determinants of investment behavior on the basis of our results.</p> </abstract>
<abstract> <p>In this paper four alternative quarterly econometric models of investment behavior are compared with regard to predictive performance. Predictive performance may be assessed in two ways: (i) We compare prediction errors for a period of prediction with errors for a period of fit. (ii) We fit investment functions for both periods and test for structural change. These two procedures may be viewed as alternative tests of the hypothesis of structural change; the second is more powerful from the statistical point of view. Test of predictive performance supplement the comparisons of alternative models given in our preceding paper [17]. Goodness of fit may be exaggerated by consideration of a wide range of alternatives and selection of the one that fits best. If goodness of fit is exaggerated, a predictive test should produce evidence of structural change between the period of fit and the period of prediction. Of course, the better an econometric model fits the data, the more stringent this criterion for predictive performance. The econometric models included in our study are those of Anderson [1], Eisner [7], Jorgenson and Stephenson [19], and Meyer and Glauber [21]. On the basis of predictive performance the ranking of the alternative models is as follows: (1) Eisner, (2) Jorgenson-Stephenson, (3) Meyer-Glauber, and (4) Anderson. This ranking is similar to that resulting from comparisons based on goodness of it presented in our preceding paper [17]. For econometric models of quarterly investment behavior, the models that fit the best also have the best predictive performance.</p> </abstract>
<abstract> <p>This paper presents a considerably abbreviated deformation method for the minimization of a quadratic function subject to linear constraints. In Section 1 the method is explained for a positive definite function. In Section 2 it is extended to singular forms.</p> </abstract>
<abstract> <p>It is often argued that discount rate changes have a "psychological" impact on the public's expectations about the future course of the economy; it is alleged that such changes affect economic activity by influencing the expectations of businesses, financial institutions, and other economic actors. This study attempts to assess the veracity of the notion that there is such an effect of expectations. We begin with the premise that any change in expectations about future net cash accruing to business enterprises really reflects changes in expectations about future economic conditions in general, and that changes in expectations about future cash flows will be reflected in the discounted present value of business firms and hence the market value of equity shares. After removing systematic components from such data, and analysis of the random component strongly suggests that there is an "announcement effect" on expectations associated with discount rate changes, and that there seems to be a consensus as to what the inferred information content of such changes portends for future economic conditions. Also, on days immediately preceding discount rate decreases, there seems to be come evidence of anticipation of the change.</p> </abstract>
<abstract> <p>In this paper we are concerned with the distribution of observed or reported incomes in the context of a model that assumes underreporting. A form of `errors-in-variables' model is considered and the Pareto distribution is shown to exhibit certain invariance properties. Under this model (a) the distribution of observed incomes suitably truncated coincides with the true distribution if and only if the distribution is of the Pareto form and (b) a variable having a linear regression on true income has a linear regression on observed income also if and only if the distribution is of the Pareto type.</p> </abstract>
<abstract> <p>A study of daily newspaper cost behavior is used to illustrate a method of estimating cost function parameters using cross-section output and price data only. The cost parameter estimates are obtained by simultaneously estimating demand functions and equations characterizing necessary conditions for profit maximization.</p> </abstract>
<abstract> <p>A method is shown for finding all solutions to the generalized von Neumann model formulated by Kemeny, Morgenstern, and Thompson. The method uses results from decomposing economic production systems to extend the algorithm of Hamburger, Thompson, and Weil.</p> </abstract>
<abstract> <p>We study a standard n-commodity model in which equilibrium positions are characterized by specified inequalities between society's marginal rates of transformation in production and a single consumer's marginal rates of substitution in consumption; these inequalities are exemplified by, but not limited to, excise taxes and subsidies. We explore circumstances under which certain increases in these "taxes" and "subsidies" can be said to decrease welfare. In order to do so, we look for conditions under which the equilibrium consumption vector is well defined by a specification of the taxes and subsidies, and find that the conditions required are stringent. Among our conclusions is the proposition that the validity of consumers' surplus measures for analyzing such problems may depend on assumptions that are more strict than their users have realized.</p> </abstract>
<abstract> <p>All over the world economists are busy evaluating major road proposals and other transport projects. But this work is largely confined to rural areas because the methods used are inappropriate for evaluating big transport schemes in towns, where traffic congestion is a dominating consideration. This paper discusses congestion as an economic problem of demand and supply, expressed as simple functions of the cost of travel, in time and money, to the road user. Road expansion in congested cities often seems to achieve nothing but more congestion. This paper demonstrates how this arises as a process of market equilibrium, and how one can assess the benefits of the road expansion in this situation. The paper then considers how the evaluation of road schemes would be affected if direct road pricing were introduced into cities as a means of controlling congestion.</p> </abstract>
<abstract> <p>In this paper an attempt is made to estimate a regression equation using a time series of cross sections. It is assumed that the coefficient vector is distributed across units with the same mean and the same variance-covariance matrix. The distribution of the coefficient vector is assumed to be invariant to translations along the time axis. A consistent and an asymptotically efficient estimator for the mean vector and an unbiased estimator for the variance-covariance matrix of the coefficient vector have been suggested. Some asymptotic procedures for testing linear hypotheses on the means and the variances of coefficients have been described. Finally, the estimation procedure is applied in the analysis of annual investment data, 1935-54, for eleven firms.</p> </abstract>
<abstract> <p>This paper presents a general formalism for calculating the effect of taxes on income distribution, and the resultant effect on income inequality. We first derive a closed form expression for income inequality (defined from a Lorenz curve) in terms of the income density function. By way of illustration, we use this expression to calculate the effect of a proportional and a lump sum tax on income inequality in a simple exponential income distribution. The results show that the effect of a lump sum tax imposed after a proportional tax is a function of the proportional tax rate, even though the proportional tax itself does not change inequality.</p> </abstract>
<abstract> <p>The Ramsey model of optimal capital accumulation is reconsidered under the additional restriction that gross investment must be nonnegative. An effective characterization of the optimal solution in open-loop form is obtained. It is shown, however, that in general no restriction can be placed on the number of intervals in which the non negativity constraint is binding.</p> </abstract>
<abstract> <p>This paper discusses one of the uses to which two powerful techniques of modern time series analysis may be put in economics: namely, the study of the precise effects of seasonal adjustment procedures on the characteristics of the series to which they are applied. Since most economic data appearing at intervals of less than a year are to a greater or lesser extent "manufactured" from more basic time series, the problem of assessing the effects of the "manufacturing" processes upon the essential characteristics of the raw material to which they are applied is not unimportant. Perhaps the most common type of adjustment applied to raw economic time series is that designed to eliminate so-called seasonal fluctuations. The precise nature of seasonality is not easy to define, but an attempt is made in Section 2.1 below. The techniques employed to study the effects of seasonal adjustment procedures are those of spectral and cross-spectral analysis. In somewhat oversimplified terms the basic idea behind these types of analysis is that a stochastic time series may be decomposed into an infinite number of sine and cosine waves with infinitesimal random amplitudes. Spectral analysis deals with a single time series in terms of its frequency "content": cross-spectral analysis deals with the relation between two time series in terms of their respective frequency "contents." The two techniques are discussed in both theoretical and practical terms. Spectral analyses have been made for about seventy-five time series of United States employment, unemployment, labor force, and various categories thereof. Cross-spectral analyses have been made of the relations between these series and the corresponding series as seasonally adjustment by the procedures used by the Bureau of Labor Statistics. Two major conclusions regarding the effects of the BLS seasonal adjustment procedures emerge from these emerge from these analyses. First, these procedures remove far more from the series to which they are applied than can properly be considered as seasonal. Second, if the relation between two seasonally adjusted series in time is compared with the corresponding relation between the original series in time, it is found that there is a distortion due to the process of seasonal adjustment itself. Both defects impair the usefulness of the seasonally adjusted series as indicators of economic conditions, but, of the two, temporal distortion is the more serious defect. Examples of some of these results are discussed below in Section 3.3.</p> </abstract>
<abstract> <p>The question of returns to scale in public utilities is a much debated issue. In this study, the productive process of electricity generation is examined and a modified substitution model is employed, permitting differentiation between returns to scale to labor and to other factors. The method employed here allows us to isolate the impact of technological progress on (steam) electricity generation. We find that increasing returns to scale prevail throughout, and that the main impact of technology was registered during the 1950's. This study covers the period 1937-59.</p> </abstract>
<abstract> <p>Firms' investment in plant and equipment is explained by a stock-adjustment model in which the coefficient of adjustment is allowed to vary. It is assumed that firms partially close the gap between desired and actual capital stock, but that the speed of adjustment depends on the firm's ability to procure funds at reasonable cost. A panel of individual firm responses to the McGraw-Hill plant and equipment survey is the principal data source, supplemented by financial statement information for the firms and two indices representing costs of debt and equity financing. The predictions generated by the regressions are aggregated for comparison with the observed aggregates.</p> </abstract>
<abstract> <p> In the recent literature on mathematical models of economic growth, attention has been devoted mainly to the existence and stability of competitive equilibria. These models are based on a rather crucial but simple savings assumption: that savings form a constant proportion of income both being evaluated in terms of numéraire&lt;/b&gt;. In this paper, the savings decision is treated as a derived decision, i.e., as an implication of the more basic behavior of utility maximization over time. Using a simple two-sector, two-commodity, two-factor model, optimal growth paths corresponding to the maximization of the sum of the discounted future stream of consumption per worker are worked out. Savings behavior and asymptotic properties of these optimal paths for varying positive discount rates are also discussed. </p> </abstract>
<abstract> <p>This is a study in interregional competition and the optimum spatial allocation of crop production in the United States. Efficient production patterns are specified by a linear programming model restrained by regional land resources and national demands. The models used are based on 122 producing regions and include as many as 500 restraints, including upper bounds on each crop category within regions. Production patterns are indicated to allow minimum national food costs and alternatives in livestock feed substitution. Three models are used and a set of national supply prices for crops is derived for each. The results have policy implications, in indicating the location and amount of land which should be withdrawn from production if the nation is to arrive at a long-run solution to its mammoth surplus problem and to lessen the treasury costs of farm subsidies.</p> </abstract>
<abstract> <p>Three separability concepts in demand analysis have been introduced by Sono [9], Leontief [5, 6], Strotz [10, 11], Gorman [3], Frisch [2], Houthakker [4], and Pearce [7]. These concepts have been used effectively to analyze the structure of consumers' preference fields, and their implications have been shown to be of primary importance to empirical study in demand analysis. In this note, we are interested in clarifying the basic structure of the three separability concepts. Necessary and sufficient conditions for a grouping of commodities to be separable in the three criteria are first stated in terms of utility functions, and then they are characterized by the Slutsky terms of the corresponding demand functions.^2</p> </abstract>
<abstract> <p>In mathematical inventory theory the (s, S) ordering policy has played a prominent role. Much work has been expended to determine when some (s, S) policy will be optimal under varying inventory situations. Simple cases exist, however, where no (s, S) policy is optimal. solutions for many of these cases can be viewed as belonging to a more general class of ordering policies, denoted multiple (s, S) policies. A study is made of when such a policy will be optimal.</p> </abstract>
<abstract> <p>Harsanyi [1], after translating Zeuthen's bargaining theory [5, Ch. 4] into modern utility terms, has shown that it implies the same outcome as Nash's theory [4], namely a settlement that maximizes the product of the utility increments of the two parties. In the same paper, Harsanyi also reviewed Hicks's comparable theory [2, pp. 140-45] and found it, understandably, distinctly inferior to Zeuthen's. The context that both Zeuthen and Hicks had in mind was labor-management bargaining, where agreements and conflicts have time dimensions. Specifically in such situations, it will be suggested, it is possible to combine the central conceptions of both Zeuthen and Hicks in a composite theory that is superior to either of the separate ones. To prepare the way for the composite theory's presentation, its components will be briefly summarized.</p> </abstract>
<abstract> <p>It is shown that in markets with a continuum of traders, the allocations associated with the Shapley value are the same as the competitive allocations.</p> </abstract>
<abstract> <p>This paper formulates the notion of stochastic equilibria as invariant probability distributions consistent with the behavior patterns of individuals and the disequilibrium adjustment mechanism of the economy. Conditions for existence, uniqueness, and stability of such equilibria are examined.</p> </abstract>
<abstract> <p>This paper examines a notion of coalitional fairness for exchange allocations. An allocation is "c-fair" if no coalition of traders could benefit from achieving the net trade of some other coalition. Properties of c-fair allocations are studied both in exchange economies with a finite number of traders and with an atomless sector.</p> </abstract>
<abstract> <p>The purpose of this paper is to set forth a general theorem on the shape of free spatial market demand curve and on the shape of the spatial competitive market demand curve. It is demonstated that the free spatial demand curve is necessarily convex to the origin regardless of the shape of the shape of the individual demands which comprise it. But the shape of the spatial competitive market deamnd curve is shown to depend upon the behavioral assumptions used in the competitive. Three basically different competitive models are presented with contrasting results. Elasticity and price effects under each type of competition are determined and evaluated as is the effect of spatial competition on prices. Different interpretations of price data tend to result from conceptions of aggregate spatial demand curves vis a vis the classical spaceless demand curve.</p> </abstract>
<abstract> <p>Social choice is studied under continuous expression of preferences by individuals. Comparison is made with simple majority rule. The existence of a social utility function under continuous expression of preferences is studied, and a straightforward topology on expressed preferences is defined.</p> </abstract>
<abstract> <p>The purpose of this paper is to analyze the accuracy of the Simon--Ando approximation for stochastic nearly-completely decomposable systems. Relations are established defining this accuracy as a function of the maximum degree of coupling (@e) between aggregates, the conditioning and the indecomposability of these aggregates. A procedure is derived by which estimates in @e^2 may be computed from aggregate eigencharacteristics. Finally, the Simon--Ando approximation is shown to be optimal in block-stochastic matrices, and the accuracy achievable by higher-order aggregation is examined.</p> </abstract>
<abstract> <p>This paper derives an asymptotically valid test for first-order autoregressive errors. The test is derived in a simultaneous system of equations context, and allows lagged endogenous variables to be present in the model.</p> </abstract>
<abstract> <p>This paper deals with the question of appropriately specifying the error structure in equations nonlinear in the parameters. An approach is presented which nests various disparate hypotheses (including those of additive normal and multiplicative lognormal error distributions) and suggests an approximate testing procedure. An example is given in which the method is applied in the context of estimating an aggregate production function for the U.S..R.</p> </abstract>
<abstract> <p>FIML is shown to be an instrumental variables estimator where the instruments embody all the over-identifying a priori restrictions. FIML is compared to the two alternative estimators 3SLS and full information instrumental variables 3SLS differs from FIML in not using all a priori restrictions in forming the instruments. The full information instrumental variables estimator when iterated to convergence yields the FIML estimate. For the case of nonlinearity in the parameters a nonlinear 3SLS and a nonlinear full information instrumental variables estimator are proposed. Both estimators are asymptotically efficient.</p> </abstract>
<abstract> <p>In this paper we derive the exact distribution of the indirect least squares estimator of the coefficients of the Cobb-Douglas production function within the context of a stochastic production model of Marschak-Andrews type. We also give a finite sample test of hypotheses on the coefficients of the production function and discuss a method of testing whether there is any systematic deviation from the profit maximizing conditions.</p> </abstract>
<abstract> <p>This paper considers the estimation of a simultaneous equations model in which the dependent variables are qualitative. The model is a simultaneous version of the multivariate logit model, and can be estimated by maximum likelihood. An example is presented, dealing with the prediction of occupation and industry of employment of a worker based on certain demographic variables.</p> </abstract>
<abstract> <p>This paper examines the determinants of the demand for labor by fourteen two-digit manufacturing industries of India, and in particular the role of factor prices and expectations, to aid understanding of the causes of the low rates of labor absorption in the manufacturing sector. This is done within the framework of neoclassical models of factor demand. A method is suggested for measuring expectations. Our results show that adverse factor prices, long adjustment lags, low output elasticities, and the shift in the industrial structure in favor of the capital goods sector are some of the more important factors responsible for the observed low rates of labor absorption. Finally, some implications of our results for studies relating to labor demand functions in general are discussed.</p> </abstract>
<abstract> <p>In this paper we estimate the Hicksian equivalent variation of consumer's surplus for a sample of public housing tenants and examine the distribution of these surpluses by household characteristics. To do this we estimate the parameters of a generalized CES utility function (imposing second order constraints as needed) and of a Cobb-Douglas utility function. The Cobb-Douglas specification is rejected statistically and benefit estimates based on it follow a significantly different distributional pattern than those estimated with the generalized CES, although there is not much difference in average benefits.</p> </abstract>
<abstract> <p>Models of the behavior of populations of self-reproducible natural resources in an economic framework have rarely anticipated the consequences of different forms of production functions. This paper investigates sufficient conditions for extinction in a very general model as well as a model having a specific production function. In the second section additional considerations relating to extinction are deduced as well as the existence of a watershed level of population. These conclusions are exemplified using data from one particular population of red deer.</p> </abstract>
<abstract> <p>We consider auctions for a single indivisible object, in the case where the bidders have information about each other which is not available to the seller. We show that the seller can use this information to his own benefit, and we completely characterize the environments in which a well chosen auction gives him the same expected payoff as that obtainable were he able to sell the object with full information about each bidder's willingness to pay. We provide this characterization for auctions in which the bidders have dominant strategies, and for those where the relevant equilibrium concept is Bayesian Nash. In both set-ups, the existence of these auctions hinges on the possibility of constructing lotteries with the correct properties.</p> </abstract>
<abstract> <p>We study two person infinitely repeated games in which players seek to minimize the complexity of their strategies. Players' preferences are assumed to depend both on repeated game payoffs and the complexity of the strategies they use. The model considered is that of Rubinstein (1986). Players simultaneously choose finite automata (Moore-machines) to implement their strategies. The complexity of a strategy is measured by the number of states in the automaton used to play the strategy. We analyze Nash equilibrium in the "machine game." Strong necessary conditions on the structure of equilibrium machine pairs are derived, under general assumptions about how players trade off repeated game payoffs against implementation costs. These structural results in turn place significant restrictions on equilibrium payoffs. We provide a complete characterization for symmetric 2 x 2 stage games, when repeated game payoffs are evaluated according to the limit of means, and complexity costs enter preferences lexicographically. We find that all Nash equilibrium payoffs must lie on one of the two "diagonals" of the payoff matrix, and show that "main" diagonal payoffs are always attained. Taken together our results suggest that the introduction of implementation costs results in a striking discontinuity in the Nash equilibrium set in terms of strategies, plays, and payoffs.</p> </abstract>
<abstract> <p>The intertemporal models developed in this paper have been stimulated by the capital asset pricing model (CAPM) of Sharpe and Lintner, and the role of factor structure in Ross' arbitrage pricing theory (APT). Suppose that some (one-dimensional) stochastic process S provides a sufficient statistic for aggregate consumption. Then a (heuristic) dynamic programming argument shows that S, market wealth, and the wealth derivative of the value function (for any agent) are all locally perfectly correlated. It follows from Merton's work that there is a linear relationship between the local mean return on a security and the local covariance of that return with the return on the market portfolio. The formal development uses martingale representation and martingale projection to obtain an intertemporal CAPM; the history of a scalar Brownian motion plays the role of a sufficient statistic. We motivate the assumption of a sufficient statistic for aggregate consumption by considering a countable set of securities whose payoffs have an approximate factor structure, where the factor components are in the information set generated by an N-dimensional Brownian motion and the idiosyncratic components are weakly correlated. The approximate factor structure on the security payoffs implies that the rates of return have (locally) an approximate factor structure. The role of the market portfolio can now be played by a set of N well-diversified portfolios.</p> </abstract>
<abstract> <p>This study incorporates household economies of scale in consumption into a utility-theoretic model of household demands. Economies of scale are modeled as arising through (possibly congestible) household public goods, through increasing returns in household production, and/or through discounts for bulk purchases. The effects of economies of scale are isolated from other influences on demands by the assumption that individuals are identical and are symmetrically treated within households. Economies of scale parameters for five goods are estimated using a theoretically plausible demand system specification and data from the U.S. Consumer Expenditure Survey on expenditures by all-adult households. Results suggest the existence of significant economies of scale in the consumption of all of the included goods (food, shelter, clothing, household furnishings and operations, and transportation), with economies being especially pronounced in the consumption of shelter.</p> </abstract>
<abstract> <p>In this paper we extend Varian's (1984) nonparametric production analysis to situations when the set of observed output, input, and price data is not consistent with profit maximization for at least one firm. In such cases, Varian's results imply that no production possibility set containing all observations can rationalize the observed data. We identify each firm whose performance, given the prices faced by it, may be found consistent with profit maximization relative to some production possibility set containing all observed output-input vectors. We show that the set E of all such firms can itself be weakly rationalized in the sense that there exists a (closed, convex, and "monotone") production possibility set that contains all the observations, and relative to which the performance of all the firms in the set E is consistent with profit maximization given their respective prices. By definition, firms not included in this largest set E of efficient observations unambiguously deviate from profit maximizing behavior for any production possibility set containing all observations. We follow Farrell (1957) and analyze these deviations into technical and allocative efficiency measures, considering as admissible all closed, convex, and "monotone" production possibility sets relative to which the performance of each firm in the set E remains consistent with profit maximization. We then describe nonparametric methods for determining the tightest upper and lower bounds on the technical, allocative, and aggregate efficiency measures evaluated relative to all such admissible production possibility sets. It is seen that the tightest upper bound on the technical efficiency measure is the same as the value computed by the nonparametric efficiency evaluation technique known as data envelopment analysis, thus establishing a link between this literature in management science/operations research and the nonparametric production analysis in economics.</p> </abstract>
<abstract> <p>This paper studies the effects of spurious detrending in regression. The asymptotic behavior of traditional least squares estimators and tests is examined in the context of models where the generating mechanism is systematically misspecified by the presence of deterministic time trends. Most previous work on the subject has relied upon Monte Carlo studies to understand the issues involved in detrending data that are generated by integrated processes and our analytical results help to shed light on many of the simulation findings. Standard F tests and Hausman tests are shown to inadequately discriminate between the competing hypotheses. Durbin-Watson statistics, on the other hand, are shown to be valuable measures of series stationarity. The asymptotic properties of regressions and excess volatility tests with detrended integrated time series are also explored.</p> </abstract>
<abstract> <p>The well known CUSUM test for structural change is investigated when there are lagged dependent variables among the regressors in a linear model. We show that both a modified CUSUM test, suggested by Dufour (1982), and the straightforward CUSUM test retain their asymptotic significance levels in dynamic models, and find that the power depends crucially on the angle between the mean regressor and the structural shift.</p> </abstract>
<abstract> <p>This paper considers estimation and testing of vector autoregression coefficients in panel data, and applies the techniques to analyze the dynamic relationships between wages and hours worked in two samples of American males. The model allows for nonstationary individual effects, and is estimated by applying instrumental variables to the quasi-differenced autoregressive equations. Particular attention is paid to specifying lag lengths, forming convenient test statistics, and testing for the presence of measurement error. The empirical results suggest the absence of lagged hours in the wage of forecasting equation. Our results also show that lagged hours is important in the hours equation, which is consistent with alternatives to the simple labor supply model that allow for costly hours adjustment or preferences that are not time separable.</p> </abstract>
<abstract> <p>Under fairly general conditions, ordinary least squares and linear instrumental variables estimators are asymptotically normal when a regression equation has nonstationary right-hand side variables. Standard formulas may be used to calculate a consistent estimate of the asymptotic variance covariance matrix of the estimated parameter vector, even if the disturbances are conditionally heteroskedastic and autocorrelated. So inference may proceed in the usual way. The key requirements are that the nonstationary variables share a common unit root and that the unconditional mean of their first differences is nonzero.</p> </abstract>
<abstract> <p>This paper extends the Pearson chi-square testing method to nondynamic parametric econometric models, in particular, to models with covariates. The paper establishes the asymptotic distribution of the test statistic under the null and local alternatives when the test statistic is based on data-dependent random cells of a general form and on an arbitrary asymptotically normal estimator. These results are attained by extending recent probabilistic results for the weak convergence of empirical processes indexed by sets. The chi-square test that is introduced can be used to test goodness-of-fit of a parametric model, as well as to test particular aspects of the parametric model that are of interest.</p> </abstract>
<abstract> <p>This paper is an exploration of the empirical implications for the behavior of consumer-workers of treating unemployment as a constraint on choice rather than the result of it. The consequences of one family member's unemployment for the constrained commodity demand functions and the labor supply functions of other family members are first examined and then the normative interpretation of unemployment's compensation is considered. The final section of the paper contains estimates of a simple set of aggregate labor supply and commodity demand functions that are based on utility maximization and that explicitly recognize the presence of both constrained and unconstrained microeconomic units.</p> </abstract>
<abstract> <p>The theory of monetary policy is examined as it pertains to the functions of money as intermediating intergenerational trade as well as providing a useful service return. Finite economic lives are shown to alter the characterization of the optimal inflation rate from that suggested by models with infinitely long lived agents. In uncertain environments with sequential trade, policy is examined as altering the range of possible trades between agents of successive generations. In some cases, fully anticipated activist feedback policies may increase expected utility from that attainable with passive policies.</p> </abstract>
<abstract> <p>This paper constructs a life-cycle model of the consumer's allocation process in which the capital market is imperfect and the consumption bundle at each instant includes both durable and nondurable goods. The nondurables are instantaneously consumed at the moment of purchase, while the durable good is accumulated and yields a flow of services over its lifetime. The durable investment is assumed to be irreversible. The consumer's optimal allocation program is shown to vary between the periods of borrowing and lending with each phase defining a different relationship between consumption and the "truncated" permanent income.</p> </abstract>
<abstract> <p>An additively decomposable inequality measure is one which can be expressed as aweighted sum of the inequality values calculated for population subgroups plus the contribution arising from differences between subgroup means. The paper derives the entire class of measures which are additively decomposable under relatively weak restrictions on the form of the index. The subclass of mean indepedent measures turns out to be a single parameter family which includes thesquare of the coefficient of variation and two entropy formulae proposed by Theil.</p> </abstract>
<abstract> <p>A general welfare framework is proposed for examining the behavior of a Tiebout type model in which consumers choose among a variety of communities providing local public services. General expressions are derived for measuring fiscal externalities and the second best nature of Tiebout taxation is explored.</p> </abstract>
<abstract> <p>We construct a general equilibrium model of international trade where each government agent has a system of tariffs as his strategic variables. Our general equilibrium model follows recent contributions allowing incompleteness and intransitivity of consumer preferences. Government agents are assumed to have incomplete information on the preferences of domestic consumers and the availability of commodities. The behavior of each government agent is to choose a system of tariffs to maximize the estimated preferences of domestic consumers with a constraint on an estimated availability of commodities. We introduce an equilibrium concept so that (a) estimated preferences and an estimated available set of commodities are compatible with an observed state of the world economy and (b) a consumption bundle intended by each government agent coincides with a consumption bundle chosen by its domestic consumers. Our major goal is to provide the existence of such an equilibrium.</p> </abstract>
<abstract> <p>Consider a closed economy with several deposits of an exhaustible resource, withthe marginal cost of extraction differing from deposit to deposit but constant for each deposit. It is widely believed that social optimality requires that deposits be exploited in strict sequence, beginning with the lowest cost deposit. It is shown that, in a general equilibrium context, with Ricardian techniques of extraction, the validity of the proposition depends on what is meant by constancy of cost. It is also believed that if there exists a high-cost substitute for the resources then the resource should be exhausted before production of the substitute is begun. It is shown that this proposition is false.</p> </abstract>
<abstract> <p>In this paper we consider the problem of the existence of a well-defined reduced form in the context of piecewise linear models. We give a general theorem which provides necessary and sufficient conditions, called coherency conditions, for such an existence. This result is applied to various kinds of models: self-selectivity models, simultaneous equation probit and tobit models, multimarkets disequilibrium models.</p> </abstract>
<abstract> <p>Statistical inference for a system of simultaneous, nonlinear, implicit equations is discussed. The discussion considers inference as an adjunct to maximum likelihood estimation rather than in a general setting. The null and non-null asymptotic distributions of the Wald test, the Lagrange multiplier test (Rao's efficient score test), and the likelihood ratio test are obtained. Several refinements in the existing theory of maximum likelihood estimation are accomplished as intermediate steps.</p> </abstract>
<abstract> <p>The assumption most appropriate for nonlinear relationships estimated on stratified cross-section data (e.g., the Current Population Surveys) is that of independent not identically distributed (i.n.i.d.) regressors, not fixed regressors. This study provides conditions which ensure the consistency and asymptotic normality of nonlenear weighted least squares estimators with i.n.i.d. regressors for both the known and estimated weights cases. A general statistic for testing hypotheses about the parameters is given, as well as a test for model misspecification. The usual conditional parameter covariance matrix estimator may be an inconsistent estimator of the unconditional covariance matrix, and a consistent estimator is provided.</p> </abstract>
<abstract> <p>This paper presents the theoretical development of bounds tests and exact tests for serial correlation in the context of a simultaneous equation model which are analogous to those developed for single equation models. The powers of the exact tests are estimated and compared using Monte Carlo simulation of a three equation model.</p> </abstract>
<abstract> <p>Much of the empirical work on investment is based on the existence of a relation between investment and the expected present value of marginal profits. In this paper we compute such a present value series, under various assumptions about demand and technology, and examine its relation to investment. We find that variations in this present value series are, surprisingly, due more to variations in the cost of capital than to variations in marginal profit. We also find that the present value series, although significantly related to investment, still leaves unexplained a large, serially correlated fraction of the movement in investment.</p> </abstract>
<abstract> <p>This paper derives the equilibrium behavior of a monopoly producer of a durable good in a continuous time framework. We show that if purchasers are subject to rational expectations then a monopolist who cannot precommit to a particular production strategy produces more at every instant of time than does a monopolist who can precommit. Asymptotically the total production of a monopolist who cannot precommit is equal to the welfare maximizing producer's; however the monopolist produces more slowly, as long as firms are subject to increasing marginal costs. Thus we formally demonstrate that the original Coase intuition regarding the efficiency of the monopoly producer of a durable good only holds in the case of constant marginal costs.</p> </abstract>
<abstract> <p>In this paper, we study a monopolistic market for an information good possessing external effects, and investigate how far the information good is diffused from its owner to demanders through trades.</p> </abstract>
<abstract> <p>A seller and a buyer make offers and counteroffers to one another until they reach an agreement, or else one side decides to terminate the negotiations. Neither side knows the value of the other of reaching an agreement. It is shown,using the concept of sequential equilibrium, that if there are known fixed costs in bargaining, then the bargaining must terminate in a single round. The side with the lower costs of waiting makes an offer which the other side either accepts or rejects by terminating the bargaining.</p> </abstract>
<abstract> <p>This paper considers multistage games with communication mechanisms that can be implemented by a central mediator. In a communication equilibrium, no player expects ex ante to gain by manipulating his reports or actions. A sequential communication equilibrium is a communication equilibrium with a conditional probability system under which no player could ever expect to gain by manipulation, even after zero-probability events. Codominated actions are defined. It is shown that a communication equilibrium is a sequential communication equilibrium if and only if it never uses codominated actions. Predominant communication equilibria are defined by iterative elimination of codominated actions and are shown to exist.</p> </abstract>
<abstract> <p> General equilibrium models with land often employ a consumption set that consists of a σ-algebra of subsets of some given set of land in, for example, R&lt;sup&gt;2&lt;/sup&gt;. The purpose of the present work is to find a set of conditions under which a complete preordering over a general σ-algebra has a continuous utility representation. </p> </abstract>
<abstract> <p>The probability density of input-output multipliers under the assumption of normality of the input coefficients is estimated. The moments and confidence intervals of the multipliers are calculated for the Central Queensland Input-Output Model.</p> </abstract>
<abstract> <p>For many years, social scientists have been interested in obtaining testable definitions of causality (Granger [12], Sims [28]). Recent works include those of Chamberlain [7] and Florens and Mouchart [8]. The present paper first clarifies the results of these latter papers by considering a unifying definition of noncausality. Then, log-likelihood ratio (LR) tests for noncausality are derived for qualitative panel data under the minimal assumption that one series is Markov. LR tests for the Markov property are also obtained. Both test statistics have closed forms. These tests thus provide a readily applicable procedure for testing noncausality on qualitative panel data. Finally, the tests are applied to French Business Survey data in order to test the hypothesis that price changes from period to period are strictly exogenous to disequilibria appearing within periods.</p> </abstract>
<abstract> <p>Some aspects of the design and analysis of short panel data are considered. It is assumed that each individual may be at any instant in one of two states, for example employed and unemployed, and that individuals may switch from one to the other state. Alternative ways of observing individuals in discrete time are compared via the Fisher information matrix. The effect of the observational interval on the efficiency of the estimation is stressed and optimal time intervals for alternative sampling schemes are computed. A generalization to a model with covariates is outlined.</p> </abstract>
<abstract> <p>A system of demand functions is said to exhibit "generalized additive separability" (GAS) if it can be written in the form xi = fi(yi, R(Y)), i = 1,..., n, where yi is the normalized price of the ith good, and R is a function of all normalized prices. That is, GAS implies that "other prices" enter the demand functions only through an index function, R. This paper shows that the demand functions corresponding to directly and indirectly additive utility functions, the Fourgeaud-Nataf demand functions, and Houthakker's self-dual addilog system exhibit GAS. The direct utility functions corresponding to indirect additivity and the self-dual addilog are characterized, and a production function interpretation of some of these results is suggested. "Generalized strong separability" (GSS) and "generalized weak separability" (GWS) are defined, and it is shown that GSS includes both direct and indirect weak separability.</p> </abstract>
<abstract> <p>The "price-specie-flow" mechanism depends on a mechanical application of the quantity theory of money, i.e., it assumes that the price level of a country is solely determined by that country's money supply, which is incorrect in an open economy. Two alternative interpretations of the equation of exchange are offered, namely, the aggregate expenditure approach and the demand for money approach; and it is shown that, in general, the Marshall-Lerner condition is neither necessary nor sufficient for the stability of the classical system.</p> </abstract>
<abstract> <p>Duality in nonlinear programming is investigated via the usual Lagrangian function in the absence of assumptions concerning convexity or differentiability of the underlying functions. Equivalent forms of the primal and dual problems are discussed along with relations between the respective optimal values. A theorem is presented which gives a weak sufficient condition for equality of primal and dual optimal values. Geometric and economic implications of these results are explored.</p> </abstract>
<abstract> <p>This paper is concerned with the econometric problems associated with estimating supply and demand schedules in disequilibrium markets. The general problem is that in the absence of an equilibrium condition the ex ante demand and supply quantities cannot in general be equated to the observed quatity traded in the market. Four methods of estimation, differing primarily in their use of information on price-setting behavior, are developed in this paper. The first method is a generalization of an earlier meothd developed by R. Quandt and is based upon the maximization of a likelihood function. The method does not require any specific assumption about price-setting behavior, and it allows the sample separation (into demand and supply regimes) to be estimated along with the coefficient estimates. The second and third methods use the change in price as a qualitative proxy in determining the sample separation. The fouth method uses the change in price as a quantitative proxy for the amount of excess demand (supply) in the market. In the final section of the paper the four methods are used to estimate a a model of the housing and mortgage market in an effort to gauge the potential usefulness of each of the methods.</p> </abstract>
<abstract> <p>This paper considers an M/M/s queuing model in which customers who arrive when k customers are present in the queuing system obtain a net benefit of a?k. The a?-sequence is assumed to be a decreasing one. If it is left to the individual customer to decide whether to join the queue or not, he will balk whenever the queue length is greater than some number, say n1. It is shown that if a balking level n2 &lt; n1 is enforced, then the customers as a group can generally expect a larger net benefit per time unit than when the balking level n1 is applied. One way to ensure social optimality is to impose a toll on the customers who join the queue. In the discussion of a possible economic interpretation of the model we point out the similarities between such a toll and a shadow price in a more conventional optimization model. It is also demonstrated that in a stochastic optimization model capacity utilization is not a sufficient price criterion.</p> </abstract>
<abstract> <p>In this paper a procedure is presented for the (asymptotically) efficient estimation of the parameters of autoregressive moving average models with exogenous variables. The estimation of distributed lag models is discussed as a particular case of models of this form. Finally a number of numerical examples are described.</p> </abstract>
<abstract> <p>The purpose of this paper is to investigate the structure of the class of market excess demand functions which can be generated by aggregating individual utility maximizing behavior. Among the results are: (i) in a region of the relative price domain an arbitrary polynomial function can be generated as an excess demand function for a particular commodity, and (ii) for any p in the relative price domain, a given configuration of excess demands and rates of change in excess demand can be generated at p if and only if it is consistent with Walras' Law.</p> </abstract>
<abstract> <p>An exact discrete model is derived from a recursive model consisting of a set of rth order stochastic linear differential equations with constant coefficients such that observations generated at equidistant points of time by the differential system satisfy the discrete model irrespective of the length of sampling interval. The difficulty of estimating the exact model subject to a priori restrictions makes it necessary to approximate the differential system by a non-recursive discrete model that maintains the structural form. This discrete model has a moving average disturbance term of order r - 1, but the co-variance function of this process is approximated by a function that is independent of the parameters of the continuous model. The eigenvalues and eigenvectors of the approximations to the differential system as well as their asymptotic variance matrices are also derived but, like the approximate asymptotic variance matrices of the parameter estimates, these variances are about biased probability limit</p> </abstract>
<abstract> <p>In an atomless economy any allocation that is not blocked by "small" coalitions is in the core. Hence, in such an economy, a competitive equilibrium is characterized by the blocking power of part of the coalitions which excludes all "big" coalitions.</p> </abstract>
<abstract> <p>Traditional empirical analysis of consumer demand has usually made the assumption of linear budget sets. That is, the consumer is assumed to purchase any desired quantity at a constant price subject to a budget constraint. However, empirical situations with nonconstant prices occur because of taxes, two part tariffs, and more generally when the cost of utilization of a durable good varies with its purchase price. This paper discusses econometric approaches to consumer demand with nonlinear budget sets. An empirical example with choice under uncertainty in the presence of nonlinear budget sets is also presented.</p> </abstract>
<abstract> <p>It is known that in large economies with strongly convex preferences, the commodity bundles agents receive at core allocations are near their demand sets.Without convexity, it is known that agents' bundles need not be near their demand sets, although these bundles will satisfy a weaker condition. In this paper, we show that for almost all sequences of economies constructed by successive sampling from any distribution of agents' characteristics, agents' bundles will be close to their demand sets.</p> </abstract>
<abstract> <p> A set of axioms is proposed, which uniquely characterize the Harsanyi [5] solution for nontransferable utility (NTU) games. These axioms appear formally identical to those of Aumann for the Shapely ("λ-transfer") NTU value; the difference lies in the range of the two solutions. Further characterizations and comparisons between these two solution concepts are then presented. </p> </abstract>
<abstract> <p>A dynamic model of insider trading with sequential auctions, structured to resemble a sequential equilibrium, is used to examine the informational content of prices, the liquidity characteristics of a speculative market, and the value of private information to an insider. The model has three kinds of traders: a single risk neutral insider, random noise traders, and competitive risk neutral market makers. The insider makes positive profits by exploiting his monopoly power optimally in a dynamic context, where noise trading provides camouflage which conceals his trading from market makers. As the time interval between auctions goes to zero, a limiting model of continuous trading is obtained. In this equilibrium, prices follow Brownian motion, the depth of the market is constant over time, and all private information is incorporated into prices by the end of trading.</p> </abstract>
<abstract> <p>A two-period (0 and T) Arrow-Debreu economy is set up with a general model of uncertainty. We suppose that an equilibrium exists for this economy. The Arrow-Debreu economy is placed in a Radner (dynamic) setting; agents may trade claims at any time during [O, T]. Under appropriate conditions it is possible to implement the original Arrow-Debreu equilibrium, which may have a infinite-dimensional commodity space, in a Radner equilibrium, which only a finite number of securities. This is done by opening the "right" set of security markets, a set which effectively completes markets for the Radner economy.</p> </abstract>
<abstract> <p>The first-order approach to principal-agent problems involves relaxing the constraint that the agent choose an action which is utility maximizing to require instead only that the agent choose an action at which his utility is at a stationary point. Although more mathematically tractable, this approach is generally invalid. This paper identifies sufficient conditions--the monotone likelihood ratio condition and convexity of the distribution function condition--for the first-order approach to be valid. The Pareto-optimal wage contract is shown to be nondecreasing in output under these same conditions.</p> </abstract>
<abstract> <p> We consider economies in which some of the firms are price takers whereas other firms are price setters. The latter firms consider the output levels for their own products as well as the prices of the inputs as given, maximize their cost, and set prices for their products according to some specific pricing rule. We give conditions under which decentralizing prices and output levels exist. This existence of equilibrium theorem covers a wide array of pricing rules, as for instance, marginal cost pricing, pricing à la Boiteux, and Aumann-Shapley pricing. </p> </abstract>
<abstract> <p>This paper develops a general theory of the aggregate implications of (S, s) inventory policies. It is shown that (S, s) policies add to the variability of demand, with the variance of orders exceeding the variance of sales. Overall, the (S, s) theory contradicts the widely held notion that retail inventories act as a buffer, protecting manufacturers from fluctuating sales.</p> </abstract>
<abstract> <p>The existence of a positive balanced growth solution for the discrete dynamic input-output (IO) model is studied. Previous work in the area invariable assumed unrealistic restrictions on the matrices A or B, such as regularity or irreducibility. In our work these restrictions are not imposed. We find that in the realistic case of reducible A, a balanced growth solution exists if each sector depends on all others for either its current account or its capital inputs. If this condition is not satisfied a balanced growth solution may still exist. Conditions for its existence relate the overall growth rate to the growth rates which groups of sectors would have if they were isolated from the rest of the economy.</p> </abstract>
<abstract> <p>Caves and Christensen [16] have provided a procedure for displaying the regular regions of a flexible functional form in the 2-good homothetic and nonhomothetic cases and in the 3-good homothetic case. We extend the procedure to the nonhomothetic 3-good case, and we apply the extended procedure to the translog, generalized Leontief, and minflex Laurent flexible functional form. In addition, we acquire the regular regions for the minflex Laurent model in the 2-good nonhomothetic case and superimpose the resulting regions on those already found by Caves and Christensen for the translog and generalized Leontief models. We find that the new minflex Laurent model generally has the largest regular regions of the three flexible functional forms. In addition, the regular region of the minflex Laurent model is found to expand as real income increases. As a result, that model is particularly well suited for use with time series data, which typically is characterized by positive long term growth trends in real income. In such applications, all recent data and future forecasts can be expected to lie within the regular region of the minflex Laurent model. Although it is possible for some of the earliest data to fall outside that regular region, the model's regular region nevertheless is sufficiently large to hold even all of those earliest data points in many data sets. The regular region of each of three models moves when the model's parameters are changed. With the generalized Leontief or translog model, the regular region's shape, location, and size are unpredictable without prior knowledge of the model's parameters. With either of those two models, the intersection of the model's regular regions, as the parameters are changed, is contained within a very small neighborhood of the one point at which we require the model to be regular. With the minflex Laurent model, the primary properties of the regular regions are invariant to the values of the parameters, and the intersection of the displayed regular regions is a very large unbounded set. The width of that intersection increases without limit as real income increases.</p> </abstract>
<abstract> <p>The first part of this paper considers the interaction between productive and nonproductive savings in a growing economy. It employs an overlapping generations model with capital accumulation and various types of rents, and gives necessary and efficient conditions for the existence of an aggregate bubble. The second part is a series of thoughts on the definition, nature, and consequences of asset bubbles. First, it derives some implications of bubbles for tests of asset pricing. Second, it demonstrates the specificity of money as an asset and shows that there is a fundamental dichtotomy in its formalization. Third, it discusses inefficiencies of price bubbles. Fourth, it shows that the financial definition of a bubble is not satisfactory for some assets.</p> </abstract>
<abstract> <p> The relationship between co-integration and error correction models, first suggested in Granger (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series x&lt;sub&gt;t&lt;/sub&gt; first achieves stationarity after differencing, but a linear combination &lt;tex-math&gt;$\alpha ^{\prime }x_{t}$&lt;/tex-math&gt; is already stationary, the time series x&lt;sub&gt;t&lt;/sub&gt; are said to be co-integrated with co-integrating vector α. There may be several such co-integrating vectors so that α becomes a matrix. Interpreting &lt;tex-math&gt;$\alpha ^{\prime }x_{t}=0$&lt;/tex-math&gt; as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on Granger (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a Monte Carlo simulation. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a series of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not M1, M3, or aggregate liquid assets. </p> </abstract>
<abstract> <p>This paper studies the random walk, in a general time series setting that allows for weakly dependent and heterogeneously distributed innovations. It is shown that simple least squares regression consistently estimates a unit root under very general conditions in spite of the presence of autocorrelated errors. The limiting distribution of the standardized estimator and the associated regression t statistic are found using functional central limit theory. New tests of the random walk hypothesis are developed which permit a wide class of dependent and heterogeneous innovation sequences. A new limiting distribution theory is constructed based on the concept of continuous data recording. This theory, together with an asymptotic expansion that is developed in the paper for the unit root case, explain many of the interesting experimental results recently reported in Evans and Savin (1981, 1984).</p> </abstract>
<abstract> <p>We consider the problem of providing incentives over time for an agent with constant absolute risk aversion. The optimal compensation scheme is found to be a linear function of a vector of N accounts which count the number of times that each of the N kinds of observable events occurs. The number N is independent of the number of time periods, so the accounts may entail substantial aggregation. In a continuous time version of the problem, the agent controls the drift rate of a vector of accounts that is subject to frequent, small random fluctuations. The solution is as if the problem were the static one in which the agent controls only the mean of a multivariate normal distribution and the principal is constrained to use a linear compensation rule. If the principal can observe only coarser linear aggregates, such as revenues, costs, or profits, the optimal compensation scheme is then a linear function of those aggregates. The combination of exponential utility, normal distributions, and linear compensation schemes makes computations and comparative statics easy to do, as we illustrate. We interpret our linearity results as deriving in part from the richness of the agent's strategy space, which makes it possible for the agent to undermine and exploit complicated, nonlinear functions of the accounting aggregates.</p> </abstract>
<abstract> <p>This paper describes a continuous time model of an economy with finitely many infinitely lived consumers and a finite number of capital goods. Two objectives are achieved. First, recursive (nonadditive) utility functionals are formulated and analyzed. Second, these preference functionals are applied to analyze the nature of efficient allocations in a dynamic economy. Two classes of global turnpike propositions are proven which provide the basis for a model of the long-run distribution of wealth. These propositions also provide new perspective regarding existing stability literature based on additive utilities.</p> </abstract>
<abstract> <p>Andersen (1970) considered the problem of inference on random effects linear models from binary response panel data. He showed that inference is possible if the disturbances for each panel member are known to be white noise with logistic distribution and if the observed explanatory variables vary over time. A conditional maximum likelihood estimator consistently estimates the model parameters up to scale. The present paper shows that inference remains possible if the disturbances for each panel member are known only to be time-stationary with unbounded support and if the explanatory variables vary enough over time. A conditional version of the maximum score estimator (Manski, 1975, 1985) consistently estimates the model parameters up to scale.</p> </abstract>
<abstract> <p>Often maximum likelihood is the method of choice for fitting an econometric model to data but cannot be used because the correct specification of the (multivariate) density that defines the likelihood is unknown. Regression with sample selection is an example. In this situation, simply put the density equal to a Hermite series and apply standard finite dimensional maximum likelihood methods. Model parameters and nearly all aspects of the unknown density itself will be estimated consistently provided that the length of the series increases with sample size. The rule for increasing series length can be data dependent. To assure in-range estimates, the Hermite series is in the form of a polynomial squared times a normal density function with the coefficients of the polynomial restricted so that the series integrates to one and has mean zero. If another density is more plausible a priori, it may be substituted for the normal. The paper verifies these claims and applies the method to nonlinear regression with sample selection and to estimation of the Stoker functional.</p> </abstract>
<abstract> <p>The expectation of the excess holding yield on a long bond is postulated to depend upon its conditional variance. Engle's (1982a) ARCH model is extended to allow the conditional variance to be a determinant of the mean and is called ARCH-M. Estimation and inference procedures are proposed and the model is applied to three interest rate data sets. In most cases the ARCH process and the time varying risk premium are highly significant. A collection of LM diagnostic tests reveals the robustness of the model to various specification changes such as alternative volatility or ARCH measures, regime changes, and interest rate formulations. The model explains and interprets the recent econometric failures of the expectations hypothesis of the term structure.</p> </abstract>
<abstract> <p>A generalization of the multinomial logit (MNL) model is developed in which discrete alternatives are ordered so as to induce stochastic correlation among alternatives in close proximity. It is designed for situations where the alternatives are ordered, but is more flexible than previous ordered models. The model belongs to the Generalized Extreme Value class introduced by McFadden,and is therefore consistent with random utility maximization. A straightforward extension can handle cases where observations have been selected on the basis of a truncated choice set. A two-stage procedure using MNL computer software provides a specification test for MNL against either of these alternative models. The models' properties are investigated through two empirical applications whose rather unsatisfactory results are very briefly described.</p> </abstract>
<abstract> <p>The problems of the existence of a competitive equilibrium in models with hidden knowledge and self-selection are well-known (Rothschild and Stiglitz (1976), Wilson (1977), Riley (1979). Recent analyses of such models argue for a particular outcome--the Pareto-dominant separating, zero-profit one (Cho and Kreps (1986), Riley (1985)). We prove the existence of such an outcome under very general conditions; and, generalizing the reactive equilibrium concept introduced by Riley (1979), we prove this outcome is the unique reactive equilibrium. This paper also introduces the strong equilibrium--the largest set of offers each of which at least breaks even on all who accept--which provides a direct and general proof of the existence of a unique reactive equilibrium.</p> </abstract>
<abstract> <p>We address the monopoly problem of designing and pricing a product line of goods distinguished by different quality and warranty levels. Consumers vary in their evaluations of these attributes, so that the problem is one of screening. It is sufficiently complex that the local approach commonly used does not work. Instead, we use new techniques for dealing with incentive constraints between nonadjacent consumer types. These techniques allow us to characterize optimal allocations that may not be monotonic. In particular, although the more eager types of buyer do pay higher prices and yield the monopoly higher profit, they may receive lower quality or lower warranty overage. We find preference restrictions that restore monotonicity: concave risk tolerance implies that warranty coverage increases in type, and constant absolute risk aversion implies that quality increases in type.</p> </abstract>
<abstract> <p>This paper examines some of the modifications required in well known propositions of general equilibrium theory when transactions require resources. Some preliminary remarks on the analysis of money in such economies are also offered.</p> </abstract>
<abstract> <p>The situation in which a seller is aware that his pricing policy will affect the probability of entry of competing suppliers is studied. The seller's optimal price policy is developed under the assumption that the entry probability is a non-decreasing function of product price and that the objective is present value maximization. It is shown that the optimal pare-entry price tends to fall as the discount rate drops, the market growth rate rises, the post-entry profit possibilities decline, or certain non-price barriers to entry fall.</p> </abstract>
<abstract> <p>Using a general definition of a generalized inverse of a singular matrix we generalized the k class and three stage least squares procedures so that they can be applied when the sample size, say T, is smaller than the number of exogenous variables, say K, in a system of equations. These generalized k class and three stage least squares estimators, in usual cases, coincide with ordinary least squares and Zellner's [7] efficient estimators respectively as long as T @&lt; K and coincide with the usual k class and three stage least squares estimators respectively as T exceeds K.</p> </abstract>
<abstract> <p>In two stage least squares estimation, when the matrix Z of predetermined variables has less than full column rank, Z′Z is singular. It is shown that in this situation 2SLS still can be used and that Ŷ - the result of the first stage--is still uniquely determined; also shown is that 2SLS will often be equivalent to OLS.</p> </abstract>
<abstract> <p>The paper is partitioned into two parts. The first contains a description of an "economic system" which allocates resources in exchange environments when consumer preferences are not selfish. This section also contains the conditions under which this economic system allocates resources optimally. The second part utilizes the allocation process described above, as well as four variants, to examine the concepts, and formal definitions, of informational decentralization and efficiency found in Hurwicz [8]. It is shown that various types of trade-offs exist between optimality in resource allocation and informational decentralization.</p> </abstract>
<abstract> <p>Non-transferable capital is an essential feature of the Fel'dman two-sector growth model. This paper is primarily an answer to the following question. Given that capital is really partially shiftable between its two sectors, when can the Fel'dman model be meaningfully used?</p> </abstract>
<abstract> <p>In this paper the problem of best (minimum variance) linear estimation of the regression coefficients in the univariate linear regression model is considered when no assumption about the rank of either the moment matrix of the regressors or the variance-covariance matrix of the error term is made. This problem has sometimes been called the "generalization of generalized least squares." Some brief examples where this problem is relevant in econometrics are given in the introductory part.</p> </abstract>
<abstract> <p>A model is considered in which a covariance-stationary exogenous process is related to an endogenous process by an unrestricted, infinite, linear distributed lag. It is shown that when an underlying continuous time model is sampled at unit intervals to yield endogenous and exogenous discrete time processes, the discrete time processes are related by a discrete time equivalent of the underlying continuous model. The relationship between the underlying continuous lag distribution and its discrete time equivalent is "close" when the exogenous process is "smooth." Even then, however, it is interesting to note that (i) a monotone continuous time distribution does not in general have a monotone discrete time equivalent and (ii) a one-sided continuous time distribution does not in general have a one-sided discrete time equivalent. The implications of the results for statistical practice are considered in the latter part of the paper.</p> </abstract>
<abstract> <p>If each automobile pays average rather than marginal social cost of a highway trip, there will be too much auto travel during periods of congestion. By using only inputs taxes, adjustments in fares on alternative transit modes, and income redistribution, we solve this problem in two ways: (i) complete (first-best) optimality in peak periods and second-best optimality in off-peak periods; or (ii) first-best optimality in off-peak periods and second-best optimality at the peak. We show that with congestion interdependence, as when automobiles and buses contribute to one another's congestion, the second-best peak solution can warrant an urban but transit fare below average cost, calling for a subsidy. And under a first-best peak solution, involving both an inputs tax and a transit fare adjustment, we show a companion second-best off-peak transit fare that will mitigate the (then inappropriate but still effective) inputs tax.</p> </abstract>
<abstract> <p>A theory of identification is developed for a general stochastic model whose probability law is determined by a finite number of parameters. It is shown under weak regularity conditions that local identifiability of the unknown parameter vector is equivalent to nonsingularity of the information matrix. The use of "reduced-form" parameters to establish identifiability is also analyzed. The general results are applied to the familiar problem of determining whether the coefficients of a system of linear simultaneous equations are identifiable.</p> </abstract>
<abstract> <p>It is shown that single-peaked, continuous utility functions do not, in general,aggregate in majority voting to a continuous social utility function. Conditions on individual preference orderings to guarantee a continuous social utility function are presented for a simple case.</p> </abstract>
<abstract> <p>The purpose of this paper is to prove an impossibility theorem for the existence of a social welfare function. Our treatment of the social welfare function differs from that of Arrow insofar as the social choice function generated through the social welfare function from the individual choice functions is defined in the disaggregated social state in his case, while in our case it is defined in the aggregated social state. We assume that every individual's choice function is defined in an n-dimensional commodity space. Then, the social choice function is defined in Arrow's case in an (m × n)-dimensional commodity space while it is defined in our case in an n-dimensional space. With our modification, the requirement insuring the existence of the social welfare function yielding social indifference surfaces becomes weaker than in Arrow's case. Nevertheless, we still obtain an impossibility.</p> </abstract>
<abstract> <p>This paper is an attempt at overcoming various logical inconsistencies which have been found in the Leontief dynamic model. It is based on an interpretation of the Leontief model in the framework of a general model of capital accumulation of the Walras-Hicks type. A particular case of such a model, corresponding to the hypothesis of static expectations, turns out to be the competitive capital theory underlying the Leontief model. This theory, capable of defining a meaningful equilibrium even when the Leontief equilibrium based onfull employment of all stocks is not possible, allows us to overcome the difficulty in retaining the economic interpretation of solutions. In the terminology of Hicks (Capital and Growth), this theory solves the problem of the traverse between two full employment equilibria. It will be evident, contrary to current opinion, that output levels and prices are indissolubly connected also inside the Leontief model. Finally, Solow's prices, which give rise to the striking "dual stability paradox," will be discussed. The ultimate reason for such a paradox will be identified as an intrinsic inconsistency in the assumption of correct expectations on which these prices are based.</p> </abstract>
<abstract> <p>This paper surveys various econometric issues that arise in estimating a relation between the logarithm of earnings, schooling, and other variables and focuses on the problem of "ability" as a left-out variable and the various solutions to it. It points out that in optimizing models the "ability bias" need not be positive and shows, using recent analyses of NLS data, that when schooling is treated symmetrically, allowing it too to be subject to errors of measurement and correlated to the disturbance in the earnings function, the usual conclusion of a significantly positive "ability bias" in the estimated schooling coefficients is not only not supported but possibly even reversed.</p> </abstract>
<abstract> <p>This review of the work done on the formulation and estimation of complete systems of consumer demand functions is primarily concerned with problems and issues around this topic. These issues are partly of a theoretical and partly of an empirical nature. Constraints on such systems, derived from theoretical considerations, are used to deal with the problems of lack of sufficient data, but can sometimes also be tested. Among the various alternative approaches, as yet no clear-cut choice can be made, although it appears that additivity of preferences is too restrictive.</p> </abstract>
<abstract> <p>Recent developments in social choice theory are critically surveyed in the light of a categorization of interpersonal aggregation problems into four distinct types that seem to require varying treatment but typically do not receive it. Informational inadequacy of the usual social choice framework is discussed in this context. A fairly thorough exploration of the correspondences between consistency conditions for choice functions and regularity properties of the binary relation of preference leads to a re-examination of the class of "impossibility" results in social choice theory, necessitating reinterpretations of various theorems (including arrow's).</p> </abstract>
<abstract> <p>A kernel of a set of alternative actions over which there is a partial order is defined in terms of optimality properties. It is shown to be the same as the generalized efficient set. A variety of theorems such as uniqueness, existence, and composition in terms of other sets are established. Related sets, such as quasi kernels and weak kernels, are also considered.</p> </abstract>
<abstract> <p>Available theorems establishing the existence of general equilibrium in models incorporating imperfectly competitive firms rely on the assumption that reaction curves are continuous functions (or convex-valued, upper hemi-continuous correspondences). However, this property has not been derived from conditions on the fundamental data of tastes, technology, and maximizing behavior. We show here that continuity may fail even in extremely simple cases, with the result that equilibrium price and/or quantity choices fail to exist. The non-pathological nature of the examples we present suggests the need for a fundamental re-examination of the way our partial and general equilibrium models of monopolistic competition fit together.</p> </abstract>
<abstract> <p>Previous attempts to estimate labor supply functions based upon the constrained maximization of a utility function with leisure and income as arguments have assumed that all time not spent at work on the job is leisure time. In this paper we formulate a model of a household in which time is allocated between work on the job, leisure, and housework, where leisure is defined to be net of time spent on housework. The model is estimated under two alternative stochastic specifications and the results compared to those obtained (i) assuming that housework time is exogenous and (ii) assuming that housework time is part of leisure time.</p> </abstract>
<abstract> <p>The notion of homotheticity is extended for production correspondences to span many special structures, with properties such as homogeneity, global homotheticity, semi-homogeneity, quasi-homogeneity, with a scaling law common to these structures which is also equivalent to linear expansion paths, thereby showing by comparison the generality of the latter. In these terms one may consider output-mix-dependent factor price deflation of cost and input-mix-dependent output price deflation of revenue.</p> </abstract>
<abstract> <p>This paper extends earlier studies in this field by Chenery-Watanabe [2] and Santhanam-Patil [3] to a comparison of the production structure of Korea at present with those of other countries and with that of Korea at various points in the past. The results of this study provide additional evidence in support of the main findings of earlier studies. It is suggested that existing methods for computing inter-industry linkages and for comparing production structures could be improved by including an analysis of both "domestic" and "international linkages".</p> </abstract>
<abstract> <p> Based on Lancaster's model of consumer behavior, Konüs' cost of living index is generalized to incorporate quality changes and new commodities. Since it is demonstrated that Laspeyres' index is an upper bound on the true index only with restrictive assumptions about the utility function, another index which is always an upper bound is also suggested. </p> </abstract>
<abstract> <p>This paper investigates an operational inconsistency between the constrained priority rationing scheme introduced by Manove in 1973 and the criterion used to determine an "optimal" priority matrix. A truncation of the optimality criterion is suggested in order to resolve this inconsistency and also reduce a potential source of bias against final demand.</p> </abstract>
<abstract> <p>The exact stochastic character of observed data from a Markov process is derived for the case where only aggregate stocks, as opposed to individual transitions, are observed. Particular attention is devoted to the distinction between data generated by a panel study, where a single group of individuals is followed over time, and that generated by random sampling, where the observed groups are not identical over time. Several alternative estimators are developed which take into account the particular stochastic structure of the data.</p> </abstract>
<abstract> <p>This paper demonstrates how a two or three component error structure can be used with seemingly unrelated regressions. Its application may be particularly useful with large panel data sets when the researcher wishes to estimate several equations simultaneously and believes that errors both between and within equations are correlated over time and across units. Relatively simple algorithms are presented for estimation of the error covariance matrix and generalized least squares coefficients.</p> </abstract>
<abstract> <p>This paper presents a model of general equilibrium stability in which agents understand that they are not at equilibrium. Rather, agents expect prices to change and contemplate the possibility that they may not be able to complete their own transactions. They optimize their actions taking account of such price changes and transaction constraints. It is shown that a necessary condition for instability is the continuing perception of new, previously unforeseen opportunities (real or imagined). Without this, old opportunities will be arbitraged away and the system will converge to equilibrium. The equilibrium approached will depend on the history of the system and may not be Walrasian if transaction constraints are present.</p> </abstract>
<abstract> <p>This paper analyzes nonlinear growth models in which agents' expectations have a role in determining present behavior. Assuming agents have perfect foresight, we develop sufficiency conditions for the local stability of a given steady state. We then briefly discuss several examples in which stability prevails.</p> </abstract>
<abstract> <p>The effects of an improvement in information on the efficiency of risk-bearing are studied under various systems of incomplete markets. With sequential futures markets for uncontingent delivery, the welfare effects are indeterminate in sign, except under special circumstances. In the presence of options markets, however, an improved information structure is almost surely beneficial.</p> </abstract>
<abstract> <p>This paper presents a model of myopic tastes, both in the context of intertemporal decision making and choice under uncertainty. Infinite dimensional consumption plans arise naturally in both contexts, either involving a denumerable number of periods or a countable number of states of the world. The essential feature of our model is that myopic behavior is formalized by defining topologies, on the space of consumption plans, which "discount" the future or improbable events.</p> </abstract>
<abstract> <p>The purpose of this paper is to construct an abstract model of a society in which each member can cooperate with others by forming a coalition, but at the same time can be influenced by the members outside the coalition. A new concept of equilibrium, called here a social coalitional equilibrium, is proposed, and a sufficient condition for its existence provided. The social coalitional equilibrium may be considered a synthesis of the Nash equilibrium (a noncooperative solution concept) and the core (a cooperative solution concept). The paper provides a broadly applicable mathematical tool for proving existence of equilibrium for models of labor-managed market economies.</p> </abstract>
<abstract> <p>The theoretical validity of consumer surplus analysis at the level of the individual agent is shown to be independent of the assumption that individual preferences are transitive. With or without transitive preferences, consumer surplus can be calculated as an appropriate area to the left of compensated demand functions. Without transitive preferences consumer surplus cannot be interpreted as a money index of utility change nor does it have an exact willingness to pay interpretation; rather it must be interpreted as a hypothetical compensation payment. Without transitive preferences the measurement of consumer surplus using ordinary demand functions to approximate compensated demand functions must rely on a heuristic rule to the effect that the "inconsistency effect" of a price change is small since full duality between ordinary and compensated demand functions does not (generally) hold.</p> </abstract>
<abstract> <p>Part I of this paper introduces a general framework for the discussion of discrete production sets and the associated programming problems which arise when a particular endowment of factors is specified. In this part of the paper we shall apply these ideas to integer programming problems with two activities and bring to bear some of the basic considerations of the theory of computational complexity. The numbering of sections, figures, and equations will follow those used in Part I.</p> </abstract>
<abstract> <p>Understanding of the relationship between producer choice of product qualities and consumer preferences has been enhanced by the development of models specialized to a particular quality attribute--durability. Durability choice is invariant with respect to market structure under certain conditions but capital market imperfections and income taxes can upset this strong independence result. Income taxes are shown to mimic the effect of a capital market distortion on a monopolist selling a durable. Moreover, contrary to previous results the tax can increase as well as lower durability. A tax on "true income" is neutral in its effect and hence tax reform provides an alternative to regulatory control of product life.</p> </abstract>
<abstract> <p>Competitive adjustment processes in labor markets where firms and workers are heterogeneous but well informed are studied. A natural notion of equilibrium for such markets is defined, and a plausible adjustment process is shown under reasonable assumptions always to converge to an equilibrium; this allows a generalization of several existence results in the literature. Finally, the relationship between market institutions (such as who makes offers) and which of the range of equilibria that heterogeneity makes possible arises, is studied. Generalizing results of Gale and Shapley and Shapley and Shubik, it is shown that all agents on a given side of the market agree on which is the best equilibrium, and that the equilibrium that emerges is the one most favored by the agents on the side of the market that makes offers in the adjustment process. The process can also be viewed as an algorithm for transportation and optimal assignment problems.</p> </abstract>
<abstract> <p>Estimation results are presented for the probability of working, the hourly wage rate, and the annual hours of work for wives in seven different age groups in both the U.S. and Canada. Federal and state or provincial taxes are incorporated into the analysis. An iterative estimation method is employed to circumvent the statistical problems resulting from the dependence of the hours of work on the tax rate, and the dependence of the tax rate on the hours of work.</p> </abstract>
<abstract> <p>For more than three decades, economic columnist Joseph A. Livingston has canvassed a panel of economists twice a year, eliciting their six-month and twelve-month forecasts for more than a dozen key variables. This study analyzes whether the experts' predictions are unbiased, and whether complete use was made of all relevant, known information (unbiasedness and completeness being necessary conditions for fully rational expectations). Little bias was found in either the half-year or full-year predictions, but extensive underutilization of information--particularly data on monetary growth--occurred.</p> </abstract>
<abstract> <p>This paper presents a precise characterization of the bias of least squares in two limited dependent variable models, the Tobit model and the truncated regression model. For the cases considered, the method of moments can be used to correct the bias of OLS. For more general cases, the results provide approximations which appear to be relatively robust.</p> </abstract>
<abstract> <p>This paper is concerned with the generalization of the Stolper-Samuelson theorem from the 2 x 2 case to the n x n case. We start by proving theorems establishing the validity of the factor price equalization theorem and the Stolper-Samuelson theorem for the n x n case. The conditions established in these theorems are then interpreted economically in terms of the generalized versions of factor intensity. It may be noted that the above results, apart from being more readily interpretable in economic terms, are of basic mathematical interest.</p> </abstract>
<abstract> <p>The purpose of this paper is to generalize to the n-commodity, n-factor case the Stolper-Samuelson condition which has established a relationship between commodity prices and factor reward rates for the two-commodity, two-factor case, and to study some necessary and/or sufficient conditions for the generalized Stolper-Samuelson conditions. Two types of generalization are studied. One is the case where the inverse of the production coefficient matrix is a Minkowski matrix. Another is the case where it is a Metzler matrix. Some results about the former have already been obtained by some economists [1, 4, 8]. But the latter case has been left unexplored so far. The main purpose of this paper is to emphasize the necessity of studying the latter case and to obtain some results corresponding to those obtained for the former case. Another purpose of this paper is to establish a univalence theorem. When all principal minors of the Jacobian matrix are positive, univalence holds. This is the theorem by Gale and Nikaido [3]. In this paper, we prove that when all principal minors of the Jacobian matrix are negative, univalence holds. This theorem cannot be obtained trivially from Gale-Nikaido's theorem, but the technique employed by them for their proof can be used for our theorem.</p> </abstract>
<abstract> <p>This paper describes empirical laws (formulated abstractly, as axioms) that lead to simultaneous measurement of utility and subjective probability under circumstances where decisions delimit which states of nature may occur.</p> </abstract>
<abstract> <p>A general definition of majority decision in terms of a hierarchy of voting councils has been given by Murakami [3, 4]. The present article establishes a set of necessary and sufficient conditions for Murakami's majority decision or representative system in terms of properties of a group decision function for two alternatives. One corollary of the general theorem is Murakami's conjecture, which says that if a group decision function is dual, strongly monotonic, and nondictatorial, then it is a representative system.</p> </abstract>
<abstract> <p>This paper gives explicit consideration to the basic role of monetary institutions in the context of a monetary growth model. The fundamental schema of this article is the following. Inflation will always affect the money rate of interest. Under the assumption that the reserves to demand deposit ratio depends on the money rate of interest, it follows that the ratio of outside money to the money supply will be influenced by inflation and ultimately the long-run equilibrium values of the real variables in a fully employed economy.</p> </abstract>
<abstract> <p>This note examines carefully the basic problems of excess demand functions that appeared in the paper "On the Stability of the Competitive Equilibrium, II" by K. J. Arrow, H. D. Block, and Leonid Hurwicz [1]. It attempts to show that nearly the same results can be obtained after some modifications of the assumptions and the methods of proof of lemmas.</p> </abstract>
<abstract> <p>Programming theory is treated initially without any assumptions about the functions and with interpretation for production and activity analysis. By proof of a general theorem about optimal and support solutions and subsequent introduction of convexity assumptions, a new method for obtaining standard propositions and an amplification of their content becomes possible. Properties entirely peculiar to linear problems are noted. Two "shadow price" interpretations are discussed, one being the standard one in which support solutions appear as prices in a fictitious market. Consideration is also given to the law of diminishing returns. Finally, vector output, with emphasis on a concept of irreducible limit output, is treated.</p> </abstract>
<abstract> <p>The paper argues that variance components models are very useful in pooling cross section and time series data because they enable us to extract some information about the regression parameters from the between group and between time-period variation--a source that is often completely eliminated in the commonly used dummy variable techniques. The paper studies the applicability and usefulness of the maximum likelihood method and analysis of covariance techniques in the analysis of this type of model, particularly when one of the covariates used is a lagged dependent variable.</p> </abstract>
<abstract> <p>Availability of data on a large number of individuals, but on each individual only over a very short period of time, has become increasingly common in a number of different fields in economics. Very often we would like to use such data to study behavioral relationships that are dynamic in character, i.e., that contain a distributed lag or other form of autogressive relationship. Since only a few observations are available over time, but a great many observations are available for different individuals at a point in time, it is exceptionally important to make the most efficient use of the data across individuals to estimate that part of the behavioral relationship containing variables that differ substantially from one individual to another, in order that the lesser amount of information over time can be used to best advantage in the estimation of the dynamic part of the relationship studied. As it turns out, the problem is far from simple: obvious devices such as the pooling of all observations and estimation by ordinary least squares, or the introduction of dummy variables for individuals, produce estimates having serious small sample bias. In earlier papers, the author and others have formulated a simple variance components model for the disturbance term in a relationship to be estimated from cross section data over time. This paper presents a series of Monte Carlo studies designed to explore the small sample properties of various types of estimates within this context. Not only is the bias of the obvious methods of estimation mentioned above confirmed, but certain serious deficiencies of the maximum likelihood approach which had been suspected earlier are also confirmed. A two-round estimation procedure is proposed which appears to work well for a wide variety of parameter values.</p> </abstract>
<abstract> <p>This note develops a slightly different formulation of one of the basic results presented in a recent paper by Wallace and Hussain [5] on error components models for disturbances in relationships designed to explain cross-sectional observations over time. In their discussion, Wallace and Hussain derive the inverse of the variance-covariance matrix of the disturbances by trial and error. Unfortunately, their formulation does lead to a "natural" interpretation of the generalized least squares estimates, or of the relationships of these estimates to other estimates in the same way diagonalization of the variance-covariance matrix by means of an appropriate orthogonal transformation does. The characteristic roots of the variance-covariance matrix for the disturbances in a three component model which has been studied by Wallace and Hussain are derived here. It is shown how knowledge of these roots and the characteristic vectors associated with them leads to a form of the inverse matrix which may be more readily interpreted, as well as a number of other useful results, including an interpretation of the poor small sample properties of estimates which incorporate dummy variables for each individual.</p> </abstract>
<abstract> <p>What can we say about the competitive equilibrium price system for an uncertain economy in which each risk concerns just one individual? Three interrelated concepts of equilibrium are considered. They show how and under which conditions the contingent price for a contract to deliver one unit of some good if some event occurs tends to be equal to the product of the sure price of the good and the probability of the event.</p> </abstract>
<abstract> <p>We study the properties of the core of large markets. We assume that traders' preferences have certain standard properties, that their preferences belong to a set which is compact with respect to a certain topology, and that there is a bound on their initial endowments. It is then found that if a market contains sufficiently many traders and if there are many traders similar to any one trader, then every core allocation is similar to a price equilibrium in a very strong sense. This fact implies a precise formulation of the following statement: for most large markets, the core decomposes into disjoint clusters of allocations, the allocations in each cluster being very similar. This statement may be interpreted as an explanation of why traders in large markets normally feel that they have little bargaining power.</p> </abstract>
<abstract> <p> The purpose of this paper is to investigate the relationship between the risk aversion function and the demand functions. Two hypotheses about risk aversion are studied: risk aversion independent of prices, and risk aversion constant on each indifference locus. The implications of these hypotheses for the utility and demand functions are then considered; in addition, the derivation of the risk aversion function from the demand functions is examined. The cases of constant (absolute and relative) risk aversion, and the problems raised by the choice of numéraire, are also dealt with. </p> </abstract>
<abstract> <p>It was suggested in [2] that an appropriate model for an oligopolistic economy is one in which the set of traders consists of some large traders and a continuum of small traders. The cores of such market models are analyzed here. Some of the results are as follows: A duopolistic market in which the duopolists are of the same type is "perfectly competitive," i.e., its core coincides with the set of competitive allocations. At any allocation in the core of any oligopolistic markets, the value of the bundle received by a small trader does not exceed the value of his initial bundle; that is, small traders can never "gain money." Conditions are given under which small traders will not "lose money" either. In addition to the case of duopoly, other conditions are given under which an oligopolistic market will be perfectly competitive.</p> </abstract>
<abstract> <p>The Liviatan estimator of the geometric distributed lag parameter is a ratio of two ordinary least squares estimators. The standard result on the distribution of the ratio of two normal variates is then used, and the problems of setting confidence limits are briefly illustrated.</p> </abstract>
<abstract> <p>The possible impact of autonomous control (vs. the price mechanism and directive control) on the functioning of an economic system is studied. It sheds some light on similarities among economic systems that are quite different in their higher functioning. The survival and stability conditions for a Leontief-type economy demonstrate the role stock signals can play in controlling the behavior of producers and consumers.</p> </abstract>
<abstract> <p>Short-term economic stabilization policy is approached as a problem in optimal control. The optimal control problem is defined as a dual discrete-time tracking problem (nominal state and nominal policy trajectories are tracked) for a linear time-invariant system with a quadratic cost functional. This problem is solved analytically, and the solution is applied to a ten-equation quarterly econometric model. Optimal stabilization policies are calculated for cost functionals designed to force single variables to follow nominal paths, to impose trade-offs between the movements of different variables, and to emphasize the use of one or another policy variable. The experimental results demonstrate that this approach is valuable both as a tool for policy planning and as a method of analyzing the dynamic properties of econometric models.</p> </abstract>
<abstract> <p>A new method for imputing benefits of government expenditures on public goods to various income classes was recently put forward by Henry Aaron and Martin McGuire. They fail to present conclusive empirical results, however, lacking a parameter whose value (they imply) is heretofore unmeasured. This note utilizes three independent, identical empirical estimates of the crucial parameter to compute believable and unambiguous estimates of the net incidence of the entire fiscal system. For the United States in 1961, it is found that benefits from public goods are regressively distributed. It is further suggested that the desire to equalize incomes requires provision of less, not more, public goods.</p> </abstract>
<abstract> <p>The equilibrium growth model is modified and used to explain the cyclical variances of a set of economic time series, the covariances between real output and the other series, and the autocovariance of output. The model is fitted to quarterly data for the post-war U.S. economy. Crucial features of the model are the assumption that more than one time period is required for the construction of new productive capital, and the non-time-separable utility function that admits greater intertemporal substitution of leisure. The fit is surprisingly good in light of the model's simplicity and the small number of free parameters.</p> </abstract>
<abstract> <p>This paper investigates the effect of costs of adjustment on the dynamic characteristics of intra-urban resource allocation. The analysis is concentrated on the development pattern of an open city within a system of many cities both in steady-state and in variable-state economies. Competitive equilibrium of the urban system as a whole is also discussed and compared to Pareto optimum allocation. Some of the results that characterize the resource allocation under static analysis are reestablished for the dynamic case as well. But others that follow from comparative statics are shown to be incorrect under the dynamic analysis.</p> </abstract>
<abstract> <p> This paper develops index number procedures for making comparisons under very general circumstances. Malmquist input, output, and productivity comparisons are defined for structures of production with arbitrary returns to scale, substitution possibilities and biases in productivity change. For translog production structures, Törnqvist output and input indexes are shown to equal the mean of two Malmquist indexes. The Törnqvist productivity index, corrected by a scale factor, is shown to equal the mean of two Malmquist productivity indexes. Similar results are given for making cost of living comparisons under general structures of consumer preferences. </p> </abstract>
<abstract> <p>We present a model of information acquisition in a competitive market in which traders can learn both from costly (and diverse) private enquiry and price, which costlessly (but partially) reveals the total amount of information known to all traders. Our major purpose is to show that an equilibrium exists in such a market: that is, there exists a rational expectations competitive equilibrium in which the amount of costly diverse information each trader acquires is endogenously determined. From this result we investigate the change in the informativeness of price relative to changes in the level of noise, the cost of acquiring information, and the distribution of traders' risk preferences.</p> </abstract>
<abstract> <p>This paper develops a model of strategic communication, in which a better-informed Sender (S) sends a possibly noisy signal to a Receiver (R), who then takes an action that determines the welfare of both. We characterize the set of Bayesian Nash equilibria under standard assumptions, and show that equilibrium signaling always takes a strikingly simple form, in which S partitions the support of the (scalar) variable that represents his private information and introduces noise into his signal by reporting, in effect, only which element of the partition his observation actually lies in. We show under further assumptions that before S observes his private information, the equilibrium whose partition has the greatest number of elements is Pareto-superior to all other equilibria, and that if agents coordinate on this equilibrium, R's equilibrium expected utility rises when agents' preferences become more similar. Since R bases his choice of action on rational expectations, this establishes a sense in which equilibrium signaling is more informative when agents' preferences are more similar.</p> </abstract>
<abstract> <p>This paper studies efficient resource allocation in a team consisting of a large number of firms and a resource allocator. We examine procedures based on a single demand message from the firms calculated using local information only. Our main result shows that for appropriately calculated demands, if firms are given (or "Grab") exactly what they demand until resources are exhausted and thereafter nothing, the per firm output converges, as the number of firms increases, to the maximal output obtainable using any decision rule including fully optimal ones requiring a complete exchange of all information. A similar result is also shown under stronger convexity conditions for demand messages defined by profit-maximizing under a (generally non-equilibrium) price system.</p> </abstract>
<abstract> <p>Competitive adjustment processes in labor markets with perfect information but heterogeneous firms and workers are studied. Generalizing results of Shapley and Shubik [7], and of Crawford and Knoer [1], we show that equilibrium in such markets exists and is stable, in spite of workers' discrete choices among jobs, provided that all workers are gross substitutes from each firm's standpoint. We also generalize Gale and Shapley's [3] result that the equilibrium to which the adjustment process converges is biased in favor of agents on the side of the market that makes offers, beyond the class of economies to which it was extended by Crawford and Knoer [1]. Finally, we use our techniques to establish the existence of equilibrium in a wider class of markets, and some sensible comparative statics results about the effects of adding agents to the market are obtained.</p> </abstract>
<abstract> <p>One of the most fundamental market mechanisms is the clearing house, where orders are accumulated over time and the market is cleared periodically. The issue addressed in this study is the statistical behavior of the market under this neutral mechanism in the framework of a tractable stochastic model which captures the underlying uncertainties and indivisibilities in the demand and supply schedules. Applying renewal theory, we derive closed-form results for the behavior of prices and quantities, and pursue the implications of the possibility of no-trade and the multiplicity of market-clearing prices.</p> </abstract>
<abstract> <p>In a portfolio problem with given asset returns, the portfolio efficient set is the set of portfolios chosen by any risk averse agent. Using an approach of Peleg and Yaari [13], we characterize the portfolio efficient set and derive some of its properties. In particular, we show that it may not be convex, proving that a central result of mean variance theory, the efficiency of the market portfolio, does not generalize. Finally, a characterization of the efficiency of several observations gives a version of revealed preference theory for incomplete markets.</p> </abstract>
<abstract> <p>In this paper we outline the computation of general equilibrium in a pure exchange economy via a fixed point decomposition procedure. For general equilibrium models of the required structure, a full equilibrium may be computed through the solution of a sequence of smaller scale `sub-equilibrium' problems. The text contains a presentation of the methods involved along with a discussion of initial computational experience for some numerical examples.</p> </abstract>
<abstract> <p>The estimation and testing of a singular equation system in the context of a general dynamic specification is considered. In an application to factor demand equations, hypotheses suggested by economic theory are expressed in terms of the long run structure of the system under alternative dynamic specifications. Variations in the dynamic specification are found to have a significant impact upon the inferences that can be made about the long run structure.</p> </abstract>
<abstract> <p>This article uses the concept of "cone of interior displacements," which extends the notion of differentiability, to set up a characterization of Pareto optima in non-convex economies. A general theorem asserting that a Pareto optimum is a PA equilibrium is given and specifications are discussed. It is finally argued that the usual formulation of the doctrine of "marginal cost pricing"--as a doctrine Pareto optimal states in a non-convex decentralized economy--has unsatisfactory logical basis, and a way of defining a minimum degree of centralization inherent to non-convex economies is suggested.</p> </abstract>
<abstract> <p>A preference ordering R is called "self-dual" by Samuelson if and only if there exists a direct utility function U representing R such that U(Z) = - U*(Z) is any non-negative n-vector and U* is the indirect utility function corresponding to U. Samuelson showed that the Cobb-Douglas preference ordering is self-dual and asked the open question as to the existence of any other self-dual case. If a preference ordering R is both self-dual and homothetic, then for the two-good case Samuelson claims to have proved that R is Cobb-Douglasian and conjectures the same to be true in the three-or-more good-case. Swamy has claimed that the Cobb-Douglas case is the only example of a preference ordering which is self-dual and either homothetic or additive. In this paper, we give two non Cobb-Douglasian examples of self-duality, one additive and the other homothetic, in order to answer the open question and refute the claims.</p> </abstract>
<abstract> <p>Edgeworth's conjecture that as the number of traders in an exchange economy increases the core approaches the set of competitive equilibria has been formalized both as a theorem about a sequence of finite economies, and as a theorem about an economy having an infinite number of agents. This paper, using nonstandard analysis, provides a synthesis of these two approaches. It is shown that the core and the set of competitive equilibria are equivalent within a non-standard exchange economy. This theorem implies a asymptotic theorem concerning the core and competitive equilibria of sequences of finite economies.</p> </abstract>
<abstract> <p>Consider a competitive economy with infinite horizon Ramsey behavior by households, present-value maximizing firms and infinitely many futures markets. If prices adjust according to excess demands, then the economy is locally stable. This is in marked contrast to the saddle-point instability exhibited by the myopic foresight dynamics in models of this genre. It is argued that infinite futures markets are idealizations of actual economic forces.</p> </abstract>
<abstract> <p>The relationship between Pareto optimal (&lt;tex-math&gt;$\theta _{s}$&lt;/tex-math&gt;) and revenue maximizing (&lt;tex-math&gt;$\theta _{r}$&lt;/tex-math&gt;) tolls is examined for queuing models that permit balking. When customers have the same value for waiting time, &lt;tex-math&gt;$\theta _{s}\equiv \theta _{r}$&lt;/tex-math&gt; provided the entrepreneur can impose a simple two-part tariff. With heterogeneous values for waiting time, &lt;tex-math&gt;$\theta _{r}$&lt;/tex-math&gt; can be greater than, equal to, or less than &lt;tex-math&gt;$\theta _{s}$&lt;/tex-math&gt;. Expanding the number of servers and charging multi-part tariffs are shown to be alternative methods for segmenting the market, and the welfare implications of these two strategies are explored.</p> </abstract>
<abstract> <p>Many environment policies use two categories of instruments: a financial incentive (most often a tax) for "polluters," and direct undertakings or financing of some restorations or maintenances or improvements of environmental qualities. The financial consequences of such policies, when optimum, are important for considerations of public finance, decentralization (financial autonomy), and equity (must polluters pay?). They turn out essentially to depend upon the mathematical structure of, first, the "environment function," i.e., the way in which environment "qualities" depend upon both deteriorating and improving activities, and, second, the various constraints of the problem. Constraints which can be expressed by functions homogeneous of any degree are shown to have no direct financial effect. Apart from the constraints' effects, budgetary equilibrium, surplus or deficit are respectively given by environment functions which present constant, decreasing, or increasing "qualitative returns to scale," i.e., "weighted" homogeneity of degree zero, positive or negative. The opposite polar cases of "cleaning" and "dilution" types of improvement technology are presented, with some other mixed simple cases and a few examples of application of the results.</p> </abstract>
<abstract> <p>Within the context of a world where some goods may be nontraded and some factors may be internationally mobile, this paper analyzes the conditions under which trade in factors can replace trade in goods in the presence of tariffs or taxes.The crucial issue regarding the perfect substitutability between trade in goods and trade in factors turns out to be whether the sufficient conditions for international equalization of prices of goods and factors are exactly met, not met, or met in excess.</p> </abstract>
<abstract> <p>Observations are made on the sources of cross-equation constraints and the ways they can be used as identification aids, on possible simplifying transformations of linear homogeneous cross-equation constraints, on a rank condition for identification of a block of equations under such linear constraints, and on a strategy for using these constraints for single-equation identification.</p> </abstract>
<abstract> <p>Some economic variables are restricted by an upper and lower limit but are continuous between the two limits. Measurements of such variables are sometimes available in their natural form and sometimes only in the form of three categories where information concerning the middle category is suppressed (unemployed, employed part time, employed full time, for example). Where such a variable is a continuous function of other variables between the two limits, the function can be estimated from data of either sort provided the function and the distribution of errors can be specified.</p> </abstract>
<abstract> <p>A method for optimization in nonlinear stochastic models is proposed in this paper, and applied to study monetary and fiscal policy in the St. Louis econometric model. Essentially, the method is to simulate the model in a number of stochastic simulations in which the co-efficients of the model are treated as random, to use the results of the stochastic simulations to find parsimonious representations of the time form of the policy multipliers by estimating autoregressive moving-average regressions for the effects of policy on the relevant endogenous variables, and then to use these equations in computing optimal policy. The method is applied to the study of monetary and fiscal policy for the St. Louis model over a 60-period horizon with encouraging and sensible results.</p> </abstract>
<abstract> <p>Although the theory of taxation and portfolio choice has been extensively developed, the current paper begins the econometric study of this subject. The research analyzes the composition of portfolios of 1,799 households in a sample in which high income individuals are greatly overrepresented. The results show that the personal income tax has a very powerful effect on individuals' demands for portfolio assets after adjusting for the effects of net worth, age, sex, and the ratio of human to nonhuman capital.</p> </abstract>
<abstract> <p>Economists as yet have relatively little evidence concerning the sign of the net cross-price effect in a family model of labor supply. Part of the problem is attributable to a lack of quality data for nonlabor income. In this paper I derive a simple indirect test which does not require accurate estimates of income effects and apply it to data from the National Longitudinal Survey.</p> </abstract>
<abstract> <p>Labor supply and demand behavior is examined under the assumption that the firm's technology depends on the simultaneous presence, during the workday, of two factors of production, either labor and capital or two types of labor differentiated by skill. If desired workdays differ, this leads to payment to both factors of a premium over their traditional sector wage and to the establishment of a workday for labor that departs from simple labor-leasure preferences. It also provides implications, in the presence of capital, for the determination of work shifts.</p> </abstract>
<abstract> <p>An approximate solution, based on the method of dynamic programming is provided for the optimal control of a system of nonlinear structural equations in econometrics with unknown parameters using a quadratic loss function. It generalizes the methods previously proposed by the author for the control of a nonlinear econometric model with constant parameters and of a linear econometric model with uncertain parameters. It is an improvement over the method of certainty equivalence which replaces the unknown parameters by their mathematical expectations and utilizes the solution for the resulting model. Since the solution is given in the form of feedback control equations, many of the useful concepts and techniques developed in the theory of optimal feedback control for linear systems are now applicable to the control of nonlinear systems using the method proposed, including the calculation of the expected loss of the system under control by analytical rather than Monte Carlo techniques.</p> </abstract>
<abstract> <p> L'objet de la présente note est de montrer la possibilité et l'intérêt d'une formulation Bayésienne-décisionelle de certains modèles économiques d'affectation. Les calculs statistiques, bien que complexes, sont connus dans leurs principles. Des adaptations ont été seulement nécessaires pour l'illustration de notre propos. </p> </abstract>
<abstract> <p>It is known that there is a one-to-one correspondence between stationary ARMAX and state space models. In order to estimate these it is necessary first to identify and further, having identified, to choose parameters. This paper discusses the properties a system of identification and parameterization might be desired to have in relation to various examples of identification. It also constructs a (known) canonical state space form (identification) out of the constants needed to specify the corresponding ARMAX form. It is argued that what is here called "simple identification" will be the best basis for identification even though some structures cannot be identified in this manner.</p> </abstract>
<abstract> <p>In a linear regression model with arbitrary disturbance covariance structure, least squares estimators subject to correct linear restrictions dominate unrestricted least squares for all estimable functions of the parameters if and only if the covariance matrix obeys conditions closely related to those of the Gauss-Markov theorem.</p> </abstract>
<abstract> <p>In this paper, we propose a procedure based on the use of the Moore-Penrose inverse of matrices for deriving unique indirect least squares (ILS) estimates of the structural parameters in the overidentified case. The procedure makes use of all reduced form estimates in deriving the unique structural estimates. The estimator is shown to be consistent. We derive the relationship between this estimator, the two stage least squares (2SLS) estimator, and instrumental variables (IV) estimators. We also derive the asymptotic distribution of the proposed estimator, and extend the procedure to a full information ILS estimator (FILS). The results of sampling experiments are summarized.</p> </abstract>
<abstract> <p>This paper is concerned with the estimation of a system of simultaneous linear differential equations that involves predetermined variables. The system is replaced by a discrete approximation that is most conveniently handled in the frequency domain. Our method of estimation is nonlinear least squares. We state conditions under which the estimators will have asymptotically desirable properties. The most notable of these is an aliasing condition on the predetermined variables.</p> </abstract>
<abstract> <p>We suggest a frequency-domain class of instrumental variables estimators for all of part of an open, linear system of differential equations. While the estimators have similar statistical properties to those of [6] they seem preferable computationally in those situations in which they can be used. The estimation procedure consists of two basic steps, the first of which we describe as consistent and the second, efficient. We discuss the possible instruments that can be used. This type of procedure, like that of [6], could also be used to efficiently estimate discrete time systems, under weak conditions on the residuals.</p> </abstract>
<abstract> <p>This paper develops a simulation model to study the income distribution effects--total and factorial-of optimum restrictions on the flows of factors and products across national boundaries. Imposing both optimum tariffs and optimum taxes on factor flows allows an increase in national income that is much larger than the sum of the two effects evaluated separately. Often there are large shifts in the incomes of factors even though total income changes only slightly.</p> </abstract>
<abstract> <p>Ostroy and Starr [3] have recently shown how money (a good which may be traded although it satisfies no excess supply/demand) allows decentralized achievement of competitive equilibrium allocations via a round of bilateral trades. Following Jevons [1], monetary exchange is here defined as any trade which decreases the utility of any participant. The consequences of this alternative definition for the Ostroy/Starr thesis are investigated and generalized to cover the case where exchange takes place multilaterally (instead of just bilaterally).</p> </abstract>
<abstract> <p>An Arrow social welfare function was designed not to incorporate any interpersonal comparisons. But some notions of equity rest on interpersonal comparisons. It is shown that a generalized social welfare function, incorporating interpersonal comparisons, can satisfy modifications of the Arrow conditions, and also a strong version of an equity axiom due to Sen. One such generalized social welfare function is the lexicographic form of Rawls' difference principle or maximin rule. This kind of generalized social welfare function is the only kind satisfying the modified Arrow conditions, the equity axiom, and a condition which underlies Suppes' grading principle.</p> </abstract>
<abstract> <p>This paper develops a unified framework for formulating econometric models of discrete/continuous consumer choices in which the discrete and continuous choices both flow from the same underlying (random) utility maximization decision. As a special case a number of models suitable for empirical application are developed where the discrete choice is among different brands of a commodity. Since these brands are essentially substitutes, the consumer prefers to buy only one brand at any time; discrete choice is which brand to select and the continuous choice is how many units to buy.</p> </abstract>
<abstract> <p>Most studies of commodity price stabilization assume that all agents behave competitively. However, many commodities suitable for stockpiling are produced by countries with a significant share of the world market, and commodity agreements themselves often result in carterlization of the market. The paper explores the consequences of market power for the choice of storage rule and the degree of price stabilization. It finds that with linear demand, dominant producers choose more stable prices than under perfect competition and price stability increases with their market share. With constant elastic demand the competitive degree of price stabilization is achieved.</p> </abstract>
<abstract> <p>This paper shows how to test firm demand and supply data for consistency with profit maximization and cost minimization models; test for special restrictions on technology such as constant returns to scale, homotheticity, and separability; recover estimates of the underlying technology; and forecast firm behavior in new situations without making any assumptions concerning the parametric form of underlying production technology.</p> </abstract>
<abstract> <p>Reformulating marginal productivity theory by replacing productivity with respect to commodities with productivity with respect to persons and then defining perfectly competitive equilibrium as an allocation at which each person receives the marginal product of his/her contribution--called a no-surplus allocation--there emerges a competitive theory of price determination. Characterizations of no-surplus allocations are given in models with a nonatomic continuum of agents and an infinite-dimensional commodity space. Comparisons between the no-surplus and Walrasian equilibrium definitions of competitive equilibrium are made and some sufficient conditions are obtained for the existence of a no-surplus allocation.</p> </abstract>
<abstract> <p>An econometric problem in estimating models of occupational choice is that the agents' forecasts of future wages and occupational tenure are unobservable. This paper solves the problem by assuming that agents have rational expectations and by considering the effects of arbitrage both within and between cohorts. The solution consists of two time series regressions of the demand and supply functions of entrants into an occupation. From these regressions one obtains estimates of the rate of return to education, the direct cost of education, and other parameters that influence the market. The model was estimated with data from the market for lawyers.</p> </abstract>
<abstract> <p>Many distributions have been used as descriptive models for the size distribution of income. This paper considers two generalized beta distributions which include many of these models as special or limiting cases. These generalized distributions have not been used as models for the distribution of income and provide a unified method of comparing many models previously considered. Expressions are reported which facilitate parameter estimation and the analysis of associated means, variances, and various measures of inequality. The distributions considered are fit to U.S. family income and their relative performance is compared.</p> </abstract>
<abstract> <p>The paper presents general solution methods for rational expectations models that can be represented by systems of deterministic first order liner differential equations with constant coefficients. One method is the continuous time adaptation of the method of Blanchard and Kahn. To obtain a unique solution there must be as many linearly independent boundary conditions as there are linearly independent state variables. Three slightly different versions of a well-known small open economy macroeconomic model are used to illustrate three fairly general ways of specifying the required boundary conditions. The first represents the standard case in which the number of stable characteristic roots equals the number of predetermined variables. The second represents the case when the number of stable roots exceeds the number of predetermined variables but a sufficient number of linear restrictions on the state variables at an initial date is given to guarantee a unique solution. The third represents the case when the "missing" initial conditions have been replaced by boundary conditions that involve linear restrictions on the values of the state variables across an initial and a future date. The method of this paper permits the numerical solution of models with large numbers of state variables. Any combination of anticipated or unanticipated, current or future and permanent or transitory shocks can be analyzed.</p> </abstract>
<abstract> <p>Estimators obtained by maximizing a likelihood function are studied in the case where the true p.d.f. does not necessarily belong to the family chosen for the likelihood function. When such a procedure is applied to the estimation of the parameters of the first order moments, it is possible to prove a necessary and sufficient condition for its consistency. Asymptotic normality is shown as well as the existence of a lower bound for the asymptotic covariance matrix. It is also seen that this bound can be reached if consistent estimates are available for the parameters of the second order moments. Finally, a necessary and sufficient condition for the consistency if the pseudo maximum likelihood estimation of the first and second moments is given.</p> </abstract>
<abstract> <p>Pseudo maximum likelihood techniques are applied to basic Poisson models and to Poisson models with specification errors. In the latter case it is shown that consistent and asymptotically normal estimators can be obtained without specifying the p.d.f. of the disturbances. These estimators are compared both from the finite sample and the asymptotic point of view. Quasi generalized PML estimators, which asymptotically dominate all PML estimators, are also proposed. Finally, bivariate and panel data Poisson models are discussed.</p> </abstract>
<abstract> <p>A general linear simultaneous equation system with a multivariate Student t disturbance vector is considered. The normal equations of the corresponding maximum likelihood estimator are used as estimator generating equations to introduce a new class of estimators. Properties of large subclasses of these estimators are determined for disturbance vectors other than the multivariate Student t.</p> </abstract>
<abstract> <p>When zero mean measurement error is added to the dependent variable for the nonlimit observations of the censored normal regression model, the conventional maximum likelihood estimator (Tobit) is inconsistent. "Correct" maximum likelihood estimation appears to be computationally difficult under various specifications for the distribution of the measurement error. Estimators based on either the expectation function or the conditional expectation function for uncensored observations remain consistent in the presence of measurement error. Eight such estimators are examined. The results of a numerical experiment suggest that several of these estimators are substantially more efficient than the conventional maximum likelihood estimator when measurement error exists and that they also will do reasonably well when it does not.</p> </abstract>
<abstract> <p>This paper examines the possible explanations for the changes in output, capital, and labor input of a sample of manufacturing plants over a number of years. Apart from the scale of operation, these changes could be attributed to three causes: technology diffusion, substitution, and improvements in X-efficiency. The empirical findings indicate that a diffusion model modified to incorporate X-efficiency improvements provides the best explanation. This suggests the need for a new approach to the specification of production functions.</p> </abstract>
<abstract> <p>The various conditions for non-intransitivity of majority rule formulated over the past decade have been concerned with choices over arbitrary, usually finite, sets of discrete alternatives. In many economic and other social choice problems, however, the possible choices constitute a point set in some appropriately defined multi-dimensional commodity or policy space. It is shown that in problems of this kind, when voter preferences can be represented by quasi-concave, differentiable utility functions, the various equilibrium conditions for majority rule are incompatible with even a very modest degree of heterogeneity of tastes, and for most purposes are probably not significantly less restrictive than the extreme condition of complete unanimity of individual preferences.</p> </abstract>
<abstract> <p>The problem of the individual's consumption and portfolio choices over time has been the focus of recent studies by a number of authors. An attempt is made here to extend these results by examining the impact of transaction costs on optimal consumption and portfolio decisions. We are able to show that these costs considerably modify available results and greatly increase the difficulty of analyzing the consumer choice problem. The major reason is that now not only wealth, but also the composition of wealth, becomes important in the decision making process. To keep the exposition reasonably manageable we consider only a constant relative risk averse utility function and confine explicit attention to a two-period horizon. Since it is now necessary to examine portfolio choices in detail, we limit portfolio opportunities to a riskless asset, cash, and a risky asset, stock, with a random return. We assume proportional transaction cost for purchases or sales of stock. Wealth is taken to be the sum of cash and stock at the beginning of a period, while income is assumed to be zero, or included in initial wealth.</p> </abstract>
<abstract> <p>The problem of collinearity suggests the search for an alternative to ordinary least squares which, although biased, might reduce the mean square error of the coefficient of interest. Two types of estimators are examined, and the corresponding mean square error loss functions are calculated.</p> </abstract>
<abstract> <p>This paper develops an iterative procedure for estimating "specific" and "income" consumer unit scales in Engel curve analysis. The proposed procedure is essentially a modification of the Prais and Houthakker method and is illustrated by means of a numerical example based on the Indian National Sample Survey data.</p> </abstract>
<abstract> <p>This paper deals with similarities and differences in the equations defining the full information maximum likelihood and three stage least squares estimators. It shows that the two sets of equations are similar, the difference being that the two estimators "purge" the jointly dependent variables differently. Hence, even if three stage least squares is iterated, it will not give an estimator which is the same as the maximum likelihood one. On the other hand, it is quite apparently asymptotically equivalent to full information maximum likelihood. A number of other results are also obtained.</p> </abstract>
<abstract> <p>The core of a game, which is an abstraction of the core or set of cooperative equilibrium states of an economy, is a fundamental notion of social equilibrium. However, except for games derived from special kinds of economic situations or satisfying restrictive (balancedness) conditions, the core is usually empty. In contrast, this paper shows that, with mild and economically natural assumptions, large games always have non-empty approximate cores. The game-theoretic framework is sufficiently general to cover a wide variety of economic situations.</p> </abstract>
<abstract> <p>This paper presents an analysis of a 2-person noncooperative bargaining game in which one party is free, subject to certain frictions, to switch between rival partners. This permits us to capture the notion of an asymmetry between "insiders" and "outsiders" in the context of a firm bargaining with its workers, in the presence of unemployment.</p> </abstract>
<abstract> <p>It is shown that cardinal value allocations may fail to be symmetric. Specifically, agents with identical preferences and identical endowments can be treated very differently at a cardinal value allocation. This casts further doubt on the interpretation of the weights as "endogenous utility comparisons."</p> </abstract>
<abstract> <p>This paper examines the implications of imposing a weak aggregation condition on inequality indices, so that the overall inequality value can be computed from information concerning the size, mean, and inequality value of each population subgroup. It is shown that such decomposable inequality measures must be monotonic transformations of additively decomposable indices. The general functional form of decomposable indices is derived without assuming that the measures are differentiable. The analysis is suitable for extension to the many other kinds of indices for which a similar relationship between the overall index value and subaggregates is desirable.</p> </abstract>
<abstract> <p>We specify and estimate a small fix-price model with quantity rationing on both the goods and labor markets. The side of the market which is rationed is random. The model is able to generate a productivity cycle and the degree of capacity utilization is endogenous. The rationing scheme, allocating supply between the various components of demand is estimated. The probabilities of being in the various regimes, the intensity of the disequilibrium on the markets, and the results of policy exercises are discussed.</p> </abstract>
<abstract> <p>We consider the dynamic regression model with lagged endogenous variables and moving average disturbances, when some observations on the endogenous variable are missing. The available data are assumed to be sampled at regular intervals of length m and can be linear combinations of the realizations of the variable over a finite number of periods. We discuss the identification of the parameters in the model. For some selected models, we evaluate the large sample variances of the maximum likelihood (ML) estimates for the incomplete data and complete data respectively. In this way, we get an indication of the loss of information when the data are incomplete. Finally, we give some results for the effects on the properties of the OLS estimator, when the interpolated series are substituted for the missing observations and we briefly discuss ways to obtain ML estimates. Our general conclusion is that when the sample is incomplete it is very important to use all available reliable a priori information to analyze the model.</p> </abstract>
<abstract> <p>The consensus in the literature is that the use of only lump-sum taxation is a necessary and sufficient condition for the dynamic consistency of optimal open-loop government policies. We show that this does not hold for models with nonhomogeneous agents. Then the stated condition is neither necessary nor sufficient. Dynamic inconsistency arises because of a shortage of appropriate government policy instruments which amounts to the consensus condition only in special cases.</p> </abstract>
<abstract> <p>I examine the effects of price discrimination on the equilibrium prices, number of firms, and level of total surplus in a monopolistically competitive market. The main finding is that uniform pricing is more (less) efficient than is price discrimination when the purchases made by the consumers who are discriminated against constitute a small (large) proportion of the total purchases.</p> </abstract>
<abstract> <p>We characterize a seller's optimal scheme for the sale of an indivisible good to one of n risk averse buyers. We also compare certain commonly used schemes, such as the high bid and second bid auctions, under the hypothesis of risk aversion.</p> </abstract>
<abstract> <p>Full information maximum likelihood estimates of an up-dated and extended version of the Klein-Goldberger Model have been completed. These supersede some earlier estimates that made simplifying approximations and dealt only with linear equations. Simulation and ex post prediction tests are made for comparison among full information maximum likelihood, two stage least squares (using principal components of predetermined variables), and ordinary least squares estimates of the same model. The relative performance of different methods varies depending on whether the simulations are one period (re-setting of lagged dependent variables every period) or dynamic (solution from fixed initial conditions). Also, the test performance varies with the number of principal components used, the best number not necessarily being large relative to the number of predetermined variables.</p> </abstract>
<abstract> <p>A special regression model is suggested to analyze economic variables possessing step-like time paths. The dependent variable is assumed not to move until the concerted action of the independent variables and the error term induces it to overcome its reaction threshold. After the theoretical solution is presented, the model is shown to be applicable both to time series, such as newsprint prices, and to cross sections, such as household durable good purchases and plant capacity increases. Finally, computational problems are briefly discussed.</p> </abstract>
<abstract> <p>The paper describes tests of the stability of three types of interindustry coefficients, industry-to-industry, commodity-to-industry, and commodity-to-commodity. Data for the first two tests are drawn from all sectors of the Irish economy, while the third test uses regression analysis to investigate sources of variation of material and labour input coefficients within one industry--shirtmaking.</p> </abstract>
<abstract> <p>This paper discusses consistent estimators for the covariance matrix of limited information estimators of simultaneous equation models. It also examines the statistical test of a priori restrictions on the coefficients of simultaneous equation models.</p> </abstract>
<abstract> <p>As of many developing countries, the foreign sector is a key determinant of the Colombian economy. Colombia has a complicated system of multiple exchange rates and many times the "peso" has been devalued to meet the real and the financial problems of the foreign sector. In order to appraise the effectiveness of devaluation and to analyze its direct and indirect impact on the economy, a complete econometric model of Colombia has been developed. A free exchange rate has been treated as an explicit endogenous variable. Alternative policy solutions under "status quo" assumptions and under controlled assumptions with respect to devaluation have been generated.</p> </abstract>
<abstract> <p>Canonical correlation coefficients measure the correlation between linear combinations of two sets of variables. For some predictive purposes it is desirable to know to what extent the variance of each of the variables of one set or the other is explained by the individual canonical functions. Formulae for this purpose are presented and comments concerning Hooper's [2] trace correlation are made.</p> </abstract>
<abstract> <p>The first section of this paper deals with the properties any demand system should have if it is derived from any utility function. We find that the utility function under consideration could not be homogeneous if we impose the condition that the income elasticity is not one for all commodities. We then examine four mayor demand systems often used in empirical studies to see if they meet the properties listed in the first section. We find that Stone's linear expenditure system and Houthakker's indirect addilog system pass the theoretical tests, whereas the double log system and Theil's differential specifications do not. We fit the two theoretically satisfactory systems to Japanese data, and find that Stone's system explains the pattern of Japanese demand far better than Houthakker's. We, thus, suggest, on a theoretical as well as empirical ground, that Stone's linear expenditure system shall be the demand system in future econometric models in Japan.</p> </abstract>
<abstract> <p>By generalizing Euler's Theorem, the classes of production functions for which scale elasticity depends either on output level alone, or on factor proportions alone, are examined.</p> </abstract>
<abstract> <p>This paper compares the simple Markov process commonly used in migration studies with an economic model of migration where interregional wage differences are the equilibrating variables. Using the economic model, it appears unlikely that regional exit and entry rates will remain stable as the population is redistributed. As a result, both theory and empirical interstate migration evidence suggest that Markov migration projections will usually understate the population changes required before stochastic equilibrium is reached.</p> </abstract>
<abstract> <p>In their paper on the three stage least squares method of estimation of a simultaneous equation system, Zellner and Theil [5] make the interesting observation that the large sample efficiency of the estimates of the parameters in the group of over-identified equations is unaffected if the three stage method is applied to this subsystem alone, ignoring all the exactly identified equations. It is shown in this paper that the estimates themselves--not just their large sample efficiency--are unaffected if one follows the above mentioned simplified procedure. This result is established for the general case of a simultaneous equation system subject to linear homogeneous a priori restrictions, whereas many other known properties of the three stage least squares method have been demonstrated only for the special case of "simple restrictions" on the structural coefficients. An error in the expression giving the estimates for the exactly identified equations in Zellner-Theil's paper is also corrected. The results of this paper have obvious significance for the problem of developing an efficient computer program for finding the three stage least squares estimates.</p> </abstract>
<abstract> <p>This article includes in an enlarged theory of equilibrium and optimality various government transactions such as production of collective goods, income transfers, direct and indirect taxes. In particular, the link between equilibrium and optimality is analysed in relation to indirect taxation. Considering the assumptions which are adopted, equilibrium does not necessarily imply optimality, but corresponds to a more restrictive concept called "private optimum." Moreover, an optimum is an equilibrium only if special assumptions are made on the nature of choices concerning social welfare goods.</p> </abstract>
<abstract> <p>Recent articles in Econometrica have questioned whether the empirical evidence from time series data supports the Keynesian hypothesis of the existence of a liquidity trap for low interest rates. This paper proposes a method by which one can obtain the maximum likelihood estimate of the minimum interest rate without imposing a priori constraints upon the parameters in the demand for money equation. Alternative estimating procedures are considered and found to yield consistent estimates of the interest rate floor.</p> </abstract>
<abstract> <p>This paper is concerned with the use of spectral analysis to analyze data generated by computer simulation experiments with models of economic systems. An example model serves to illustrate two different applications of spectral analysis. First, spectral analysis is used to construct confidence bands and to test hypotheses for the purpose of comparing the results of the use of two or more alternative economic policies. Second, spectral analysis is employed as a technique for validating an econometric model.</p> </abstract>
<abstract> <p>The permanent income hypothesis implies that people save because they rationally expect their permanent income to decline; they save "for a rainy day." It follows that saving should be at lease as good a predictor of declines in labor income as any other forecast that can be constructed from publicly available information. The paper tests this hitherto ignored implication of the permanent income hypothesis, using quarterly aggregate data for the period 1953-84 in the United States. By contrast with much of the recent literature, the results here are valid when income is stationary in first differences rather than levels.</p> </abstract>
<abstract> <p>In affiliated private value auctions, each bidder has perfect information regarding his/her own value for the object at auction, but higher values of the item for one bidder make higher values for other bidders more likely. We report on a series of experiments examining three key implications of these auctions: (i) in a first-price auction, public information about rivals' values increases expected revenue, (ii) an English auction increases expected revenue compared to a first-price auction, and (iii) a second-price auction is isomorphic to an English auction. In examining these issues, we compare predictions of some ad hoc bidding models with Nash equilibrium predictions. In the first-price auction experiments, Nash equilibrium bidding theory organizes the data better than either of two ad hoc bidding models. Public information about others' valuations does increase average revenue, but the increase in revenue is smaller and less reliable than predicted under risk neutral Nash equilibrium bidding. Lower average revenue might be attributed to risk aversion, while the high variability is attributed to a sizable frequency of individual bidding errors relative to the theory. Bidding theory precisely organizes English auction outcomes after a brief initial learning period. The dominant strategy equilibrium does not organize second-price auctions nearly as well, as market prices persistently exceed predicted prices. The difference between English and second-price outcomes is attributed to effects of different information flows, inherent in the structure of the two institutions, on eliminating bidding errors. Revenue impacts of these two institutions, relative to a first-price auction, are examined in light of observed bidding patterns.</p> </abstract>
<abstract> <p>The local power of test statistics is analyzed by extending the notion of Pitman sequences to sequences of data-generating process (DGP's) that approach the null hypothesis without necessarily satisfying the alternative. A space of probability densities is defined and endowed with the structure of an infinite-dimensional Hilbert manifold, which permits a geometrical interpretation of hypothesis testing. The three classical test statistics--LR, Wald, and LM--are shown to tend asymptotically to the same random variable under all sequences of local DGP's. The power of these statistics is seen to depend on the null, the alternative, and the sequence of DGP's in a simple and geometrically intuitive way. Moreover, for any test statistic that is asymptotically chi-squared under the null, there exists an "implicit alternative hypothesis" against which that statistic will have highest power, and which coincides with the explicit alternative for the classical test statistics.</p> </abstract>
<abstract> <p>In an extensive form game, a strategy profile is a sequential equilibrium if there are consistent beliefs at all information sets which, with the strategy profile, are sequentially rational at every information set. Along the equilibrium path, consistent beliefs are computed using the strategy profile and Bayes' rule. Consistent out-of-equilibrium beliefs, on the other hand, are derived by taking the closure of belief-strategy profile pairs, for totally mixed strategies. Kreps and Wilson (1982) claim that consistent beliefs are structurally consistent; that is, out-of-equilibrium beliefs can be rationalized by some single alternative conjecture as to the strategy opponents have used. We show by example that this is incorrect. Moreover, while equilibrium beliefs are structurally consistent in some cases, the alternative conjectures that justify them are inconsistent with the hypothesis (implicit in sequential rationality) that there will be no further defections from the initially hypothesized equilibrium strategies. These difficulties disappear if one relaxes the requirement of structural consistency, so that out-of-equilibrium conjectures may be formed as convex combinations of conjectures strategies of opponents. But such convex combinations result in out-of-equilibrium conjectures in which opponents' strategies are correlated. Correlation arises "naturally" if one adopts the perspective that no action can really have zero probability. But if one admits this sort of correlation, then other forms of correlation might also seem reasonable, including correlation that is not permitted under consistency and sequential rationality.</p> </abstract>
<abstract> <p>This paper provides two results that are useful in proving the existence of and characterizing separating equilibria in signaling games. These are games in which a privately informed agent takes an action which is observed by uniformed agents before they in turn choose actions. Depending on the equilibrium, the privately informed agent's action could reveal the private information, partially or completely. If the private information is completely revealed, the equilibrium is called separating. A key element in the analysis of separating equilibria is the examination of the implied incentive compatibility constraints (namely, that no type should have an incentive to take an action other than the prescribed equilibrium action). It is shown that these constraints together with one of two conditions imply differentiability of strategies when the set of possible types is an interval. The first condition, implied by sequentiality in many games, is an initial value condition on the informed agent's strategy. The second condition is a monotonicity condition which is similar to the familiar single crossing condition. The monotonicity condition turns out to be necessary and sufficient for there to be a strictly monotonic strategy satisfying the incentive compatibility constraints. As a direct consequence of these two results, the analysis of Milgrom and Roberts (1982) is considerably strengthened.</p> </abstract>
<abstract> <p>We propose a refinement of sequential equilibrium for extensive form games by generalizing a restriction proposed for signaling games in Cho and Kreps (1987).^2 The restriction is that beliefs must not assign positive weight to the possibilities that can be excluded through reasonable introspection based on the data available as common knowledge. A new technique is developed in order to prove the existence of forward induction equilibrium, which consists of two steps. First, we establish the generic existence of forward induction equilibrium by exploiting the results of Kohlberg and Mertens (1986). Then, we show that the forward induction equilibrium correspondence is upper hemi-continuous in the outcome space with respect to the changes of parameters of the game.</p> </abstract>
<abstract> <p>We discuss the unity between the two standard approaches to noncooperative solution concepts for games. The decision-theoretic approach starts from the assumption that the rationality of the players is common knowledge. This leads to the notion of correlated rationalizability. It is shown that correlated rationalizability is equivalent to a posteriori equilibrium--a refinement of subjective correlated equilibrium. Hence a decision-theoretic justification for the equilibrium approach to game theory is provided. An analogous equivalence result is proved between independent rationalizability, which is the appropriate concept if each player believes that the others act independently, and conditionally independent a posteriori equilibrium. A characterization of Nash equilibrium is also provided.</p> </abstract>
<abstract> <p>A general equilibrium model of markets for commodities and assets is considered. We develop the notion of a price system that admits no arbitrage opportunity, and we demonstrate the fundamental role of this concept for the theory of the existence of a competitive equilibrium. A price system admits no arbitrage opportunity for a consumer if every bundle of desired commodities has a positive market value. We prove that the existence of a price system that admits no arbitrage opportunity for all consumers is sufficient for the existence of an equilibrium.</p> </abstract>
<abstract> <p>The problem of time inconsistency arises from two different sources. First, as shown by Calvo (1978), there is an incentive for each government to engage in an initial unanticipated inflation. Second, as discussed by Lucas and Stokey (1983), there is an incentive for each government to deviate from the path of taxes announced by the preceding government. In this paper it is shown that these two sources of time inconsistency can be removed by a particular method of debt management, involving both nominal and indexed government bonds of various maturities.</p> </abstract>
<abstract> <p>We investigate the effect of opening a forward or futures market on spot price or real exchange rate variability in a two-agent, two-good, two-state general equilibrium model. We derive a linear approximation to the change in spot price variability which results from opening such a market. This is shown to depend upon such familiar parameters as substitution elasticities, marginal propensities to consume, and degrees of risk aversion. Our analysis highlights the importance of the income transfers which occur as a result of capital gains and losses in the forward market. We find that there is some presumption in favor of the view that opening a forward market reduces spot price variability. The presumption is strengthened the less risk averse are agents.</p> </abstract>
<abstract> <p>This paper characterizes all utility derived demand systems having Engel curves that are linear in both income and an arbitrary function of income. This class encompasses virtually all utility derived demand systems that have been estimated in the past using aggregate data with explicit treatment of the problem of aggregation across individuals. It includes extensions of the PIGLOG and PIGL classes that have similar properties to these classes, but allow for more general Engel curve shapes. This paper extends Gorman's study of these forms primarily by characterizing systems of rank two. The application of the characterized systems to problems of nesting, separability, flexibility, Engel curve analysis, estimation, and aggregation are briefly discussed.</p> </abstract>
<abstract> <p>This is Part I of a paper concerning an iterative decentralized process design to allocate resources optimally in decomposable environments that are possibly characterized by indivisibilities and other nonconvexities. Important steps of the process involve randomization. In Part I we present the basic models and results, together with examples showing that certain assumptions can be satisfied in both classical and nonconvex cases. Part II will go further with such examples in showing that our process yields optimal allocations in environments in which the competitive mechanism fails, as well as show how abstract conditions used in Part I can be verified in terms of properties of preferences and production functions that are familiar to economists.</p> </abstract>
<abstract> <p>This paper deals with the relationship between the monopoly price and the socially optimal price in the presence of consumption diseconomies. The above relationship is derived from characteristics of the utility function.</p> </abstract>
<abstract> <p>Linear decision rules for controlling complex systems are often obtained by matrix inversion, but transform methods offer an alternative approach that yields insights into the structure of the decision problem of maximizing expected payoffs under constraints.</p> </abstract>
<abstract> <p>This paper is a study on optimal consumption over time under the assumption of uncertainty about prices and interest rates. A stochastic model has been developed in which present wealth is either consumed or invested in a future commodity and loans. The possibility of lending and borrowing allows the model to capture some of the aspects of a monetary economy, such as the effects of actual or anticipated inflation on optimal plans. Future prices follow a Markovian process. Conditions for the existence of optimal policies are given for concave utility functions in general, but constant elasticity utility functions are studied in greater detail. The case in which the expected value of the discounted stream of utilities is infinite, is also investigated.</p> </abstract>
<abstract> <p>Recently, a partial ordering "more risky than" on the set of distribution functions has been established by several authors. The following problem is investigated in this paper: characterize the maximal elements (efficient points) in the above ordering of a given set X of random variables. For a closed and convex set X of discrete random variables we give an essentially complete characterization of the efficient random variables by means of their systems of efficiency prices.</p> </abstract>
<abstract> <p>The problem under consideration is to select the smallest majority size such that at least one social state is not defeated by any other when restrictions on possible voters' preferences are explicitly known, and when populations of arbitrary size are possible. This problem is formulated in mathematical programming terms. The special structure of the formulation is discussed, and some results concerning bounds on the majority sizes are established.</p> </abstract>
<abstract> <p>Changes in asset prices are shown to produce only substitution effects in a broad class of portfolio-choice models. Wealth effects are identically zero unless the individual's stocks of assets are subject to unanticipated changes.</p> </abstract>
<abstract> <p>The model &lt;tex-math&gt;$Y_{it}=\Sigma _{k}(\beta _{k}+\delta _{ik}+y_{tk})x_{ikt}=\varepsilon _{it}$&lt;/tex-math&gt; with &lt;tex-math&gt;$\delta _{ik}$&lt;/tex-math&gt; and &lt;tex-math&gt;$y_{tk}$&lt;/tex-math&gt; random is considered as a means of pooling the time series of a cross-section sample. The model is placed in a mixed analysis of variance framework. Relationships between various estimation criteria are derived and their asymptotic properties compared. Some implementation problems are also discussed.</p> </abstract>
<abstract> <p>This paper obtains Edgeworth or Gram-Charlier expansions for the t ratio of instrumental variable and k-class estimators, and uses them to give approximations to the confidence intervals obtained from these t ratios. These confidence intervals for large sample size are more accurate than the usual asymptotic confidence interval.</p> </abstract>
<abstract> <p>In this paper we analyze an aggregate general equilibrium model in which the use of money is motivated by a cash-in-advance constraint, applied to purchases of a subset of consumption goods. The system is subject to both real and monetary shocks, which are economy-wide and observed by all. We develop methods for verifying the existence of, characterizing, and explicitly calculating equilibria.</p> </abstract>
<abstract> <p>For every range of admissible incomes we characterize the class of Engel curves with the property that if an economy has, first, a price independent distribution of income and, second, preferences which are identical across consumers and generate Engel curves in the class, then the corresponding aggregate demand function satisfies the Weak Axiom of Revealed Preference. This class is defined by two simple conditions. The no torsion condition says that, in the relevant range of income, the Engel curve is contained in a plane through the origin (which may depend on the price vector). The uniform curvature condition says that in addition, the Engel curve is either convex or concave to the origin. We also study the nonidentical preferences case and give a sufficient condition on Engel curves (called positive association) for the Weak Axiom to be satisfied in the aggregate.</p> </abstract>
<abstract> <p>This paper uses data for a cross-section of 798 U.S. families to estimate a life-cycle consumption model that incorporates endogenous liquidity constraints. The model assumes that families maximize intertemporal utility subject to the constraint that net worth always exceed a portion of consumption expenditure (transactions balances) and a minimal amount of equity in home(s) and automobile(s). Liquidity constraints shorten the family's effective planning horizon, which extends to a time that the family expects to first encounter binding borrowing constraints. Families with short multiperiod planning horizons are liquidity constrained but do not necessarily consume in accordance with their contemporaneous disposable income. The estimated model explains over 60 per cent of the consumption variance of families in the lower 99.1 per cent of the wealth distribution. Wealthier families appear to plan substantial bequests. Liquidity constrained families are estimated to constitute 19.4 per cent of the population sampled, a group that accounts for 16.7 per cent of consumption in the population sampled. More than half of these families have short multiperiod horizons. In-sample simulations of the model suggest that a temporary tax has three to four times more impact on aggregate consumption than it would if liquidity constraints were not in effect.</p> </abstract>
<abstract> <p>We derive lower bounds on the asymptotic variances for regular distribution-free estimators of the parameters of the binary choice model and the censored regression (Tobit) model. A distribution-free (or semiparametric) estimator is one that does not require any assumption about the distribution of the stochastic error term in the model, apart from regularity conditions. For the binary choice model, we obtain an explicit lower bound for the asymptotic variance for the slope parameters, or more generally the parameters of a nonlinear regression function in the underlying latent variable model, but we find that there is no regular semiparametric estimator of the constant term (identified by requiring the error distribution to have zero median). Lower bounds are also obtained under the further assumption that the error distribution is symmetric, and in this case there is a finite lower bound for the constant term too. Comparison of the bounds with those for the classical parametric problem shows the loss of information due to lack of a priori knowledge of the functional form of the error distribution. We give the conditions for equality of the parametric and semiparametric lower bounds (in which case adaptive estimation may be possible), both with and without the assumption of a symmetric error distribution. In general, adaptive estimation is not possible, but one special case where these conditions hold is when the regression function is linear and the explanatory variables have a multivariate normal distribution. The Tobit model considered here is the censored nonlinear regression model, with a fixed censoring point. We again give an explicit lower bound for the asymptotic variance for the regression parameters, this time including a constant term (if the error term has zero median). Comparison with the corresponding lower bound for the parametric case shows that adaptive estimation is in general not possible for this model.</p> </abstract>
<abstract> <p>The purpose of this paper is to investigate testable implications of equilibrium asset pricing models. We derive a general representation for asset prices that displays the role of conditioning information. This representation is then used to examine restrictions implied by asset pricing models on the unconditional moments of asset payoffs and prices. In particular, we analyze the effect of information omission on the mean-variance frontier of one-period returns on portfolios of securities. Also, we deduce an information extension of equilibrium pricing functions that is useful in deriving restrictions on the unconditional moments of payoffs and prices.</p> </abstract>
<abstract> <p>Several partners jointly own an asset that may be traded among them. Each partner has a valuation for the asset; the valuations are known privately and drawn independently from a common probability distribution. We characterize the set of all incentive-compatible and interim-individually-rational trading mechanisms, and give a simple necessary and sufficient condition for such mechanisms to dissolve the partnership ex post efficiently. A bidding game is constructed that achieves such dissolution whenever it is possible. Despite incomplete information about the valuation of the asset, a partnership can be dissolved ex post efficiently provided no single partner owns too large a share; this contrasts with Myerson and Satterthwaite's result that ex post efficiency cannot be achieved when the asset is owned by a single party.</p> </abstract>
<abstract> <p>The interim preferences of buyers over three forms of auction are determined. The buyers are risk averse and ex ante identical, have private values, and are unaware at the time they bid of how many others are eligible to bid. The auctions compared are a second-price auction (SPA), a first-price auction (FPA), and a first-price auction conducted under a policy of revealing to the eligible bidders their actual number (FPA-R). If their types are independently distributed, the buyers prefer the SPA to the FPA-R to the FPA if they exhibit decreasing absolute risk aversion, and they are indifferent between all three auctions if they have constant absolute risk aversion. Their preferences are biased away from the SPA if their types are affiliated: they then prefer the FPA to the SPA if they exhibit constant absolute risk aversion, and the comparison is ambiguous if they exhibit decreasing absolute risk aversion. Affiliation also biases the buyer's preferences toward, and the seller's preferences away from, the revealing policy: assuming constant or decreasing or no risk aversion, the buyers prefer the FPA-R to the FPA, and the expected price is greater in the FPA than the FPA-R.</p> </abstract>
<abstract> <p>This paper studies the sequential equilibria of signaling games. It introduces a new solution concept, divine equilibrium, that refines the set of sequential equilibria by requiring that off-the-equilibrium-path beliefs satisfy an additional restriction. This restriction rules out implausible sequential equilibria in many examples. We show that divine equilibria exist by demonstrating that a sequential equilibrium that fails to be divine cannot be in a stable component. However, the stable component of signaling games is typically smaller than the set of divine equilibria. We demonstrate this fact through examples. We also present a characterization of the stable equilibria in generic signaling games.</p> </abstract>
<abstract> <p>This paper examines a market with asymmetric information where there are many signals available and where both the costs of signalling and product value may depend on many privately known characteristics. Under a weak condition on the relationship between the marginal cost of increasing the signals and the product value, a separating set exists whereby the value of every seller's product is inferred from the seller's optimal choice of signals. The separating set constructed is Pareto-dominant and corresponds to recently proposed equilibrium notions in signalling and screening models.</p> </abstract>
<abstract> <p>This paper addresses two related issues: (a) the "preference reversal" phenomenon and transitivity of preferences and (b) the observability of preference relations by experimental methods. In the first part, we adopt the framework of the theory of expected utility with rank-dependent probabilities to show that the well known "preference reversal" phenomenon can be consistent with transitive preferences. When this phenomenon is consistent with transitive preferences, it violates the von Neumann-Morgenstern independence axiom. Consequently, the "preference reversal" phenomenon may be explicated by other nonexpected utility theories. The experiments that produced the evidence on "preference reversal" attempted to elicit the certainty equivalents of given lotteries. The second part of this paper shows that the experimental design used in many of these experiments will elicit the certainty equivalents of given lotteries if and only if the respondent's preferences can be represented by expected utility functionals. Furthermore, we show that a more general class of experiments is not immune to "preference reversal" if non-expected utility preferences are admitted.</p> </abstract>
<abstract> <p>This paper proposes a simple modification of a conventional method of moments estimator for a discrete response model, replacing response probabilities that require numerical integration with estimators obtained by Monte Carlo simulation. This method of simulated moments (MSM) does not require precise estimates of these probabilities for consistency and asymptotic normality, relying instead on the law of large numbers operating across observations to control simulation error, and hence can use simulations of practical size. The method is useful for models such as high-dimensional multinomial probit (MNP), where computation has restricted applications.</p> </abstract>
<abstract> <p>A general central limit theorem is proved for estimators defined by minimization of the length of a vector-valued, random criterion function. No smoothness assumptions are imposed on the criterion function, in order that the results might apply to a broad class of simulation estimators. Complete analyses of two simulation estimators, one introduced by Pakes and the other by McFadden, illustrate the application of the general theorems.</p> </abstract>
<abstract> <p>This paper is concerned with the use of power properties of tests in econometric applications. Inverse power functions are defined. These functions are designed to yield summary measures of power that facilitate the interpretation of test results in practice. Simple approximations are introduced for the inverse power functions of Wald, likelihood ratio, Lagrange multiplier, and Hausman tests. These approximations readily convey the general qualitative features of the power of a test. Examples are provided to illustrate their usefulness in interpreting test results.</p> </abstract>
<abstract> <p>The overidentifying restrictions of the intertemporal capital asset pricing model are usually rejected when tested using data on consumption growth and asset returns, particularly when additively separable, constant relative risk utility is attributed to the representative agent. This article investigates the extent to which specification error can explain these rejections. The empirical strategy is limited information maximum likelihood in conjunction with seminonparametric (expanding parameter space) representations for both the law of motion and utility. We find that consumption growth and asset returns display conditional heterogeneity, but this fact does not account for rejection of the overidentifying restrictions as might be anticipated from the work of Hansen, Singleton, and others using generalized method of moments methods. We also find that expansion of the parameter space in the direction of nonseparable utility causes the overidentifying restrictions to be accepted. Our estimation strategy provides information on the manner in which the restrictions distort the law of motion. In particular, imposition of additively separable, constant relative risk aversion utility causes the conditional variance of consumption growth to be overpredicted, the conditional covariance of asset returns with consumption growth to be overpredicted, and an equity premium. Imposition of nonseparable seminonparametric utility causes distortion in these same directions, though the distortions are much smaller which is consistent with the outcomes of the tests of the restrictions.</p> </abstract>
<abstract> <p>A test for the ex ante efficiency of a given portfolio of assets is analyzed. The relevant statistic has a tractable small sample distribution. Its power function is derived and used to study the sensitivity of the test to the portfolio choice and to the number of assets used to determine the ex post mean-variance efficient frontier. Several intuitive interpretations of the test are provided, including a simple mean-standard deviation geometric explanation. A univariate test, equivalent to our multivariate-based method, is derived, and it suggests some useful diagnostic tools which may explain why the null hypothesis is rejected. Empirical examples suggest that the multivariate approach can lead to more appropriate conclusions than those based on traditional inference which relies on a set of dependent univariate statistics.</p> </abstract>
<abstract> <p>Multi-period decisions are decisions which determine an individual's payoffs in several periods in the future. This paper examines the theoretical foundations of the prevalent weighted average assumption. More specifically, we use a multi-period interpretation of the famous Ellsberg paradox in decision under uncertainty to show that in many cases of interest additively-separable functionals (in general) and weighted average ones (in particular) do not seem appropriate for the representation of the decision maker's preferences. We then suggest replacing the sure-thing principle, which may be used to axiomatize a weighted average functional, by a weaker version of it. Using the weakened axiom in Schmeidler's nonadditive measure model (reinterpreted for the multi-period context) yields an axiomatization of a larger class of decision rules which are representable by a weighted average of the utility in each period and the utility variation between each two consecutive periods. The weighted average assumption is a special case of the generalized model, a case in which the decision maker is variation neutral. Similarly, we define and characterize variation aversion and variation liking, and show an example of the economic implications of these properties.</p> </abstract>
<abstract> <p> We propose a new framework for games in continuous time that conforms as closely as possible to the conventional discrete-time framework. In this paper, we take the view that continuous time can be viewed as "discrete time, but with a grid that is infinitely fine." Specifically, we define a class of continuous-time strategies with the following property: when restricted to an arbitrary, increasingly fine sequence of discrete-time grids, any profile of strategies drawn from this class generates a convergent sequence of outcomes, whose limit is independent of the sequence of grids. We then define the continuous-time outcome to be this limit. Because our continuous-time model conforms so closely to the conventional discrete-time model, we can readily compare the predictions of the two frameworks. Specifically, we ask two questions. First, is discrete-time with a very find grid a good proxy for continuous time? Second, does every subgame perfect equilibrium in our model have a discrete-time analog? Our answer to the first question is the following "upper hemi-continuity" result: Suppose a sequence of discrete-time ε-subgame-perfect equilibria increasingly closely approximates (in a special sense) a given continuous-time profile, with ε converging to zero along as the period length shrinks. Then the continuous-time profile will be an exact equilibrium for the corresponding continuous-time game.Our second answer is a lower hemi-continuity result that holds under weak conditions. Fix a perfect equilibrium for a continuous-time game and a positive ε. Then for any sufficiently fine grid, there will exist an ε-subgame perfect equilibrium for the corresponding game played on that grid which "is within ε of" the continuous-time equilibrium. Our model yields sharp predictions in a variety of industrial organization applications. We first consider several variants of a familiar preemption model. Next, we analyze a stylized model of a patent race. Finally, we obtain a striking uniqueness result for a class of "repeated" games. </p> </abstract>
<abstract> <p>Monte Carlo (MC) is used to draw parameter values from a distribution defined on the structural parameter space of an equation system. Making use of the prior density, the likelihood, and Bayes' Theorem it is possible to estimate posterior moments of both structural and reduced form parameters. The MC method allows a rather liberal choice of prior distributions. The number of elementary operations to be preformed need not be an explosive function of the number of parameters involved. The method overcomes some existing difficulties of applying Bayesian methods to medium size models. The method is applied to a small scale macro model. The prior information used stems from considerations regarding short and long run behavior of the model and form extraneous observations on empirical long term ratios of economic variables. Likelihood contours for several parameter combinations are plotted, and some marginal posterior densities are assessed by MC.</p> </abstract>
<abstract> <p>In this paper an explicit and computationally convenient expansion of the exact finite sample distribution function of a quasi-maximum likelihood spectral estimator is given. In the majority of practical situations it will be necessary to estimate certain nuisance parameters of the distribution. Therefore, a method of evaluating these parameters is suggested an some Monte-Carlo evidence concerning the practical implementation of the results is given.</p> </abstract>
<abstract> <p>A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.</p> </abstract>
<abstract> <p>The widely used Cochrane-Orcutt and Hildreth-Lu procedures for estimating the parameters of a linear regression model with first-order autocorrelation typically ignore the first observation. An alternative maximum likelihood procedure which incorporates the first observation and the stationarity condition of the error process is proposed in this paper. It is similar to the Cochrane-Orcutt procedure, and appears to be at least as computationally efficient. This estimator is superior to the conventional ones on theoretical grounds, and sampling experiments suggest that it may yield substantially better estimates in some circumstances.</p> </abstract>
<abstract> <p>This paper considers procedures for testing for autocorrelation when there are missing observations on both the dependent and explanatory variables. These procedures include Durbin-Watson type tests given the vector of residuals, tests based on a set of uncorrelated residuals, and large sample likelihood ratio and Wald tests.</p> </abstract>
<abstract> <p>In empirical analysis of data consisting of repeated observations on economic units (time series on a cross section) it is often assumed that the coefficients of the quantitative variables (slopes) are the same, whereas the coefficients of the qualitative variables (intercepts or effects) vary over units or periods.This is the constant-slope variable-intercept framework. In such an analysis an explicit account should be taken of the statistical dependence that exists between the quantitative variables and the effects. It is shown that when this is done, the random effect approach and the fixed effect approach yield the same estimate for the slopes, the "within" estimate. Any matrix combination of the "within" and "between" estimates is generally biased. When the "within" estimate is subject to a relatively large error a minimum mean square error can be applied, as is generally done in regression analysis. Such an estimator is developed here from a somewhat different point of departure.</p> </abstract>
<abstract> <p>A parametric procedure for linear programming is shown to solve the bilinear complementarity problem, of which a special case is the problem of computing a competitive equilibrium of a piecewise linear economic model.</p> </abstract>
<abstract> <p>Wage and price controls in the form of inflation ceilings are introduced into a dynamic extension of a variant of the IS-LM model. The controls do not alter the location of the equilibrium; rather, they affect the path and speed of adjustment of the economy. When slack exists, controls can be useful for lowering the rate of inflation and increasing employment and aggregate demand. However, when excess demand is present, controls although reducing the excess demand pressures and lowering the inflation rate may necessitate some form of commodity rationing.</p> </abstract>
<abstract> <p>Green and Laffont have characterized certain appealing dominant-strategy revelation mechanisms as precisely the mechanisms introduced by Groves, but they have established the characterization only for unstructured sets of public alternatives: if the set has some natural structure, their proof generally requires that pathological preferences be admissible. It is shown here that the same characterization holds on sets in R^n, even when only nice preferences are admitted; this greatly extends the usefulness of the characterization.</p> </abstract>
<abstract> <p>A social choice function is exactly and strongly consistent if for each profile of true preferences of individuals it possesses a strong equilibrium point which yields the same social choice as that corresponding to the profile of true preferences. We construct exactly and strongly consistent anonymous social choice functions, and investigate various additional desirable properties of our construction.</p> </abstract>
<abstract> <p>The paper discusses certain problems raised recently by B. Peleg, about the existence of equilibria and `nice' equilibria for various classes of voting games. First it is shown that under practically every non-dictatorial voting procedure, and for every sincere preference profile, one can find a Nash equilibrium with nice properties. Secondly, a Paretian and anonymous decision procedure is constructed, under which one can always find equilibria with certain attractive properties. Lastly, it is shown that if the number of alternatives is large enough, then for some sincere preference profile, no equilibrium may exist under many decision procedures.</p> </abstract>
<abstract> <p>A choice function C on X identifies, for each subset Y of X, a set C(Y) of "best" alternatives in Y. A. computationally viable choice function is one for which a member of the choice set C(Y) can be located by a computational procedure which does not waste time, generates reasonably satisfactory intermediate alternatives, and adjusts easily as new alternatives become available. Computational viability is defined precisely and the class of computationally viable choice functions is neatly characterized. In addition, the various types of binary choice functions are distinguished in terms of computational criteria.</p> </abstract>
<abstract> <p>A generalization of the St. Petersburg paradox has led Menger to observe that utility functions must be bounded to insure existence of expected utility when probability distributions are unrestricted. It is clear that the admissible set of utility functions can be expanded as restrictions are imposed on the distribution functions under consideration. This paper provides a schema for determining the admissible utility functions for each probability distribution set defined by a minimum order requirement on the moments of the distribution.</p> </abstract>
<abstract> <p>We consider dynamic choice behavior under conditions of uncertainty, with emphasis on the timing of the resolution of uncertainty. Choice behavior in which an individual distinguishes between lotteries based on the times at which their uncertainty resolves is axiomatized and represented, thus the result is choice behavior which cannot be represented by a single cardinal utility function on the vector of payoffs. Both descriptive and normative treatments of the problem are given and are shown to be equivalent. Various specializations are provided, including an extension of "separable" utility and representation by a single cardinal utility function.</p> </abstract>
<abstract> <p>Given a known demand schedule for a mineral at each instant of time and many deposits with different known extraction costs per ton (different qualities) and different known sizes, how should exploitation be organized? How does an exogenous change in the size of deposit i or in the extraction costs per unit in deposit i affect the program of exploitation? These questions are investigated for the case of extraction costs constant per ton for deposit i. The comparative static analysis parallels that for a problem with many income classes in location theory.</p> </abstract>
<abstract> <p>The present paper analyzes cropping of a self-reproducible natural resource under competitive conditions. Situations which may lead to extermination of the resource are described. In particular, it is shown that a production function with increasing returns to scale does not exclude the possibility of extermination, as suggested in [1].</p> </abstract>
<abstract> <p>The work extends the results of an earlier paper by Hahn; it presents a model of a barter economy with a sequence of markets in which each trader operates with his own transaction technology. An equilibrium is proved to exist and the efficiency of equilibrium is discussed.</p> </abstract>
<abstract> <p>The core is the set of all unblocked allocations. Implicit in this definition is the idea that if an allocation is proposed which could be blocked, some coalition will form and issue a counterproposal which it can enforce. A process of successive counterproposals based on this idea is shown to converge in a finite period of time (amost surely) to the core.</p> </abstract>
<abstract> <p>This paper contains a relatively simple proof for the stability of Edgeworthian recontracting. It also applies that proof to a random recontracting process in an economy with a finite number of utility vectors, in which the recontracting process is a Markov chain and all non-core utility vectors correspond to transient states.</p> </abstract>
<abstract> <p>Market determination of the value in exchange (price) of money is considered in a general equilibrium finite horizon model. The possibility of the price of money being zero in equilibrium and the role of taxes (payable in money) in preventing a zero price are considered.</p> </abstract>
<abstract> <p>Assume that to each pair (x, y) of elements of the set A there corresponds probability p(x, y) that y will be chosen as having more of a certain attribute, if the pair (x, y) is presented to the subject. It is shown that if the p(x, y) satisfy a certain set of (logically independent) conditions, then there exists an (essentially unique) measure f of the attribute of elements of A, such that probabilities p(x, y) depend only on differences between f(x) and f(y).</p> </abstract>
<abstract> <p>This paper contains the formulation of the theoretical restrictions on the labor supply functions of the husband and wife in a model of family labor supply in a manner that makes them readily amenable to test and, if they are not rejected, to imposition onto the data. It also contains an application of the proposed empirical counterparts of the theoretical equations to cross-sectional data from the 1960 U.S. Census of Population.</p> </abstract>
<abstract> <p>In the theory of production it is frequently assumed that the scale elasticity is decreasing with scale. There are, however, very few empirical studies that provide support for this assumption. The results of this study, based on individual establishment data, suggest quite clearly that the scale elasticity is decreasing with scale in Norwegian mining and manufacturing.</p> </abstract>
<abstract> <p>This paper deals with the characterization and properties of "information improvement" which Theil has applied in Economics. A functional equation in three variables is formed. The solutions of this functional equation under suitable boundary conditions are defined as information-improvement functions. The "information improvement" is then defined in terms of information-improvement functions.</p> </abstract>
<abstract> <p>Necessary and sufficient conditions are determined under which a truncated approximation to generalized least squares is more efficient than ordinary least squares. For the general case, the necessary conditions are unlikely to be fulfilled. For a first order Markov model in a second order world, the sufficient conditions are satisfied only when one of the second order roots is very small, and therefore the first order assumption is approximately true. When the class of possible exogenous variables is limited to those typical of economic time series, the sufficient conditions are satisfied for a wider range of cases. Relative efficiencies are computed for a variety of cases.</p> </abstract>
<abstract> <p>This paper presents an axiomatic characterization of commodities for which the consumer faces a choice of quality rather than a choice of quantity. Properties of individual and market demand functions for commodities of differing quality levels are derived. Properties of comparative static price changes in response to supply changes at one or more quality levels are also developed. The analysis is then applied to price changes in housing markets.</p> </abstract>
<abstract> <p>This paper establishes certain conditions for the validity of Nagar's approximations to the mean and second moments of econometric estimations, where the estimators are rational functions of the OLS estimator of the reduced form coefficients, and of the corresponding estimate of the equation error variance matrix. The criteria depend upon the existence, and asymptotic orders of magnitude, of the moments of the estimators.</p> </abstract>
<abstract> <p>This paper is concerned with the problem of estimating demand and supply schedules in disequilibrium markets. The results of Fair and Jaffee are expanded in three ways. (1) Their directional method I is modified to yield consistent estimates. (2) A maximum likelihood alternative to their quantitative method is proposed. (3) The price equation is generalized to be a multivariate, stochastic function, and a method is proposed for estimating demand and supply schedules in this case.</p> </abstract>
<abstract> <p>This paper presents a method for obtaining the large sample distribution of the mean deviation and related measures of income inequality.</p> </abstract>
<abstract> <p>We model an oligopoly facing uncertain demand in which each firm chooses as its strategy a "supply function" relating its quantity to its price. Such a strategy allows a firm to adapt better to the uncertain environment than either setting a fixed price or setting a fixed quantity; commitment to a supply function may be accomplished in practice by the choice of the firm's size and structure, its culture and values, and the incentive systems and decision rules for its employees. In the absence of uncertainty, there exists an enormous multiplicity of equilibria in supply functions, but uncertainty, by forcing each firm's supply function to be optimal against a range of possible residual demand curves, dramatically reduces the set of equilibria. Under uncertainty, we prove the existence of a Nash equilibrium in supply functions for a symmetric oligopoly producing a homogeneous good and give sufficient conditions for uniqueness. We perform comparative statics with respect to firms' costs, the industry demand, the nature of the demand uncertainty, and the number of firms, and sketch the extension to differentiated products. Firms' equilibrium supply functions are steeper with marginal cost curves that are steeper relative to demand, fewer firms, more highly differentiated products, and demand uncertainty that is relatively greater at higher prices. The steeper are the supply functions firms choose in equilibrium, the more closely competition resembles the Cournot model (which exogenously imposes vertical supply functions--fixed quantities); with flatter equilibrium supply functions, competition is closer to the Bertrand model (which exogenously imposes horizontal supply functions--fixed prices).</p> </abstract>
<abstract> <p>The paper provides conditions on the primitives of a continuous-time economy under which there exist equilibria obeying the Consumption-Based Capital Asset Pricing Model (CCAPM). The paper also extends the equilibrium characterization of interest rates of Cox, Ingersoll, and Ross (1985) to multi-agent economies. We do not use a Markovian state assumption.</p> </abstract>
<abstract> <p>The problem considered is inference in a simple errors-in-variables model where consistent estimation is impossible without introducing additional exact prior information. The probabilistic prior information required for Bayesian analysis is found to be surprisingly light: despite the model's lack of identification a proper posterior is guaranteed for any bounded prior density, including those representing improper priors. This result is illustrated with the improper uniform prior, which implies marginal posterior densities obtainable by integrating the likelihood function; surprisingly, the posterior mode for the regression slope is the usual least squares estimate.</p> </abstract>
<abstract> <p>Methods for the systematic application of Monte Carlo integration with importance sampling to Bayesian inference in econometric models are developed. Conditions under which the numerical approximation of a posterior moment converges almost surely to the true value as the number of Monte Carlo replications increases, and the numerical accuracy of this approximation may be assessed reliably, are set forth. Methods for the analytical verification of these conditions are discussed. Importance sampling densities are derived from multivariate normal of Student t approximations to local behavior of the posterior density at its mode. These densities are modified by automatic rescaling along each axis. The concept of relative numerical efficiency is introduced to evaluate the adequacy of a chosen importance sampling density. The practical procedures based on these innovations are illustrated in two different models.</p> </abstract>
<abstract> <p>Properties of t ratios associated with the LIML, TSLS, and OLS estimators in a structural form estimation are studied in this paper. The existence of moments of these t ratios including the LIML form is proved first. Second, Monte Carlo simulations are performed to find out real sizes of the t test and the likelihood ratio test. Third, asymptotic expansions of the distributions of t ratios are derived under the null hypothesis to find out deviations of real sizes from nominal sizes theoretically. The asymptotic power functions are also derived, and t ratios associated with the LIML and TSLS estimators are proved asymptotically as powerful as the likelihood ratio test.</p> </abstract>
<abstract> <p>We consider the null hypothesis that a time series has a unit root with possibly nonzero drift against the alternative that the process is "trend-stationary." The interest is that we allow under both the null and alternative hypotheses for the presence of a one-time change in the level or in the slope of the trend function. We show how standard tests of the unit root hypothesis against trend stationary alternatives cannot reject the unit root hypothesis if the true data generating mechanism is that of stationary fluctuations around a trend function which contains a one-time break. This holds even asymptotically. We derive test statistics which allow us to distinguish the two hypotheses when a break is present. Their limiting distribution is established and selected percentage points are tabulated. We apply these tests to the Nelson-Plosser data set and to the postwar quarterly real GNP series. In the former, the break is due to the 1929 crash and takes the form of a sudden change in the level of the series. For 11 out of the 14 series analyzed by Nelson and Plosser we can reject at a high confidence level the unit root hypothesis. In the case of the postwar quarterly real GNP series, the break in the trend function occurs at the time of the oil price shock (1973) and takes the form of a change in the slope. Here again we can reject the null hypothesis of a unit root. If one is ready to postulate that the 1929 crash and the slowdown in growth after 1973 are not realizations of an underlying time-invariant stochastic process but can be modeled as exogenous, then the conclusion is that most macroeconomic time series are not characterized by the presence of a unit root. Fluctuations are indeed stationary around a deterministic trend function. The only "shocks" which have had persistent effects are the 1929 crash and the 1973 oil price shock.</p> </abstract>
<abstract> <p>This paper gives a solution to the problem of estimating coefficients of index models, through the estimation of the density-weighted average derivative of a general regression function. We show how a normalized version of the density-weighted average derivatives can be estimated by certain linear instrumental variables coefficients. Both of the estimators are computationally simple, root-N-consistent and asymptotically normal; their statistical properties do not rely on functional form assumptions on the regression function or the distribution of the data. The estimators, based on sample analogues of the product moment representation of the average derivative, are constructed using nonparametric kernel estimators of the density of the regressors. Asymptotic normality is established using extensions of classical U-statistic theorems, and asymptotic bias is reduced through use of a higher-order kernel. Consistent estimators of the asymptotic variance-covariance matrices of the estimators are given, and a limited Monte Carlo simulation is used to study the practical performance of the procedures.</p> </abstract>
<abstract> <p>It is suggested that the most natural mathematical model for a market with "perfect competition" is one in which there is a continuum of traders (like the continuum of points on a line). It is shown that the core of such a market coincides with the set of its "equilibrium allocations," i.e., allocations which constitute a competitive equilibrium when combined with an appropriate price structure.</p> </abstract>
<abstract> <p>A recent contribution to estimation of the coefficients of a system of simultaneous linear equations is the three-stage least-squares procedure of Zellner and Theil. This paper points out the relationship between this procedure and the method of instrumental variables, thus simplifying the exposition of the procedure. It further shows that the procedure yields estimates which are asymptotically at least as efficient as two-stage least-squares estimates, and that iteration of the procedure does not improve its asymptotic efficiency.</p> </abstract>
<abstract> <p>The asymptotic covariance matrix of the full-information maximum-likelihood estimator is derived. The paper then compares the asymptotic efficiency of three estimators: full-information maximum likelihood, three-stage least squares, and linearized maximum likelihood. It is shown that all three are efficient if the covariance matrix of the contemporaneous structural disturbances is unknown. If, however, some elements of this covariance matrix are known a priori, then three-stage least squares is no longer efficient.</p> </abstract>
<abstract> <p>This paper proves in the context of maximum likelihood estimation of linear stochastic models of the Cowles Commission type [2], that if the model is fully identified and stable and the error variance matrix unrestricted, three-stage least-squares estimates differ asymptotically from full maximum likelihood estimates by order 1/T, where T is the number of time periods. When the full maximum likelihood estimates are best asymptotic normal so are the three-stage least-squares estimates.</p> </abstract>
<abstract> <p>This paper extends an earlier study by one of the authors. A set of postulates concerning a utility function of an infinite consumption program implies the existence of a utility scale such that postponement of each of two programs by the same time delay cannot increase, and generally diminishes, the difference of their utilities. This property of "time perspective" allows previous results concerning "impatience" to be extended and generalized.</p> </abstract>
<abstract> <p>This paper studies the property of the dynamic paths of a model in which neutral technical progress in Harrod's sense exists and in which the labor efficiency of newly installed capital goods increases at an exogenously given constant rate. The relative stability of a balanced growth path is shown.</p> </abstract>
<abstract> <p>This paper concerns utility functions for money. A measure of risk aversion in the small, the risk premium or insurance premium for an arbitrary risk, and a natural concept of decreasing risk aversion are discussed and related to one another. Risks are also considered as a proportion of total assets.</p> </abstract>
<abstract> <p>This paper presents a model in which (1) the market demand for urban automobile travel is a function of a time-price as well as a money-price and (2) the market supply is represented by a flow function that is derived from assumed relationships between traffic density and average speed. Two qualitatively different types of traffic congestion are identified. Marginal cost pricing in terms of both time and money taxes is proposed as an efficient and feasible means of controlling both types of traffic congestion. Using the results of existing empirical studies, tax schedules for three types of urban roads are computed.</p> </abstract>
<abstract> <p>This paper strengthens the Radner turnpike theorem by examining, under a few reasonable additional assumptions, the mode of the convergence of efficient paths to the von Nuemann ray. Continual proximity to the turnpike is established for long-term efficient paths of economic growth except in a common number of initial and final consecutive periods. This result consolidates the Radner theorem and thereby confirms what the originators of the turnpike proposition undoubtedly had in mind.</p> </abstract>
<abstract> <p>This paper discusses two forecasting experiments involving models of the water-melon market. The first experiment compares the forecasts of an interdependent model estimated by limited information, single equation with those of a model using least squares reduced form. The second experiment compares the forecasts of the interdependent model with those of a causal chain model. It is found that the forecasts of the interdependent model are generally better than those of the alternative models.</p> </abstract>
<abstract> <p>Mixed estimation, in contrast to pure estimation, in regression analysis was proposed sometime ago by Theil and Goldberger. More recently Theil has proposed an f-class of these estimates. In this paper we analyze the bias to order 1/T, T being the number of observations, and the moment matrix to order 1/T^2 of a member of the f-class.</p> </abstract>
<abstract> <p>The purpose of this note is to demonstrate the equivalence of indirect least squares and Hoch's generalized method of estimating the production function parameters. It is further shown that these estimates are, under appropriate assumptions, maximum likelihood estimates, and their asymptotic properties are noted.</p> </abstract>
<abstract> <p>Turning now to the Friedman theory of consumption, there is a basic assumption which is fundamental to this theory which must be challenged. This is that the utility function can be assumed to be homogeneous in the value of consumption at different dates. For this to be true for all possible sets of prices for the different commodities it would be necessary for the utility function to be completely homogeneous in the quantities of all the goods consumed. And, if this were true, then a change in the level of income would produce proportionate changes in the consumption of all commodities. This is so contrary to common sense and the results of empirical research that the assumption of complete homogeneity must be dismissed. It follows that Friedman's basic assumption of the homogeneity of the utility function considered as a function of the total value of consumption at different periods could only hold true for certain special sets of prices. And it seems a priori rather unlikely that one of these special sets of prices should happen to coincide with the actual and expected market prices.</p> </abstract>
<abstract> <p>This paper concerns itself with the following problems in univariate multiple regression analysis: (1) least squares estimation of the regression coefficients when these are assumed to be subject to a set of linear restrictions; (2) testing one set of linear restrictions when another (possibly vacuous) set of linear restrictions is assumed to hold. In (2) it is assumed that the true residuals are normally distributed, and in both cases it is assumed that the independent variables are fixed variates, and that the variance-covariance matrix of the dependent variable is known in any sample up to multiplication by an unknown scalar. The formulas for the estimators and test statistics are given in terms of the original variables, and a simple proof is provided of the unbiasedness of the F test. Extensive use is made of the properties of idempotent matrices, and geometric interpretations of the results are briefly described.</p> </abstract>
<abstract> <p>Theorem B of [1] is false as it stands (Section 1). It is true if the preference order is defined by a finite number of linear functions (Section 2) or alternatively, if the archimidean assumption [1, (1.2)] is replaced by either of the stronger forms [1, (4.1)] or [1, (4.2)] (Section 3). A corresponding correction must be made for Theorem C of [1] (Section 4).</p> </abstract>
<abstract> <p>A single long-run player plays a simultaneous-move stage game against a sequence of opponents who play only once, but observe all previous play. Let the "Stackelberg strategy" be the pure strategy to which the long-run player would most like to commit himself. If there is positive prior probability that the long-run player will always play the Stackelberg strategy, then his payoff in any Nash equilibrium exceeds a bound that converges to the Stackelberg payoff as his discount factor approaches one. When the stage game is not simultaneous move, this result must be modified to account for the possibility that distinct strategies of the long-run player are observationally equivalent.</p> </abstract>
<abstract> <p>I analyze and estimate an extended life cycle model of consumption in which utility depends on the path of consumption and bequests. The theoretical section gives conditions under which consumption and wealth will decline with age. An important determinant is a boundary condition on the consumption path caused by annuities. Using panel data from the Retirement History Survey on the wealth of the retired elderly, I estimate the parameters of the model, which are the degree of mortality risk aversion, the subjective time rate of discount, and the marginal utility of bequests. The estimation method involves solving for the optimal consumption trajectory from the first-order conditions of the dynamic optimization, and from the boundary conditions. The results indicate that the consumption path is sensitive to variations in mortality rates, meaning that mortality risk aversion is moderate, and certainly much smaller than what is typically assumed in the literature. The marginal utility of bequests is small; therefore, desired bequests, which are estimated from model simulations, are small on average. Apparently most bequests are accidental, the result of uncertainty about the date of death. The parameter estimates imply that although consumption and wealth paths may rise at early ages, eventually they will fall as mortality rates become large. Falling wealth with age is confirmed in the raw data: average wealth holdings of the elderly decline with age, and the majority of individuals dissave as they age.</p> </abstract>
<abstract> <p>In this paper, we examine the consequences of adopting the random utility hypothesis as an approach for randomizing a system of demand equations. Random utility models are appealing since they allow the usual assumption of deterministic utility maximizing behavior by each consumer to co-exist with the apparent randomness across individuals which is exhibited by data. Our results show that the use of random utility models implies that the disturbances of the demand equations may not be homoskedastic but must be functions of prices and/or income. If the demand system is generated by random utility maximization, then empirical studies of demand which have assumed homeskedastic disturbances will suffer the usual inferential difficulties. A possible explanation for the common rejection of demand theory axioms in applied work, therefore, is provided by the fact that most previous demand studies have, in fact, assumed homoskedasticity. A sampling experiment provides some evidence that ignoring the heteroskedasticity leads to an incorrect test size for a test of symmetry. Heteroskedastic-consistent tests are compared to the usual tests. At least one is found to work reasonably well in small samples in the presence of heteroskedasticity.</p> </abstract>
<abstract> <p>The past decade has seen the econometric implementation of macroeconomic multi-market fix-price models for a number of European countries. The procedure in use, the full information maximum likelihood (FIML) method, unfortunately becomes very cumbersome and seems out of reach when additional features are incorporated in the model (disaggregation into micro markets, opinion surveys,...). One purpose of the present work is to prove the fruitfulness of the following estimation strategy: use Monte-Carlo simulations to compute the first and second order moments of the endogenous variables, and maximize a resulting pseudo likelihood function to estimate the parameters. We first describe the PML method in the context of the so-called canonical disequilibrium model. We then apply it to a small aggregated macroeconomic model previously studied under FIML by Artus, Avouyi-Dovi, Laroque (1985), where we allow for a disaggregation into micro markets. The results we obtain are strikingly similar to theirs. This both demonstrates in this example the robustness of the FIML procedure to the introduction of micro markets and stresses the usefulness of the PML method in obtaining reliable estimates at a smaller cost.</p> </abstract>
<abstract> <p>This paper deals with the problem of aggregation where the focus of the analysis is whether to predict aggregate variables using macro or micro equations. The Grunfeld-Griliches prediction criterion for choosing between aggregate and disaggregate equations is generalized to allow for contemporaneous covariances between the disturbances of micro equations and the possibility of different parametric restrictions on the equations of the disaggregate model. A new test is proposed of the hypothesis of `perfect aggregation' which tests the validity of aggregation either through coefficient equality or through the stability over time of the composition of the regressors across the micro units. The tools developed in the paper are then applied to employment demand functions for the UK economy disaggregated by 40 industries. Firstly a set of unrestricted log-linear dynamic specifications are estimated for the disaggregate equations and then linear parameter restrictions are imposed as appropriate. Corresponding unrestricted and restricted aggregate equations are estimated. Two different levels of aggregation are considered: aggregation over the 23 manufacturing industries and aggregation over all 40 industries of the economy. In both cases the hypothesis of perfect aggregation is firmly rejected. For the manufacturing industries the prediction criterion marginally favors the aggregate equation but over all industries the disaggregated equations are strongly preferred.</p> </abstract>
<abstract> <p> In this paper we consider how boundedly rational agents learn rational expectations. The assumption that agents are boundedly rational is made operational by imposing computability constraints on the economy: all equilibrium price functions or forecasts of future equilibrium prices are required to be computable. Computable functions are defined, as in the computer science literature, as functions whose values can be calculated using some finite algorithm. The paper examines two learning environments. In the first, agents have perfect information about the state of nature. In this case, the theory of machine inference can be applied to show that there is a broad class of computable economies whose rational expectations equilibria can be learned by inductive inference. In the second environment, agents do not have perfect information about the state of nature. In this case, a version of Gödel's incompleteness theorem applicable to the theory of computable functions yields the conclusion that rational expectations equilibria cannot be learned. </p> </abstract>
<abstract> <p> This paper presents a model of the business cycle with perfect foresight where the mere presence of inventories is responsible for the appearance of the cycle. The basic assumption of the model is that the price system does not adjust instantaneously to its competitive value. Then inventory holding destabilizes the tâtonnement dynamics and creates the cycle. A Wicksellian cumulative process generates both the booms, where the real rate of return on cash is smaller than the natural rate of interest obtained by the inventory holders, and the recessions, where inventories are dominated by money balances. </p> </abstract>
<abstract> <p>This paper develops a class of recursive, but not necessarily expected utility, preferences over intertemporal consumption lotteries. An important feature of these general preferences is that they permit risk attitudes to be disentangled from the degree of intertemporal substitutability. Moreover, in an infinite horizon, representative agent context these preference specifications lead to a model of asset returns in which appropriate versions of both the atemporal CAPM and the intertemporal consumption-CAPM are nested as special cases. In our general model, systematic risk of an asset is determined by covariance with both the return to the market portfolio and consumption growth, while in each of the existing models only one of these factors plays a role. This result is achieved despite the homotheticity of preferences and the separability of consumption and portfolio decisions. Two other auxiliary analytical contributions which are of independent interest are the proofs of (i) the existence of recursive intertemporal utility functions, and (ii) the existence of optima to corresponding optimization problems. In proving (i), it is necessary to define a suitable domain for utility functions. This is achieved by extending the formulation of the space of temporal lotteries in Kreps and Porteus (1978) to an infinite horizon framework. A final contribution is the integration into a temporal setting of a broad class of a temporal non-expected utility theories. For homogeneous members of the class due to Chew (1985) and Dekel (1986), the corresponding intertemporal asset pricing model is derived.</p> </abstract>
<abstract> <p> This paper investigates various properties of a two-sector putty-clay model in which physical durability of capital as well as its economic lifetime is a choice variable. Our model adopts Wicksell-Åkerman's hypothesis that the production of capital goods of greater durability can be accomplished only by incurring greater costs per unit of capital produced. Thus it is a synthesis of Wicksell's durability and putty-clay models. The case in which all capital goods are infinitely durable and are scrapped solely by obsolescence (i.e., pure obsolescence case) will appear as a special case of our model. </p> </abstract>
<abstract> <p>When a continuous time model is estimated from its non-recursive discrete approximation, the presence of identities and exogenous variables in the system does not preclude the use of standard procedures. However, if we wish to use the exact discrete model for estimation purposes, the treatment of identities and exogenous variables is not so staightforward. It is found that the procedure based on the exact discrete model is unlikely to be affected by the presence of identities, but when exogenous variables occur in the system some sort of approximation is usually necessary before the model can be estimated with discrete data. An approximate model is constructed to deal with the latter case and the asymptotic properties of estimators derived from this model are investigated.</p> </abstract>
<abstract> <p>The empirical distribution functions of the least squares estimators and test statistics in a system of stochastic difference equation are studied. The general conclusion is that the empirical distributions cannot be closely approximated using the normal distribution theory.</p> </abstract>
<abstract> <p>The paper compares the power functions of the conventional tests of significance based on asymptotic theory with those of some alternative tests suggested by Dhrymes and Richardson and Rohr and also a test that was suggested earlier by Anderson and Rubin in a simple two equation model by means of Monte Carlo experiments.</p> </abstract>
<abstract> <p> A family of tests of significance is developed for coefficients in a single equation of a simultaneous system. Different members of this family are distinguished by the k-class estimator on which they are based, and on the alternative hypothesis against which they test. The size of the test is found when the disturbances are small, and the test is shown to be consistent if plim k = 1 or k = λ, the limited information maximum likelihood value. </p> </abstract>
<abstract> <p>In the first section of this paper the overidentifying restrictions on a system of linear simultaneous equations are expressed in terms of restrictions on the reduced form parameters. These restrictions provide the basis of a test of the structure using only the unrestricted reduced form parameter estimates. Under H&lt;sub&gt;0&lt;/sub&gt; the test proposed is asymptotically equivalent to a likelihood ratio test. The test may be applied as a single equation or complete system procedure and it may be presented as either a χ &lt;sup&gt;2&lt;/sup&gt; or an F statistic. The case is also made here for system overidentification tests rather than single equation procedures, the arguments being drawn from the statistical literature on hypothesis testing by induction. The computational advantages of the present proposals are substantial when compared to FIML based likelihood ratio tests and Monte Carlo experiments confirm that a system version of the test performs well in large samples. The system version of the test behaves like the FIML likelihood ratio test in large sample situations both under H&lt;sub&gt;0&lt;/sub&gt; and H&lt;sub&gt;1&lt;/sub&gt;. However, the Monte Carlo studies indicate that both the single equation and system versions of the test perform poorly in small samples.</p> </abstract>
<abstract> <p>This paper investigates the possibility of arriving at the mixed-strategy solution of a zero-sum two-person game through an iterative learning process. Learning takes place during repeated play of the game, in which the players have no direct knowledge of the payoff matrix but are allowed to record what happens during play. In this context, all members of a wide class of behaviorally plausible learning mechanisms are shown to be locally unstable for "almost all" zero-sum two-person games with mixed-strategy solutions.</p> </abstract>
<abstract> <p>The existence and location of "best" decisions under majority rule is examined under a variety of symmetry and indifference contour assumptions. These contours have properties which make them at least as important as the conventional Euclidean contours.</p> </abstract>
<abstract> <p>A consistent theory of demand is possible without the transitivity axiom. Here it is shown that a class of nontransitive orderings can be represented by a continuous numerical function, in such a way that an individual's demand function may be found by solving a constrained maximum problem.</p> </abstract>
<abstract> <p>It is the purpose of this paper to show first that in a competitive system satisfying a generalized Walras' law all principal minors of order less than n of the Jacobian determinant of the system of excess supply functions will ordinarily change sign somewhere on the domain of all positive prices whether demand is bounded or not. Conditions for uniqueness of equilibrium are proposed for the case of bounded demand which allow minors of the Jacobian to change signs. Unbounded demand was treated in Part I. Existence of equilibrium is proved under a generalized Walras law.</p> </abstract>
<abstract> <p> Cette note discute les conditions d'existence d'un noyau non vide dans une économie de propriété privée où des possibilités de production sont associées à chaque coalition en fonction de sa composition. On étudie les relations entre noyau et équilibre concurrentiel dans le cas particulier où il existe un ensemble d'entreprises et où l'ensemble de production d'une coalition est égal à la somme des ensembles de production des entreprises dont les membres de la coalition possèdent une majorité des parts. </p> </abstract>
<abstract> <p>This paper proposes a method for measuring the numerical value of the conjectural variation which has been a key concept in oligopoly theories. The statistical property of the estimator for the conjectural variation is examined and two statistical tests are presented. One is a test for the hypothesis that the conjectural variation is a specified value. The other is designed for the hypothesis that there is a certain type of collusion among oligopoly firms. These are applied to the Japanese flat glass industry.</p> </abstract>
<abstract> <p>In this paper we construct a model that describes the behavior of the foreign exchange market and Exchange Fund. Cross-spectral and regression analysis of daily data are used to show that official intervention contributed significantly to the short-run stability of Canadian exchange rates.</p> </abstract>
<abstract> <p>This paper is concerned with general equilibrium under a weakened version of Walras' Law. Conditions for uniqueness of equilibrium are derived which include the more standard conditions as a special case and which are at the same time free of some of the weaknesses of received theory.</p> </abstract>
<abstract> <p>How should rule-of-thumb priorities be assigned for the rationing of intermediate goods in an economy marked by shortages and non-scarcity prices? A method is developed for setting these priorities in an optimal way, and the method is applied to Soviet data as an illustration.</p> </abstract>
<abstract> <p>The purposes of this paper are to establish propositions about the behavior of the supply curve of labor to the individual firm and to estimate the distribution of the elasticities of this supply curve by firm. The main statistical problem faced is the possibility that labor quality increases with firm size so that one does not know how to interpret the relationship of firm size and measured wages. My solution to that problem is to look at the relationship of wages to firm size relative to population density, since I expect the "quality" and the labor supply effects to differ markedly in this respect.</p> </abstract>
<abstract> <p>An intertemporal model for the capital market is deduced from the portfolio selection behavior by an arbitrary number of investors who act so as to maximize the expected utility of lifetime consumption and who can trade continuously in time. Explicit demand functions for assets are derived, and it is shown that, unlike the one-period model, current demands are affected by the possibility of uncertain changes in future investment opportunities. After aggregating demands and requiring market clearing, the equilibrium relationships among expected returns are derived, and contrary to the classical capital asset pricing model, expected returns on risky assets may differ from the riskless rate even when they have no systematic or market risk.</p> </abstract>
<abstract> <p>This paper simplifies and extends some previous work on domain restriction. With the help of an easy arithmetical result, two types of necessary and sufficient conditions are established for transitivity and quasi-transitivity under majority rule and transitivity under non-minority rule.</p> </abstract>
<abstract> <p>The economic theory of individual choice most frequently assumes that individual preferences are weak orders; this implies, among other things, a virtually perfect discriminating power on the part of individual decision makers. R. D. Luce's theory of semiorders generalizes the weak order concept to allow imperfect discrimination when choices are close. This paper examines the demand implications of the semiorder axioms and states conditions on demand that are necessary and sufficient for the revealed ordering to be a semiorder.</p> </abstract>
<abstract> <p>This paper is concerned with showing differentiability and measure theoretic properties on demand functions. The main results are roughly as follows. (i) Demand is differentiable and the Slutsky equation holds for almost all prices if demand satisfies a Lipschitz condition in income (except possibly for a closed cone of prices of measure zero) and utility is concave. This includes the homothetic case which is given special attention in Section 4. (ii) For the Slutsky equation to indicate demand behavior in the large, it is sufficient that along any given indifference curve the ratio of changes in price to changes in quantity be bounded from zero (Section 5). (iii) Even with almost everywhere differentiable demand derived from continuously differentiable utility, most change in demand does not necessarily take place where the Slutsky equation is valid (Section 5). (iv) By way of proof of Theorems 1-3, it is shown that the maximand in a Lagrange problem is differentiable under appropriate conditions on the function being maximized (Theorem 6, Section 3, and Appendix). (v) For preferences as in (ii) and for almost all wealths, equilibrium is locally unique (Section 6).</p> </abstract>
<abstract> <p>Two principal questions are treated: (i) Which equilibrium conditions (or, which types of factor demand equations) based on the neoclassical single-capital-good model are unchanged when there are many different capital goods, only one of which is used at any one time? (ii) Are there any "pseudo" productions functions (of "capital" and labor) which correctly describe behavior by profit-rate maximizing entrepreneurs in the many-capital-goods model? A new surrogate production model is developed to resolve these questions. It is shown, inter alia, that if one wishes to predict changes in labor and value capital requirements in response to changes in factor prices, the neoclassical marginal rate of substitution relationship can be justified as the basis for the econometric specification.</p> </abstract>
<abstract> <p>In most empirical work, the investigator's understanding of the economic process under study is only minimally reflected in the econometric methodology. This paper suggests that in many cases, the construction of a small-scale simulation can "prepare" the data for regression in a manner which takes cognizance of the theory of the process. Regression is then used to scale the output of the simulation up to observed magnitudes of the variable to be predicted. The simulation has the function of exploring for the nature of the nonlinearities and interactions and thus replaces the usual search for a form which maximizes R^2. The simulation may also be helpful where colinear data are a problem. An example is presented in which the effects of wages, unemployment rates, and labor turnover on poverty are studied through a "prepared" regression.</p> </abstract>
<abstract> <p>It is known that prices only cannot usually be utilized to coordinate a linear economic system. This paper considers a linear economic system, formally represented as a linear programming model which is interpreted as a resource-allocation problem. An algorithm founded on the idea of associating with each resource a linearly increasing price schedule rather than a constant price is developed. The paper hence demonstrates that a mechanism rather similar to a pure price mechanism can be used both to find and sustain an optimal allocation of resources in a linear economic system.</p> </abstract>
<abstract> <p>Using the result that under the null hypothesis of no misspecification an asymptotically efficient estimator must have zero asymptotic covariance with its difference from a consistent but asymptotically inefficient estimator, specification tests are devised for a number of model specifications in econometrics. Local power is calculated for small departures from the null hypothesis. An instrumental variable test as well as tests for a time series cross section model and the simultaneous equation model are presented. An empirical model provides evidence that unobserved individual factors are present which are not orthogonal to the included right-hand-side variable in a common econometric specification of an individual wage equation.</p> </abstract>
<abstract> <p>In recent years more and more emphasis has been placed on model discrimination procedures. In this paper we propose some new procedures for the selection of the most adequate regression model. Properties of those procedures are analyzed and compared. Their relationship with classical informal procedures is fully discussed. Our procedures are called the information criteria because we base our loss function on the Kullback-Leibler information measure of the distance between two probability density functions. The basic framework of our approach was originated by Akaike in his sequence of papers.</p> </abstract>
<abstract> <p>Since dynamic regression equations are often obtained from rational distributed lag models and include several lagged values of the dependent variable as regressors, high order serial correlation in the disturbances is frequently a more plausible alternative to the assumption of serial independence than the usual first order autoregressive error model. The purpose of this paper is to examine the problem of testing against general autoregressive and moving average error processes. The Lagrange multiplier approach is adopted and it is shown that the test against the nth order autoregressive error model is exactly the same as the test against the nth order moving average alternative. Some comments are made on the treatment of serial correlation.</p> </abstract>
<abstract> <p>There has been increasing concern recently over the use of the simple first order Markov form to model error autocorrelation in regression analysis. The consequence of misspecifying the error model will be especially serious when the regressors include lagged values of the dependent variable. The purpose of this paper is to develop Lagrange multiplier tests of the assumed error model against specified ARMA alternatives. It is shown that all of the tests can be regarded as asymptotic tests of the significance of a coefficient of determination, and a table is provided which gives details of two general tests and several special cases.</p> </abstract>
<abstract> <p>A class of parametric tests for heteroscedasticity in linear models is discussed. For models with nonstochastic regressors, new exact tests within this class are suggested which utilize existing tables of the distribution of the von Neumann ratio and of the Durbin-Watson bounding ratios. "Bound tests" for heteroscedasticity in least squares regression are proposed. A rigorous treatment of tests within this class for heteroscedasticity in the errors of structural relations in dynamic simultaneous equations models is provided.</p> </abstract>
<abstract> <p>This paper provides necessary and sufficient conditions for it to be optimal to base decisions on estimates of the parameters that characterize a decision problem (e.g., profit maximization with an estimated price elasticity of demand). We show that the separation of parameter estimation from decision making generally yields lower utility than an integrated approach which takes account of estimation uncertainty. We evaluate the decision bias in the parameter estimation method and show that the resulting utility loss can be substantial.</p> </abstract>
<abstract> <p>We start with a study of the piece-wise linear function determining how the maximum of the objective varies with the right-hand members of the constraints. Generation of this function needs dual prices. The latter being used only in their validity sets, the two proposed procedures avoid the main difficulty to which the Kornai-Liptak procedure is subject. Basically, the subsystems propose at each step new production processes out of various projects which are often not well specified. The central agency sends prices or resource allocations.</p> </abstract>
<abstract> <p>We study the time consistency of optimal monetary policy in a framework akin to the one in [12, Ch. 1] but we assume away lump sum taxation--all taxes are distortionary. Our major result is that under perfect foresight (as defined in [8, 23]) optimal monetary policy is bound to be time inconsistent. The paper is closely related to the previous works of Auernheimer [2], and Kydland and Prescott [15].</p> </abstract>
<abstract> <p>This paper is a theoretical examination of the stochastic behavior of equilibrium asset prices in a one-good, pure exchange economy with identical consumers. A general method of constructing equilibrium prices is developed and applied to a series of examples.</p> </abstract>
<abstract> <p>Are there non-dictatorial outcome functions (game forms) such that (1) for every preference profile there exists a Nash equilibrium, and (2) for every profile, every Nash equilibrium outcome is Pareto optimal? For more than two persons, if indifferences are ruled out or if only "weak" Pareto optimality is required, the answer is shown to be in the affirmative. For two persons it is in the negative. Among other questions explored are voting system interpretations and symmetry properties of outcome functions.</p> </abstract>
<abstract> <p>It is demonstrated that, under regularity assumptions on individuals' preferences, for an open dense set of exchange economies indexed by initial endowments, the core does not possess the equal treatment property. The assumptions made on individuals' preferences are subsequently shown to characterize an open dense subject of the space of preferences.</p> </abstract>
<abstract> <p>We give an elementary statement and proof of a core equivalence theorem.</p> </abstract>
<abstract> <p>This survey of the use of structural equation models and methods by social scientists emphasizes the treatment of unobservable variables and attempts to redress economists' neglect of the work of Sewall Wright.</p> </abstract>
<abstract> <p>It is shown that every voting procedure determines a partition of the set of all voters into separate committees, the latter forming a hierarchical structure.</p> </abstract>
<abstract> <p>It is now popular to construct economic models in differential equation form. Perhaps the most serious econometric problem faced when dealing with a differential equation system is the practical difficulty of finding consistent estimates of the important structural parameters. In this paper a simple three-equation Phillips model is considered and consistent estimates of the structural parameters are provided by the minimum-distance procedure. The small-sample distributions of these estimates are investigated by the Monte Carlo method; and the results are then compared with those of the three-stage least-squares estimates found by making a discrete approximation to the system of differential equations.</p> </abstract>
<abstract> <p>The multi-period control problem analyzed assumes the data are generated by the simple regression model with an unknown slope coefficient. There is a tradeoff between stabilization and experimentation to learn more about the unknown coefficient. When parameter uncertainty is large, experimentation becomes an important consideration.</p> </abstract>
<abstract> <p>This paper discusses in general the use of a priori information in regression analysis and explores in detail the use of mixtures of normal prior distributions. It includes an application to distributed lag analysis.</p> </abstract>
<abstract> <p>A social decision rule is one that produces a social decision for each configuration of individuals' decisions. Such a rule is representative if it produces a social decision that is the result of repeatedly applying the rule of simple majority decision to decisions obtained by that rule. We give necessary and sufficient conditions for a social decision rule for two alternatives to be representative.</p> </abstract>
<abstract> <p>The paper deals with a measure theoretic model of a pure exchange economy. There are two kinds of traders: "big" traders, represented by atoms of the measure space, and "small" traders, represented by the atomless part of the measure space. The restriction of an allocation to the atomless sector is called competitive if there exists a price vector such that the consumption of every "small" trader is a maximal element (in terms of his preference) in the budget set defined by that price vector and by his initial endowment. We consider the set of allocations that are not blocked by any atomless, coalition, or by the complement of any atomless coalition, and call it the @?"2-core. The main results of the paper consist in defining sufficient conditions under which allocations in the @?"2-core have a competitive restriction to the atomless sector, and vice versa. The economic implications and significance of the results are briefly discussed.</p> </abstract>
<abstract> <p>This paper contains the derivation of the exact finite sample distribution function of the LIML identifiability test statistic associated with an overidentified nondynamic structural equation which contains two endogenous variables. Some of the properties of the moments of this statistic are also investigated.</p> </abstract>
<abstract> <p>Sufficient conditions (in terms of restrictions on individual preferences) have been already established in the literature for transitivity of simple majority decisions when individual preferences are quasi-transitive (but not necessarily transitive) and also for transitivity of multi-stage majority decisions when individual preferences are transitive. This paper establishes sufficient conditions for transitivity of multi-stage majority decisions when individual preferences are quasi-transitive, but not necessarily transitive. This constitutes a generalization of several results proved earlier.</p> </abstract>
<abstract> <p>The paper discusses the allocation of output among consumption and two types of capital with different gestation periods. Along an optimal path we show that the imputed prices of capital goods, from the time they start production, do not exceed the prices of output, which are not less than the marginal instantaneous utility of consumption. A simple numerical example helps to illustrate some further implications of the model.</p> </abstract>
<abstract> <p>An axiomatization is presented for a Savage-type conditional subjective expected utility model. The axioms consist of extensions of the Herstein-Milnor [11] axioms for "measurable" utility, a generalization of an averaging condition in Bolker [4], and several structural conditions. The structural conditions are examined in some detail, and examples are given to show what happens to the numerical model when they do not hold. The numerical model expresses the utility of an act (or mixed act), given an event, as a weighted sum of the utilities of the act given events that partition the initial event, the weights being personal probabilities for the partition events conditioned on the initial event. The theory is compared to Savage's theory [18] and to a version of the theory of Luce and Krantz [14] for conditional expected utility.</p> </abstract>
<abstract> <p>The concepts of conditional risk aversion, the conditional risk premium, and risk independence pertaining to multiattributed utility functions are defined. The latter notion is then generalized to what is called utility independence. A number of theorems useful for simplifying the assessment of multiattributed utility functions given certain risk independence and utility independence assumptions are stated.</p> </abstract>
<abstract> <p>The exact finite sample properties of an asymptotically unbiased but nonconsistent estimator of a structural variance are examined briefly.</p> </abstract>
<abstract> <p>An asymptotic expansion of a confluent hypergeometric series is used to approximate the exact finite sample distribution function of a nonconsistent GCL structural variance estimator. A theoretical result is used to motivate the specification of a simple algorithm under which we may accept or reject the use of the asymptotic distribution function of the GCL estimator to approximate the exact distribution function.</p> </abstract>
<abstract> <p> This paper deals with single-equation estimators in a simultaneous system of linear stochastic equations and approximates the distribution function of the two-stage least-squares estimators up to terms whose order of magnitude is &lt;tex-math&gt;$1/\sqrt{N}$&lt;/tex-math&gt;, where N is the sample size. For fixed N, an approximation to the OLS distribution function is also obtained up to terms whose order of magnitude 1/μ, where μ &lt;sup&gt;2&lt;/sup&gt; is what is referred to in the literature as the concentration parameter. </p> </abstract>
<abstract> <p>The topological properties of dynamic heterogeneous capital good models are examined, and it is found that the savings hypothesis crucially influences the dimension of the manifold consisting of the locus of backward solutions from stationary equilibrium. If not all capital gains are saved, the convergent manifold is generally of higher dimension than it is if no income from capital gains is spent on consumption. Accordingly, the characteristic equation for the associated linear system near stationary equilibrium may have more than half its roots with negative real parts, and thus in general the model does not possess a "regular saddlepoint property."</p> </abstract>
<abstract> <p>We present a stochastic model of the employment process in which both the worker's search for jobs and the employer's search for workers are simple Markov processes. An employer's hiring decision is determined by the type of worker applying. Dynamic programming methods are used to find the optimal hiring policy by analyzing the interaction between the two processes. The steady-state distribution of worker unemployment by type is derived.</p> </abstract>
<abstract> <p>Given n observations on a system of linear stochastic difference equations with appropriate initial conditions, and given a prior density (possibly diffuse) of its parameters, this paper obtains the predictor of the time series k periods into the future with minimum mean squared error. Completely analytical solution is given for predictions from the first-order univariate system, and, in the general higher-order multivariate case, for k up to 5.</p> </abstract>
<abstract> <p>This paper derives the asymptotic distribution of restricted and unrestricted reduced form estimators and compares their asymptotic efficiency. It also shows that the same techniques yield the asymptotic distribution of dynamic multipliers.</p> </abstract>
<abstract> <p>S. Bochner's concept of a subordinate stochastic process is proposed as a model for speculative price series. A general class of finite-variance distributions for price changes is described, and a member of this class, the lognormal-normal, is tested against previously proposed distributions for speculative price differences. It is shown with both discrete Bayes' tests and Kolmogorov-Smirnov tests that finite-variance distributions subordinate to the normal fit cotton futures price data better than members of the stable family.</p> </abstract>
<abstract> <p>In this paper we are concerned with the following question: in any economy with several public goods, what are the conditions under which the conventional optimality rule of equality between the sum of marginal rates of substitution and the marginal rate of transformation still holds even in the presence of distortionary taxation? Two cases are considered. In the first case, the taxes may be arbitrary. In the second case, the taxes are optimally chosen.</p> </abstract>
<abstract> <p>Techniques are described whereby the distribution of completed unemployment spell lengths may be inferred from the distribution of in-process unemployment spell lengths recorded each month in the Current Population Survey. An extension is proposed whereby the complete population joint distribution of labor market transition probabilities can be estimated using only Current Population Survey information.</p> </abstract>
<abstract> <p>The paper puts forward an approach to the analysis of the problem of strategic voting in democratic choice. It is argued that the notion of K-stability (or other similar concepts of equilibrium) is not adequate as an instrument for analyzing strategic voting. An alternative framework of analysis is suggested and using this framework, the possibility of sincere voting is examined in the context of a class of democratic systems.</p> </abstract>
<abstract> <p>Many demand system specifications employed in empirical studies are generated by utility functions which belong to the class that is characterized in the dual by what we call the "Gorman polar form." This class has the attractive property that membership is equivalent to the satisfaction of necessary and sufficient conditions for aggregation across consumers. We characterize the class of (direct) preference orderings that is dual to the Gorman polar form; this class includes, as special cases, homotheticity, affine homotheticity, and homotheticity to minus infinity. We also specify and estimate a member of this class which generalizes previously estimated specifications of Gorman polar forms.Finally, employing a likelihood ratio test, we reject the hypothesis that preferences are described by the S-branch utility tree (or, of course, any of its special cases) and concomitantly reject the hypothesis of affine homotheticity of preferences.</p> </abstract>
<abstract> <p>The conditions under which aggregate CES demand behavior is consistent with polytomous choice by micro demanders are explored. Several special cases are treated in which either the relative efficiency or the relative input price is assumed to vary randomly over the micro units. In each case it is shown that the random variable must have either a log-logistic (Burr) distribution function or a generalization thereof if the aggregate and the micro behavior are to be consistent.</p> </abstract>
<abstract> <p>This paper explores a new approach to the estimation of a joint production technology. Pseudo data, which are obtained by solving a petrochemical process model for alternative relative prices, are used to estimate a price possibility frontier with 3 inputs and 6 outputs. Unlike traditional data sources, pseudo data are not constrained by historical price variation, technologies, and environmental controls. As an econometric exercise, the approximation of the process model's detailed piecewise linear production surface by a single equation "generalized" functional form, the translog, raises a host of interesting empirical and methodological questions.</p> </abstract>
<abstract> <p>Arrow and Hahn's book General Competitive Analysis offers (Section 6.4) an existence theorem for non-competitive general equilibrium. Their formulation is apparently very general, but it becomes seriously restrictive in the presence of increasing returns to scale. An alternative existence theorem is presented here that obviates these shortcomings.</p> </abstract>
<abstract> <p>To date, the most widely used method for empirical analysis of multiple alternative qualitative choices has been an extension of binary logit analysis called conditional logit analysis. Although this method is extremely attractive because of its computational simplicity, it is burdened with a property termed the "independence of irrelevant alternatives" that is quite unrealistic in many choice situations. We have proposed in this paper a computationally feasible method of estimation not constrained by the independence restriction and which allows for a much richer range of human behavior than does the conditional logit model. An important characteristic of the model is provision for correlation among the random components of utility and, as a by-product, the explicit allowance for variation in tastes across individuals for the attributes of alternatives. We have demonstrated the model and compared it with the logit one by analyzing the travel mode choice decisions of commuters to the central business district of Washington, D.C. Substantial differences are found in predictions based on the two models. The example allows three alternatives. Extension to four or five is quite feasible.</p> </abstract>
<abstract> <p>An efficient estimator for regressions in which the parameter vector can take any of several values is devised. It is shown that although the likelihood function is unbounded, the likelihood equations have a consistent root. An initial consistent estimator is provided. One Newton step provides efficient estimates. Applications to nonlinear models and contaminated normal models are suggested.</p> </abstract>
<abstract> <p>An over-identified model could be defined as an exactly identified model that is subject to over-identifying restrictions. One could therefore define a constrained indirect least squares estimator for systems of equations similar to generalized least squares estimators under constraints for single equations. The estimator differs from three stage least squares in using the indirect least squares estimated covariance instead of the two stage least squares estimated covariance. With linear constraints, the estimator is linear. Under the overall null hypothesis with all constraints obtaining, the constrained indirect least squares estimator has the same asymptotic properties as the full information maximum likelihood estimator. The main advantage of the estimator lies in its easy adaptability to the multiple comparisonist's preferred testing procedure given the exactly identified model as maintained hypothesis. In this paper we stay with the likelihood principle and the corresponding preliminary Wald-type multiple χ &lt;sup&gt;2&lt;/sup&gt; tests.</p> </abstract>
<abstract> <p>A gap is filled in a proof of a central limit theorem, for regression in a time series context, which has been used in some econometric theory.</p> </abstract>
<abstract> <p>There are two types of mathematical economists, one who applies existing mathematics to economic problems (the best example is Court) and the other who anticipates new mathematical problems within economics. Taking Marx as the second type of economist (Section 1), I discuss two of his problems: the fundamental Marxian theorem (Section 2) and the transformation problem (Section 3). In Section 2 I propose a generalisation of the theorem to the effect that the theorem does not need the labour theory of value and hence is independent of any criticisms of that theory. In Section 3 it is seen that the transformation problem is formally identical with the Markov chain process transforming the initial position to the ergodic position.</p> </abstract>
<abstract> <p>The core is the set of all allocations that cannot be blocked by any coalition no matter how improbable is its formation. Extending the insights of the Continuum School, this article shows that restrictions placed on the formation of coalitions do not enlarge the core "very much" if the finite economy is "large."</p> </abstract>
<abstract> <p>This analysis generalizes the concept of a private-goods contract curve to an economy with public goods. It provides a derivation of the properties of the set of Pareto-optimal points in n-space. The paper concentrates on situations where the taxation system for financing the public goods is externally imposed; the efficiency goal is thus "mechanism-constrained Pareto optimality." The results of the main analysis are applied to assess performance of several group decision procedures.</p> </abstract>
<abstract> <p>The introduction of ratchet effects into the consumption and investment functions of the Smithies model does not provide an endogenous explanation of cyclical fluctuations and growth, contrary to Smithies' expectation. As in other cycle models, the growth trend can only be explained by exogenous forces.</p> </abstract>
<abstract> <p>Arrow's impossibility theorem can be viewed as requiring that each subset of two social alternatives be a potential feasible subset or environment, with transitive and complete social choices over these subsets for each profile of individual preference orders. The feasibility assumption for every two-alternative subset is relaxed with consequent changes in the social ordering condition. An Arrow-type impossibility result still obtains when the set of social alternatives is the union of two disjoint sets, each of which has two or more elements, and when {x, y} is feasible whenever x is from one set and y is from the other. Variants of the basic theorem are included, one of which requires that strict binary social choices be acyclic.</p> </abstract>
<abstract> <p>This paper presents a model of an economy in which the formation of equilibrium is completely explained by the independent optimizing behavior of individual agents, and thus reformulates the concept of equilibrium in a way which is essentially related to that of stability.</p> </abstract>
<abstract> <p>This paper presents the results of sampling experiments that were designed to test the conjecture that under certain conditions the exact distribution functions of estimators and test statistics in a simultaneous equations model are not affected by the presence of lagged endogenous variables. The experimental data support the conjecture in almost every case.</p> </abstract>
<abstract> <p>A nonparametric framework for deriving the asymptotic MSE-optimal predictor for a multiplicative model is presented. The resulting predictor is compared to several known competitors in a limited Monte Carlo experiment.</p> </abstract>
<abstract> <p>This paper deals with the two-stage least squares (2SLS) estimator of structural parameters in a system of M linear structural equations. The structural equation being estimated consists of three endogenous variables and some exogenous variables. The exact mean of the 2SLS estimator has been worked out.</p> </abstract>
<abstract> <p>Regularity conditions in social welfare analysis can be seen as ways of making specific types of information "inadmissible" in welfare judgements: the evaluation is made invariant with respect to information of those types. In this sense, these conditions--often reflecting "principles" of judgement--serve also as informational constraints. In this lecture alternative approaches to social welfare evaluation are examined in this light. The analysis covers both Arrovian and Bergson--Samualsonian social welfare functions as well as principles underlying utilitarianism, Rawlsian conception of justice, notions of liberty and egality, and "historical" theories of rights and entitlements (e.g., Marx or Nozick).</p> </abstract>
<abstract> <p>In this paper we study the possibility of constructing satisfactory social choice mechanisms whose outcomes are determined by a combination of voting and chance. The following theorem is obtained: if a social choice mechanism does not leave "too much" to chance and satisfies a unanimity condition, then it is either uniformly manipulable or dictatorial. The result contributes to the program suggested by Gibbard [2] for the study of the extent to which social choice mechanisms in which chance plays a role can be freed from strategic manipulation.</p> </abstract>
<abstract> <p>In Kaneko [6] we considered the relationship between the ratio equilibria and the core of the voting game G(N, W,r), in which a fixed ratio is given. In this paper we present a new voting game G(N, W) in which no fixed ratio is given, and consider the relationship between the ratio equilibria and the core of G(N, W). We prove that the core of G(N, W) coincides with the ratio equilibria.</p> </abstract>
<abstract> <p>Much of the development of economic theory has been based on assumptions that credit economic agents with perfect discriminating power and perfect consistency in their choice behavior, but recently a range of less restrictive psychological choice theories has been introduced into economic analysis. These theories may be classed into two types--those that treat preference (and hence choice) as a probabilistic phenomenon, and those that retain the deterministic structure of the traditional analysis but relax in some way the assumption that individual preference be transitive. In previous work other have examined the implications for economic equilibrium of asuming preferences to be random; our purpose in this paper is to examine the equilibrium implications of a particularly attractive deterministic choice theory, that of semiordered preferences. We first examine an exchange economy in which the agents' preferences are semiorders that are not too nonconvex; such an economy is shown to have an equilibrium, and further, for interior equilibria, it is shown that a range of allocations to consumers is consistent with equilibrium. Finally, we show that in a semiorder market economy the set of price equilibria has a non-empty interior relative to the price simplex.</p> </abstract>
<abstract> <p>A bargaining situation is described by a set of alternative which are feasible to n individuals when they do cooperate, and an alternative which comes about when they do not cooperate. The paper addresses the question of which cooperative outcome will be chosen. A Nash-type approach is used to prove that, under plausible axioms describing the underlying bargaining process, the individuals must be doing interpersonal comparison of utility. The model and the solution overcome some difficulties recently described by Nydegger and Owen.</p> </abstract>
<abstract> <p>Four conditions are shown to imply together that a solution function for two-person bargaining problems must equalize gains in some ordinal utility scales. These conditions are: weak Pareto optimality, strong individual rationality, composition, and uniformity. The composition condition relates to sequences of bargaining problems. The uniformity condition requires that the solution function must be invariant under enough ordinal utility transformationsto move any threat point to the origin.</p> </abstract>
<abstract> <p> L'étude de C. Fourgeaud et A. Nataf visant à dégager toutes les classes de fonctions de consommation en prix et revenu réels compatibles avec la théorie des choix, contient une erreur; l'indice de prix associé à une de ces classes s'avère plus général que celui dégagé par ces auteurs. Dans cette note, après avoir corrigé cette erreur, on examine l'utilité économique et empirique de ces classes qui sont apparues jusqu'ici dépourvues d'intérêt, en y introduisant des quantités obligées. Il en résulte une généralisation intéressante du célebre système linéaire de dépenses dont on explicite la forme de l'indicateur d'utilité indirecte. </p> </abstract>
<abstract> <p>A recent paper by D. J. Roberts and A. Postlewaite [1] shows a possible justification for the assumption that participants in a market behave as price-takers when the number of participants becomes large. The present note compares this justification with other approaches to the same question and argues that the various approaches are complementary rather than alternative since, in order to demonstrate the viability of a system with price-taking behavior, all types of feasible deviations from such behavior must be explored. The note introduces a taxonomy for such deviations which may be useful in comparing various approaches and contributions found in the literature.</p> </abstract>
<abstract> <p>The cost of a day of hospital care has increased 600 per cent in the past 20 years while the general consumer price index rose only 100 percent. The current paper explains the rising cost of hospital care with the aid of an econometric model. the essence of the explanation is that the explosion of hospital cost reflects a rapid change in the quality and style of hospital care and not an increasing cost of providing the same product. The dramatic change in the quality of care is primarily due to the increased demand caused by the growth of private and public insurance. The paper presents an analytic of the hospital industry in which quality affectsthe demand for hospital services and in which the purchase or private insurance is endogenous. Estimates of key equations of the model based on a cross-section of time series for the individual states for the years 1958 through 1973 are presented. The price adjustment process and dynamic multipliers are discussed.</p> </abstract>
<abstract> <p>The optimal policy prescription in response to congestion on a traffic network involves taxes levied so as to increase the private costs of vehicle use by the amount of the costs imposed on other users of the system. However, technical and political constraints may make this taxation policy infeasible. Using assumptions on the effect of taxation on the level and structure of demandfor transportation services, this paper provides guidelines for taxation (and possibly subsidization) in a multi-mode traffic system in which such constraints are effective.</p> </abstract>
<abstract> <p>A specialization of the Edgeworth type formulae due to Chambers [4] to approximate the marginal distribution of an econometric estimator is presented, and its application to improvement of the use of asymptotic limits in significance testing is discussed. Appendices discuss the validity of Nagar approximations to estimator moments, the exact distribution of the instrumental variable estimator, the Edgeworth approximations for 3SLS and FIML estimators, and the use of Monte Carlo procedures for assessing the appropriate probability to attach to a given significance test.</p> </abstract>
<abstract> <p>A multiple equation nonlinear regression model with serially independent disturbances is considered. The estimation of the parameters in this model by maximum likelihood and minimum distance methods is discussed and our main subject is the relationship between these procedures. We establish that if the number of observations in a sample is sufficiently large, the iterated minimum distance procedure converges almost surely and the limit of this sequence of iterations is the quasi-maximum likelihood estimator.</p> </abstract>
<abstract> <p>A regression model in which the disturbances exhibit a certain type of heteroscedasticity is considered. Maximum likelihood methods of estimation are developed and compared with the two-step estimation procedure. A likelihood ratio test for heteroscedasticity is suggested.</p> </abstract>
<abstract> <p>In a multiplicative model it is usual to assume that the logarithm of the disturbance variable is normally distributed with unknown variance σ &lt;sup&gt;2&lt;/sup&gt; and with a mean which is either zero or &lt;tex-math&gt;$-{\textstyle\frac{1}{2}}\sigma ^{2}$&lt;/tex-math&gt; according to the viewpoint taken of the object of the model. It is shown that, for each of several estimation criteria, the two assumptions lead to precisely the same point estimators of the exponents of the explanatory variables in the model and of the conditional mean, median, and mode of the dependent variable for specified values of the explanatory variables. An improved form is also given of the estimator of the conditional mean proposed by Teekens and Koerts [8].</p> </abstract>
<abstract> <p>This paper applies a simple heuristic interpolator to estimating the Lorenz curve from grouped data. Precise conditions are given to insure that the method yields a convex curve. The results are illustrated on United States tax data.</p> </abstract>
<abstract> <p>Using cross-section data on white married women for the year 1967, a model of labor supply which permits statistical estimation of a "coefficient of tax perception" is studied. The model allows for the possibility that the wage may depend upon the number of hours worked. The results suggest that marginal tax rates have an important impact on labor market behavior.</p> </abstract>
<abstract> <p>The static aspect of the Le Chatelier-Samuelson principle in the local version is fully characterized in an extremum formulation with differing systems of auxiliary constraints. The local characterization results in an interesting finding. With the systems of effective constraints which restrict only as man involved variables as the number of constraints, the variations of an extremum system, as consequences of changes in parametric indices of exogeneous conditions, will be only quantitative. With the systems of effective constraints which restrict more or all the involved variables, however, the consequential variations will be qualitative as well as quantitative. Necessary and sufficient conditions are investigated for constrained saddle points to exhibit a generalized Le Chatelier-Samuelson principle. An economic argument along the lines of demand theory is that the generalized principle among constrained demands is established if and only if the Jacobian of the constrained demands with effective constraints exhibits a dominant complementarity for the new constraint with its coefficients non-negative. The Marshallian conjectures about short run and long run are extensively discussed and rigorously proved with the results.</p> </abstract>
<abstract> <p>Defining each good (factor) in each different country as a distinct good (factor), one can use Scarf's algorithm to solve general equilibrium problems for a trading world. The dimension of such problems grows very fast with the number of goods (factors) and countries, making computations extremely costly. Here we present a method for solving general equilibrium problems for a trading world which can be applied to a class of problems, and which requires considerably less computation time than the direct application of Scarf's algorithm.</p> </abstract>
<abstract> <p>A general equilibrium trade model with more produced commodities than factors is specified under the assumption that industry production functions are homothetic. The main issue under consideration is that of local and global determinateness of the economy's production pattern. The paper also examines the impact of exogenous commodity price and endowment changes on output levels and factor returns.</p> </abstract>
<abstract> <p>This paper presents conditions on the coefficients of Nth order linear constant coefficient functional equations in generalized differences, necessary and sufficient for asymptotic stability. These conditions are analogous to the Schur-Cohn conditions for difference equations in dated form, and to the Routh-Hurwitz conditions for differential equations.</p> </abstract>
<abstract> <p>Bayesian theory for rational individual decision making under uncertainty prescribes that the decision maker define independently a set of beliefs (probability assessments for the states of the world) and a system of values (utilities). The decision is then made by maximizing expected utility. We attempt to generalize the model to group decision making. It is assumed that the group's belief depends only on individual beliefs and the group's values only on individual values, that the belief aggregation procedure respects unanimity, and that the entire procedure guarantees Pareto optimality. We prove that only trivial (dictatorial) aggregation procedures for beliefs are possible.</p> </abstract>
<abstract> <p>The concept of a dominance solvable voting scheme is presented as a weakening of the strategy-proofness requirement: it relies on successive elimination of dominated strategies and generalizes the well known concept of "sophisticated voting." Dominance solvable decision schemes turn out to contain many usual voting procedures such as voting by veto, kingmaker, and voting by binary choices. The procedure of voting by elimination is proved to be an anonymous dominance solvable voting scheme which always selects an efficient alternative.</p> </abstract>
<abstract> <p>This paper considers a problem faced by players who are involved in a sequence of games: not necessarily the same games, not necessarily with the same opponents, and not necessarily under conditions of complete information. The players are assumed to act in response to stationary Markovian hypotheses which they form about the actions of their opponents. Conditions are explored which require that these hypotheses be correct on average and that the players actions be optimal in response to their hypotheses.</p> </abstract>
<abstract> <p>Defining a choice as a function which picks a subset of every set of alternatives, we consider a list of over thirty conditions (expressed as functional inequalities) which a choice may satisfy, demonstrating &amp;-semilattices formed by certain sublists, thus summarily presenting as "synopses" all the implications obtaining between logical conjunctions formed within these sublists. The list studied includes many well known conditions, such as Plott's [13] path independence, for which we offer over a dozen new characterizations of various types.</p> </abstract>
<abstract> <p>This paper develops a matrix-measure of multivariate risk aversion which is related to a notion of risk premium and states the restrictions that must be imposed upon the matrix-measures of two utility functions in order that one require a higher risk premium than another for every small multivariate risk. A necessary and sufficient condition for comparability of global attitude towards risk is that the local restrictions hold over the entire domain. The usefulness of the measures of risk aversion is discussed within the context of a multivariate risk-sharing problem.</p> </abstract>
<abstract> <p>This paper provides an explanation for the observed widespread use of a money good with little intrinsic value. The explanation derives from the result that if agents expect some particular good to play the role of money, then sequential, pairwise trading leads to an economy-wide Pareto optimal allocation.</p> </abstract>
<abstract> <p>This paper studies the existence, uniqueness, and stability of equilibrium for a class of random dynamical systems that arise frequently in the study of Markovtemporary equilibrium models and in models arising from maximization behavior over time. It is seen that equilibria in these models have very nice properties.</p> </abstract>
<abstract> <p>It is well known that in the Sidrauski monetary intertemporal optimizing model the steady state capital stock is invariant to the rate of inflation. This paper shows that the rate of accumulation of capital is not invariant to the rate of inflation and that, for the constant relative risk aversion family of utility functions (except logarithmic), the rate of capital accumulation is faster the higher the growth rate of money. Money is thus not neutral on transition paths.</p> </abstract>
<abstract> <p>The partial adjustment model has been used in many areas of applied economics as a description of optimal behavior in the face of adjustment costs. The model requires a specification of how expectations are formed; for example static or adaptive expectations are frequently specified. This paper analyzes the implications of a rational expectations specification. In applications, a series which measures expectations is usually not available, and various proxies have been suggested. If expectations are rational, one can use the actual value of a variable as an estimate of what agents had expected. This device can be used to obtain consistent estimates of the parameters of the partial adjustment model.</p> </abstract>
<abstract> <p>If one assumes, as is typically done in modelling continuous time systems, that: (i) a forecast is for an infinitesimally short future period; (ii) forecasters have instantaneous access to relevant information as it becomes available and have some ability to store that information; (iii) the variable being forecast is differentiable; then this paper shows how expectations must satisfy perfect myopic foresight. The implications of relaxing (i) and (ii), and thereby allowing for forecast errors, are discussed. In either case, a finite delay leading to a differential-difference equation (system) is generated. The implications of this for modelling continuous time systems are discussed and the procedure is illustrated with a simple example.</p> </abstract>
<abstract> <p>This paper develops behavioral relationships explaining investors' demands for longterm bonds, using three alternative hypotheses about investors' expectations of future bond prices (yields). The results, based on U.S. data for six major categories of bond market investors, consistently support an autoregressive expectations model. The results also have implications for further aspects of investors' portfolio behavior, including expectations formation, response to inflation, and speed of adjustment.</p> </abstract>
<abstract> <p>Previous estimates of the magnitude of highway congestion costs have employed equations relating the external cost imposed on motorists by an additional vehicle to the speed of traffic flow or the ratio of traffic flow to the maximum capacity of the road. While those equations may be accurate for rural roads and expressways, they may be less accurate on city streets where delays at intersections are a dominant factor in congestion costs. This study replaces single speed-volume equations with a traffic simulation model that replicates the queuing of vehicles at traffic lights, the dispersion of platoons of vehicles as they move from one intersection to another, and the interaction of intersecting traffic flows on an urban street network. The model is used with actual Toronto road and traffic data to produce new estimates of congestion costs on specific streets during the morning rush hour. The model produces a surprisingly high average congestion cost during the morning rush hour and a poor correlation of the results with those that would be estimated for the same traffic flows by the single equation models. The simulation technique allows the calculation of congestion costs on a street-by-street basis, generating the detailed information that would be necessary for a complex congestion pricing scheme.</p> </abstract>
<abstract> <p>In this paper we consider the lognormal, gamma, beta, and Singh-Maddala functions as descriptive models for the distribution of family income for 1960 and 1969 through 1975. Least squares and two efficient estimation techniques are used to estimate the unknown parameters. Alternative functional forms for comparable estimation techniques can then be contrasted and estimation techniques for a given functional form can be compared. We note that estimates of population characteristics depend upon the functional form and estimation technique selected.</p> </abstract>
<abstract> <p>A new approximation based on the saddlepoint method of approximating integrals is derived for the probability density of the k-class estimator in the case of the equation with two endogenous variables. The two tails of the density are approximated by different functions, each of which bears a close relationship with the exact density in the same region of the distribution. Corresponding approximations are also derived for the distribution function and the method of derivation should be useful in other applications. Some brief numerical results are reported which illustrate the performance of the new approximation.</p> </abstract>
<abstract> <p>The paper deals with a measure theoretic model of a pure exchange economy. There are two kinds of traders: "big" traders, represented by atoms of the measure space, and "small" traders, represented by the atomless part of the measure space. The restriction of an allocation to the atomless sector is called competitive if there exists a price vector such that the consumption of every "small" trader is a maximal element (in terms of his preference) in the budget set defined by that price vector and by his initial endowment. We consider the set of allocations that are not blocked by any atomless coalition, or by the complement of any atomless coalition, and call it the &lt;tex-math&gt;$\scr{I}^{2}\text{-core}$&lt;/tex-math&gt;. The main results of the paper consist in defining sufficient conditions under which allocations in the &lt;tex-math&gt;$\scr{I}^{2}\text{-core}$&lt;/tex-math&gt; have a competitive restriction to the atomless sector, and vice versa. The economic implications and significance of the results are briefly discussed.</p> </abstract>
<abstract> <p>Necessary and sufficient conditions are given for an aggregation procedure to be a "point system" or "generalized point system." In Sections 4 and 5 various "runoff" systems and their properties are discussed but no equivalent characterization is obtained.</p> </abstract>
<abstract> <p>It is often claimed that models of money and growth which assume perfect foresight are dynamically unstable. If the system is initially in a steady state with zero inflation, it is claimed that a once-and-for-all increase in the money supply will set off a process of ever-accelerating deflation. Here we set forth an alternative view according to which such an increase in the money supply produces a once-and-for-all increase in the price level that is just sufficient to keep the system at its steady state. Consequently, the system is stable, making it possible to perform meaningful comparative static experiments. Our view provides a way of reconciling traditional analysis with recent dynamic analyses.</p> </abstract>
<abstract> <p>A concept of competitive equilibrium is proposed for a game. The following three reasons are given, proving that this is really a generalization of the concept used in economic theory: (i) The mathematical definition is the same; (ii) an existence theorem is proved in the same way; (iii) when the number of players becomes very large, in a suitable way, the core and the equilibrium are the same. The concept and utilization of a characteristic function, a generalization of the one defined by von Neumann and Morgenstern, form the basis of this paper.</p> </abstract>
<abstract> <p>This paper sets out a general criterion for the identifiability of a statistical system, based on Kullback's information integral. It is shown that the general identification problem is equivalent to a maximisation problem, or where parameter restrictions are present, a problem in nonlinear programming. The relationship of this criterion to that based on the information matrix of the underlying distribution is also exhibited.</p> </abstract>
<abstract> <p>The paper provides several axiomatizations of the concept of "path independence"as applied to choice functions defined over finite sets. The axioms are discussed in terms of their relationship to "rationality" postulates and their meaning with respect to social choice models.</p> </abstract>
<abstract> <p>The preliminary test estimator (or predictor) is studied in the context of linear normal regression models. The estimator (or predictor) obtained after the preliminary test is known to be inadmissible with respect to squared error loss under certain assumptions. Nevertheless, it is still widely used in practical application of regression analysis, particularly in econometrics. The object of the present paper is to tabulate the optimal significance points in the preliminary test for practical use. The optimality is based on the minimax regret principle. It is shown that if, as is usual, we take the significance point equal to the 5 per cent point or 10 per cent point, the risk is extremely large for some parameter value. The optimal significance point of the preliminary t test decreases slightly as the degrees of freedom increase,but it is nearly constant; i.e., it lies between 1.370 to 1.380 if d.f. is greater than 6.</p> </abstract>
<abstract> <p>The existence of an equilibrium is proven for a two-period model in which there are spot transactions and futures transactions in the first period and spot markets in the second period. Prices at that date are viewed with subjective uncertainty by all traders. This introduces the possibility of speculation. Conditions for the existence of a competitive equilibrium include restriction on the nature of price expectations.</p> </abstract>
<abstract> <p>In this paper we generalize the family of single equation k-class estimators to systems of equations. The systems k-class estimator with k = 1 is the 3SLS estimator. After developing the asymptotic properties we introduce a further member of the systems k-class, the systems LVR estimator. A systems version of Basmann's identifiability test statistic is also considered.</p> </abstract>
<abstract> <p> Dans un article déjà ancien, les Professeurs C. Fourgeaud et A. Nataf [7] s'étaient attachés à définir la forme la plus générale prise par un système complet de fonctions de demande lorsque ces fonctions ne dépendent que du revenu réel et du prix réel du bien considéré. Ce papier retrace un essai d'interprétation et une expérience d'estimation numérique de ces fonctions. On sait que le système de Fourgeaud et Nataf constitue une généralisation intéressante du bien connu système linéaire de dépenses de R. Stone. Il permet en effet d'introduire un peu plus de flexibilité dans les effets de substitution permis par le modèle mais surtout il accroît fortement la richesse des effets revenu. Dans cette étude cette possibilité est appliquée à la prise en compte d'effets revenus de courte et de longue période, le but étant de pouvoir disposer d'un modèle de cheminement applicable aux études de planification à moyen terme. </p> </abstract>
<abstract> <p>This paper reports on some mathematical and analytical properties of a static nonlinear model of a national economy or, more generally, of a multisectored economy. The model is a nonlinear version of the well-known linear input-output model of Leontief. Conditions are given for the nonlinear model to be workable in the sense that (i) there is a unique nonnegative vector of output production levels for each nonnegative final-demand vector, and (ii) the vector of output levels depends in a certain reasonable manner on the final-demand vector. Attention is also focused on several other properties of the model of economic interest. For example, it is shown that propositions completely analogous to the LeChatelier-Samuelson principle in both the weak and strong forms for workable Leontief systems are valid within the context of the nonlinear model.</p> </abstract>
<abstract> <p>A summation social choice function is a social choice function whose choice sets are determinable from maximum sums of utilities that preserve individual preference. Assuming the set of alternatives is finite and individual preferences are irreflexive and transitive, a unanimity-type condition is shown to be necessary and sufficient for a social choice function to be a summation social choice function. The effects of conditions of voter independence, anonymity, and neutrality are noted.</p> </abstract>
<abstract> <p>This paper develops a theory of the firm's demand for labor when workers, at the time they are hired, know that they may later be laid off. The derived behavior shows how the firm, in response to price falls of increasing severity, will first reduce hours of work. After a minimum work week has been reached layoffs start. The point at which this occurs depends upon the income workers expect to receive if they are laid off. Since unemployment insurance (UI) benefits are an important determinant of this income level, they influence the number of layoffs. Given the absence of an effective incentive tax in practice, we would predict that the present UI system encourages layoffs. An effective, incentive tax could stop this encouragement. Two possible wage strategies are explored, both of which are consistent with the basic layoff and hours model. The flexible wage policy has the advantage of giving no incentive to the firm to default on the (privately) efficient layoff rules derived earlier, but seems to be inconsistent with observed short-run wage policy. The fixed wage policy has its strength and weakness the other way around.</p> </abstract>
<abstract> <p>A household time-allocative model which explicitly takes into account the economic contribution of children in agricultural areas of less-developed countries is applied to direct-level data pertaining to the rural population of India. Joint family decisions concerning fertility and the allocation of male and female child time to schooling and work activities are examined empirically in a simultaneous equations system. The properties of the formal model are used to derive inferences from the parameter estimates with respect to the shadow price configuration influencing these joint decisions.</p> </abstract>
<abstract> <p>This paper estimates all possible multidimensional interaction effects in a logit model of homeownership, assesses the relative importance of these interactions, and interprets the results in light of existing theories of housing consumption.</p> </abstract>
<abstract> <p>We develop a demand model from a utility tree possessing interactions at all levels. The model is both highly flexible and globally integrable. We use an approach to recursive subaggregation permitting convenient estimation with an unlimited number of goods, and we apply the approach to the construction of a food price forecasting model.</p> </abstract>
<abstract> <p>A treatment of taxation based on considerations of political and economic power in a majority-vote democratic context is the topic of this article. Agents are endowed with gross incomes and have concave von Neumann-Morgenstern utilities for money. Taxation policies are decided by majority vote, but each citizen retains a certain basic right that prevents the majority from arbitrarily expropriating his income. The resulting non-transferable utility cooperative game is analyzed by means of the Harsanyi-Shapley-Nash value.</p> </abstract>
<abstract> <p>This paper explores a new approach to the Nash bargaining problem in which the axiom of symmetry is dropped and it is assumed that the final allocation depends on both the status quo and the threat point. The resulting final allocation, unlike that formalized by Nash, cannot be represented by a simple analytic expression; rather, it leads to a whole class of solutions. Properties of the final allocation are analyzed. It is shown that for every initial allocation there exists a Nash fiber, corresponding to the Nash allocation, that it is possible to determine the sign of the derivates of the final allocation with respect to changes in the threat point, and that a "Slutsky-like" equation relates these derivatives to the derivatives with respect to the initial allocation. It is also shown that, under certain conditions, as play is respected the final allocation asymptotically converges to the Nash allocation.</p> </abstract>
<abstract> <p>This paper studies conditions under which aggregate demand behavior will satisfy the usual revealed preference axioms. Assuming a fixed distribution of income and the hypothesis that individual demand is homogeneous in income, it is shown that the weak axiom of revealed preference or the congruence axiom will hold in the aggregate if each individual demand satisfies the corresponding axiom. it is also shown that the hypothesis of homogeneity in income is not necessary for the weak axiom to hold in the aggregate.</p> </abstract>
<abstract> <p>We construct a monthly model of supply and demand for chartered banks' loans to business firms in Canada. Recent disequilibrium econometric methods are used to take into account possibilities of rationing.</p> </abstract>
<abstract> <p>This paper considers the problem of estimating the parameters of linear demand and supply equations in cases where prices are set exogenously and do not, in general, clear the market. The particular feature of the problem is that the quantities demanded and supplied are not explictly observed. Instead we observe the quantity actually transacted - assumed to be determined as the smaller of these two. The maximum likelihood estimator is examined and the asymptotic properties are derived.</p> </abstract>
<abstract> <p>In considering the problems of inference in nonlinear regression models the statistical and computational aspects of parameter estimation are discussed, and model selection procedures for both nested and nonnested hypotheses are analyzed including an optimal sequential testing procedure for ordered nested hypotheses. A distinction is made between tests of specification and tests of misspecification, and is discussed in relationship to the Wald, likelihood ratio, and Lagrange multiplier hypothesis testing principles. Degrees of freedom or small sample adjustments to the asymptotically valid test statistics are also discussed. The choice of production function from a class of CES functions, with either additive or multiplicative error specifications, for a cross section of UK industries, provides an application of the theory.</p> </abstract>
<abstract> <p>This paper examines the estimation of a normal contemporaneous simultaneous equation model in which some of the exogenous variables are measured with error.The theory for asymptotically efficient least squares estimation is developed. The primary result is a structural least squares estimator which offers certain computational advantages relative to the full information maximum likelihood estimator.</p> </abstract>
<abstract> <p>This paper considers the least squares estimator of @?^2 in the linear model with disturbances generated by a first-order autoregressive process. It is well known that the estimator is biased. In this paper an attempt is made to establish bounds for the bias. These bounds depend on n, k, and @r, where n is the number of observations, k is the number of parameters, and @r is the (positive) coefficient of the autoregressive process.</p> </abstract>
<abstract> <p>This paper surveys alternative testing criteria in the linear multivariate regression model, and investigates the possibility of conflict among them. We consider the asymptotic Wald, likelihood ratio (LR), and Lagrange multiplier (LM) tests. These three test statistics have identical limiting chi-square distributions; thus their critical regions coincide. A strong result we obtain is that a systematic numerical inequality relationship exists; specifically, Wald @&gt; LR @&gt; LM. Since the equality relationship holds only if the null hypothesis is exactly true in the sample, in practice there will always exist a significance level for which the asymptotic Wald, LR, and LM tests will yield conflicting inference. However, when the null hypothesis is true, the dispersion among the test statistics will tend to decrease as the sample size increases. We illustrate relationships among the alternative testing criteria with an empirical example based on the three reduced form equations of Klein's Model I of the United States economy, 1921-1941.</p> </abstract>
<abstract> <p>A limiting feature of several theoretically superior "shrinkage" estimators for the linear regression model lies in the fact that there must be a certain degree of orthogonality in regressors in order for them to dominate the ordinary least squares estimator. In this paper we apply variants of pre-test and Stein estimators to data on international trade, and discuss their merits in light of the limitations imposed by the non-orthogonality of these and other sets of economic data.</p> </abstract>
<abstract> <p>It has been conjectured that no system of voting can preclude strategic voting--the securing by a voter of an outcome he prefers through misrepresentation of his preferences. In this paper, for all significant systems of voting in which chance plays no role, the conjecture is verified. To prove the conjecture, a more general theorem in game theory is proved: a game form is a game without utilities attached to outcomes; only a trivial game form, it is shown, can guarantee that whatever the utilities of the players may be, each player will have a dominant pure strategy.</p> </abstract>
<abstract> <p>Transitivity-like properties for binary social choices on a triple of alternatives are shown to follow from simple conditions that apply within each voter preference profile, coupled with structural profile restrictions such as those used in single-peakedness. These results are compared to results obtained under the simple majority rule. The special intraprofile conditions used in the main theorem are related to interprofile conditions such as independence, neutrality, and monotonicity.</p> </abstract>
<abstract> <p>This paper analyzes the problem of inducing the members of an organization to behave as if they formed a team. Considered is a conglomerate-type organization consisting of a set of semi-autonomous subunits that are coordinated by the organization's head. The head's incentive problem is to choose a set of employee compensation rules that will induce his subunit managers to communicate accurate information and take optimal decisions. The main result exhibits a particular set of compensation rules, an optimal incentive structure, that leads to team behavior. Particular attention is directed to the informational aspects of the problem. An extended example of a resource allocation model is discussed and the optimal incentive structure is interpreted in terms of prices charged by the head for resources allocated to the subunits.</p> </abstract>
<abstract> <p>The effect of economic constraints upon fertility are analyzed within the theory of household production and allocation of time. The interaction of individual components of family income and the direct economic costs of children are shown to have an increasingly large impact upon Swedish fertility as industrialization proceeds.</p> </abstract>
<abstract> <p>This paper concerns experiment design for regression analysis. New procedures for handling uncertainty about regression functional form are suggested. The discussion stresses applications to subsidy experiments in economics.</p> </abstract>
<abstract> <p>The properties of systems of investment equations derived under the hypothesis of present value maximization are investigated. The possibility that either the optimal time rate of change in some factor or the stationary level of some stock may increase with its own rental rate is shown to be consistent with the hypothesis in the case of more than one factor. A condition necessary for this result is that marginal products depend on the rates of which factor levels are justified.</p> </abstract>
<abstract> <p>The purpose of this paper is twofold. The first is to obtain a nice characterization of production possibility sets generalizing the well-known nonsubstitution theorem. The second is to obtain a relationship between indispensability of nonproducible commodities and desirable properties of production possibility sets.</p> </abstract>
<abstract> <p>The limited information maximum likelihood and two-stage least squares estimates have the same asymptotic normal distribution; the ordinary least squares estimate has another asymptotic normal distribution. This paper considers more accurate approximations to the distributions of the so-called "k-class" estimates. An asymptotic expansion of the distribution of such an estimate is given in terms of an Edgeworth or Gram-Charlier series (of which the leading term is the normal distribution). The development also permits expression of the exact distribution in several forms. The distributions of the two-stage least squares and ordinary least squares estimates are transformed to doubly-noncentral F distributions. Numerical comparisons are made between the approximate distributions and exact distributions calculated by the second author.</p> </abstract>
<abstract> <p> For fixed sample size N, the distribution functions of the k-class estimators (k non-stochastic) are approximated up to terms whose order of magnitude is 1/μ, where μ &lt;sup&gt;2&lt;/sup&gt; is what is referred to in the literature as the concentration parameter. Similar results are also given for the double k-class estimators. </p> </abstract>
<abstract> <p>The paper proves the asymptotic normality of a generalized least squares estimator utilizing estimated autocovariances of the residual in a regression equation having a residual following a mixed autoregressive, moving-average process. It also proves the asymptotic normality of the best linear unbiased estimator and shows that the two asymptotic distributions are the same.</p> </abstract>
<abstract> <p>When there is error in the deflating variable of a multiple linear regression, the ordinary least squares slope estimators are inconsistent unless one of the following conditions obtains: (i) the intercept of the ratio regression is zero, (ii) the mean of each independent ratio variable is zero, or (iii) the error in the deflator is systematic (has zero variance). The intercept estimator may be inconsistent even when the slope estimators are consistent; for the intercept estimator to be consistent it is sufficient that the slope estimators are consistent and, in addition, the expected value of the proportional error in the reciprocal of the deflator is zero. When there is inconsistency, the estimated regression tends to indicate a proportional relationship between the dependent ratio variable and each of the independent ratio variables (an important application of this result concerns the estimation of distributed lag accelerator relationships). Given assumptions about the distribution of error in the deflator, consistent estimators can be evaluated; an empirical application suggests that, as a rule, inconsistency is small even when the errors in the deflator are quite large.</p> </abstract>
<abstract> <p>The paper compares the power of two tests for serial correlation in regression models with lagged dependent variables, recently suggested by Durbin, with that of the likelihood ratio test by means of two sets of Monte-Carlo experiments--one in which the exogenous series is taken to be the quarterly GNP series for the USA and the other in which the exogenous series is generated by a known autoregression.</p> </abstract>
<abstract> <p>A distributed lag estimator is developed here from Bayesian priors regarding the"smoothness" of the lag curve. "Smoothness" priors of the dth degree are represented by a normal density function with zero mean of the difference of order d + 1 of the coefficients, where d will usually be one to zero. Such probabilistic priors, which do not imply any parametrization of the lag curve, are, it is contended here, a more accurate representation of the kind of prior knowledge that has led many researchers to use the polynomial distributed lag estimation procedure, and other parametrization procedures, in the past. The estimator developed here is, moreover, very simple in its implementation. All that is needed is any least squares regression program.</p> </abstract>
<abstract> <p>The existence of an equilibrium solution for the open expanding economy model of Morgenstern-Thompson is proven under weaker assumptions than originally used.The proof is based on a theorem of the Farkas type formulated for finite dimensional linear spaces ordered by means of cones.</p> </abstract>
<abstract> <p>It has already been proven that when production is described by differentiable supply functions, the equilibrium correspondence is continuous for "most" economies [7 and 8]. The present paper generalizes this result to situations where production activities are described through a certain class of correspondences: namely, those which have a smooth graph and are such that the profit function is differentiable.</p> </abstract>
<abstract> <p>This paper develops an econometric model of production technology in terms of the "ex ante-ex post" description of production possibilities. Ex ante and ex post substitution characteristics are allowed to differ from one another and parametric representations are derived which provide arbitrary second-order approximations to the true underlying characteristics. Nested in the maintained model are the specialized "putty-putty", "putty-clay", and "clay-clay" structures of technology; and hypothesis tests are developed to test for the applicability of these specialized structures. An example is provided in which the hypotheses that fossil fuel electricity generation can be characterized by putty-clay or clay-clay technologies are tested using data drawn from individual United States electricity generating plants.</p> </abstract>
<abstract> <p>This paper proposes a simple and straightforward method of finding a consistent and efficient set of sectoral capacity growth rates and output and investment levels in a dynamic input-output (IO) model with given production capacities at a certain "base period" of time and given consumption targets for a later "terminal period." The basic problem is that terminal capacities have to be consistent with terminal production requirements as given by the consumption targets, intermediate input requirements, and investment, where both the latter are treated endogenously. The basis for endogenous investment is (a) the assumption that the planning period as a whole is characterized by a set of constant sectoral growth rates and (b) the assumption that investment does not create any excess capacity. These two assumptions form the core of the notions of consistency and efficiency respectively. The method proposed is an integrated iterative procedure with endogenous revision of the division of sectors between those operating at full capacity (bottleneck) and the rest, output and investment levels, and rates of growth. The method is, in fact, a straightforward adaptation of the standard method of solving an IO model by power series expansion. The paper also discusses the method of target revision taken in conjunction with prior bounds on growth rates and certain aspects of the relation between investment and technology in the frame of the model proposed. It ends with a brief review of the literature, criticizing, in particular, the methods and approaches followed in a large number of applied planning models to tackle the set of issues discussed.</p> </abstract>
<abstract> <p>Proposals for the computation of competitive equilibria in the presence of taxation contained in recent joint work by the author are applied to a model of the United Kingdom economy and tax system for the period 1968-1970. Difficulties of model specification and parameterization are also discussed. Results provide indications of efficiency, distributional, and welfare impacts for a number of alternative tax changes.</p> </abstract>
<abstract> <p>Consider a small country with a strictly convex production possibility set, where non-traded goods as well as traded goods are produced. Suppose that tariffs are the only causes of the distortions in this economy. In the present paper, we will prove that in this economy reduction of the highest tariff rate to the level of the second highest rate will improve welfare if (i) inferior goods do not exist, (ii) the good on which the highest tariff rate is imposed is substitutive to all the other goods both in consumption and in production, and (iii) the non-traded goods are substitutive to all the other goods both in consumption and in production.</p> </abstract>
<abstract> <p>The sets of local demand functions which would generate either linear or nonlinear schedules in a heterogenous as well as homogeneous space economy are determined by iterative applications of the separation of variables technique for solving differential equations. Nonlinear delivered price schedules which reflect profit maximization objectives are thereby identifiable. In turn, these schedules can be distinguished from those which involve strictly predatory price behavior.</p> </abstract>
<abstract> <p>The paper analyzes borrowing and lending on uncertain future income, with a positive probability of bankruptcy. Creditor and debtor play a strategic game, in which it is shown that optimal creditor behavior is not generally well defined. The model suggests that under uncertainty the availability of credit may be restricted below that which would be predicted by classical microeconomic theory.</p> </abstract>
<abstract> <p>Social choice lottery rules are analyzed for two-candidate elections with voters who may be uncertain about whom they prefer. A voter's uncertainty is reflected by a nonobservable choice probability of voting for candidate A rather than candidate B, given that he votes. Lottery rules are based on the votes for A and B; they are to be monotonic and symmetric in voters and in candidates. Given n voters, all lottery rules are convex combinations of about n/2 basic rules ranging from the coin-flip rule to simple majority. Candidate A's win probability and two measures of expected voter satisfaction are examined as functions of the individuals' choice probabilities and the lottery rules. Comparisons are made between simple majority and the proportional lottery rule which assigns social choice probability of j/n to A when A gets j of n votes. Each of simple majority and the proportional lottery rule satisfies attractive properties that are not satisfied by the other rule.</p> </abstract>
<abstract> <p>Formal analogies between (dynamic) systems and economics have been pointed out by Paul A. Samuelson. The Le Chatelier-Samuelson principle, as a common property of extremum equilibrium (optimal) systems in respect to exogenous conditions, is such an analogy with theory of rational behavior and preference. The weak principle classifies a generalized weak principle. We shall argue that this generalized principle is "the economic integrability condition" itself in a broader sense and, going beyond that, asserts the existence of a maximal system so as for any affiliated systems, such as the equilibrium system, to be imbeddable therein. In consumer theory, this principle, with its identity with a maximal ordinal utility representation (without any regularity conditions), is implied by, or becomes equivalent for a non-satiable individual to, the (modified) strong axiom of revealed preference. The present characterization of the weak principles, making use of the mathematical results of R. Tyrrell Rockafellar [36, 37], completes Samuelson's general principle of economic method [40, pp. 22].</p> </abstract>
<abstract> <p>A convenient technique used for solving several types of economic problems is to cast them as optimization problems having the line integral of the (inverse) demand and/or supply functions in the objective. But if these functions are non-integrable (having a nonsymmetric Jacobian matrix) then alternative techniques must be used. This paper surveys these and proposes an additional solution technique, based on making the given functions integrable by making a small number of substitutions and iteratively adjusting a parameter associated with each substitution until a solution is found. The technique is particularly applicable when certain neoclassical assumptions are satisfied.</p> </abstract>
<abstract> <p>This paper presents extended tables for the Durbin and Watson [3 and 4] bounds test. The tables can be used for samples with 6 to 200 observations and for as many as 20 regressors.</p> </abstract>
<abstract> <p>In a socialist economy the shortage in consumer goods, housing shortage, disturbances in material supply, shortage in investment goods, and labor shortage are to be traced back to common main causes. Certain properties of the economic mechanism permanently reproduce shortage. First we study the microanalytics of the producer firm. Efforts at increasing production may hit three different upper constraints: the constraints of physical resources, demand constraints, and the firm's budget constraint. The system can best be characterized according to which of these constraints is in effect. From this aspect resource-constrained and demand-constrained systems are discerned. In the former it is bottlenecks of production and not buyers' demand that delimit production; in the latter the case is the reverse. A socialist economy in its "classical" form belongs to the former type. This is connected with the question of whether the firm's budget constraint is "hard" or "soft". If it is hard, spending of the firm will be effectively delimited by its financial abilities. If it is soft, then because losses are almost automatically compensated by the state the firm's demand becomes almost insatiable. The macroanalytical part of the paper demonstrates the mechanism of a chronic shortage economy with the aid of a hydraulic analogy. In it the sector of firms "pumps out" the slack of the system. This phenomenon is due primarily to the effect of "investment hunger" shown to be the result of an irresistible expansion drive. Finally, the paper briefly touches on the interdepencies of shortage, inflation, and employment.</p> </abstract>
<abstract> <p>In large and complicated management systems, solutions with the same indices, but with different degrees of aggregation, are used and coordinated. For example, in hierarchical systems, those in higher management levels make decisions with more aggregated indices than those in lower management levels. Managers of an individual subsystem within a large and complicated system use detailed information about their own subsystem and aggregated information (in some degree or other) about other subsystems. The principal idea of the iterative aggregation method is to consecutively recompute the aggregated indices characterizing the activities of the whole system, followed by a recomputation of the detailed indices characterizing each of its subsystems. From a theoretical point of view these methods are generalizations of some classes of iterative and decomposition methods.</p> </abstract>
<abstract> <p>A model of a cooperative game without side payments is used to study the core of an economy subject to increasing returns. The major result is that quasi-convexity and subhomogeneity of the cost function along with restrictive but plausible conditions on preferences are sufficient for the existence of a core. Implications are considered for both partial and general equilibrium analysis.</p> </abstract>
<abstract> <p>This paper reconsiders necessary conditions for extinction of an animal population. Containing many of the models previously developed in the literature, the general model proposed here suggests that (i) conditions for extinction depend exclusively on the relation of a minimum viable population size to the minimum population size at which any exploitation is profitable, (ii) necessary conditions for extinction need not be characterized in terms of returns-to-scale parameters or growth rates as other authors have contended, and (iii) possible short-run shut-down may save a population from extinction if the fish population at the time of shutdown is large enough to sustain the species.</p> </abstract>
<abstract> <p>Technical change in general milk processing is estimated on the basis of a homothetic frontier production function allowing neutrally variable scale elasticity. The results show that technical progress is characterized by a rapid increase in optimal scale and a small capital saving bias, increasing the marginal productivity of labor relative to capital. To characterize technical change, Salter's measures of bias and technical advance are utilized and interpreted within the framework of the efficiency concepts of Farrell.</p> </abstract>
<abstract> <p>A decomposable inequality measure is defined as a measure such that the total inequality of a population can be broken down into a weighted average of the inequality existing within subgroups of the population and the inequality existing between them. Thus, decomposable measures differ only by the weights given to the inequality within the subgroups of the population. It is proven that the only zero-homogeneous "income-weighted" decomposable measure is Theil's coefficient (T) and that the only zero-homogeneous "population-weighted" decomposable measure is the logarithm of the arithmetic mean over the geometric mean (L). More generally, it is proved that T and L are the only decomposable inequality measures such that the weight of the "within-components" in the total inequality of a partitioned population sum to a constant. More general decomposable measures are also analyzed.</p> </abstract>
<abstract> <p>An economy with production is considered where the labor time is the same for all working agents and for all types of labor. It is shown that a private goods equilibrium exists if the labor time is fixed. An equilibrium with public goods exists if the labor time is variable but the distribution of consumers over types of labor is given, where labor time appears as a public good. Optimality properties of solutions where the equilibrium conditions of both types of equilibrium are fulfilled simultaneously are analyzed.</p> </abstract>
<abstract> <p>This paper is a study of econometric problems and methods involved in interpreting the variation between unemployed job seekers in the length of time they are out of work in the light of search theories. In particular we propose in equation (7.4) a parametric form for the duration of unemployment distribution which allows one to study the temporal variation in the chances of an unemployed man returning to work as well as to estimate a regression error variance. The study is illustrated by calculations using data from interviews with a sample of British unskilled workers.</p> </abstract>
<abstract> <p>This article first illuminates possible discrepancies between normal and logistic models in the bivariate dichotomous case of qualitative response analyses. Then a Cox-type test statistic for separate families of hypotheses is proposed for the purpose of comparing the two models. An asymptotic distribution of the new test statistic is derived, and also the consistency of the test is shown. Before applying the Cox-type tests to actual data, Berkson's minimum chi-square estimators of the two models are explained in Section 3. In Section 4, logistic, normal, and linear models are worked out for a set of economic data, and the Cox-type tests are applied to compare them.</p> </abstract>
<abstract> <p>In this paper, a class of statistical models which generate simultaneous equation models with both discrete and continuous endogenous variables is introduced. This class of models can also be regarded as a new class of switching simultaneous equation models which are of general interest. Identification and estimation problems are investigated. Several simple consistent two stage methods are proposed. The consistency of those estimators is proved. Two step maximum likelihood procedures are then developed.</p> </abstract>
<abstract> <p>The efficiency gain from observing the sample classification in a disequilibrium or switching model is analyzed. The problem is set up as one of comparing the precisions of estimates based on a joint density with those based on a marginal density. Asymptotic results are obtained for a simple model.</p> </abstract>
<abstract> <p>For a general vector linear time series model we prove the strong consistency and asymptotic normality of parameter estimates obtained by maximizing a particular time domain approximation to a Gaussian likelihood, although we do not assume that the observations are necessarily normally distributed. To solve the normal equations we set up a constrained Gauss-Newton iteration and obtain the properties of the iterates when the sample size is large. In particular we show that the iterates are efficient when the iteration begins with a &lt;tex-math&gt;$\sqrt{N}\text{-consistent}$&lt;/tex-math&gt; estimator. We obtain similar results to the above for a frequency domain approximation to a Gaussian likelihood. We use the asymptotic estimation theory to obtain the asymptotic distribution of several familiar test statistics for testing nonlinear equality constraints.</p> </abstract>
<abstract> <p>Structural and stochastic neutrality have refutable implications for aggregate economic time series only in conjunction with other maintained hypotheses. Simple and commonly employed maintained hypotheses lead to restrictions on measures of feedback and their decomposition by frequency. These restrictions also suggest an empirical interpretation of the notional long and short runs. It is found that a century of annual U.S. data, and postwar monthly data, consistently support structural superneutrality of money with respect to output and the real rate of return and consistently reject its superneutrality with respect to velocity. A quantitative characterization of the long run is suggested.</p> </abstract>
<abstract> <p>A model of aggregate economic activity is formulated which emphasizes the effects of borrowing constraints in the presence of uninsurable risk. An important determinant of current income level is shown to be the cross-sectional distribution of wealth. As this distribution evolves endogenously, the model is capable of producing rich dynamics from a simple specification of exogenous shocks. The model shows that this phenomena can contribute to observed asset price volatility.</p> </abstract>
<abstract> <p>This paper develops a model of labor supply for married women which takes into account both the joint decision on participation and hours, and the nonlinear shape of the budget constraint due to taxation. The model can explain the absence of observations of the tax kink by assuming the existence of optimization errors in addition to errors capturing taste variation. The estimates of the model, which are obtained with British micro data, suggest that the overall wage elasticity is about 2 and that participation is more responsive to wages than hours of work.</p> </abstract>
<abstract> <p>It is commonly believed that textbook publishers attempt to "kill off" competition from used textbooks through yearly edition changes. In the context of Wicksell's model of durable goods, Peter Swan has shown that such "planned obsolescence" is never optimal: a monopolist seller of durable goods maximizes profits by setting product durability equal to the competitive or socially optimal level, and efficiently extracts consumer surplus through sales price alone. This paper formulates a monopolist seller's choice of price and durability as the solution to a Stackelberg game between the monopolist and consumers. We employ a new equilibrium model of a durable goods market which, unlike Wicksell's model, recognizes that scrappage of durables is endogenously determined. We show that with endogenous scrappage, consumers have a substitution possibility which constrains the profits of a monopolist seller. This constraint on profits causes the monopolist to distort durability from the socially optimal level. We derive conditions under which this distortion takes its most extreme form: the monopolist kills off competition from used durables by producing new assets of zero durability.</p> </abstract>
<abstract> <p>This paper amends the Aumann and Kurz single commodity "Power and Taxes" model in several ways: A linear production technology is assumed, incentive effects are introduced, and tax schedules are restricted to be linear. A theorem is stated which characterizes the linear tax schedules which are the NTU solutions of the model. The solutions of an example are computed, providing a perspective on a result of the Aumann and Kurz model that equilibrium marginal tax rates are not less than 50 per cent. For this example, equilibrium marginal tax rates are less than 50 per cent; incentive effects appear to be responsible for the low tax rates.</p> </abstract>
<abstract> <p> This paper studies conditions for competitiveness of strategic market games where agents set prices and quantities (a game is said to be competitive if its Nash equilibria are Walrasian). Some necessary conditions are first derived which show that discontinuities in the strategic outcome functions may be an essential element for competitiveness. Then a set of economically meaningful sufficient conditions for competitiveness are derived. These bridge the gap between visions of competition "à la Bertrand" and some recent non-Walrasian concepts. </p> </abstract>
<abstract> <p>This paper investigates the effect that common knowledge of public information has on individual beliefs. We assume that n individuals start with the same prior beliefs over a finite probability space, and then each observes private information. We prove that if an admissible statistic of their posterior probabilities of an event becomes common knowledge, then everyone's posterior probabilities for that event must be the same. The class of admissible statistics includes any statistic which is an invertible function of a stochastically monotone function. We also prove that if information partitions are finite, an iterative procedure of public announcement of the statistic--where the statistic is publicly announced and then individuals recompute posterior probabilities based on their previous information plus the announced value of the statistic--converges in a finite number of steps to the common knowledge situation described above. The result has applications to asymmetric information models in economics, where private information becomes incorporated into an aggregate, publicly observed statistic such as a price or quantity in a market.</p> </abstract>
<abstract> <p>We consider an economy in which agents may or may not communicate with each other. Coalitions can form only between linked agents. We consider two cases: agents must communicate directly to be in the same coalition or in the second case indirectly. We consider the communication to be random. The economy may then be represented by a stochastic graph; the admissible coalitions are then stochastic and thus so is the core of an economy. We demonstrate that if the probability that agents are linked with each other does not tend to zero too fast as this number increases, then the probability that a coalition will form and block any non-Walrasian allocation tends to one, as the number of agents goes to infinity.</p> </abstract>
<abstract> <p>This paper relates the issue of efficiency in communication to the problem of designing games that implement a given objective in Nash equilibrium. A message process that "realizes" (or computes) the objective is used to construct a game that implements it in Nash equilibrium. Any efficient encoding of information that occurs in the message process causes a reduction in the size of the strategy space of the game that is constructed. Necessary and (stronger) sufficient conditions on the message process are given for this construction.</p> </abstract>
<abstract> <p>Simultaneous bargaining over more issues by two bargainers is treated by taking sums of bargaining games and requiring bargaining solutions to satisfy certain (super-) additivity axioms. A (new) characterization of a family of so-called proportional solutions is given with the aid of three axioms: (partial) superadditivity, homogeneity, and weak Pareto optimality. Requiring, besides individual rationality and Pareto continuity, the axioms of restricted additivity, scale transformation invariance, and Pareto optimality, yields an alternative characterization of a family of solutions consisting of all nonsymmetric extensions of Nash's solution. Also these solutions exhibit a (limited) proportionality property. Further, the relation with the Super-Additive solution of Perles and Maschler is discussed, and also the link with Myerson's results on proportional and utilitarian solutions.</p> </abstract>
<abstract> <p>In this paper several results are established which provide for the consistent estimation of macroeconomic effects using cross-section data, for general assumptions on the movement of the population distribution over time. We show that macroeconomic effects are always consistently estimated by linear instrumental variables coefficients, where the instruments are determined by the form of distribution movement. This leads to a natural way to assess the biases in OLS coefficients as estimators of macroeconomic effects, provides a nonparametric macroeconomic interpretation of linear instrumental variables coefficients when the true microeconomic behavioral model is unknown, and gives a nonparametric interpretation of standard regression decomposition statistics such as R^2 relative to the information costs of nonlinearities in aggregation. All of the results are valid without imposing any testable restrictions on the cross-section data.</p> </abstract>
<abstract> <p>This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling.</p> </abstract>
<abstract> <p>In an economy with incomplete markets, firms' profits at different dates and contingencies cannot be aggregated into a single index and so profit maximization is not well-defined. In this paper we propose an objective for firms to pursue which is a generalization of the idea of profit maximization. We show that, if firms' managers can transfer current income between shareholders at the first date, and if shareholders have what we call competitive perceptions concerning the effect of a change in production plan on share prices, then each firm will maximize a weighted sum of shareholders' private valuations of the firm's production plan, where the weights are the initial shareholdings. We then define, and prove the existence of, a competitive equilibrium in which firms pursue this proposed objective. Finally, we analyze the optimality properties of the competitive equilibrium.</p> </abstract>
<abstract> <p>If buyers are less well-informed about product quality than sellers, market prices will reflect average quality. Sellers of high quality products therefore have an incentive to engage in some distinguishing activity which operates as a signal to potential buyers. This paper explores the viability of such signalling or "informational equilibria." It is established that with a continuum of quality levels there is no Nash equilibrium. An alternative non-cooperative equilibrium concept is then developed in which potential price searching agents take account of possible reactions by other agents. It is shown that there is a unique "reactive" informational equilibrium.</p> </abstract>
<abstract> <p>We study a team with many processes; the output process depends on the resources allocated to it by the resource manager, on a local decision by the process manager, and on a (random) parameter of the process. We compare two communication patterns: (1) resource allocations are based on full information, but local decisions are based only on corresponding local information; (2) all decisions are based on full information. We show that, if the criterion is expected average output per process, and if the process parameters are independent and identically distributed, then (under certain regularity assumptions) for "large" teams the additional communication among process managers in (2) over (1) has approximately no value.</p> </abstract>
<abstract> <p>It is well known that the returns on various betting opportunities at a racetrack are determined by a competitive bidding of the bettors in a natural environment of their decision making. In this paper, two simple bets of unknown but identical winning probabilities are identified. An analysis of 1,089 observations shows that the data are consistent with the hypothesis that both bets are identically priced, an implication of an efficient speculative market.</p> </abstract>
<abstract> <p>This article considers an economy in which there is one public good financed by means of commodity taxes (lump sum transfers being not available). The first part of the paper is devoted to the study of tax equilibria. Sufficient conditions for the existence of an equilibrium with respect to a given tax system are given. When the tax system is modified, the structure of the corresponding set of tax equilibria is analyzed, and continuity properties of equilibria (with respect to the tax system) are stated. In the second part, attention is focused on the Pareto ranking of tax equilibria. In a given equilibrium, the directions of policy tools changes for a Pareto improvement (if any) are characterized. The "size" of the set of second best Pareto optima in the set of tax equilibria is evaluated.</p> </abstract>
<abstract> <p>We investigate the notion of the Nash social welfare function and make a fundamental assumption that there exists a distinguished alternative called an origin, which represents one of the worst states for all individuals in the society. Under this assumption, in Sections 1 and 2, we formulate several rationality criteria that a reasonable social welfare function should satisfy. Then we introduce the Nash social welfare function and the Nash social welfare indices which are the images of the welfare function. The function is proved to satisfy the criteria. In Section 3 it is shown that the Nash social welfare function is the unique social welfare function that satisfies the criteria. Then, in Section 4, we examine two examples which display plausibility of the welfare function.</p> </abstract>
<abstract> <p>This paper is concerned with the sources of variation in the earnings of American scientists over the decade 1960-70. It focuses on sources of variation over time as well as at a point in time. Earnings variation over time is decomposed into sources due to measurable variables representing an earnings function, a random effect individual variance component in the level of earnings, a random effect individual component in earnings growth, and a serially correlated transitory component. Maximum likelihood estimates of the implied parameters are presented and a method for obtaining GLS estimates of the earnings regression function, when the residual variance components are given, is explored.</p> </abstract>
<abstract> <p>This paper is concerned with the identification and estimation of the parameters in a dynamic simultaneous equations model with stationary disturbances when both the endogenous and exogenous variables are subject to random measurement errors. A frequency domain approach is suggested to fully utilize the information contained in the data. The first part of this paper explores the identification criteria. The second part of this paper suggests estimation methods for such a model. Both full information and limited information estimation methods are studied and their respective gains and losses are evaluated.</p> </abstract>
<abstract> <p>The identifiability of linear dynamic models with autocorrelated errors is considered. Without a priori assuming relative left primeness of the structures, global identifiability conditions in the case of affine cross-equation restrictions and local identifiability conditions in the case of continuously differentiable cross-equation restrictions are derived.</p> </abstract>
<abstract> <p>This lecture surveys the history and recent resurgence of interest in models with errors in variables and substantive unobservable variables. Among several examples of such models, special attention is paid to a schooling-occupation-income achievement model in which identification and estimation are based on the variance-components structure (across families) of the unobserved individual ability variable.</p> </abstract>
<abstract> <p>This paper extends the single equation regression model with the truncated dependent variable considered by Tobin [10] and Amemiya [1] to multivariate and simultaneous equation models and proposes a computationally simple consistent estimator.</p> </abstract>
<abstract> <p>The paper presents maximum likelihood methods for estimating four types of disequilibrium models. In each case the model includes three equations: the demand equation, the supply equation, and the condition that quantity observed is the minimum of quantity demanded and quantity supplied. The first model consists of just these equations. In the second model one knows whether one is on the demand function or the supply function by looking at the direction of the change in price. In the third model the price change is assumed to be proportional to excess demand. In the fourth model the price change is a stochastic function of excess demand and possibly other exogenous variables. Some illustrative calculations are presented using the housing starts model considered by Fair and Jaffee in an earlier issue of this journal.</p> </abstract>
<abstract> <p>This paper presents a comparison of three distributed lag estimators: OLS, the Almon procedure, and the Hannan "inefficient" method. Each method is compared for sample sizes of 50 and 100 for several alternative distributed lag shapes and residual process structures. The results not only reveal the relative performance of these estimators, but also provide evidence on each method's performance under misspecification with respect to lag length and the residual process.</p> </abstract>
<abstract> <p>The purpose of this paper is to give a systematic account of the significance that antagonism among opinions of individuals has on the theory of social choice. We define an "intensity" of antagonism among individuals and an index of "possibility" of constructing social welfare functions, and show that when there is less antagonism in a society, then there is more "possibility" of constructing social welfare functions. Since antagonism is an opposite concept to similarity, this relation can be viewed as a formal realization of Arrow's assertion [1, Ch. 7] that sets preference similarity as the basis of social welfare judgement.</p> </abstract>
<abstract> <p>A two equation model that explains the simultaneous determination of wage and price inflation and their interaction with inflationary expectations (of the adaptive type) and real variables is presented. A dynamic definition of the long run trade off between inflation and unemployment is introduced and applied to the model in order to find conditions under which the model is stable or, in economic terminology, has a long run trade off. The model is then applied to the U.S. economy during the period 1949-1970. It is found that for all speeds of adjustment in expectations between 0 and 1 there is a permanent trade off.</p> </abstract>
<abstract> <p>This paper demonstrates how the short-run static stability conditions of the two-sector growth model can be derived by converting the problem into a two-person, two-good exchange model. It is shown that the only necessary and sufficient condition for short-run stability is the Marshall-Lerner condition, and that the Drandakis (sufficient) condition follows easily from the former.</p> </abstract>
<abstract> <p>A pairwise trading process is formulated subject to conditions of non-negativity of traders' holdings, quid pro quo, and a limited number of trading opportunities. The following points are made: (i) there is a centralized procedure that achieves the equilibrium allocation for an arbitrary economy; (ii) it is not in general possible to find a decentralized procedure that achieves the equilibrium allocation for an arbitrary economy; and (iii) in a monetary economy there is a decentralized procedure that achieves the equilibrium allocation. The usefulness of money is that it allows decentralization of the trading process.</p> </abstract>
<abstract> <p>The distribution of personal income is approximated by a two-parameter gamma density function (Pearson Type III). The two parameters may be considered as indicators of scale and of inequality, respectively. Maximum likelihood estimates of the parameters are derived from a random sample using graphical techniques, and a likelihood ratio test for the hypothesis that the inequality parameter is the same for different distributions is presented. The derivation of both the estimates and the test statistic requires computing the arithmetic and geometric means from the sample. An empirical application, including a comparison of the gamma and lognormal distributions to demonstrate the better fit of the gamma, is made to personal income data in the United States for the years 1960 to 1969. Using the gamma density, inequality is shown to decrease when unemployment or inflation decreases, or when the real national product increases.</p> </abstract>
<abstract> <p>Our model of a multi-sector infinite horizon economy contains uncertainty about future input-output possibilities and labor supply. Future utilities are not discounted. Even under perfect certainty a competitive program which satisfies some transversality condition (e.g., a bounded price system) does not have to be optimal. We prove that if the technology satisfies some concavity condition, a necessary and sufficient condition for optimality of any competitive program is that the expected present value of the inputs (or outputs) be bounded over time.</p> </abstract>
<abstract> <p>The classical one-sector problem of optimal growth theory over an infinite horizon in continuous times with a convex-concave production function is studied. Consumption and investment are subject to the inequality phase constraint. The basic instrument is a theory of optimal control. Results of the qualitive analysis are presented on the phase diagram.</p> </abstract>
<abstract> <p>The main theorem in this paper is one in a long series of theorems which show the existence of equilibrium in economies without convex preferences, without convex consumption sets, or without complete and transitive preorderings. Here, it is proved that a competitive equilibrium exists in a large economy with not necessarily convex consumption sets and where preferences are continuous and transitive. As an additional assumption the continuity of the wealth distribution with respect to the Borel-Lebesque measure is required.</p> </abstract>
<abstract> <p>This paper surveys some recent developments in equilibrium analysis based on a differential viewpoint. They deal with the structure of the set of equilibria, with the study of regular and singular economies, with the determinateness of the number of equilibria, and with an application to characterizing economies having a unique equilibrium. Equilibrium analysis from the differential viewpoint turns out to be formally similar to models encountered in Thom's catastrophe theory understood as a general theory of bifurcation phenomena.</p> </abstract>
<abstract> <p>The existence of comprehensive sets in R^3 which cannot be transformed onto convex sets by means of ordinal transformations (i.e., continuous, strictly increasing transformations of the coordinate axes) is established. Such sets can be realized as the disposable hulls of pure exchange economies with convex preference orderings.</p> </abstract>
<abstract> <p>In this paper the process of exchange is formulated as a noncooperative game. The game is analogous to the familiar institution of competitive bidding in a sealed-tender auction in which one agent (the auctioneer) chooses among trades offered by the other agents (the bidders). Using various regularity assumptions it is shown that this noncooperative game has a Nash equilibrium which yields an allocation in the core of the corresponding cooperative game of exchange. Also, as the bidders are replicated by division this allocation (aggregated by types) converges to a Walrasian allocation in which each type's budget constraint (using the efficiency prices) is satisfied. Thus it appears that bidding is a competitive process for achieving a cooperative outcome, and in the limit, a market outcome.</p> </abstract>
<abstract> <p>It has recently been shown that the utility of playing a game with side payments depends on a parameter called strategic risk posture. The Shapley value is the risk neutral utility function for games with side payments. In this paper, utility functions are derived for bargaining games without side payments, and it is shown that these functions are also determined by the strategic risk posture. The Nash solution is the risk neutral utility function for bargaining games without side payments.</p> </abstract>
<abstract> <p>Where alternatives, players, and strategies for each player are finitely many, a game form assigns a lottery over alternatives to each configuration of players' strategies. It is straightforward iff it guarantees that each player, whatever his utilities, will have a dominant strategy. It is unilateral iff only one player can influence the outcome, and duple if it restricts the final outcome to a fixed pair of alternatives. Any straightforward game form, it is shown, is, on a domain which gives each player a dominant strategy for each utility scale, a probability mixture of game forms, each unilateral or duple.</p> </abstract>
<abstract> <p>This paper deals with the problem of computing optimal strategies in a model of intertemporal choice under uncertainty for a competitive firm. The paper presents a method for computing the optimal strategies for (i) investment, (ii) production, (iii) financing, and (iv) consumption or dividends when (i) the entrepreneur's utility function displays constant absolute risk aversion, (ii) the production technology is certain and of the activity analysis variety, and (iii) prices are uncertain and serially independent. It is shown that the task of finding the optimal investment and production strategies reduces to a concave programming problem while the optimal financing and consumption strategies can be computed by analytical methods after the investment and production strategies are obtained. When prices are normally distributed, the optimal production and investment strategies can be found by means of quadratic programming.</p> </abstract>
<abstract> <p>Single equation models of fertility behavior are frequently subject to specification error and often fail to capture the dynamic nature of household decision-making. This paper takes a first step in resolving these difficulties by estimating the fertility equation within a simultaneous system and by tracing the time path of fertility determinants via Boot and Theil multipliers. The estimates obtained are reasonable in both sign and magnitude and the dynamic character of fertility behavior is borne out by the finding that ordinary least squares estimates generally under- or over-estimate the total effects of changes in fertility determinants. Of particular interest is an intergenerational variable which captures the effect of socio-economic status on fertility behavior.</p> </abstract>
<abstract> <p>The regression relation between regularly sampled Y(t) and X"19t),..., X"N(t) implied by an underlying model in which time enters more generally is studied. The underlying model includes continuous distributed lags, discrete models, and stochastic differential equations as special cases. The relation between parameters identified by regular samplings of Y and X"j and those of the underlying model is characterized. Sufficient conditions for identification of the underlying model in the limit as disaggregation over time proceeds are set forth. Empirical evidence presented suggests that important gains can be realized from temporal disaggregation in the range of conventional measurement frequencies for macroeconomic data.</p> </abstract>
<abstract> <p>For the two sample linear heteroscedastic regression model, moments of a popular two stage Aitken estimator are derived analytically. Even for small samples and/or near homoscedastic errors, the two stage procedure is surprisingly efficient relative to both unweighted least squares and the Gauss-Markov estimator. These exact results are compared with the author's previous calculations derived from Nagar approximations.</p> </abstract>
<abstract> <p>In Pesaran [9], the test developed by Cox for comparing separate families of hypotheses was applied to the choice between two non-nested linear single-equation econometric models. In this paper, the analysis is extended to cover multivariate nonlinear models whenever full information maximum likelihood estimation is possible. This allows formal comparisons not only of competing explanatory variables but also of alternative functional forms. The largest part of the paper derives the results and shows that they are recognizable as generalizations of the single-equation case. It is also shown that the calculation of the test statistic involves very little computation beyond that necessary to estimate the models in the first place. The paper concludes with a practical application of the test to the analysis of the U.S. consumption function and it is demonstrated that formal tests can give quite different results to conventional informal selection procedures. Indeed, in the case examined, five alternative hypotheses, some of which appear to perform quite satisfactorily, can all be rejected using the test.</p> </abstract>
<abstract> <p>In this paper a reduced form estimator is developed which combines the corresponding restricted 3SLS and the unrestricted LS estimators. This estimator is similar to the `positive part' Stein-like estimators proposed by Baranchik [2] and S. Sclove [16] in the classical multivariate regression context. It is shown that, whereas the restricted (derived) 3SLS and 2SLS reduced form estimates possess no finite moments (hence have unbounded risk), the modified Stein-like reduced form (MSRF) estimator has finite moments of up to order (T - n - m), where T is the sample size, n and m are the number of the endogenous and the non-stochastic exogenous variables in the system. Furthermore it is argued that, asymptotically, the difference between the MSRF and the 3SLS estimators is negligible.</p> </abstract>
<abstract> <p>This paper develops a new family of biased estimators, namely the double k-class, for the parameters of the general linear regression model. We note that James and Stein[7] Stein-rule estimator in the regression context is a member of the family of double k-class. The conditions for the existence of the moments and expressions for the exact and approximate bias, moment matrix, and the risk function of the double k-class estimator are analyzed.</p> </abstract>
<abstract> <p>Keynes' general attitude toward mathematical economics and econometrics, respectively, is discussed in Sections 2-3. The remainder of the paper is devoted to a description and analysis of the interaction between the Keynesian revolution of the mid-1930's and the revolution that had actually started somewhat earlier with respect to the preparation of current official estimates of national income. In this connection an attempt is made to explain why Kuznets' work in the U.S. in the early 1930's was immediately integrated into official national income estimates in the U.S., where Colin Clark's work in Britain was not--with the result that official British national income estimates did not begin to appear until almost a decade later.</p> </abstract>
<abstract> <p>A process which combines a planning procedure for the allocation of final products and a multilateral nonrecontracting trading process for allocating primary and intermediate goods is defined and shown to satisfy Malinvaud's criteria for evaluating planning procedures. Central processing costs are lower than in the Malinvaud procedure since the central planner only collects information on final products.</p> </abstract>
<abstract> <p>We consider a situation in which demand for a product increases with the amount of it already in existence, up to some level of saturation. We focus on the effect of such a phenomenon on the decisions of a producer. A producer who takes this growth pattern into account can obtain a larger discounted profit than one who looks at the market myopically. We show that, under general conditions, the producer's optimal output sequence here leads to lower prices, so that the consumers benefit simultaneously.</p> </abstract>
<abstract> <p>This paper is devoted to the analysis of the dynamic behavior of a sequence of temporary equilibria. The model chosen is a generalization of Samuelson's pure consumption loan model as introduced by J. M. Grandmont and G. Laroque in [5]. Three main results are given. First there is an open and dense subset U of economies for which, near stationary equilibria and cycles, the dynamics take the standard form of an ordinary difference equation. Then conditions are obtained so that, for an economy E in U, stationary equilibria are locally asymptotically stable; these conditions are discussed in the case where there is only one good in addition to money. Last, it is proven that the qualitative behavior of trajectories of E near stationary equilibria and cycles is preserved under small perturbations; i.e., one has a property of local structural stability; this is true in particular with respect to changes in the individual expectations of the agents.</p> </abstract>
<abstract> <p>In this paper, we focus on capital aggregation in a general equilibrium model of production. Various potential aggregates involving intrasectoral and intersectoral, as well as full aggregation are discussed in connection with the various aggregation procedures. It will be shown that the satisfaction of the Gorman conditions allows for full aggregation within a general equilibrium model of production. We shall derive new conditions for aggregation using a composite commodity approach that appears to be somewhat weaker than the conditions associated with restrictions-on-functional-form theorems. Our main conditions relate to the equality of sectoral labor shares. The data for testing those conditions appear to be readily available. It is shown that the equal labor share condition can be applied to models with joint and nonjoint products. In addition, the conditions for aggregation are derived for a model with many primary inputs and also for a model with unequal rates of depreciation. Two sections are devoted to the main correspondences between certain aggregation procedures in the literature from the point of view of a general equilibrium model. The implications of our analysis for the form of the unit cost function and of the aggregate production function are discussed. In particular, if our aggregation condition holds, then the aggregate production function can be Cobb-Douglas, if one of the sectoral forms is also Cobb-Douglas, irrespective of the forms of the other sectoral production functions.</p> </abstract>
<abstract> <p>Charitable contributions are an important source of basic finance for a wide variety of private nonprofit organizations that perform quasi-public functions. The tax treatment of charitable contributions substantially influences the volume and distribution of these gifts. The current study presents new estimates of the price and income elasticities of charitable giving. The parameter estimates are then used with the United States Treasury Tax File to simulate the effects of several possible alternatives to the current tax treatment of charitable giving.</p> </abstract>
<abstract> <p>This paper considers an income maximizing life cycle model of human capital accumulation with the objective of simultaneous estimation of the parameters of the model from observations of the age-earnings profile. The earnings profile, which is a solution to the optimal control problem, is nonlinear and therefore was estimated by nonlinear least squares. The parameter estimates are all quite precise (in the standard error sense) and seem to be intuitively reasonable. The results given here suggest that optimal control models can be verified by direct estimation of the solution.</p> </abstract>
<abstract> <p>This paper explores the implications of the additive social welfare function and the max-min criterion on the optimal distributions of labor and income.</p> </abstract>
<abstract> <p>We generalize to directionally dense but otherwise arbitrary production the Foster-Sonneschein theorem that increases in price distortion reduce consumer welfare. No convexity assumptions appear as in another generalization due to Kawamata. The many consumer case is considered but found to be problematic.</p> </abstract>
<abstract> <p>This paper develops a household production model of individual behavior to focus on the choice of recreational activities. This framework is used to propose a generalized approach for measuring the consumer surplus associated with a natural resource development project. The model is applied to the proposed Mineral King project in California. The results indicate that the project is unlikely to yield a positive net present value.</p> </abstract>
<abstract> <p>Economic researchers are rarely able to conduct surveys or design experiments to obtain evidence with which to assess theories or hypotheses but must rely on information, such as the national income accounting data, compiled by the government bureau of statistics. The bureau revises its national income estimates as more information becomes available or as a result of changes in methods of estimation or minor changes in definitions or classifications. The purpose of this paper is to show that the correlation structures and the autoregressive moving average representations of a number of Australian quarterly time series extracted from the income accounts are relatively insensitive to data revision. The same is true of the cross correlation functions between the "pre-whitened" series.</p> </abstract>
<abstract> <p>The statistical properties of the certainty equivalence control rule and of the least squares estimates generated by this rule are examined experimentally in a linear model with two unknown parameters. It is found that the least squares certainty equivalence rule converges to its true value with probability one and is asymptotically efficient, having an asymptotic distribution with a variance as small as any other strongly consistent rule. However, while a linear combination of the parameter estimates is consistent, the evidence does not confirm that the individual estimates themselves are consistent. If these converge to their true values at all, they do so very slowly (on the order of (log t)^-^1).</p> </abstract>
<abstract> <p>This is the second part of a paper concerning an iterative decentralized1 process designed to allocate resources optimally in decomposable environments that are possibly characterized by indivisibilities and other non convexities. Important steps of the process involve randomization. In Part I we presented the basic models and results, together with examples showing that certain assumptions can be satisfied in both classical and non convex cases. Part II goes further with such examples in showing that our process yields optimal allocations in environments in which the competitive mechanism fails, and also shows how abstract conditions used in Part I can be verified in terms of properties of preferences and production functions that are familiar to economists.</p> </abstract>
<abstract> <p>Direct or indirect additivity of production or utility functions implies dependence of substitution effects on income effects. This dependence is eliminated by implicit additivity, or strong separability along isoquants or indifference surfaces. The present study proposes and analyzes two models, with direct an indirect implicit additivity, respectively, which are generally non-homothetic, non-CES, and include less than 3n parameters for n goods. They give rise to log-linear systems of estimable demand relations. Many other models, such as Cobb-Douglas, CES, Direct and Indirect Addilog, CRESH, CDE, and Non-homothetic CES, are simple, testable special cases of either or both of these models.</p> </abstract>
<abstract> <p>This paper is concerned with clarifying the relationship between the certainty equivalence and first order certainty equivalence results. The effects of applying the first order certainty equivalence result are examined by analyzing the effects on the control vector of a one period stochastic control problem of a change in elements of the covariance matrix of parameters.</p> </abstract>
<abstract> <p>Generalizing the Sharpe-Lintner capital asset pricing model, Dieffenbach [4] presents a model of securities markets in a private enterprise economy in a multiperiod competitive equilibrium with uncertainty. Risk premiums on securities depend on the covariances of holding period returns with the return on the market portfolio and with a multiperiod cost-of-living index. This paper develops a quantitative theory of that relationship suitable for empirical estimation and testing. Whether the Arrow-Pratt relative risk aversion of a representative investor is greater or less than one is important in the theory; the empirical results for the United States suggest that this value exceeds one. A theoretical and empirical application of the theory to the term structure of United States Treasury securities concludes the paper. Mean observed returns are consistent with theoretical predictions for medium and long term securities, but the differences of mean observed returns among bills of different maturities exceed the theoretical predictions.</p> </abstract>
<abstract> <p>A model is developed to describe the allocation of laborers to tasks. The relation between the distributions of earnings and abilities is found to depend on the presence of comparative advantage. Various features of the distribution of earnings are explained in terms of the time taken by a given grade of labor to perform a task of a given difficulty.</p> </abstract>
<abstract> <p>This paper discusses the asymptotic behavior of the neoclassical two-sector growth model when the steady-state conditions are not fulfilled, and derives the asymptotic growth rates for cases in which Hicks neutral technical progress occurs in the investment sector, or Harrod neutral technical progress occurs at different rates in the two sectors. The last section compares the asymptotic properties of this model with the standard steady-state properties of the two-sector growth model.</p> </abstract>
<abstract> <p>Suppose that the coefficients of an input-output matrix, A, are random variables but that we have ascertained their expected values, &lt;tex-math&gt;$\underline{E}A$&lt;/tex-math&gt;. What will be the relation of the Leontief inverse of &lt;tex-math&gt;$\underline{E}A,(I-\underline{E}A)^{-1}$&lt;/tex-math&gt;, to the expected value of the inverse, &lt;tex-math&gt;$\underline{E}(I-A)^{-1}$&lt;/tex-math&gt;? Will one or the other be uniformly greater? We will show that if all coefficients of A are independent, then the expected value of the inverse is uniformly greater than or equal to the inverse of the expected value. If, on the other hand, the column and row sums of the coefficient matrix are fixed, and smaller than one, so that the variables are not independent, then, in the two-by-two case, the opposite is true of the off-diagonal elements._pg 493-498</p> </abstract>
<abstract> <p>A two-person bargaining problem is considered. It is shown that under four axioms that describe the behavior of players there is a unique solution to such a problem. The axioms and the solution presented are different from those suggested by Nash. Also, families of solutions which satisfy a more limited set of axioms and which are continuous are discussed.</p> </abstract>
<abstract> <p>Changes in the mathematical form of theoretic models of an economy during the past four decades and the growth of mathematical economics in that period are considered in relation to each other and to the development of the Econometric Society. The fit of the mathematical form to the economic content of theoretic models, their separation in a completed axiomatic theory and their interplay in its elaboration are examined. Consequences of the axiomatization of economic theory are analyzed.</p> </abstract>
<abstract> <p>In markets with satiation, competitive equilibria may fail to exist, because no matter what the prices are, the satiation points of some traders may be in the interiors of their budget sets. Thus some traders will be using less than the maximum budget available to them, creating a total budget excess. This suggests a revision of the equilibrium concept that allows the budget excess to be divided among all the traders, as dividends. Each trader's budget is then the sum of his dividend and the market value of his endowment. A given system of dividends and prices defines a dividend equilibrium if it generates equal supply and demand. This in itself is not satisfactory because it is too broad: Every Pareto optimal allocation is sustained by some system of dividends and prices. However, the Shapely value yields much more specific information. We prove that, when there are many individually insignificant agents, every Shapely value allocation is generated by a system of dividends and prices in which all dividends are nonnegative and depend only on the net trade sets of the agents, not on their utilities. Moreover, the dependence is monotonic; the larger the net trade set, the higher the dividend. The same result holds for markets with fixed prices, which can be analyzed formally as a special case of markets with satiation. On a more technical level, our analysis has some unusual features. We use a finite-type asymptotic model, rather than a nonatomic continuum. Surprisingly, the results are qualitatively different. (The continuum is too rough a tool for our problem, and leads to inconclusive results.) Also, small coalitions play a critical role in our analysis. (We are led to equations in which the first-order terms cancel; the second-order terms, which take events of small probability into account, become decisive.)</p> </abstract>
<abstract> <p>This paper estimates the incidence of response errors in the Current Population Survey. It proposes a procedure for adjusting the Bureau of Labor Statistics' gross flows data on labor market transitions to account for these errors. Although the findings are not definitive because the procedure makes particular assumptions regarding the stochastic process generating response errors, they illustrate the potentially substantial effect of response errors on studies of labor market behavior. The adjustment procedure suggests that because measurement errors give rise to spurious transitions between labor market states, the labor market may be less dynamic than previously thought. The results imply that conventional measures may understate the duration of unemployment by as much as eighty per cent, and overstate the frequency of labor force entry and exit by even more.</p> </abstract>
<abstract> <p>In this study we use five biennial panels of males in the Retirement History survey to estimate a model of self assessed health status and retirement status, both of which are categorical dependent variables whose residuals may be correlated. The health variable can take one of four states ranging from better health than others the same age to dead. The retirement variable has two states: working or not working full time. Our maximum likelihood estimator requires ten-way integration of the multivariate normal density function. We outline the Gaussian quadrature formulae used in calculating these multivariate distribution functions. We find many significant variables. Perhaps of most interest is the trade off in retirement between Social Security benefits and wage rates and the sharp decline in retirement before age 62, the earliest date nondisabled men are eligible for benefits. We also find that health is worse for those whose longest occupation is unskilled labor.</p> </abstract>
<abstract> <p>This paper examines how corporate taxation of multijurisdictional firms using formula apportionment affects the incentives faced by individual firms and individual states. Under formula apportionment, a firm's tax payments to a given state depend on its total profits nationally (or internationally) times an average of the fractions of the firm's total property, payroll, and sales located in that state. This apportionment of a firm's total profits among states, based on three separate factors, in effect creates three separate taxes, each with complicated incentive effects. A large part of our analysis is concerned with the component of the tax tied to the allocation of property. Under this tax, price distortions differ in general among firms within the same state, creating incentives for firms producing in different states to merge their operations. State tax policies are also affected by this apportionment formula: states choose inefficiently low tax rates and are encouraged to shift to direct taxation of property. The component of the tax based on payroll creates many similar incentives. With this tax, however, the marger of firms producing different goods is discouraged.When a sales component to the tax is added, there are incentives for the cross-hauling of output, with production in low tax rates states sold in high tax rate states, and conversely. None of the above distortions are created when the corporate tax uses separate accounting to divide a firm's profits among states. The final section presents an alternative apportionment formula which retains the administrative advantages of existing law, yet creates the same incentives as separate accounting as long as there are no economic profits.</p> </abstract>
<abstract> <p>The Nash equilibrium concept may be extended gradually when the rules of the game are interpreted in a wider and wider sense, so as to allow preplay or even intraplay communication. A well-known extension of the Nash equilibrium is Aumann's correlated equilibrium, which depends only on the normal form of the game. Two other solution concepts for multistage games are proposed here: the extensive form correlated equilibrium, where the players can observe private extraneous signals at every stage and the communication equilibrium where the players are furthermore allowed to transmit inputs to an appropriate device at every stage. We show that the set of payoffs associated with each solution concept has a canonical representation (in the spirit of the revelation principle) and is a convex polyhedron. We also provide for each concept a "super canonical" game such that the set of payoffs associated with the solution concept is precisely the set of Nash equilibrium payoffs of this game.</p> </abstract>
<abstract> <p>The formulation of optimal policy in linear rational expectations models is studied using methods analogous to the classical design techniques utilized in linear systems engineering. Specifically, the policy-maker's present-value-like objective function is converted, using the convolution transform, to an equivalent frequency domain, "spectral utility" function. Then the residue calculus and Wiener-Hopf methods are used to maximize spectral utility through the choice of a complex function which represents a sequence of distributed lag coefficients to be applied to current and past values of instrument variables. The solution to this problem is a closed form expression for the decision rule of the dominant player in a particular type of linear-quadratic dynamic game.</p> </abstract>
<abstract> <p>The axiomatic derivation of mobility indices for first-order Markov chain models in discrete time is extended to continuous-time models. Many of the logical inconsistencies among axioms noted in the literature for the discrete time models do not arise for continuous time models. It is shown how mobility indices in continuous time Markov chains may be estimated from observations at two points in time. Specific attention is given to the case in which the states are fractiles, and an empirical example is presented.</p> </abstract>
<abstract> <p>Studies in the production structure of any national economy, as revealed in its input-output table, have been closely related to the question of the interindustrial dependence or hierarchical structures of productive sector leading from primary to final production. In particular, the notion of hierarchy provides a useful tool when one wants to draw inferences on the structural change or international difference of industrial structures. The standard technique for studying this notion is to triangularize the input-output table by interchanging sectors in order to maximize the entries below the main diagonal. In a perfect triangularized table, the entries above the main diagonal should be zero. For example, such a strong one-way interdependence relation as cotton-textiles-clothing can easily establish the hierarchy. Due to the existence of circular relations like coal-steel-mining equipment-coal, it is not possible to triangularize the input-output table perfectly. This paper extends and revises the previous triangulation method which is based on a permutation theorem deriving from the interchange of adjoining two industrial groups, into that among vhree industrial groups (Theorem 2). The new algorithm based on Theorem 2 is demonstrated by actually computing the suboptimal orderings for the four input-output tables for such more developed countries (MDC's) as the United States, Italy, Norway, Japan, and the two tables for such less developed countries (LDC's) as India and Korea. The empirical results suggest that sectors can be arranged in a similar hierarchical order among MDC's and LDC's. Transport Equipment, Machinery, Apparel, Leather and Products, Grain Mill Products, and Processed Foods, which link directly to the final demand, are recorded as higher-order sectors. Trade, Transport, and services are lower-order ones, and Energy sectors such as Electric Power, Coal Products, Coal Mining, Petroleum, Petroleum Products and Natural Gas are the lowest. Consequently, this paper provides some more evidence in support of the similarity of hierarchical structures of production among these countries.</p> </abstract>
<abstract> <p>This paper proposes alternatives to maximum likelihood estimation of the censored and truncated regression models (known to economists as "Tobit" models). The proposed estimators are based upon symmetric censoring or truncation of the upper tail of the distribution of the dependent variable. Unlike methods based on the assumption of identically distributed Gaussian errors, the estimators are semiparametric, in the sense that they are consistent and asymptotically normal for a wide class of (symmetric) error distributions with heteroskedasticity of unknown form. The paper gives the regularity conditions and proofs of these large sample properties, demonstrates how to construct consistent estimators of the asymptotic covariance matrices, and presents the results of a simulation study for the censored case. Extensions and limitations of the approach are also considered.</p> </abstract>
<abstract> <p>This paper studies the estimation of coefficients B in single index models such that E(y@?X) = F(@a+X'@B), where the function F is misspecified or unknown. A general connection between behavioral derivatives and covariane estimators is established, which shows how @B can be estimated up to scale using information on the marginal distribution of X. A sample covariance estimator and an instrumental variables slope coefficient vector are proposed, which are constructed using appropriately defined score vectors of the X distribution. The framework is illustrated using several common limited dependent variable models, and extended to multiple index models, including models of selection bias and multinomial discrete choice. The asymptotic distribution of the instrumental variables estimator is established, when the X distribution is modeled up to a finite parameterization. The asymptotic bias in the OLS coefficients of y regressed on X is analyzed.</p> </abstract>
<abstract> <p>A general model of arbitrator behavior in conventional and final-offer arbitration is developed that is based on an underlying notion of an appropriate award in a particular case. This appropriate award is defined as a function of the facts of the case independently of the offers of the parties. In conventional arbitration the arbitration award is argued to be a function of both the offers of the parties and the appropriate award. The weight that the arbitrator puts on the appropriate award relative to the offers is hypothesized to be a function of the quality of the offers as measured by the difference between the offers. In final-offer arbitration it is argued that the arbitrator chooses the offer that is closest to the appropriate award. The model is implemented empirically using data gathered from practicing arbitrators regarding their decisions in twenty-five hypothetical cases. The estimates of the general model strongly support the characterizations of arbitrator behavior in the two schemes. In addition, no substantial differences were found in the determination of the appropriate award implicit in conventional arbitration decisions and the determination of the appropriate award implicit in the final-offer decisions.</p> </abstract>
<abstract> <p>Fully modified least squares (FM-OLS) regression was originally designed in work by Phillips and Hansen (1990) to provide optimal estimates of cointegrating regressions. The method modifies least squares to account for serial correlation effects and for the endogeneity in the regressors that results from the existence of a cointegrating relationship. This paper provides a general framework which makes it possible to study the asymptotic behavior of FM-OLS in models with full rank I(1) regressors, models with I(1) and I(0) regressors, models with unit roots, and models with only stationary regressors. This framework enables us to consider the use of FM regression in the context of vector autoregressions (VAR's) with some unit roots and some cointegrating relations. The resulting FM-VAR regressions are shown to have some interesting properties. For example, when there is some cointegration in the system, FM-VAR estimation has a limit theory that is normal for all of the stationary coefficients and mixed normal for all of the nonstationary coefficients. Thus, there are no unit root limit distributions even in the case of the unit root coefficient submatrix (i.e., I&lt;sub&gt;n - r&lt;/sub&gt;, for an n-dimensional VAR with r cointegrating vectors). Moreover, optimal estimation of the cointegration space is attained in FM-VAR regression without prior knowledge of the number of unit roots in the system, without pretesting to determine the dimension of the cointegration space and without the use of restricted regression techniques like reduced rank regression. The paper also develops an asymptotic theory for inference based on FM-OLS and FM-VAR regression. The limit theory for Wald tests that rely on the FM estimator is shown to involve a linear combination of independent chi-squared variates. This limit distribution is bounded above by the conventional chi-squared distribution with degrees of freedom equal to the number of restrictions. Thus, conventional critical values can be used to construct valid (but conservative) asymptotic tests in quite general FM time series regressions. This theory applies to causality testing in VAR's and is therefore potentially useful in empirical applications.</p> </abstract>
<abstract> <p> We examine the second order properties of various quantities of interest in the partially linear regression model. We obtain a stochastic expansion with remainder &lt;tex-math&gt;$O_{P}(n^{-2\ \mu})$&lt;/tex-math&gt;, where μ &lt; 1/2, for the standardized semiparametric least squares estimator, a standard error estimator, and a studentized statistic. We use the second order expansions to correct the standard error estimates for second order effects, and to define a method of bandwidth choice. A Monte Carlo experiment provides favorable evidence on our method of bandwidth choice. </p> </abstract>
<abstract> <p>A new asymptotic theory of regression is introduced for possibly nonstationary time series. The regressors are assumed to be generated by a linear process with martingale difference innovations. The conditional variances of these martingale differences are specified as autoregressive stochastic volatility processes, with autoregressive roots which are local to unity. We find conditions under which the least squares estimates are consistent and asymptotically normal. A simple adaptive estimator is proposed which achieves the same asymptotic distribution as the generalized least squares estimator, without requiring parametric assumptions for the stochastic volatility process.</p> </abstract>
<abstract> <p>This paper proposes two consistent one-sided specification tests for parametric regression models, one based on the sample covariance between the residual from the parametric model and the discrepancy between the parametric and nonparametric fitted values; the other based on the difference in sums of squared residuals between the parametric and nonparametric models. We estimate the nonparametric model by series regression. The new test statistics converge in distribution to a unit normal under correct specification and grow to infinity faster than the parametric rate (n&lt;sup&gt;-1/2&lt;/sup&gt;) under misspecification, while avoiding weighting, sample splitting, and non-nested testing procedures used elsewhere in the literature. Asymptotically, our tests can be viewed as a test of the joint hypothesis that the true parameters of a series regression model are zero, where the dependent variable is the residual from the parametric model, and the series terms are functions of the explanatory variables, chosen so as to support nonparametric estimation of a conditional expectation. We specifically consider Fourier series and regression splines, and present a Monte Carlo study of the finite sample performance of the new tests in comparison to consistent tests of Bierens (1990), Eubank and Spiegelman (1990), Jayasuriya (1990), Wooldridge (1992), and Yatchew (1992); the results show the new tests have good power, performing quite well in some situations. We suggest a joint Bonferroni procedure that combines a new test with those of Bierens and Wooldridge to capture the best features of the three approaches.</p> </abstract>
<abstract> <p>Sufficient conditions for Nash equilibrium in an n-person game are given in terms of what the players know and believe--about the game, and about each other's rationality, actions, knowledge, and beliefs. Mixed strategies are treated not as conscious randomizations, but as conjectures, on the part of other players, as to what a player will do. Common knowledge plays a smaller role in characterizing Nash equilibrium than had been supposed. When n = 2, mutual knowledge of the payoff functions, of rationality, and of the conjectures implies that the conjectures form a Nash equilibrium. When n ≥ 3 and there is a common prior, mutual knowledge of the payoff functions and of rationality, and common knowledge of the conjectures, imply that the conjectures form a Nash equilibrium. Examples show the results to be tight.</p> </abstract>
<abstract> <p>We add a round of pre-play communication to a finite two-player game played by a population of players. Pre-play communication is cheap talk in the sense that it does not directly enter the payoffs. The paper characterizes the set of strategies that are stable with respect to a stochastic dynamic adaptive process. Periodically players have an opportunity to change their strategy with a strategy that is more successful against the current population. Any strategy that weakly improves upon the current poorest performer in the population enters with positive probability. When there is no conflict of interest between the players, only the efficient outcome is stable with respect to these dynamics. For general games the set of stable payoffs is typically large. Every efficient payoff recurs infinitely often.</p> </abstract>
<abstract> <p>The paper studies the representation and characterization of risks generated by a continuum of random variables. The Main Theorem is a characterization of a broad class of continuum processes in terms of the decomposition of risk into aggregate and idiosyncratic components, and in terms of the approximation of the continuum process by finite collections of random variables. This characterization is used to study decision making problems with anonymous and state-independent payoffs. An Extension Theorem shows that if such a payoff function is defined on simple processes, then it has a unique continuous extension to the class of processes characterized in this paper. This extension is formulated without reference to sample realizations and with minimal restrictions on the patterns of correlation between the random variables. As an application, the theory is used to develop a new model of large games which emphasizes the explicit description of the players' randominizations. This model is used to study the class of environments in which Schmeidler's (1973) representation of strategic uncertainty in large games is valid.</p> </abstract>
<abstract> <p>We consider an auction in which k identical objects of unknown value are auctioned off to n bidders. The k highest bidders get an object and pay the k + 1st bid. Bidders receive a signal that provides information about the value of the object. We characterize the unique symmetric equilibrium of this auction. We then consider a sequence of auctions A&lt;sub&gt;r&lt;/sub&gt; with n&lt;sub&gt;r&lt;/sub&gt; bidders and k&lt;sub&gt;r&lt;/sub&gt; objects. We show that price converges in probability to the true value of the object if and only if both &lt;tex-math&gt;$k_{r}\rightarrow \infty $&lt;/tex-math&gt; and &lt;tex-math&gt;$n_{r}-k_{r}\rightarrow \infty $&lt;/tex-math&gt;, i.e., both the number of objects and the number of bidders who do not receive an object go to infinity.</p> </abstract>
<abstract> <p>A number of papers have shown that a strict Nash equilibrium action profile of a game may never be played if there is a small amount of incomplete information (see, for example, Carlsson and van Damme (1993a)). We present a general approach to analyzing the robustness of equilibria to a small amount of incomplete information. A Nash equilibrium of a complete information game is said to be robust to incomplete information if every incomplete information game with payoffs almost always given by the complete information game has an equilibrium which generates behavior close to the Nash equilibrium. We show that many games with strict equilibria have no robust equilibrium and examine why we get such different results from existing refinements. If a game has a unique correlated equilibrium, it is robust. A natural many-player many-action generalization of risk dominance is a sufficient condition for robustness.</p> </abstract>
<abstract> <p>The standard formalization of the econometric analysis of treatment response assumes that each member of a population of interest receives one of a set of mutually exclusive and exhaustive treatments, and that the outcome under the realized treatment is observable. Outcomes under the nonrealized treatments are necessarily unobservable; hence these outcomes are censored. This paper investigates what may be learned about treatment response when it is assumed that response functions are monotone, semi-monotone, or concave-monotone. The analysis assumes nothing about the process of treatment selection and imposes no cross-individual restrictions on response. The basic idea is to determine, for every member of the population, the set of response functions that pass through that person's realized (treatment, outcome) pair and that are consistent with the functional-form assumption imposed. These person-specific findings are then explicitly aggregated across the population to determine what can be learned about the distribution of response. The findings have application to the econometric analysis of market demand and of production.</p> </abstract>
<abstract> <p>We consider the problem of estimation in a panel data sample selection model, where both the selection and the regression equation of interest contain unobservable individual-specific effects. We propose a two-step estimation procedure, which "differences out" both the sample selection effect and the unobservable individual effect from the equation of interest. In the first step, the unknown coefficients of the "selection" equation are consistently estimated. The estimates are then used to estimate the regression equation of interest. The estimator proposed in this paper is consistent and asymptotically normal, with a rate of convergence that can be made arbitrarily close to n&lt;sup&gt;-1/2&lt;/sup&gt;, depending on the strength of certain smoothness assumptions. The finite sample properties of the estimator are investigated in a small Monte Carlo simulation.</p> </abstract>
<abstract> <p> General characterizations of valid confidence sets and tests in problems which involve locally almost unidentified (LAU) parameters are provided and applied to several econometric models. Two types of inference problems are studied: (i) inference about parameters which are not identifiable on certain subsets of the parameter space, and (ii) inference about parameter transformations with discontinuities. When a LAU parameter or parametric function has an unbounded range, it is shown under general regularity conditions that any valid confidence set with level 1 - α for this parameter must be unbounded with probability close to 1 - α in the neighborhood of nonidentification subsets and will have a nonzero probability of being unbounded under any distribution compatible with the model: no valid confidence set which is almost surely bounded does exist. These properties hold even if "identifying restrictions" are imposed. Similar results also obtain for parameters with bounded ranges. Consequently, a confidence set which does not satisfy this characterization has zero coverage probability (level). This will be the case in particular for Wald-type confidence intervals based on asymptotic standard errors. Furthermore, Wald-type statistics for testing given values of a LAU parameter cannot be pivotal functions (i.e., they have distributions which depend on unknown nuisance parameters) and even cannot be usefully bounded over the space of the nuisance parameters. These results are applied to several econometric problems: inference in simultaneous equations (instrumental variables (IV) regressions), linear regressions with autoregressive errors, inference about long-run multipliers and cointegrating vectors. For example, it is shown that standard "asymptotically justified" confidence intervals based on IV estimators (such as two-stage least squares) and the associated "standard errors" have zero coverage probability, and the corresponding t statistics have distributions which cannot be bounded by any finite set of distribution functions, a result of interest for interpreting IV regressions with "weak instruments." Furthermore, expansion methods (e.g., Edgeworth expansions) and bootstrap techniques cannot solve these difficulties. Finally, in a number of cases where Wald-type methods are fundamentally flawed (e.g., IV regressions with poor instruments), it is observed that likelihood-based methods (e.g., likelihood-ratio tests and confidence sets) combined with projection techniques can easily yield valid tests and confidence sets. </p> </abstract>
<abstract> <p>This paper develops a search-theoretic model of technological change that accounts for some puzzling trends in industrial research, patenting, and productivity growth. In the model, researchers sample from probability distributions of potential new production techniques. Past research generates a technological frontier representing the best techniques for producing each good in the economy. Technological breakthroughs, resulting in patents, become increasingly hard to find as the technological frontier advances. This explains why patenting has been roughly constant as research employment has risen sharply over the last forty years. Productivity is determined by the position of the technological frontier and hence by the stock of past research. If researchers sample from Pareto distributions, then productivity growth is proportional to the growth of the research stock. The Pareto specification accounts for why productivity growth has neither risen as research employment has grown nor fallen as patenting has failed to grow. The growth of research employment itself is driven, in equilibrium, by population growth. Calibrating the model's four parameters, the implied social return to research is over twenty percent.</p> </abstract>
<abstract> <p>We identify sufficient conditions which guarantee that aggregate demand is approximately linear in income and satisfies a restricted form of the law of demand in a market where income is price dependent. The conditions involve the way preferences are distributed: loosely speaking, preferences have to be sufficiently heterogeneous. The definition of heterogeneity we use is similar to Grandmont (1992) but it is applied to homothetic transformations as introduced in Jerison (1982) and Grandmont (1987) and allows for the possibility of atoms. Another condition is that the distributions of preferences and income are independent. In exchange and production economies, our result guarantees the uniqueness of the price equilibrium and its stability under the Walras' tatonnement.</p> </abstract>
<abstract> <p>This paper introduces random versions of successive approximations and multigrid algorithms for computing approximate solutions to a class of finite and infinite horizon Markovian decision problems (MDPs). We prove that these algorithms succeed in breaking the "curse of dimensionality" for a subclass of MDPs known as discrete decision processes (DDPs).</p> </abstract>
<abstract> <p>The strategy method asks experienced subjects to program strategies for a game. This paper reports on an application to a 20-period supergame of an asymmetric Cournot duopoly. The final strategies after three programming rounds show a typical structure. Typically, no expectations are formed and nothing is optimized. Instead of this, fairness criteria are used to determine cooperative goals, called "ideal points." The subjects try to achieve cooperation by a "measure-for-measure policy," which reciprocates movements towards and away from the ideal point by similar movements. A strategy tends to be more successful the more typical it is.</p> </abstract>
<abstract> <p>This paper develops asymptotic distribution theory for single-equation instrumental variables regression when the partial correlations between the instruments and the endogenous variables are weak, here modeled as local to zero. Asymptotic representations are provided for various statistics, including two-stage least squares (TSLS) and limited information maximum likelihood (LIML) estimators, Wald statistics, and statistics testing overidentification and endogeneity. The asymptotic distributions are found to provide good approximations to sampling distributions with 10-20 observations per instrument. The theory suggests concrete guidelines for applied work, including using nonstandard methods for construction of confidence regions. These results are used to interpret Angrist and Krueger's (1991) estimates of the returns to education: whereas TSLS estimates with many instruments approach the OLS estimate of 6%, the more reliable LIML estimates with fewer instruments fall between 8% and 10%, with a typical 95% confidence interval of (5%, 15%).</p> </abstract>
<abstract> <p>Virtually all applications of time-varying conditional variance models use a quasi-maximum-likelihood estimator (QMLE). Consistency of a QMLE requires an identification condition that the quasi-log-likelihood have a unique maximum at the true conditional mean and relative scale parameters. We show that the identification condition holds for a non-Gaussian QMLE if the conditional mean is identically zero or if a symmetry condition is satisfied. Without symmetry, an additional parameter, for the location of the innovation density, must be added for identification. We calculate the efficiency loss from adding such a parameter under symmetry, when the parameter is not needed. We also show that there is no efficiency loss for the conditional variance parameters of a GARCH process.</p> </abstract>
<abstract> <p>The literature on the aggregation of (S, s) policies has ignored the impact of aggregate behavior on the individual's optimization problem. In the case of pricing, the feedback effects are clear. Not only do pricing strategies determine the evolution of the price level, but the evolution of the price level also influences the optimal pricing strategies. In this paper, we provide a consistent treatment of aggregation and optimization. We use this model to analyze three issues in the menu cost pricing literature: the relationship between strategic complementarity and the real effects of money; the relationship between the variance of the money supply and the correlation between money and output; and the relationship between the cost of price adjustment and the size of price adjustment.</p> </abstract>
<abstract> <p> Misspecification tests for parametric models, f(y, θ), that examine data for failure of moment conditions implied by the maintained parametric distribution are interpreted as score tests of &lt;tex-math&gt;$H_{0}\colon \lambda ={\bf 0}$&lt;/tex-math&gt; in the context of a parametric family of distributions r(y; θ, λ). This family contains the maintained distribution as a special case (λ = 0) and has the property that only in that special case do the chosen moment conditions hold. A likelihood ratio test of &lt;tex-math&gt;$H_{0}\colon \lambda ={\bf 0}$&lt;/tex-math&gt; therefore constitutes an alternative test of the validity of the moment conditions. This test admits a Bartlett correction, unlike conventional moment tests for which adjustments based on second order asymptotic theory may behave badly. The dependence of the Bartlett correction and of the O(n&lt;sup&gt;-1/2&lt;/sup&gt;) local power of the test on the way in which r(y; θ, λ) is constructed is studied. In many cases the correction can be made to vanish leading to a specification test whose distribution is chi-square to order &lt;tex-math&gt;$O_{p}(n^{-2})$&lt;/tex-math&gt;. </p> </abstract>
<abstract> <p>We demonstrate that despite variables that are integrated, the fundamental issues on structural equation modeling raised by the Cowles Commission remain valid and standard estimation and testing procedures can still be applied. A basic framework linking the multiple time series model and the dynamic simultaneous equation model is provided and implications under the long-run cointegrating relations are discussed. Conditions for identifying both the short-run dynamics and long-run equilibrium conditions are given. Limiting properties of the least squares and simultaneous equation estimators under cointegration are derived. Implications for hypothesis testing are also discussed.</p> </abstract>
<abstract> <p>To accommodate the observed pattern of risk-aversion and risk-seeking, as well as common violations of expected utility (e.g., the certainty effect), we introduce and characterize a weighting function according to which an event has greater impact when it turns impossibility into possibility, or possibility into certainty, that when it merely makes a possibility more or less likely. We show how to compare such weighting functions (of different individuals) with respect to the degree of departure from expected utility, and we present a method for comparing an individual's weighting functions for risk and for uncertainty.</p> </abstract>
<abstract> <p>This paper considers the theory of market versus optimal product diversity in the light of two recent advances in oligopoly theory. The first is the development of discrete choice models to describe heterogeneous consumer tastes, and the application of such models to oligopolistic competition. The second advance is the proof that logconcavity of the consumer taste density guarantees the existence of a price equilibrium. We analyze an oligopoly model with price competition and free entry, taking explicit account of the integer constraint. Under the Chamberlinian symmetry assumption (that tastes are i.i.d.), we first show that logconcavity of the taste density implies there is excessive market provision of variety when each consumer buys one unit of the product from one of the firms. We then show that this result extends to price-sensitive individual demands by proving that the equilibrium number of firms is at least as great as that which would be provided at the second-best social optimum subject to a zero-profit constraint for firms. Our results call into question previous findings for representative consumer models that left open the possibility of insufficient product diversity.</p> </abstract>
<abstract> <p>This paper considers the problem of social evaluation in a model where population size, individual lifetime utilities, lengths of life, and birth dates vary across states. In an intertemporal framework, we investigate principles for social evaluation that allow history to matter to some extent. Using an axiom called independence of the utilities of the dead, we provide a characterization of critical-level generalized utilitarian rules. As a by-product of our analysis, we show that social discounting is ruled out in an intertemporal welfarist environment. A simple population-planning example is also discussed.</p> </abstract>
<abstract> <p>We study the strategic equilibria of a negotiation game where potential buyers are affected by identity-dependent, negative externalities. The unique equilibrium of long, finitely repeated generic games can either display delay--where a transaction can take place only in several stages before the deadline--or, in spite of the random element in the game, a well-defined buyer exists that obtains the object with probability close to one.</p> </abstract>
<abstract> <p>A common interest game is a game in which there exists a unique pair of payoffs which strictly Pareto-dominates all other payoffs. We consider the undiscounted repeated game obtained by the infinite repetition of such a two-player stage game. We show that if supergame strategies are restricted to be computable within Church's thesis, the only pair of payoffs which survives any computable tremble with sufficiently large support is the Pareto-efficient pair. The result is driven by the ability of the players to use the early stages of the game to communicate their intention to play cooperatively in the future.</p> </abstract>
<abstract> <p>This paper investigates stability properties of evolutionary selection dynamics in normal-form games. The analysis is focused on deterministic dynamics in continuous time and on asymptotic stability of sets of population states, more precisely of faces of the mixed-strategy space. The main result is a characterization of those faces which are asymptotically stable in all dynamics from a certain class, and we show that every such face contains an essential component of the set of Nash equilibria, and hence a strategically stable set in the sense of Kohlberg and Mertens (1986).</p> </abstract>
<abstract> <p>Two-person repeated games with no discounting are considered where there is uncertainty about the type of the players. If there is a possibility that a player is an automaton committed to a particular pure or mixed stage-game action, then this provides a lower bound on the Nash equilibrium payoffs to a normal type of this player. The lower bound is the best available and is robust to the existence of other types. The results are extended to the case of two-sided uncertainty. This work extends Schmidt (1993) who analyzed the restricted class of conflicting interest games.</p> </abstract>
<abstract> <p>We present three distinct approaches to perfect and proper equilibria for infinite normal form games. In the first two approaches, players "tremble" in the infinite games playing full support approximate best responses to others' strategies. In the strong approach, a tremble assigns high probability to the set of pure best responses; in the weak approach, it assigns high probability to a neighborhood of this set. The third, limit-of-finite approach applies traditional refinements to sequences of successively larger finite games. Overall, the strong approach to equilibrium refinement most fully respects the structure of infinite games.</p> </abstract>
<abstract> <p>We apply nonparametric regression models to estimation of demand curves of the type most often used in applied research. From the demand curve estimators we derive estimates of exact consumers surplus and deadweight loss, which are the most widely used welfare and economic efficiency measures in areas of economics such as public finance. We also develop tests of the symmetry and downward sloping properties of compensated demand. We work out asymptotic normal sampling theory for kernel and series nonparametric estimators, as well as for the parametric case. The paper includes an application to gasoline demand. Empirical questions of interest here are the shape of the demand curve and the average magnitude of welfare loss from a tax on gasoline. In this application we compare parametric and nonparametric estimates of the demand curve, calculate exact and approximate measures of consumers surplus and deadweight loss, and give standard error estimates. We also analyze the sensitivity of the welfare measures to components of nonparametric regression estimators such as the number of terms in a series approximation.</p> </abstract>
<abstract> <p>This paper uses the Panel Study of Income Dynamics to test whether risk-sharing is complete between or within American families. The tests accommodate a wide variety in the configuration and availability of family data. The test results reject inter- as well as intra-family full risk-sharing even assuming that leisure is endogenous or that leisure and consumption are nonseparable.</p> </abstract>
<abstract> <p>This paper examines the effect of cash transfers and food stamp benefits on family labor supply and welfare participation among two-parent families. The Aid to Families with Dependent Children--Unemployed Parent Program has provided cash benefits to two-parent households since 1961. Despite recent expansions, little is known about the program's effect on labor supply and welfare participation. I develop a model of family labor supply in which hours of work for the husband and wife are chosen to maximize family utility subject to a family budget constraint accounting for AFDC-UP benefits and other tax and transfer programs. The husband's and wife's labor supply decisions are restricted to no work, part-time work, and full-time work. Maximum likelihood techniques are used to estimate parameters of the underlying hours of work and welfare participation equations. The estimates are used to determine the magnitude of the work disincentive effects of the AFDC-UP program, and to simulate the effects of changes in AFDC-UP benefit and eligibility rules on family labor supply and welfare participation. The results suggest that labor supply and welfare participation among two-parent families are highly responsive to changes in the benefit structure under the AFDC-UP program.</p> </abstract>
<abstract> <p>This paper examines how, in the presence of individual risk, economic efficiency can be achieved without an unrealistically large number of contingent claims. Market uncertainty is specified in such a way that general types of individual risk and collective risk are properly accounted for and so that, specifically, market clearing is always satisfied ex post as well as ex ante. We show that consistency of beliefs and optimality of allocation can be guaranteed with an appropriate array of pure Arrow securities to spread collective risk and mutual insurance policies to pool individual risk. When there is individual risk common to like groups of individuals, pooling risk by means of mutual insurance permits substantial economizing on market transactions, as compared to those required if dealing instead with the full complement of pure Arrow securities. We show that if there are N households (consisting of H types), each facing the possibility of being in S individual states together with T collective states, then ensuring Pareto optimally requires only H(S - 1)T independent mutual insurance policies plus T pure Arrow securities. Our results also help to clarify the question of which missing markets may affect allocational efficiency.</p> </abstract>
<abstract> <p>The aim of this paper is to explain the fact that certain properties of binary relations are frequently observed in natural language while others do not appear at all. Three features of binary relation are studied: (i) the ability to use the relation to indicate nameless elements; (ii) the accuracy with which the vocabulary spanned by the relation can be used to approximate the actual terms to which a user of the language wishes to refer; (iii) the ease with which the relation can be described by means of examples. It is argued that linear orderings are optimal according to the first criteria while asymmetric relations are optimal according to the second. From among complete and asymmetric relations (tournaments), those which are transitive are optimal according to the third criterion.</p> </abstract>
<abstract> <p>We present and analyze a model of noncooperative bargaining among n participants, applied to situations describable as games in coalitional form. This leads to a unified solution theory for such games that has as special cases the Shapley value in the transferable utility (TU) case, the Nash bargaining solution in the pure bargaining case, and the recently introduced Maschler-Owen consistent value in the general nontransferable utility (NTU) case. Moreover, we show that any variation (in a certain class) of our bargaining procedure which generates the Shapley value in the TU setup must yield the consistent value in the general NTU setup.</p> </abstract>
<abstract> <p>This paper develops an asymptotic theory of Bayesian inference for time series. A limiting representation of the Bayesian data density is obtained and shown to be of the same general exponential form for a wide class of likelihoods and prior distributions. Continuous time and discrete time cases are studied. In discrete time, an embedding theorem is given which shows how to embed the exponential density in a continuous time process. From the embedding we obtain a large sample approximation to the model of the data that corresponds to the exponential density. This has the form of discrete observations drawn from a nonlinear stochastic differential equation driven by Brownian motion. No assumptions concerning stationarity or rates of convergence are required in the asymptotics. Some implications for statistical testing are explored and we suggest tests that are based on likelihood ratios (or Bayes factors) of the exponential densities for discriminating between models.</p> </abstract>
<abstract> <p>Many econometric testing problems involve nuisance parameters which are not identified under the null hypotheses. This paper studies the asymptotic distribution theory for such tests. The asymptotic distributions of standard test statistics are described as functionals of chi-square processes. In general, the distributions depend upon a large number of unknown parameters. We show that a transformation based upon a conditional probability measure yields an asymptotic distribution free of nuisance parameters, and we show that this transformation can be easily approximated via simulation. The theory is applied to threshold models, with special attention given to the so-called self-exciting threshold autoregressive model. Monte Carlo methods are used to assess the finite sample distributions. The tests are applied to U.S. GNP growth rates, and we find that Potter's (1995) threshold effect in this series can be possibly explained by sampling variation.</p> </abstract>
<abstract> <p>This article commemorates Ragnar Frisch (1895-1973), the first editor of Econometrica, and relates the events which finally led to the establishment of the journal. How the name of the journal was settled, and what it might have been, is revealed. The editorial views, style, and habits of Ragnar Frisch are recounted.</p> </abstract>
<abstract> <p>Continuous-time Markov processes can be characterized conveniently by their infinitesimal generators. For such processes there exist forward and reverse-time generators. We show how to use these generators to construct moment conditions implied by stationary Markov processes. Generalized method of moments estimators and tests can be constructed using these moment conditions. The resulting econometric methods are designed to be applied to discrete-time data obtained by sampling continuous-time Markov processes.</p> </abstract>
<abstract> <p>Individual income is much more variable than aggregate per capita income. I argue that aggregate information is therefore not very important for individual consumption decisions and study models of life-cycle consumption in which individuals react optimally to their own income process but have incomplete or no information on economy-wide variables. Since individual income is less persistent than aggregate income consumers will react too little to aggregate income variation. Aggregate consumption will be excessively smooth. Since aggregate information is slowly incorporated into consumption, aggregate consumption will be autocorrelated and correlated with lagged income. On the other hand, the model has the same prediction for micro data as the standard permanent income model. The second part of the paper provides empirical evidence on individual and aggregate income processes. Different models for individual income are fit to quarterly data from the Survey of Income and Program Participation making various adjustments for measurement error. Calibrating the consumption model using the estimated parameters for the income process yields predictions which qualitatively correspond to the empirical findings for aggregate consumption but do not match them well in magnitude.</p> </abstract>
<abstract> <p>This paper develops techniques for empirically analyzing demand and supply in differentiated products markets and then applies these techniques to analyze equilibrium in the U.S. automobile industry. Our primary goal is to present a framework which enables one to obtain estimates of demand and cost parameters for a class of oligopolistic differentiated products markets. These estimates can be obtained using only widely available product-level and aggregate consumer-level data, and they are consistent with a structural model of equilibrium in an oligopolistic industry. When we apply the techniques developed here to the U.S. automobile market, we obtain cost and demand parameters for (essentially) all models marketed over a twenty year period.</p> </abstract>
<abstract> <p>This paper develops and estimates a model of the U.S. Automobile Industry. On the demand side, discrete choice model is adopted, that is estimated using micro data from the Consumer Expenditure Survey. The estimation results are used in conjunction with population weights to derive aggregate demand. On the supply side, the automobile industry is modelled as an oligopoly with product differentiation. Equilibrium is characterized by the first order conditions of the profit maximizing firms. The estimation results are used in counterfactual simulations to investigate two trade policy issues: the effects of the VER, and exchange rate pass-through.</p> </abstract>
<abstract> <p>In this paper we propose an estimation method for the empirical study of theoretical auction models. We focus on first-price sealed bid and descending auctions and we adopt the private value paradigm, where each bidder is assumed to have a different private value, only known to him, for the object that is auctioned. Following McFadden (1989) and Pakes and Pollard (1989), our proposed method is based on simulations. Specifically, the method relies on a simulated nonlinear least squares objective function appropriately adjusted so as to obtain consistent estimates of the parameters of interest. We illustrate the proposed method by studying a market of agricultural products, where descending auctions are used. Our analysis takes into account heterogeneity of the auctioned objects and the fact that only the winning bid is observed. We estimate the parameters that characterize the distribution of the unobserved private values for each auctioned object.</p> </abstract>
<abstract> <p>This study demonstrates the possibility of ergodically chaotic optimal accumulation in the case in which future utilities are discounted arbitrarily weakly. For this purpose, we use a two-sector model with Leontief production functions and construct a condition under which the optimal transition function is unimodal and expansive. We demonstrate that the set of parameter values satisfying that condition is nonempty no matter how weakly the future utilities are discounted.</p> </abstract>
<abstract> <p>There are several solutions to the Nash bargaining problem in the literature. Since various authors have expressed preferences for one solution over another, we find it useful to study preferences over solutions in their own right. We identify a set of appealing axioms on such preferences that lead to unanimity in the choice of solution, which turns out to be the solution of Nash.</p> </abstract>
<abstract> <p>This paper provides a fairly systematic study of general economic conditions under which rational asset pricing bubbles may arise in an intertemporal competitive equilibrium framework. Our main results are concerned with nonexistence of asset pricing bubbles in those economies. These results imply that the conditions under which bubbles are possible--including some well-known examples of monetary equilibria--are relatively fragile.</p> </abstract>
<abstract> <p>This paper proposes a Bayesian approach to a vector autoregression with stochastic volatility, where the multiplicative evolution of the precision matrix is driven by a multivariate beta variate. Exact updating formulas are given to the nonlinear filtering of the precision matrix. Estimation of the autoregressive parameters requires numerical methods: an importance-sampling based approach is explained here.</p> </abstract>
<abstract> <p>This paper extends the Kiyotaki-Wright search model of fiat money to allow for divisible money and goods. The extension allows me to examine the standard issues in monetary economics, such as the neutrality and superneutrality of money, by severing the artificial link in the Kiyotaki-Wright model between the money supply and the number of money holders. It is shown that money is neutral, but not superneutral. Money growth generates a trading opportunity effect: it changes the fraction of different agents in the economy and hence changes the probability with which agents have a successful match. In addition, money growth has a negative effect on the real money balance that is familiar in Walrasian monetary models. The balance of the two effects can imply a positive optimal money growth rate.</p> </abstract>
<abstract> <p>This paper considers the problem of determining the number of nonparametric factors in a multivariate nonparametric relationship. The definition given is broad enough to encompass a number of potential applications in econometrics including inferring the rank of demand, testing whether for a given set of instruments it is possible to identify a linear model, and testing arbitrage pricing theory. The paper gives methods for testing hypotheses concerning the number of factors, using series and kernel based nonparametric methods and also considers the problem of estimating the number of factors. The methods are compared in a small simulation study and an application to determining the rank of demand systems is considered.</p> </abstract>
<abstract> <p> We consider a family of rank tests based on the regression rank score process introduced by Gutenbrunner and Jurečková (1992) to test the unit root hypothesis in economic time series. In contrast to tests based on least squares methods, the rank tests are asymptotically Gaussian under the null hypothesis, and have excellent power--particularly under innovation processes exhibiting heavy tails. </p> </abstract>
<abstract> <p>We present a finite system of polynomial inequalities in unobservable variables and market data that observations on market prices, individual incomes, and aggregate endowments must satisfy to be consistent with the equilibrium behavior of some pure trade economy. Quantifier elimination is used to derive testable restrictions on finite data sets for the pure trade model. A characterization of observations on aggregate endowments and market prices that are consistent with a Robinson Crusoe's economy is also provided.</p> </abstract>
<abstract> <p>Technological change and deregulation have caused a major restructuring of the telecommunications equipment industry over the last two decades. Our empirical focus is on estimating the parameters of a production function for the equipment industry, and then using those estimates to analyze the evolution of plant-level productivity. The restructuring involved significant entry and exit and large changes in the sizes of incumbents. Firms' choices on whether to liquidate, and on input quantities should they continue, depended on their productivity. This generates a selection and a simultaneity problem when estimating production functions. Our theoretical focus is on providing an estimation algorithm which takes explicit account of these issues. We find that our algorithm produces markedly different and more plausible estimates of production function coefficients than do traditional estimation procedures. Using our estimates we find increases in the rate of aggregate productivity growth after deregulation. Since we have plant-level data we can introduce indices which delve deeper into how this productivity growth occurred. These indices indicate that productivity increases were primarily a result of a reallocation of capital towards more productive establishments.</p> </abstract>
<abstract> <p>This is a one-agent Bayesian model of learning by doing and technology choice. The more the agent uses a technology, the better he learns its parameters, and the more productive he gets. This expertise is a form of human capital. Any given technology has bounded productivity, which therefore can grow in the long run only if the agent keeps switching to better technologies. But a switch of technologies temporarily reduces expertise: The bigger is the technological leap, the bigger the loss in expertise. The prospect of a productivity drop may prevent the agent from climbing the technological ladder as quickly as he might. Indeed, an agent may be so skilled at some technology that he will never switch again, so that he will experience no long-run growth. In contrast, someone who is less skilled (and therefore less productive) at that technology may find it optimal to switch technologies over and over again, and therefore enjoy long-run growth in output. Thus the model can give rise to overtaking.</p> </abstract>
<abstract> <p>This paper extends the spatial theory of voting to an institutional structure in which policy choices depend upon not only the executive but also the composition of the legislature. Voters have incentives to be strategic since policy reflects the outcome of a simultaneous election of the legislature and the executive and since the legislature's impact on policy depends upon relative plurality. To analyze equilibrium in this game between voters, we apply "coalition proof" type refinements. The model has several testable implications which are consistent with voting behavior in the United States. For instance, the model predicts: (a) split-tickets where some voters vote for one party for president and the other for congress; (b) for some parameter values, a divided government with different parties controlling the executive and the majority of the legislature; and (c) the midterm electoral cycle with the party holding the presidency always losing votes in midterm congressional elections.</p> </abstract>
<abstract> <p>This paper constructs a space of states of the world representing the exhaustive uncertainty facing each player in a strategic situation. The innovation is that preferences are restricted primarily by "regularity" conditions and need not conform with subjective expected utility theory. The construction employs a hierarchy of preferences, rather than of beliefs as in the standard Bayesian model. The framework is sufficiently general to accommodate uncertainty averse preferences, such as exhibited in the Ellsberg paradox, and to allow common knowledge of expected utility (or Choquet expected utility) to be well-defined formally. Applications include the provision of (i) foundations for a Harsanyi-style game of incomplete information, and (ii) a rich framework for the axiomatization of solution concepts for complete information normal form games.</p> </abstract>
<abstract> <p>This paper presents a new, probabilistic model of learning in games which investigates the often stated intuition that common knowledge of strategic intent may arise from repeated interaction. The model is set in the usual repeated game framework, but the two key assumptions are framed in terms of the likelihood of beliefs and actions conditional on the history of play. The first assumption formalizes the basic intuition of the learning approach; the second, the indeterminacy that inspired resort to learning models in the first place. Together the assumptions imply that, almost surely, play will remain almost always within one of the stage game's "minimal inclusive sets." In important classes of games, including those with strategic complementarities, potential functions, and bandwagon effects, all such sets are singleton Nash.</p> </abstract>
<abstract> <p>Several recent papers have emphasized that long-term relationships can be efficiently governed by short-term contracts, provided that there is no asymmetric information at the contracting dates. This excludes not only adverse selection problems, but also moral hazards problems in which hidden actions generate private information over the future. In this paper we show, in the context of a multiperiod principal-agent relationship with adverse selection, that renegotiable short-term contracts can be as efficient as long-term renegotiation-proof contracts. On the other hand, spot contracting may fail to achieve long-run efficiency not only because of intertemporal smoothing, as with symmetric information, but also because of ratchet effects and time inconsistency in the optimal structure of informational rents. Thus some limited commitment seems to be a necessary and sufficient condition to achieve long-run efficiency.</p> </abstract>
<abstract> <p> We introduce and analyze "multistage situations," which generalize "multistage games" (which, in turn, generalize "repeated games"). One reason for this generalization is to avoid the perhaps unrealistic constraint--inherent to noncooperative games--that the set of strategy tuples must be a Cartesian product of the strategy sets of the players. Another reason is that in most economic and social activities (e.g., in sequential bargaining without a rigid protocol) the "rules of the game" are rather amorphous; the procedures are rarely pinned down. Such social environments can, however, be represented as multistage situations and be effectively analyzed through the theory of social situations. The paper contributes to the theory of social situations within the framework of multistage situations (e.g., the existence of a largest conservative stable standard of behavior which yields a definition that extends "subgame perfect equilibrium paths"), and to the theory of multistage games (e.g., the existence of ε-generalized perfect equilibrium, for all ε &gt; 0). It also provides an equivalence theorem between subgame perfection and the largest conservative stable standard of behavior for multistage games. The usefulness of our approach is further illustrated by our notion of "k-rationality" whereby (at each substitution) players look only k steps ahead. </p> </abstract>
<abstract> <p>This paper examines how proportional transaction costs, short-sale constraints, and margin requirements affect inferences based on asset return data about intertemporal marginal rates of substitution (IMRSs). It is shown that small transaction costs can greatly reduce the required variability of IMRSs. This suggests that the low variability of many parametric, aggregate consumption based IMRSs need not be inconsistent with asset return data. Euler inequalities for a transaction cost economy with power utility are tested using aggregate consumption data and returns on stocks and short maturity U.S. Treasury bills. In the majority of cases there is little evidence against power utility specifications with low risk-aversion parameters. The results are obtained with transaction costs on stocks as small as .5% of price, and are in sharp contrast to the strong rejection of the analogous Euler equalities for a frictionless economy.</p> </abstract>
<abstract> <p>Our general subject is model determination methods and their use in the prediction of economic time series. The methods suggested are Bayesian in spirit but they can be justified by classical as well as Bayesian arguments. The main part of the paper is concerned with model determination, forecast evaluation, and the construction of evolving sequences of models that can adapt in dimension and form (including the way in which any nonstationarity in the data is modelled) as new characteristics in the data become evident. The paper continues some recent work on Bayesian asymptotics by the author and Werner Ploberger (1995), develops embedding techniques for vector martingales that justify the role of a class of exponential densities in model selection and forecast evaluation, and implements the modelling ideas in a multivariate regression framework that includes Bayesian vector autoregressions (BVAR's) and reduced rank regressions (RRR's). It is shown how the theory in the paper can be used: (i) to construct optimized BVAR's with data-determined hyperparameters; (ii) to compare models such as BVAR's, optimized BVAR's, and RRR's; (iii) to perform joint order selection of cointegrating rank, lag length, and trend degree in a VAR; and (iv) to discard data that may be irrelevant and thereby reset the initial conditions of a model.</p> </abstract>
<abstract> <p>The asymptotic power envelope is derived for point-optimal tests of a unit root in the autoregressive representation of a Gaussian time series under various trend specifications. We propose a family of tests whose asymptotic power functions are tangent to the power envelope at one point and are never far below the envelope. When the series has no deterministic component, some previously proposed tests are shown to be asymptotically equivalent to members of this family. When the series has an unknown mean or linear trend, commonly used tests are found to be dominated by members of the family of point-optimal invariant tests. We propose a modified version of the Dickey-Fuller t test which has substantially improved power when an unknown mean or trend is present. A Monte Carlo experiment indicates that the modified test works well in small samples.</p> </abstract>
<abstract> <p>This paper proposes three classes of consistent one-sided tests for serial correlation of unknown form for the residual from a linear dynamic regression model that includes both lagged dependent variables and exogenous variables. The tests are obtained by comparing a kernel-based normalized spectral density estimator and the null normalized spectral density estimator and the null normalized spectral density, using a quadratic norm, the Hellinger metric, and the Kullback-Leibler information criterion respectively. Under the null hypothesis of no serial correlation, the three classes of new test statistics are asymptotically N(0, 1) and equivalent. The null distributions are obtained without having to specify any alternative model. Unlike some conventional tests for serial correlation, the null distributions of our tests remain invariant when the regressors include lagged dependent variables. Under a suitable class of local alternatives, the three classes of the new tests are asymptotically equally efficient. Under global alternatives, however, their relative efficiencies depend on the relative magnitudes of the three divergence measures. Our approach provides an interpretation for Box and Pierce's (1970) test, which can be viewed as a quadratic norm based test using a truncated periodogram. Many kernels deliver tests with better power than Box and Pierce's test or the truncated kernel based test. A simulation study shows that the new tests have good power against an AR(1) process and a fractionally integrated process. In particular, they have better power than the Lagrange multiplier tests of Breusch (1978) and Godfrey (1978) as well as the portmanteau tests of Box and Pierce (1970) and Ljung and Box (1978). The cross-validation procedure of Beltrao and Bloomfield (1987) and Robinson (1991a) works reasonably well in determining the smoothing parameter of the kernel spectral estimator and is recommended for use in practice.</p> </abstract>
<abstract> <p>In this paper, we develop several consistent tests in the context of a nonparametric regression model. These include tests for the significance of a subset of regressors and tests for the specification of the semiparametric functional form of the regression function, where the latter covers tests for a partially linear and a single index specification against a general nonparametric alternative. One common feature to the construction of all these tests is the use of the Central Limit Theorem for degenerate U-statistics of order higher than two. As a result, they share the same advantages over most of the corresponding existing tests in the literature: (a) They do not depend on any ad hoc modifications such as sample splitting, random weighting, etc. (b) Under the alternative hypotheses, the test statistics in this paper diverge to positive infinity at a faster rate than those based on ad hoc modifications.</p> </abstract>
<abstract> <p>Monte Carlo experiments have shown that tests based on generalized-method-of-moments estimators often have true levels that differ greatly from their nominal levels when asymptotic critical values are used. This paper gives conditions under which the bootstrap provides asymptotic refinements to the critical values of t tests and the test of overidentifying restrictions. Particular attention is given to the case of dependent data. It is shown that with such data, the bootstrap must sample blocks of data and that the formulae for the bootstrap versions of test statistics differ from the formulae that apply with the original data. The results of Monte Carlo experiments on the numerical performance of the bootstrap show that it usually reduces the errors in level that occur when critical values based on first-order asymptotic theory are used. The bootstrap also provides an indication of the accuracy of critical values obtained from first-order asymptotic theory.</p> </abstract>
<abstract> <p> Well-behaved infinite signaling games may have no sequential equilibria. We prove that adding cheap talk to these games "solves" the nonexistence problem: the limit of sequential equilibrium outcomes of finite approximating games is a sequential equilibrium outcome of the cheap-talk extension of the limit game. In addition, when the signaling space has no isolated points, any cheap-talk sequential equilibrium outcome can be approximated by a sequential ε-equilibrium of the game without cheap talk. </p> </abstract>
<abstract> <p>Recent evolutionary models have introduced "small mutation rates" as a way of refining predictions of long-run behavior. We show that this refinement effect can only be obtained by restrictions on how the magnitude of the effect of mutation on evolution varies across states of the system. In particular, given any model of the effect of mutations, any invariant distribution of the "mutationless" process is close to an invariant distribution of the process with appropriately chosen small mutation rates.</p> </abstract>
<abstract> <p>This paper studies the effects of unions on the structure of wages, using an estimation technique that explicitly accounts for misclassification errors in reported union status, and potential correlations between union status and unobserved productivity. The econometric model is estimated separately for five skill groups using a large panel data set formed from the U.S. Current Population Survey. The results suggest that unions raise wages more for workers with lower levels of observed skills. In addition, the patterns of selection bias differ by skill group. Among workers with lower levels of observed skill, unionized workers are positively selected, whereas union workers are negatively selected from among those with higher levels of observed skill.</p> </abstract>
<abstract> <p>We propose a nonparametric estimation procedure for continuous-time stochastic models. Because prices of derivative securities depend crucially on the form of the instantaneous volatility of the underlying process, we leave the volatility function unrestricted and estimate it nonparametrically. Only discrete data are used but the estimation procedure still does not rely on replacing the continuous-time model by some discrete approximation. Instead the drift and volatility functions are forced to match the densities of the process. We estimate the stochastic differential equation followed by the short-term interest rate and compute nonparametric prices for bonds and bond options.</p> </abstract>
<abstract> <p>Suppose an observed time series is generated by a stochastic volatility model--i.e., there is an unobservable state variable controlling the volatility of the innovations in the series. As shown by Nelson (1992), and Nelson and Foster (1994), a misspecified ARCH model will often be able to consistently (as a continuous time limit is approached) estimate the unobserved volatility process, using information in the lagged residuals. This paper shows how to more efficiently estimate such a volatility process using information in both lagged and led residuals. In particular, this paper expands the optimal filtering results of Nelson and Foster (1994) and Nelson (1994) to smoothing and to filtering with a random initial condition.</p> </abstract>
<abstract> <p>This paper provides a proof of the consistency and asymptotic normality of the quasi-maximum likelihood estimator in GARCH(1,1) and IGARCH(1,1) models. In contrast to the case of a unit root in the conditional mean, the presence of a "unit root" in the conditional variance does not affect the limiting distribution of the estimators; in both models, estimators are normally distributed. In addition, a consistent estimator of the covariance matrix is available, enabling the use of standard test statistics for inference.</p> </abstract>
<abstract> <p>This paper proposes some tests for parameter constancy in linear regressions. The tests use weighted empirical distribution functions of estimated residuals and are asymptotically distribution free. The local power analysis reveals that the proposed tests have nontrivial local power against a wide range of alternatives. In particular, the tests are capable of detecting error heterogeneity that is not necessarily manifested in the form of changing variances. The model allows for both dynamic and trending regressors. The residuals may be obtained based on any root-n consistent estimator (under the null) of regression parameters. As an intermediate result, some weak convergence for (stochastically) weighted sequential empirical processes is established.</p> </abstract>
<abstract> <p>We study the collective choice of fiscal policy in a "federation" with two levels of government. Local policy redistributes across individuals and affects the probability of aggregate shocks, whereas federal policy shares international risk. There is a tradeoff between risk-sharing and moral hazard: federal risk-sharing may induce local governments to enact policies that increase local risk. We analyze this tradeoff under alternative fiscal constitutions. In particular, we contrast a vertically ordered system like the EC with a horizontally ordered federal system like the US. Alternative arrangements create different incentives for policymakers and voters, and give rise to different political equilibria. Under appropriate institutions, centralization of functions and power can mitigate the moral hazard problem.</p> </abstract>
<abstract> <p>In most states, unemployment insurance recipients accepting part-time work can earn up to a specific amount (the "disregard") with no reduction in benefits. Benefits are then reduced on a dollar for dollar basis for earnings in excess of the disregard. The disregard varies both across states and within a state over time. This paper analyzes the effects of changes in the disregard on job search behavior. A continuous-time job search model is developed and under general conditions an increase in the disregard is shown to increase both the part-time and overall re-employment hazards. Data from the Current Population Survey's Displaced Worker Supplements are used to test these predictions. Estimates from a competing risks model with correlated risks and time-varying coefficients shows that increasing the disregard significantly increases the conditional probability of part-time re-employment during the first three months of joblessness.</p> </abstract>
<abstract> <p>A model of social distance is presented that is useful for understanding social decisions. Status and conformity in previous models are discussed, and then a generalization is described. In this generalization agents have inherited positions in social space and an expected value of trade between two individuals as a function of the difference in their initial positions. An example of this system is constructed in which there is class stability. Agents who are initially close interact strongly while those who are socially distant have little interaction. In this example inherited social position, which may be interpreted as social class, plays a dominant role. The relevance of this model to social decisions such as the choice of educational attainment and childbearing is discussed in the context of specific ethnographic examples. Class position may play a dominant role in these decisions.</p> </abstract>
<abstract> <p>We analyze two-candidate elections in which voters are uncertain about the realization of a state variable that affects the utility of all voters. Each voter has noisy private information about the state variable. We show that the fraction of voters whose vote depends on their private information goes to zero as the size of the electorate goes to infinity. Nevertheless, elections fully aggregate information in the sense that the chosen candidate would not change if all private information were common knowledge. Equilibrium voting behavior is to a large extent determined by the electoral rule, i.e., if a candidate is required to get at least x percent of the vote in order to win the election, then in equilibrium this candidate gets very close to x percent of the vote with probability close to one. Finally, if the distribution from which preferences are drawn is uncertain, then elections will generally not satisfy full information equivalence and the fraction of voters who take informative action does not converge to zero.</p> </abstract>
<abstract> <p>The concept of adaptively rational equilibrium (A.R.E.) is introduced. Agents adapt their beliefs over time by choosing from a finite set of different predictor or expectations functions. Each predictor is a function of past observations and has a performance or fitness measure which is publicly available. Agents make a rational choice concerning the predictors based upon their past performance. This results in a dynamics across predictor choice which is coupled to the equilibrium dynamics of the endogenous variables. As a simple, but typical, example we consider a cobweb type demand-supply model where agents can choose between rational and naive expectations. In an unstable market with (small) positive information costs for rational expectations, a high intensity of choice to switch predictors leads to highly irregular equilibrium prices converging to a strange attractor. The irregularity of the equilibrium time paths is explained by the existence of a so-called homoclinic orbit and its associated complicated dynamical phenomena. Thus local instability and global complicated dynamics may be a feature of a fully rational notion of equilibrium.</p> </abstract>
<abstract> <p>This paper introduces a conditional Kolmogorov test of model specification for parametric models with covariates (regressors). The test is an extension of the Kolmogorov test of goodness-of-fit for distribution functions. The test is shown to have power against &lt;tex-math&gt;$1/\sqrt{n}$&lt;/tex-math&gt; local alternatives and all fixed alternatives to the null hypothesis. A parametric bootstrap procedure is used to obtain critical values for the test.</p> </abstract>
<abstract> <p>In this paper we derive the asymptotic distribution of the test statistic of a generalized version of the integrated conditional moment (ICM) test of Bierens (1982, 1984), under a class of &lt;tex-math&gt;$\sqrt{n}\text{-}\text{local}$&lt;/tex-math&gt; alternatives, where n is the sample size. The generalized version involved includes neural network tests as a special case, and allows for testing misspecification of dynamic models. It appears that the ICM test has nontrivial local power. Moreover, for a class of "large" local alternatives the consistent ICM test is more powerful than the parametric t test in a neighborhood of the parametric alternative involved. Furthermore, under the assumption of normal errors the ICM test is asymptotically admissible, in the sense that there does not exist a test that is uniformly more powerful. The asymptotic size of the test is case-dependent: the critical values of the test depend on the data-generating process. In this paper we derive case-independent upperbounds of the critical values.</p> </abstract>
<abstract> <p>This paper considers reputation effects in a repeated game between two long-run players, one of whom is relatively patient and may be committed to a fixed strategy which punishes the opponent for increasingly long periods whenever inappropriate actions are taken. By following this commitment strategy, the more patient player can induce a sufficiently patient opponent to experiment to see whether the commitment strategy is also being followed off the equilibrium path. This can guarantee the more patient player (in any Nash equilibrium) a payoff close to the highest feasible payoff consistent with the individual rationality of the other player. For an arbitrary degree of patience of the less patient player a weaker result is established: the more patient player is guaranteed an average equilibrium payoff close to the static Stackelberg payoff. This latter result generalizes that of Schmidt (1993), who establishes the same result for the restricted class of games of conflicting interests.</p> </abstract>
<abstract> <p>Allowing for incomplete information, this paper characterizes the social choice functions that can be approximated by the equilibrium outcomes of a mechanism: incentive compatibility is necessary and almost sufficient for virtual Bayesian implementability. In conjunction with a second condition, Bayesian incentive consistency, incentive compatibility is also sufficient. This new condition is weak--under standard topological and informational assumptions, it is satisfied by every social choice function. The type sets of the agents are taken to be arbitrary (possibly infinite) measurable spaces. An example shows that there are virtually (in fact, exactly) Bayesian implementable social choice functions that are not virtually implementable in iteratively undominated strategies.</p> </abstract>
<abstract> <p>Equivalence of the core and the set of Walrasian allocations has long been taken as one of the basic tests of perfect competition. The present paper examines this basic test of perfect competition in economies with an infinite dimensional space of commodities and a large finite number of agents. In this context, we cannot expect equality of the core and the set of Walrasian allocations; rather, as in the finite dimensional context, we look for theorems establishing core convergence (that is, approximate decentralization of core allocations in economies with a large finite number of agents). Previous work in this area has established that core convergence for replica economies and core equivalence for economies with a continuum of agents continue to be valid in the infinite dimensional context under assumptions much the same as those needed in the finite dimensional context. For general large finite economies, however, we present here a sequence of examples of failure of core convergence. These examples point to a serious disconnection between replica economies and continuum economies on the one hand, and general large finite economies on the other hand. We identify the source of this disconnection as the measurability requirements that are implicit in the continuum model, and which correspond to compactness requirements that have especially serious economic content in the infinite dimensional context. We also obtain a positive result. When the commodity space is L^1, the space of integrable functions on a finite measure space, we establish core convergence under the assumptions that marginal utility goes to zero as consumption tends to infinity and the per capita social endowment lies above a consumption bundle which is equidesirable with respect to the preferences. This positive result depends on a version of the Shapley-Folkman theorem for L&lt;sup&gt;1&lt;/sup&gt;.</p> </abstract>
<abstract> <p>Consider a two-player discounted repeated game in which each player optimizes with respect to a prior belief about his opponent's repeated game strategy. One would like to argue that if beliefs are cautious, then each player's best response will be in the support, loosely speaking, of his opponent's belief and that, therefore, players will learn as the game unfolds to predict the continuation path of play. If this conjecture were true, a convergence result due to Kalai and Lehrer would imply that the continuation path of the repeated game would asymptotically resemble that of a Nash equilibrium. One would thus have constructed a theory in which Nash equilibrium behavior is a necessary long-run consequence of optimization by cautious players. This paper points out an obstacle to such a theory. Loosely put, in many repeated games, if players optimize with respect to beliefs that satisfy a diversity condition termed neutrality, then each player will choose a strategy that his opponent was certain would not be played.</p> </abstract>
<abstract> <p>This paper reports a laboratory experiment designed to examine the price formation process in a simple market institution, the single call market. The experiment features random values and costs each period, so each period generates a new price formation observation. Other design features are intended to enhance the predictive power of the Bayesian Nash equilibrium (BNE) theory developed recently for this trading institution. We find that the data support several qualitative implications of the BNE, but that subjects' bid and ask behavior is not as responsive to changes in the pricing rule as the BNE predictions. Bids and asks tend to reveal more of the underlying values and costs than predicted, particularly when subjects are experienced. Nevertheless, observed trading efficiency falls below the BNE prediction. The results offer more support for the BNE when subjects compete against Nash "robot" opponents. A simple learning model accounts for several of the deviations from BNE.</p> </abstract>
<abstract> <p>This paper develops a general framework for modeling choice under uncertainty that extends subjective expected utility to include nonseparabilities, state-dependence, and the effect of subjective or ill defined consequences. This is accomplished by not including consequences among the formal primitives. Instead, the effect of consequences is modeled indirectly, through conditional preferences over acts. The main results concern the aggregation of conditional utilities to form an unconditional utility, including the case of additive aggregation. Applications, obtained by further specifying the structure of acts and conditional preferences, include disappointment, regret, and the subjective value of information.</p> </abstract>
<abstract> <p>The starting point of this paper is a simple, regular dynamic game in which subgame-perfect equilibrium fails to exist. Examination of this example shows that existence would be restored if players were allowed to observe the output of a public-randomization device. The main result of the paper shows that the introduction of public randomization yields existence not only in the example, but also in a large class of dynamic games. It is also argued that the introduction of public randomization is the minimal robust extension of subgame-perfect equilibrium in this class of games.</p> </abstract>
<abstract> <p>Rubinstein's alternating-offers bargaining model is enriched by assuming that players' payoffs in disagreement periods are determined by a normal form game. It is shown that such a model can have multiple perfect equilibria, including inefficient ones, provided that players are sufficiently patient. Delay is possible even though there is perfect information and the players are fully rational. The length of delay depends only on the payoff structure of the disagreement game and not on the discount factor. Not all feasible and individually rational payoffs of the disagreement game can be supported as average disagreement payoffs. Indeed, some negotiation games have a unique perfect equilibrium with immediate agreement.</p> </abstract>
<abstract> <p>This paper studies moral hazard contracts that may be renegotiated after an agent chooses an unobservable effort. Unlike in previous models, a contract here contains only one compensation scheme, and the agent has all the bargaining power in the renegotiation stage. Using a relatively weak forward-induction refinement, all equilibria are shown to be (second-best) efficient. Renegotiation occurs in every equilibrium. If the effort set is rich, the only equilibrium initial contract is a sales contract, i.e., a scheme which "sells the project" to the agent. This captures the idea that a party (the principal) who has an inherently weak renegotiation position will sometimes insist on a simple initial contract.</p> </abstract>
<abstract> <p>We analyze optimal mechanisms in environments where sellers are privately informed about quality. A methodology is provided for deriving conditions that are necessary and sufficient to determine when two simple trading environments maximize either social or private surplus. The commonly used auction mechanism is frequently inefficient in procurement environments. Often, the optimal mechanism is simply to order potential suppliers and to tender take-it-or-leave-it offers to each sequentially. We completely characterize the environments in which either mechanism is optimal. In doing so, we develop a general methodology that determines when and if a given trading institution is optimal.</p> </abstract>
<abstract> <p>We consider a model where two agents, privately informed about their own characteristics, play a (normal form) game on behalf of two uninformed principals. We analyze the existence of precommitment effects through public announcements of contracts, in a model where agency contracts, designed ex-ante, can always be secretly renegotiated, at the ex-ante and interim stages. We show that the existence of precommitment effects depends both on the strategic complementarity of the agents' actions and on the direct effect of the opponents' actions on each principal's welfare. In our model, the possibility of renegotiation is crucial for the existence of precommitment effects. The results are introduced through an example of Cournot and Bertrand competition between firms, viewed as vertical structures.</p> </abstract>
<abstract> <p>We show that, under certain regularity conditions, if the distribution of income is price independent and satisfies a condition on the shape of its graph, then total market demand, F(p), is monotone; i.e., given two positive prices, p and q, one has &lt;latex&gt;$(p - q). (F(p) - F(q)) &lt; 0$&lt;/latex&gt;. These results allow for density functions increasing on some intervals, like unimodal distributions or even densities with more than one peak. Similar assumptions on the distribution of endowments, yield a restricted monotonicity property on aggregate excess demand, where, now, wealth is determined by market prices. This property guarantees uniqueness and stability of equilibrium of the Walrasian pure exchange economy.</p> </abstract>
<abstract> <p>With the same normalization as that for standard parametric statistics, and centered at a parameter of interest, many semiparametric estimates based on n observations have been shown to be root-n-consistent and asymptotically normal. In the context of semiparametric averaged derivative estimates, we go further by showing that the rate of convergence of the finite-sample distribution to the normal limit distribution can equal that of standard parametric statistics.</p> </abstract>
<abstract> <p>Using a Simulated Method of Moments approach, I evaluate a representative consumer asset pricing model in which the consumer is assumed to have time nonseparable preferences of several forms. Examining the model's implications for several moments of asset returns, I find evidence for the local substitution of consumption with habit formation occurring over longer periods of time. The interaction between these two effects is important. I also show that, when accounting for sampling error, a model with local substitution and long-run habit persistence is consistent with the Hansen and Jagannathan (1991) bounds.</p> </abstract>
<abstract> <p>Existence of equilibrium with incomplete markets is problematic because demand functions are typically not continuous. Discontinuities occur at prices for which a marketed asset suddenly becomes redundant. We show that this discontinuity disappears if we allow an agent in the economy to introduce a new asset when such redundancies occur. This enables us to prove existence with incomplete markets using a standard path-following argument. Hence, available algorithms for path-following in R&lt;sup&gt;K&lt;/sup&gt; can be applied to compute equilibria in the GEI case. We demonstrate this by computing equilibrium for a numerical example.</p> </abstract>
<abstract> <p>In the previous paper Keisler (1995) we introduced a random price adjustment model for an exchange economy which is decentralized in that the trades permitted to an agent and the resulting price changes depend only on the commodity vector currently held by that agent, and not on the whole economy. Our model is an exchange economy with a finite set of commodities, a market maker who adjusts prices, a large finite set of agents who trade only with the market maker, and a parameter λ ∈ (0, 1) which determines the rate of price adjustment. Each agent has an initial endowment and a preference relation. At each discrete time, one agent is chosen at random and makes the trade which maximizes his preferences subject to the budget constraint at the current price vector. Then the market maker adjusts the price vector according to the following rule: if x is the change in the commodity vector of the agent who just traded, p is the old price, and α is the number of agents, then the new price is p + (α&lt;sup&gt;-λ&lt;/sup&gt;)x. Thus both the price vector and the commodity allocation change randomly with time. We obtain asymptotic results as the number of agents goes to infinity, subject to stability assumptions on the price paths. It was shown in the earlier paper that with probability arbitrarily close to one the price path in our model will approximate the price path of the corresponding tatonnement process on a rapid time scale, and will then remain close to a limit price. In this paper we show that, with probability arbitrarily close to one, the economy will approach a competitive equilibrium, and the process will be feasible in the sense that the market maker's inventory is approximately constant over time. Our main assumption is that the price adjustment rule is stable throughout the trading process. This is stronger than the classical tatonnement assumption that the price adjustment rule is stable at the initial endowment. We show that many classical examples of exchange economies satisfy our assumptions, and then give an example where stability at the initial endowment holds but the stronger stability assumption needed for our results fails.</p> </abstract>
<abstract> <p>Typically, work on mechanism design has assumed that all private information can be captured in a single scalar variable. This paper explores one way in which this assumption can be relaxed in the context of the multiproduct nonlinear pricing problem. It is shown that the firm will choose to exclude some low value consumers from all markets. A class of cases that allow explicit solution is derived by making use of a multivariate form of "integration by parts." In such cases the optimal tariff is cost-based.</p> </abstract>
<abstract> <p>Inequality measures are often used to summarize information about empirical income distributions. However the resulting picture of the distribution and of changes in the distribution can be severely distorted if the data are contaminated. The nature of this distortion will in general depend upon the underlying properties of the inequality measure. We investigate this issue theoretically using a technique based on the influence function, and illustrate the magnitude of the effect using a simulation. We consider both direct nonparametric estimation from the sample, and indirect estimation using a parametric model; in the latter case we demonstrate the application of a robust estimation procedure. We apply our results to two micro-data examples.</p> </abstract>
<abstract> <p>This paper presents a method for estimating the model Λ(Y) = β'X + U, where Y is a scalar, Λ is an unknown increasing function, X is a vector of explanatory variables, β is a vector of unknown parameters, and U has unknown cumulative distribution function F. It is not assumed that Λ and F belong to known parametric families; they are estimated nonparametrically. This model generalizes a large number of widely used models that make stronger a priori assumptions about Λ and/or F. The paper develops n&lt;sup&gt;1/2&lt;/sup&gt;-consistent, asymptotically normal estimators of Λ, F, and quantiles of the conditional distribution of Y. Estimators of β that are n&lt;sup&gt;1/2&lt;/sup&gt;-consistent and asymptotically normal already exist. The results of Monte Carlo experiments indicate that the new estimators work reasonably well in samples of size 100.</p> </abstract>
<abstract> <p>It is widely known that conditional covariances of asset returns change over time. Researchers doing empirical work have adopted many strategies for accommodating conditional heteroskedasticity. Among the popular strategies are: (a) chopping the available data into short blocks of time and assuming homoskedasticity within the blocks, (b) performing one-sided rolling regressions, in which only data from, say, the preceding five year period is used to estimate the conditional covariance of returns at a given date, and (c) performing two-sided rolling regressions, in which covariances are estimated for each date using, say, five years of lags and five years of leads. Another model--GARCH--amounts to a one-sided weighted rolling regression. We develop continuous record asymptotic approximations for the measurement error in conditional variances and covariances when using these methods. We derive asymptotically optimal window lengths for standard rolling regressions and optimal weights for weighted rolling regressions. As an empirical example, we estimate volatility on the S &amp; P 500 stock index using daily data from 1928 to 1990.</p> </abstract>
<abstract> <p>We investigate the separate effects of a training program on the duration of participants' subsequent employment and unemployment spells. This program randomly assigned volunteers to treatment and control groups. However, the treatments and controls experiencing subsequent employment and unemployment spells are not generally random (or comparable) subsets of the initial groups because the sorting process into subsequent spells is very different for the two groups. Standard practice in duration models ignores this sorting process, leading to a sample selection problem and misleading estimates of the training effects. We propose an estimator that addresses this problem and find that the program studied, the National Supported Work Demonstration, raised trainees' employment rates solely by lengthening their employment durations.</p> </abstract>
<abstract> <p>The act of choosing can have particular relevance in maximizing behavior for at least two distinct reasons: (1) process significance (preferences may be sensitive to the choice process, including the identity of the chooser), and (2) decisional inescapability (choices may have to be made whether or not the judgemental process has been completed). The general approach of maximizing behavior can--appropriately formulated--accommodate both concerns, but the regularities of choice behavior assumed in standard models of rational choice will need significant modification. These differences have considerable relevance in studies of economic, social, and political behavior.</p> </abstract>
<abstract> <p>This paper provides an empirical analysis of how the U.S. Social Security and Medicare insurance system affects the labor supply of older males in the presence of incomplete markets for loans, annuities, and health insurance. We estimate a dynamic programming (DP) model of the joint labor supply and Social Security acceptance decision, focusing on a sample of males in the low to middle income brackets whose only pension is Social Security. The DP model delivers a rich set of predictions about the dynamics of retirement behavior, and comparisons of actual vs. predicted behavior show that the DP model is able to account for a wide variety of phenomena observed in the data, including the pronounced peaks in the distribution of retirement ages at 62 and 65 (the ages of early and normal eligibility for Social Security benefits, respectively). We identify a significant fraction of "health insurance constrained" individuals who have no form of retiree health insurance other than Medicare, and who can only obtain fairly priced private health insurance via their employer's group health plan. The combination of significant individual risk aversion and a long tailed (Pareto) distribution of health care expenditures implies that there is a significant "security value" for these individuals to remain employed until they are eligible for Medicare coverage at age 65. Overall, our model suggests that a number of heretofore puzzling aspects of retirement behavior can be viewed as artifacts of particular details of the Social Security rules, whose incentive effects are especially strong for lower income individuals and those who do not have access to fairly priced loans, annuities, and health insurance.</p> </abstract>
<abstract> <p>Numerous experimental studies indicate that people tend to reciprocate favors and punish unfair behavior. It is hypothesized that these behavioral responses contribute to the enforcement of contracts and, hence, increase gains from trade. It turns out that if only one side of the market has opportunities for reciprocal responses, the impact of reciprocity on contract enforcement depends on the details of the pecuniary incentive system. If both sides of the market have opportunities for reciprocal responses, robust and powerful reciprocity effects occur. In particular, reciprocal behavior causes a substantial increase in the set of enforceable actions and, hence, large efficiency gains.</p> </abstract>
<abstract> <p>While optimally weighted GMM estimation has desirable large sample properties, its small sample performance is poor in some applications. We propose a computationally simple alternative, for weakly dependent data generating mechanisms, based on minimization of the Kullback-Leibler Information Criterion. Conditions are derived under which the large sample properties of this estimator are similar to GMM, i.e., the estimator will be consistent and asymptotically normal, with the same asymptotic covariance matrix as GMM. In addition, we propose overidentifying and parametric restrictions tests as alternatives to analogous GMM procedures.</p> </abstract>
<abstract> <p>When applied to groups, the Revelation Principle postulates a Bayesian-Nash behavior between agents. Their binding agreements are unenforceable or the principal can prevent them at no cost. We analyze instead a mechanism design problem in which the agents can communicate between themselves and collude under asymmetric information. We characterize the set of implementable collusion-proof contracts both when the principal offers anonymous and nonanonymous contracts. After having isolated the nexi and the stakes of collusion we proceed to a normative analysis, perform some comparative statics, discuss our concept of collusion-proofness, and provide some insights about transaction costs in side contracting.</p> </abstract>
<abstract> <p>To obtain consistency and asymptotic normality, a generalized method of moments (GMM) estimator typically is defined to be an approximate global minimizer of a GMM criterion function. To compute such an estimator, however, can be problematic because of the difficulty of global optimization. In consequence, practitioners usually ignore the problem and take the GMM estimator to be the result of a local optimization algorithm. This yields an estimator that is not necessarily consistent and asymptotically normal. The use of a local optimization algorithm also can run into the problem of instability due to flats or ridges in the criterion function, which makes it difficult to know when to stop the algorithm. To alleviate these problems of global and local optimization, we propose a stopping-rule (SR) procedure for computing GMM estimators. The SR procedure eliminates the need for global search with high probability. And, it provides an explicit SR for problems of stability that may arise with local optimization problems.</p> </abstract>
<abstract> <p>This paper presents a procedure for analyzing a model in which the parameter vector has two parts: a finite-dimensional component θ and a nonparametric component λ. The procedure does not require parametric modeling of λ but assumes that the true density of the data satisfies an index restriction. The idea is to construct a parametric model passing through the true model and to estimate θ by setting the score for the parametric model to zero. The score is estimated nonparametrically and the estimator is shown to be &lt;tex-math&gt;$\sqrt N$&lt;/tex-math&gt; consistent and asymptotically normal. The estimator is then shown to attain the semiparametric efficiency bound characterized in Begun et al. (1983) for multivariate nonlinear regression, simultaneous equations, partially specified regression, index regression, censored regression, switching regression, and disequilibrium models in which the error densities are unknown.</p> </abstract>
<abstract> <p>This paper presents a semiparametric procedure to analyze the effects of institutional and labor market factors on recent changes in the U.S. distribution of wages. The effects of these factors are estimated by applying kernel density methods to appropriately weighted samples. The procedure provides a visually clear representation of where in the density of wages these various factors exert the greatest impact. Using data from the Current Population Survey, we find, as in previous research, that de-unionization and supply and demand shocks were important factors in explaining the rise in wage inequality from 1979 to 1988. We find also compelling visual and quantitative evidence that the decline in the real value of the minimum wage explains a substantial proportion of this increase in wage inequality, particularly for women. We conclude that labor market institutions are as important as supply and demand considerations in explaining changes in the U.S. distribution of wages from 1979 to 1988.</p> </abstract>
<abstract> <p>Contemporary tests for structural change deal with detections of the "one-shot" type: given an historical data set of fixed size, these tests are designed to detect a structural break within the data set. Due to the law of the iterated logarithm, one-shot tests cannot be applied to monitor out-of-sample stability each time new data arrive without signalling a nonexistent break with probability one. We propose and analyze two real-time monitoring procedures with controlled size asymptotically: the fluctuation and CUSUM monitoring procedures. We extend an invariance principle in the sequential testing literature to obtain our results. Simulation results show that the proposed monitoring procedures indeed have controlled asymptotic size. Detection timing depends on the magnitude of parameter change, the signal to noise ratio, and the location of the out-of-sample break point.</p> </abstract>
<abstract> <p>This paper develops procedures for inference about the moments of smooth functions of out-of-sample predictions and prediction errors, when there is a long time series of predictions and realizations. The aim is to provide tools for analysis of predictive accuracy and efficiency, and, more generally, of predictive ability. The paper allows for nonnested and nonlinear models, as well as for possible dependence of predictions and prediction errors on estimated regression parameters. Simulations indicate that the procedures can work well in samples of size typically available.</p> </abstract>
<abstract> <p>This paper presents optimal tests for parameter instability in the GMM framework. The new tests include tests that are optimal for both one-sided and two-sided alternatives. One of the optimal tests for two-sided alternatives is the GMM generalization of the test presented in Andrews and Ploberger (1994) for the likelihood framework. The new tests include a class of optimal tests that direct the test's power to specific locations in the sample. One of these optimal tests has the attractive feature of a normal distribution under the null hypothesis.</p> </abstract>
<abstract> <p>We examine in this paper a new natural restriction on utility functions, namely that adding an unfair background risk to wealth makes risk-averse individuals behave in a more risk-averse way with respect to any other independent risk. This concept is called risk vulnerability. It is equivalent to the condition that an undesirable risk can never be made desirable by the presence of an independent, unfair risk. Moreover, under risk vulnerability, adding an unfair background risk reduces the demand for risky assets. Risk vulnerability generalizes the concept of properness (individually undesirable, independent risks are always jointly undesirable) introduced by Pratt and Zeckhauser (1987). It implies that the two first derivatives of the utility function are concave transformations of the original utility function. Under decreasing absolute risk aversion, a sufficient condition for risk vulnerability is local properness, i.e. r" ≥ r'r, where r is the Arrow-Pratt coefficient of absolute risk aversion.</p> </abstract>
<abstract> <p>We consider the situation where a single consumer buys a stream of goods from different sellers over time. The true value of each seller's product to the buyer is initially unknown. Additional information can be gained only by experimentation. For exogeneously given prices the buyer's problem is a multi-armed bandit problem. The innovation in this paper is to endogenize the cost of experimentation to the consumer by allowing for price competition between the sellers. The role of prices is then to allocate intertemporally the costs and benefits of learning between buyers and sellers. We examine how strategic aspects of the oligopoly model interact with the learning process. All Markov perfect equilibria (MPE) are efficient. We identify an equilibrium which besides its unique robustness properties has a strikingly simple, seemingly myopic pricing rule. Prices below marginal cost emerge naturally to sustain experimentation. Intertemporal exchange of the gains of learning is necessary to support efficient experimentation. We analyze the asymptotic behavior of the equilibria.</p> </abstract>
<abstract> <p>We propose a method to test for liquidity constraints which relies on using the within period marginal rate of substitution condition as a benchmark to evaluate the intertemporal Euler equation. If spot markets for nondurable goods exist, but financial markets either do not exist, or are imperfect, we show how the comparison of first order conditions involving the relevant spot and intertemporal prices can be used to detect the imperfection. We apply our methodology to a large sample of U.S. households, drawn from twelve years of the Consumer Expenditure Survey, allowing for a general nonseparable preference structure. Our estimates of first order conditions do not indicate the presence of liquidity constraints, with the possible exception of young households.</p> </abstract>
<abstract> <p>Tests for stochastic dominance, based upon extensions of the Goodness of Fit Test to the nonparametric comparison of income distributions, are proposed, implemented, and compared with indirect tests of second order stochastic dominance currently utilized in income distribution studies.</p> </abstract>
<abstract> <p>In this paper, we define different concepts of noncausality for continuous-time processes, using conditional independence and decomposition of semi-martingales. These definitions extend the ones already given in the case of discrete-time processes. As in the discrete-time setup, continuous-time noncausality is a property concerned with the prediction horizon (global versus instantaneous noncausality) and the nature of the prediction (strong versus weak noncausality). Relations between the resulting continuous-time noncausality concepts are then studied for the class of decomposable semi-martingales, for which, in general, the weak instantaneous noncausality does not imply the strong global noncausality. The paper than characterizes these different concepts of noncausality in the cases of counting processes and Markov processes.</p> </abstract>
<abstract> <p>We investigate the classical Anscombe—Aumann model of decision-making under uncertainty without the completeness axiom. We distinguish between the dual traits of "indecisiveness in beliefs" and "indecisiveness in tastes." The former is captured by the Knightian uncertainty model, the latter by the single-prior expected multi-utility model. We characterize axiomatically the latter model. Then we show that, under independence and continuity, these two models can be jointly characterized by means of a partial completeness property.</p> </abstract>
<abstract> <p>An autoregressive model with Markov regime-switching is analyzed that reflects on the properties of the quasi-likelihood ratio test developed by Cho and White (2007). For such a model, we show that consistency of the quasi-maximum likelihood estimator for the population parameter values, on which consistency of the test is based, does not hold. We describe a condition that ensures consistency of the estimator and discuss the consistency of the test in the absence of consistency of the estimator.</p> </abstract>
<abstract> <p>In nonlinear panel data models, the incidental parameter problem remains a challenge to econometricians. Available solutions are often based on ingenious, model-specific methods. In this paper, we propose a systematic approach to construct moment restrictions on common parameters that are free from the individual fixed effects. This is done by an orthogonal projection that differences out the unknown distribution function of individual effects. Our method applies generally in likelihood models with continuous dependent variables where a condition of non-surjectivity holds. The resulting method-of-moments estimators are root-N consistent (for fixed T) and asymptotically normal, under regularity conditions that we spell out. Several examples and a small-scale simulation exercise complete the paper.</p> </abstract>
<abstract> <p>We analyze subprime consumer lending and the role played by down payment requirements in screening high-risk borrowers and limiting defaults. To do this, we develop an empirical model of the demand for financed purchases that incorporates both adverse selection and repayment incentives. We estimate the model using detailed transaction-level data on subprime auto loans. We show how different elements of loan contracts affect the quality of the borrower pool and subsequent loan performance. We also evaluate the returns to credit scoring that allows sellers to customize financing terms to individual applicants. Our approach shows how standard econometric tools for analyzing demand and supply under imperfect competition extend to settings in which firms care about the identity of their customers and their postpurchase behavior.</p> </abstract>
<abstract> <p>We study a dynamic setting in which stochastic information (news) about the value of a privately informed seller's asset is gradually revealed to a market of buyers. We construct an equilibrium that involves periods of no trade or market failure. The no-trade period ends in one of two ways: either enough good news arrives, restoring confidence and markets reopen, or bad news arrives, making buyers more pessimistic and forcing capitulation that is, a partial sell-off of low-value assets. Conditions under which the equilibrium is unique are provided. We analyze welfare and efficiency as they depend on the quality of the news. Higher quality news can lead to more inefficient outcomes. Our model encompasses settings with or without a standard static adverse selection problem—in a dynamic setting with sufficiently informative news, reservation values arise endogenously from the option to sell in the future and the two environments have the same equilibrium structure.</p> </abstract>
<abstract> <p>This paper studies endogenous risk-taking by embedding a concern for status (relative consumption) into an otherwise conventional model of economic growth. We prove that if the intertemporal production function is strictly concave, an equilibrium must converge to a unique steady state in which there is recurrent endogenous risk-taking. (The role played by concavity is clarified by considering a special case in which the production function is instead convex, in which there is no persistent risk-taking.) The steady state is fully characterized. It displays features that are consistent with the stylized facts that individuals both insure downside risk and gamble over upside risk, and it generates similar patterns of risk-taking and avoidance across environments with quite different overall wealth levels. Endogenous risk-taking here is generally Pareto-inefficient. A concern for status thus implies that persistent and inefficient risk-taking hinders the attainment of full equality.</p> </abstract>
<abstract> <p>We study the asymptotic distribution of Tikhonov regularized estimation of quantile structural effects implied by a nonseparable model. The nonparametric instrumental variable estimator is based on a minimum distance principle. We show that the minimum distance problem without regularization is locally ill-posed, and we consider penalization by the norms of the parameter and its derivatives. We derive pointwise asymptotic normality and develop a consistent estimator of the asymptotic variance. We study the small sample properties via simulation results and provide an empirical illustration of estimation of nonlinear pricing curves for telecommunications services in the United States.</p> </abstract>
<abstract> <p>Stochastic sequential bargaining models (Merlo and Wilson (1995, 1998)) have found wide applications in different fields including political economy and macroeconomics due to their flexibility in explaining delays in reaching an agreement. This paper presents new results in nonparametric identification and estimation of such models under different data scenarios.</p> </abstract>
<abstract> <p>In this paper, we introduce a notion of continuous implementation and characterize when a social choice function is continuously implementable. More specifically, we say that a social choice function is continuously (partially) implementable if it is (partially) implementable for types in the model under study and it continues to be (partially) implementable for types "close" to this initial model. Our results show that this notion is tightly connected to full implementation in rationalizable strategies.</p> </abstract>
<abstract> <p>For a finite game with perfect recall, a refinement of its set of Nash equilibria selects closed connected subsets, called solutions. Assume that each solution's equilibria use undominated strategies and some of its equilibria are quasi-perfect, and that all solutions are immune to presentation effects; namely, if the game is embedded in a larger game with more pure strategies and more players such that the original players' feasible mixed strategies and expected payoffs are preserved regardless of what other players do, then the larger game's solutions project to the original game's solutions. Then, for a game with two players and generic payoffs, each solution is an essential component of the set of equilibria that use undominated strategies, and thus a stable set of equilibria as defined by Mertens (1989).</p> </abstract>
<abstract> <p>We present a simple way to estimate the effects of changes in a vector of observable variables X on a limited dependent variable Y when Y is a general nonseparable function of X and unobservables, and X is independent of the unobservables. We treat models in which Y is censored from above, below, or both. The basic idea is to first estimate the derivative of the conditional mean of Y given X at x with respect to x on the uncensored sample without correcting for the effect of x on the censored population. We then correct the derivative for the effects of the selection bias. We discuss nonparametric and semiparametric estimators for the derivative. We also discuss the cases of discrete regressors and of endogenous regressors in both cross section and panel data contexts.</p> </abstract>
<abstract> <p>This paper discusses a consistent bootstrap implementation of the likelihood ratio (LR) co-integration rank test and associated sequential rank determination procedure of Johansen (1996). The bootstrap samples are constructed using the restricted parameter estimates of the underlying vector autoregressive (VAR) model that obtain under the reduced rank null hypothesis. A full asymptotic theory is provided that shows that, unlike the bootstrap procedure in Swensen (2006) where a combination of unrestricted and restricted estimates from the VAR model is used, the resulting bootstrap data are I(1) and satisfy the null co-integration rank, regardless of the true rank. This ensures that the bootstrap LR test is asymptotically correctly sized and that the probability that the bootstrap sequential procedure selects a rank smaller than the true rank converges to zero. Monte Carlo evidence suggests that our bootstrap procedures work very well in practice.</p> </abstract>
<abstract> <p>This paper studies the behavior, under local misspecification, of several confidence sets (CSs) commonly used in the literature on inference in moment (in)equality models. We propose the amount of asymptotic confidence size distortion as a criterion to choose among competing inference methods. This criterion is then applied to compare across test statistics and critical values employed in the construction of CSs. We find two important results under weak assumptions. First, we show that CSs based on subsampling and generalized moment selection (Andrews and Soares (2010)) suffer from the same degree of asymptotic confidence size distortion, despite the fact that asymptotically the latter can lead to CSs with strictly smaller expected volume under correct model specification. Second, we show that the asymptotic confidence size of CSs based on the quasi-likelihood ratio test statistic can be an arbitrary small fraction of the asymptotic confidence size of CSs based on the modified method of moments test statistic.</p> </abstract>
<abstract> <p>We examine challenges to estimation and inference when the objects of interest are nondifferentiable functionals of the underlying data distribution. This situation arises in a number of applications of bounds analysis and moment inequality models, and in recent work on estimating optimal dynamic treatment regimes. Drawing on earlier work relating differentiability to the existence of unbiased and regular estimators, we show that if the target object is not differentiable in the parameters of the data distribution, there exist no estimator sequences that are locally asymptotically unbiased or α-quantile unbiased. This places strong limits on estimators, bias correction methods, and inference procedures, and provides motivation for considering other criteria for evaluating estimators and inference procedures, such as local asymptotic minimaxity and one-sided quantile unbiasedness.</p> </abstract>
<abstract> <p>Empirical evidence suggests that perfectionism can affect choice behavior. When striving for perfection, a person can desire to keep normatively appealing options feasible even if she persistently fails to use these options later. For instance, she can "pay not to go to the gym," as in DellaVigna and Malmendier (2006). By contrast, some perfectionists may avoid normatively important tasks for fear of negative self-evaluation of their performance. This paper models perfectionist behaviors in Gul and Pesendorfer's (2001) menu framework where agents may be tempted to deviate from their long-term normative objectives. In addition to self-control costs, I identify a utility component that reflects emotional costs and benefits of perfectionism. My model is derived from axioms imposed on preferences over menus in an essentially unique way.</p> </abstract>
<abstract> <p>Observers often interpret boom—bust episodes in asset markets as speculative frenzies where asymmetrically informed investors buy overvalued assets hoping to sell to a greater fool before the crash. Despite its intuitive appeal, however, this notion of speculative bubbles has proven difficult to reconcile with economic theory. Existing models have been criticized on the basis that they assume irrationality, that prices are somewhat unresponsive to sales, or that they depend on fragile, knife-edge restrictions. To address these issues, I construct a rational version of Abreu and Brunnermeier (2003), where agents invest growing endowments into an asset, fueling appreciation and eventual overvaluation. Riding bubbles is optimal as long as the growth rate of the bubble and the probability of selling before the crash are high enough. This probability increases with the amount of noise in the economy, as random short-term fluctuations make it difficult for agents to infer information from prices.</p> </abstract>
<abstract> <p>This paper examines the incentives offered by frictionless markets to innovate asset-backed securities by owners who maximize the assets' values. Assuming identical preferences across investors with heterogeneous risk-sharing needs, we characterize economies in which competition provides insufficient incentives to innovate so that, in equilibrium, financial markets are incomplete in all (pure strategy) equilibria, even when innovation is essentially costless. Thus, value maximization does not generally result in complete markets.</p> </abstract>
<abstract> <p>In this paper, we derive and experimentally test a theoretical model of speculation in multiperiod asset markets with public information flows. The speculation arises from the traders' heterogeneous posteriors as they make different inferences from sequences of public information. This leads to overpricing in the sense that price exceeds the most optimistic belief about the real value of the asset. We find evidence of speculative overpricing in both incomplete and complete markets, where the information flow is a gradually revealed sequence of imperfect public signals about the state of the world. We also find evidence of asymmetric price reaction to good news and bad news, another feature of equilibrium price dynamics under our model. Markets with a relaxed short-sale constraint exhibit less overpricing.</p> </abstract>
<abstract> <p>This paper argues that, in the presence of intersectoral input—output linkages, microeconomic idiosyncratic shocks may lead to aggregate fluctuations. We show that, as the economy becomes more disaggregated, the rate at which aggregate volatility decays is determined by the structure of the network capturing such linkages. Our main results provide a characterization of this relationship in terms of the importance of different sectors as suppliers to their immediate customers, as well as their role as indirect suppliers to chains of downstream sectors. Such higher-order interconnections capture the possibility of "cascade effects" whereby productivity shocks to a sector propagate not only to its immediate downstream customers, but also to the rest of the economy. Our results highlight that sizable aggregate volatility is obtained from sectoral idiosyncratic shocks only if there exists significant asymmetry in the roles that sectors play as suppliers to others, and that the "sparseness" of the input—output matrix is unrelated to the nature of aggregate fluctuations.</p> </abstract>
<abstract> <p>This paper establishes existence of a stationary Markov perfect equilibrium in general stochastic games with noise—a component of the state that is nonatomically distributed and not directly affected by the previous period's state and actions. Noise may be simply a payoff-irrelevant public randomization device, delivering known results on the existence of correlated equilibrium as a special case. More generally, noise can take the form of shocks that enter into players' stage payoffs and the transition probability on states. The existence result is applied to a model of industry dynamics and to a model of dynamic electoral competition.</p> </abstract>
<abstract> <p>Two players announce bargaining postures to which they may become committed and then bargain over the division of a surplus. The share of the surplus that a player can guarantee herself under first-order knowledge of rationality is determined (as a function of her probability of becoming committed), as is the bargaining posture that she must announce in order to guarantee herself this much. This "maxmin" share of the surplus is large relative to the probability of becoming committed (e.g., it equals 30% if the commitment probability is 1 in 10 and equals 13% if the commitment probability is 1 in 1000), and the corresponding bargaining posture simply demands this share plus compensation for any delay in reaching agreement.</p> </abstract>
<abstract> <p>According to standard theory, the set of implementable efficient outcome functions is greatly reduced if the mechanism or contract can be renegotiated ex post. In some cases, contracts can achieve nothing and so, for example, the hold-up problem may be severe. This paper shows that if the mechanism is designed in such a way that sending a message involves a small cost, then renegotiation essentially does not restrict the set of efficient implementable functions. Given a weak preference-reversal condition, any Pareto-efficient, bounded social choice function can be implemented in subgame-perfect equilibrium in a renegotiation-proof manner, for any strictly positive message cost. The key point is that messages themselves can act as punishments.</p> </abstract>
<abstract> <p>In this paper we study identification and estimation of a correlated random coefficients (CRC) panel data model. The outcome of interest varies linearly with a vector of endogenous regressors. The coefficients on these regressors are heterogenous across units and may covary with them. We consider the average partial effect (APE) of a small change in the regressor vector on the outcome (cf. Chamberlain (1984), Wooldridge (2005a)). Chamberlain (1992) calculated the semiparametric efficiency bound for the APE in our model and proposed a √N-consistent estimator. Nonsingularity of the APE's information bound, and hence the appropriateness of Chamberlain's (1992) estimator, requires (i) the time dimension of the panel (T) to strictly exceed the number of random coefficients (p) and (ii) strong conditions on the time series properties of the regressor vector. We demonstrate irregular identification of the APE when T = p and for more persistent regressor processes. Our approach exploits the different identifying content of the subpopulations of stayers—or units whose regressor values change little across periods—and movers—or units whose regressor values change substantially across periods. We propose a feasible estimator based on our identification result and characterize its large sample properties. While irregularity precludes our estimator from attaining parametric rates of convergence, its limiting distribution is normal and inference is straightforward to conduct. Standard software may be used to compute point estimates and standard errors. We use our methods to estimate the average elasticity of calorie consumption with respect to total outlay for a sample of poor Nicaraguan households.</p> </abstract>
<abstract> <p>This paper analyzes the properties of standard estimators, tests, and confidence sets (CS's) for parameters that are unidentified or weakly identified in some parts of the parameter space. The paper also introduces methods to make the tests and CS's robust to such identification problems. The results apply to a class of extremum estimators and corresponding tests and CS's that are based on criterion functions that satisfy certain asymptotic stochastic quadratic expansions and that depend on the parameter that determines the strength of identification. This covers a class of models estimated using maximum likelihood (ML), least squares (LS), quantile, generalized method of moments, generalized empirical likelihood, minimum distance, and semi-parametric estimators. The consistency/lack-of-consistency and asymptotic distributions of the estimators are established under a full range of drifting sequences of true distributions. The asymptotic sizes (in a uniform sense) of standard and identification-robust tests and CS's are established. The results are applied to the ARMA(1, 1) time series model estimated by ML and to the nonlinear regression model estimated by LS. In companion papers, the results are applied to a number of other models.</p> </abstract>
<abstract> <p>Estimating structural models is often viewed as computationally difficult, an impression partly due to a focus on the nested fixed-point (NFXP) approach. We propose a new constrained optimization approach for structural estimation. We show that our approach and the NFXP algorithm solve the same estimation problem, and yield the same estimates. Computationally, our approach can have speed advantages because we do not repeatedly solve the structural equation at each guess of structural parameters. Monte Carlo experiments on the canonical Zurcher bus-repair model demonstrate that the constrained optimization approach can be significantly faster.</p> </abstract>
<abstract> <p>The widely used estimator of Berry, Levinsohn, and Pakes (1995) produces estimates of consumer preferences from a discrete-choice demand model with random coefficients, market-level demand shocks, and endogenous prices. We derive numerical theory results characterizing the properties of the nested fixed point algorithm used to evaluate the objective function of BLP's estimator. We discuss problems with typical implementations, including cases that can lead to incorrect parameter estimates. As a solution, we recast estimation as a mathematical program with equilibrium constraints, which can be faster and which avoids the numerical issues associated with nested inner loops. The advantages are even more pronounced for forward-looking demand models where the Bellman equation must also be solved repeatedly. Several Monte Carlo and real-data experiments support our numerical concerns about the nested fixed point approach and the advantages of constrained optimization. For static BLP, the constrained optimization approach can be as much as ten to forty times faster for large-dimensional problems with many markets.</p> </abstract>
<abstract> <p>In this paper, we propose a method to evaluate the effect of a counterfactual change in the unconditional distribution of a single covariate on the unconditional distribution of an outcome variable of interest. Both fixed and infinitesimal changes are considered. We show that such effects are point identified under general conditions if the covariate affected by the counterfactual change is continuously distributed, but are typically only partially identified if its distribution is discrete. For the latter case, we derive informative bounds, making use of the available information. We also discuss estimation and inference.</p> </abstract>
<abstract> <p>This paper considers the estimation problem of structural models for which empirical restrictions are characterized by a fixed point constraint, such as structural dynamic discrete choice models or models of dynamic games. We analyze a local condition under which the nested pseudo likelihood (NPL) algorithm converges to a consistent estimator, and derive its convergence rate. We find that the NPL algorithm may not necessarily converge to a consistent estimator when the fixed point mapping does not have a local contraction property. To address the issue of divergence, we propose alternative sequential estimation procedures that can converge to a consistent estimator even when the NPL algorithm does not.</p> </abstract>
<abstract> <p>Seemingly absent from the arsenal of currently available "nearly efficient" testing procedures for the unit root hypothesis, that is, tests whose asymptotic local power functions are virtually indistinguishable from the Gaussian power envelope, is a test admitting a (quasi-)likelihood ratio interpretation. We study the large sample properties of a quasi-likelihood ratio unit root test based on a Gaussian likelihood and show that this test is nearly efficient.</p> </abstract>
<abstract> <p>The single crossing property plays a crucial role in economic theory, yet there are important instances where the property cannot be directly assumed or easily derived. Difficulties often arise because the property cannot be aggregated: the sum or convex combination of two functions with the single crossing property need not have that property. We introduce a new condition characterizing when the single crossing property is stable under aggregation, and also identify sufficient conditions for the preservation of the single crossing property under multidimensional aggregation. We use our results to establish properties of objective functions (convexity, logsupermodularity), the monotonicity of optimal decisions under uncertainty, and the existence of monotone equilibria in Bayesian games.</p> </abstract>
<abstract> <p>I propose a new mechanism design approach to the problem of ranking standard auctions with two heterogeneous bidders. A key feature of the approach is that it may be possible to rank two auctions even if neither dominates the other for all combinations of types. The approach simplifies the analysis and unifies results in the existing literature. Roughly speaking, the first-price auction is more profitable than the second-price auction when the strong bidder's distribution is flatter and more disperse than the weak bidder's distribution. Applications include auctions with one-sided externalities. Moreover, contrary to previous work, reserve prices are easily handled. Finally, the method can be extended to some environments with many bidders.</p> </abstract>
<abstract> <p>We develop results for the use of Lasso and post-Lasso methods to form first-stage predictions and estimate optimal instruments in linear instrumental variables (IV) models with many instruments, p. Our results apply even when p is much larger than the sample size, n. We show that the IV estimator based on using Lasso or post-Lasso in the first stage is root-n consistent and asymptotically normal when the first stage is approximately sparse, that is, when the conditional expectation of the endogenous variables given the instruments can be well-approximated by a relatively small set of variables whose identities may be unknown. We also show that the estimator is semiparametrically efficient when the structural error is homoscedastic. Notably, our results allow for imperfect model selection, and do not rely upon the unrealistic "beta-min" conditions that are widely used to establish validity of inference following model selection (see also Belloni, Chernozhukov, and Hansen (2011b)). In simulation experiments, the Lasso-based IV estimator with a data-driven penalty performs well compared to recently advocated many-instrument robust procedures. In an empirical example dealing with the effect of judicial eminent domain decisions on economic outcomes, the Lasso-based IV estimator outperforms an intuitive benchmark. Optimal instruments are conditional expectations. In developing the IV results, we establish a series of new results for Lasso and post-Lasso estimators of nonparametric conditional expectation functions which are of independent theoretical and practical interest. We construct a modification of Lasso designed to deal with non-Gaussian, heteroscedastic disturbances that uses a data-weighted 𝓁₁-penalty function. By innovatively using moderate deviation theory for self-normalized sums, we provide convergence rates for the resulting Lasso and post-Lasso estimators that are as sharp as the corresponding rates in the homoscedastic Gaussian case under the condition that log p = o(n 1/3 ). We also provide a data-driven method for choosing the penalty level that must be specified in obtaining Lasso and post-Lasso estimates and establish its asymptotic validity under non-Gaussian, heteroscedastic disturbances.</p> </abstract>
<abstract> <p>We investigate the welfare properties of the one-sector neoclassical growth model with uninsurable idiosyncratic shocks. We focus on the notion of constrained efficiency used in the general equilibrium literature. Our characterization of constrained efficiency uses the first-order condition of a constrained planner's problem. This condition highlights the margins of relevance for whether capital is too high or too low: the factor composition of income of the (consumption-)poor. Using three calibrations commonly considered in the literature, we illustrate that there can be either over- or underaccumulation of capital in steady state and that the constrained optimum may or may not be consistent with a nondegenerate long-run distribution of wealth. For the calibration that roughly matches the income and wealth distribution, the constrained inefficiency of the market outcome is rather striking: it has much too low a steady-state capital stock.</p> </abstract>
<abstract> <p>We present a model for the equilibrium movement of capital between asset markets that are distinguished only by the levels of capital invested in each. Investment in that market with the greatest amount of capital earns the lowest risk premium. Intermediaries optimally trade off the costs of intermediation against fees that depend on the gain they can offer to investors for moving their capital to the market with the higher mean return. The bargaining power of an investor depends on potential access to alternative intermediaries. In equilibrium, the speeds of adjustment of mean returns and of capital between the two markets are increasing in the degree to which capital is imbalanced between the two markets, and can be reduced by competition among intermediaries.</p> </abstract>
<abstract> <p>In many financial markets, dealers have the advantage of observing the orders of their customers. To quantify the economic benefit that dealers derive from this advantage, we study detailed data from Canadian Treasury auctions, where dealers observe customer bids while preparing their own bids. In this setting, dealers can use information on customer bids to learn about (i) competition, that is, the distribution of competing bids in the auction, and (ii) fundamentals, that is, the ex post value of the security being auctioned. We devise formal hypothesis tests for both sources of informational advantage. In our data, we do not find evidence that dealers are learning about fundamentals. We find that the "information about competition" contained in customer bids accounts for 13—27% of dealers' expected profits.</p> </abstract>
<abstract> <p>Firms are more productive, on average, in larger cities. Two main explanations have been offered: firm selection (larger cities toughen competition, allowing only the most productive to survive) and agglomeration economies (larger cities promote interactions that increase productivity), possibly reinforced by localized natural advantage. To distinguish between them, we nest a generalized version of a tractable firm selection model and a standard model of agglomeration. Stronger selection in larger cities left-truncates the productivity distribution, whereas stronger agglomeration right-shifts and dilates the distribution. Using this prediction, French establishment-level data, and a new quantile approach, we show that firm selection cannot explain spatial productivity differences. This result holds across sectors, city size thresholds, establishment samples, and area definitions.</p> </abstract>
<abstract> <p>This paper studies information aggregation in dynamic markets with a finite number of partially informed strategic traders. It shows that, for a broad class of securities, information in such markets always gets aggregated. Trading takes place in a bounded time interval, and in every equilibrium, as time approaches the end of the interval, the market price of a "separable" security converges in probability to its expected value conditional on the traders' pooled information. If the security is "non-separable," then there exists a common prior over the states of the world and an equilibrium such that information does not get aggregated. The class of separable securities includes, among others, Arrow—Debreu securities, whose value is 1 in one state of the world and 0 in all others, and "additive" securities, whose value can be interpreted as the sum of traders' signals.</p> </abstract>
<abstract> <p>We consider tests of a simple null hypothesis on a subset of the coefficients of the exogenous and endogenous regressors in a single-equation linear instrumental variables regression model with potentially weak identification. Existing methods of subset inference (i) rely on the assumption that the parameters not under test are strongly identified, or (ii) are based on projection-type arguments. We show that, under homoskedasticity, the subset Anderson and Rubin (1949) test that replaces unknown parameters by limited information maximum likelihood estimates has correct asymptotic size without imposing additional identification assumptions, but that the corresponding subset Lagrange multiplier test is size distorted asymptotically.</p> </abstract>
<abstract> <p>We consider model based inference in a fractionally cointegrated (or cofractional) vector autoregressive model, based on the Gaussian likelihood conditional on initial values. We give conditions on the parameters such that the process X t is fractional of order d and cofractional of order d — b; that is, there exist vectors β for which βʹX t is fractional of order d — b and no other fractionality order is possible. For b = 1, the model nests the I(d — 1) vector autoregressive model. We define the statistical model by 0 &lt; b ≤ d, but conduct inference when the true values satisfy 0 ≤ d₀ — b₀ &lt; 1/2 and b₀ ≠ 1/2, for which ${{\mathrm{\beta }}^{\prime }}_{0}{\mathrm{X}}_{\mathrm{t}}$ is (asymptotically) a stationary process. Our main technical contribution is the proof of consistency of the maximum likelihood estimators. To this end, we prove weak convergence of the conditional likelihood as a continuous stochastic process in the parameters when errors are independent and identically distributed with suitable moment conditions and initial values are bounded. Because the limit is deterministic, this implies uniform convergence in probability of the conditional likelihood function. If the true value b₀ &gt; 1/2, we prove that the limit distribution of ${\mathrm{T}}^{{\mathrm{b}}_{0}}(\hat{\mathrm{\beta }}-{\mathrm{\beta }}_{0})$ is mixed Gaussian, while for the remaining parameters it is Gaussian. The limit distribution of the likelihood ratio test for cointegration rank is a functional of fractional Brownian motion of type II. If b₀ &lt; 1/2, all limit distributions are Gaussian or chi-squared. We derive similar results for the model with d = b, allowing for a constant term.</p> </abstract>
<abstract> <p>In the context of a dynamic, stochastic, general equilibrium model, we perform classical maximum likelihood and Bayesian estimations of the contribution of anticipated shocks to business cycles in the postwar United States. Our identification approach relies on the fact that forward-looking agents react to anticipated changes in exogenous fundamentals before such changes materialize. It further allows us to distinguish changes in fundamentals by their anticipation horizon. We find that anticipated shocks account for about half of predicted aggregate fluctuations in output, consumption, investment, and employment.</p> </abstract>
<abstract> <p>We propose a model of monopolistic competition with additive preferences and variable marginal costs. Using the concept of "relative love for variety," we provide a full characterization of the free-entry equilibrium. When the relative love for variety increases with individual consumption, the market generates pro-competitive effects. When it decreases, the market mimics anti-competitive behavior. The constant elasticity of substitution is the only case in which all competitive effects are washed out. We also show that our results hold true when the economy involves several sectors, firms are heterogeneous, and preferences are given by the quadratic utility and the translog.</p> </abstract>
<abstract> <p>This paper proposes a dynamic politico-economic theory of fiscal policy in a world comprising a set of small open economies, whose driving force is the intergenerational conflict over debt, taxes, and public goods. Subsequent generations of voters choose fiscal policy through repeated elections. The presence of young voters induces fiscal discipline, that is, low taxes and low debt accumulation. The paper characterizes the Markov-perfect equilibrium of the voting game in each economy, as well as the stationary equilibrium debt distribution and interest rate of the world economy. The equilibrium can reproduce some salient features of fiscal policy in modern economies.</p> </abstract>
<abstract> <p>This paper is concerned with tests and confidence intervals for parameters that are not necessarily point identified and are defined by moment inequalities. In the literature, different test statistics, critical-value methods, and implementation methods (i.e., the asymptotic distribution versus the bootstrap) have been proposed. In this paper, we compare these methods. We provide a recommended test statistic, moment selection critical value, and implementation method. We provide data-dependent procedures for choosing the key moment selection tuning parameter κ and a size-correction factor η.</p> </abstract>
<abstract> <p>This paper considers local and global multiple-prior representations of ambiguity for preferences that are (i) monotonic, (ii) Bernoullian, that is, admit an affine utility representation when restricted to constant acts, and (iii) locally Lipschitz continuous. We do not require either certainty independence or uncertainty aversion. We show that the set of priors identified by Ghirardato, Maccheroni, and Marinacci's (2004) "unambiguous preference" relation can be characterized as a union of Clarke differentials. We then introduce a behavioral notion of "locally better deviation" at an act and show that it characterizes the Clarke differential of the preference representation at that act. These results suggest that the priors identified by these preference statements are directly related to (local) optimizing behavior.</p> </abstract>
<abstract> <p>Belief disagreements have been suggested as a major contributing factor to the recent subprime mortgage crisis. This paper theoretically evaluates this hypothesis. I assume that optimists have limited wealth and take on leverage so as to take positions in line with their beliefs. To have a significant effect on asset prices, they need to borrow from traders with pessimistic beliefs using loans collateralized by the asset itself. Since pessimists do not value the collateral as much as optimists do, they are reluctant to lend, which provides an endogenous constraint on optimists' ability to borrow and to influence asset prices. I demonstrate that the tightness of this constraint depends on the nature of belief disagreements. Optimism concerning the probability of downside states has no or little effect on asset prices because these types of optimism are disciplined by this constraint. Instead, optimism concerning the relative probability of upside states could have significant effects on asset prices. This asymmetric disciplining effect is robust to allowing for short selling because pessimists that borrow the asset face a similar endogenous constraint. These results emphasize that what investors disagree about matters for asset prices, to a greater extent than the level of disagreements. When richer contracts are available, relatively complex contracts that resemble some of the recent financial innovations in the mortgage market endogenously emerge to facilitate betting.</p> </abstract>
<abstract> <p>This paper investigates the behavior of asset prices in an endowment economy in which a representative agent with power utility consumes the dividends of multiple assets. The assets are Lucas trees; a collection of Lucas trees is a Lucas orchard. The model generates return correlations that vary endogenously, spiking at times of disaster. Since disasters spread across assets, the model generates large risk premia even for assets with stable cashflows. Very small assets may comove endogenously and hence earn positive risk premia even if their cashflows are independent of the rest of the economy. I provide conditions under which the variation in a small asset's price-dividend ratio can be attributed almost entirely to variation in its risk premium.</p> </abstract>
<abstract> <p>We investigate the role of deeply rooted pre-colonial ethnic institutions in shaping comparative regional development within African countries. We combine information on the spatial distribution of ethnicities before colonization with regional variation in contemporary economic performance, as proxied by satellite images of light density at night. We document a strong association between pre-colonial ethnic political centralization and regional development. This pattern is not driven by differences in local geographic features or by other observable ethnic-specific cultural and economic variables. The strong positive association between pre-colonial political complexity and contemporary development also holds within pairs of adjacent ethnic homelands with different legacies of pre-colonial political institutions.</p> </abstract>
<abstract> <p>We investigate the testable implications of the theory of stable matchings. We provide a characterization of the matchings that are rationalizable as stable matchings when agents' preferences are unobserved. The characterization is a simple nonparametric test for stability, in the tradition of revealed preference tests. We also characterize the observed stable matchings when monetary transfers are allowed and the stable matchings that are best for one side of the market: extremal stable matchings. We find that the theory of extremal stable matchings is observationally equivalent to requiring that there be a unique stable matching or that the matching be consistent with unrestricted monetary transfers.</p> </abstract>
<abstract> <p>A group of peers must choose one of them to receive a prize, everyone cares only about winning, not about who gets the prize if someone else. An award rule is impartial if one's message never influences whether or not one wins the prize. We explore the consequences of impartiality when each agent nominates a single (other) agent for the prize. On the positive side, we construct impartial nomination rules where both the influence of individual messages and the requirements to win the prize are not very different across agents. Partition the agents in two or more districts, each of size at least 3, and call an agent a local winner if he is nominated by a majority of members of his own district; the rule selects a local winner with the largest support from nonlocal winners, or a fixed default agent in case there is no local winner. On the negative side, impartiality implies that ballots cannot be processed anonymously as in plurality voting. Moreover, we cannot simultaneously guarantee that the winner always gets at least one nomination, and that an agent nominated by everyone else always wins.</p> </abstract>
<abstract> <p>We consider a standard social choice environment with linear utilities and independent, one-dimensional, private types. We prove that for any Bayesian incentive compatible mechanism there exists an equivalent dominant strategy incentive compatible mechanism that delivers the same interim expected utilities for all agents and the same ex ante expected social surplus. The short proof is based on an extension of an elegant result due to Gutmann, Kemperman, Reeds, and Shepp (1991). We also show that the equivalence between Bayesian and dominant strategy implementation generally breaks down when the main assumptions underlying the social choice model are relaxed or when the equivalence concept is strengthened to apply to interim expected allocations.</p> </abstract>
<abstract> <p>We study selection rules: voting procedures used by committees to choose whether to place an issue on their agenda. At the selection stage of the model, committee members are uncertain about their final preferences. They only have some private information about these preferences. We show that voters become more conservative when the selection rule itself becomes more conservative. The decision rule has the opposite effect. We compare these voting procedures to the designation of an agenda setter among the committee and to a utilitarian social planner with all the ex interim private information.</p> </abstract>
<abstract> <p>This paper extends the subjective expected utility model of decision making under uncertainty to include incomplete beliefs and tastes. The main results are two axiomatizations of the multiprior expected multiutility representations of preference relations under uncertainty. The paper also introduces new axiomatizations of Knightian uncertainty and the expected multiutility model with complete beliefs.</p> </abstract>
<abstract> <p>We consider the estimation of dynamic panel data models in the presence of incidental parameters in both dimensions: individual fixed-effects and time fixed-effects, as well as incidental parameters in the variances. We adopt the factor analytical approach by estimating the sample variance of individual effects rather than the effects themselves. In the presence of cross-sectional heteroskedasticity, the factor method estimates the average of the cross-sectional variances instead of the individual variances. The method thereby eliminates the incidental-parameter problem in the means and in the variances over the cross-sectional dimension. We further show that estimating the time effects and heteroskedasticities in the time dimension does not lead to the incidental-parameter bias even when T and N are comparable. Moreover, efficient and robust estimation is obtained by jointly estimating heteroskedasticities.</p> </abstract>
<abstract> <p>We study the asymptotic distribution of three-step estimators of a finite-dimensional parameter vector where the second step consists of one or more nonparametric regressions on a regressor that is estimated in the first step. The first-step estimator is either parametric or nonparametric. Using Newey's (1994) path-derivative method, we derive the contribution of the first-step estimator to the influence function. In this derivation, it is important to account for the dual role that the first-step estimator plays in the second-step nonparametric regression, that is, that of conditioning variable and that of argument.</p> </abstract>
<abstract> <p>We study a two-stage model where the agent has preferences over menus as in Dekel, Lipman, and Rustichini (2001) in the first period and then makes random choices from menus as in Gul and Pesendorfer (2006) in the second period. Both preference for flexibility in the first period and strictly random choices in the second period can be, respectively, rationalized by subjective state spaces. Our main result characterizes the representation where the two state spaces align, so the agent correctly anticipates her future choices. The joint representation uniquely identifies probabilities over subjective states and magnitudes of utilities across states. We also characterize when the agent completely overlooks some subjective states that realize at the point of choice.</p> </abstract>
<abstract> <p>We characterize and prove the existence of Nash equilibrium in a limit order market with a finite number of risk-neutral liquidity providers. We show that if there is sufficient adverse selection, then pointwise optimization (maximizing in p for each q) in a certain nonlinear pricing game produces a Nash equilibrium in the limit order market. The need for a sufficient degree of adverse selection does not vanish as the number of liquidity providers increases. Our formulation of the nonlinear pricing game encompasses various specifications of informed and liquidity trading, including the case in which nature chooses whether the market-order trader is informed or a liquidity trader. We solve for an equilibrium analytically in various examples and also present examples in which the first-order condition for pointwise optimization does not define an equilibrium, because the amount of adverse selection is insufficient.</p> </abstract>
<abstract> <p>We take cohorts of entering freshmen at the United States Air Force Academy and assign half to peer groups designed to maximize the academic performance of the lowest ability students. Our assignment algorithm uses nonlinear peer effects estimates from the historical pre-treatment data, in which students were randomly assigned to peer groups. We find a negative and significant treatment effect for the students we intended to help. We provide evidence that within our "optimally" designed peer groups, students avoided the peers with whom we intended them to interact and instead formed more homogeneous subgroups. These results illustrate how policies that manipulate peer groups for a desired social outcome can be confounded by changes in the endogenous patterns of social interactions within the group.</p> </abstract>
<abstract> <p>Few microfinance-funded businesses grow beyond subsistence entrepreneurship. This paper considers one possible explanation: that the structure of existing microfinance contracts may discourage risky but high-expected-return investments. To explore this possibility, I develop a theory that unifies models of investment choice, informal risk-sharing, and formal financial contracts. I then test the predictions of this theory using a series of experiments with clients of a large microfinance institution in India. The experiments confirm the theoretical predictions that joint liability creates two potential inefficiencies. First, borrowers free-ride on their partners, making risky investments without compensating partners for this risk. Second, the addition of peer-monitoring overcompensates, leading to sharp reductions in risk-taking and profitability. Equity-like financing, in which partners share both the benefits and risks of more profitable projects, overcomes both of these inefficiencies and merits further testing in the field.</p> </abstract>
<abstract> <p>A dynamic structural model of labor supply, welfare participation, and food stamp participation is estimated using the 1992, 1993, and 1996 panels of the Survey of Income and Program Participation. Details of various policies including welfare time limits, work requirements, and Earned Income Tax Credit (EITC) are incorporated formally in the budget constraint. Policy simulations reveal that the economy accounts for half of the increase in the labor supply of female heads of family between 1992 and 1999. A time limit results in a larger efficiency gain than a work requirement or a direct reduction in welfare benefits. A reform package can lead to both a reduction in the government expenditure and an improvement in utility. The EITC expansion results in a substantial efficiency gain among individuals with the lowest expected wage. These individuals are almost unaffected by the economic expansion, but their income and utility increase significantly under the reform package.</p> </abstract>
<abstract> <p>I investigate the role of demand shocks in the ready-mix concrete industry. Using Census data on more than 15,000 plants, I estimate a model of investment and entry in oligopolistic markets. These estimates are used to simulate the effect of eliminating short-term local demand changes. A policy of smoothing the volatility of demand has a market expansion effect: The model predicts a 39% increase in the number of plants in the industry. Since bigger markets have both more plants and larger plants, a demand-smoothing fiscal policy would increase the share of large plants by 20%. Finally, the policy of smoothing demand reduces entry and exit by 25%, but has no effect on the rate at which firms change their size.</p> </abstract>
<abstract> <p>Dynamic models of ambiguity aversion are increasingly popular in applied work. This paper shows that there is a strong interdependence in such models between the ambiguity attitude and the preference for the timing of the resolution of uncertainty, as defined by the classic work of Kreps and Porteus (1978). The modeling choices made in the domain of ambiguity aversion influence the set of modeling choices available in the domain of timing attitudes. The main result is that the only model of ambiguity aversion that exhibits indifference to timing is the maxmin expected utility of Gilboa and Schmeidler (1989). This paper examines the structure of the timing nonindifference implied by the other commonly used models of ambiguity aversion. This paper also characterizes the indifference to long-run risk, a notion introduced by Duffie and Epstein (1992). The interdependence of ambiguity and timing that this paper identifies is of interest both conceptually and practically—especially for economists using these models in applications.</p> </abstract>
<abstract> <p>We derive the analogue of the classic Arrow—Pratt approximation of the certainty equivalent under model uncertainty as described by the smooth model of decision making under ambiguity of Klibanoff, Marinacci, and Mukerji (2005). We study its scope by deriving a tractable mean-variance model adjusted for ambiguity and solving the corresponding portfolio allocation problem. In the problem with a risk-free asset, a risky asset, and an ambiguous asset, we find that portfolio rebalancing in response to higher ambiguity aversion only depends on the ambiguous asset's alpha, setting the performance of the risky asset as benchmark. In particular, a positive alpha corresponds to a long position in the ambiguous asset, a negative alpha corresponds to a short position in the ambiguous asset, and greater ambiguity aversion reduces optimal exposure to ambiguity. The analytical tractability of the enhanced Arrow—Pratt approximation renders our model especially well suited for calibration exercises aimed at exploring the consequences of model uncertainty on equilibrium asset prices.</p> </abstract>
<abstract> <p>News—or foresight—about future economic fundamentals can create rational expectations equilibria with non-fundamental representations that pose substantial challenges to econometric efforts to recover the structural shocks to which economic agents react. Using tax policies as a leading example of foresight, simple theory makes transparent the economic behavior and information structures that generate non-fundamental equilibria. Econometric analyses that fail to model foresight will obtain biased estimates of output multipliers for taxes; biases are quantitatively important when two canonical theoretical models are taken as data generating processes. Both the nature of equilibria and the inferences about the effects of anticipated tax changes hinge critically on hypothesized information flows. Different methods for extracting or hypothesizing the information flows are discussed and shown to be alternative techniques for resolving a non-uniqueness problem endemic to moving average representations.</p> </abstract>
<abstract> <p>We argue that positive co-movements between land prices and business investment are a driving force behind the broad impact of land-price dynamics on the macroeconomy. We develop an economic mechanism that captures the co-movements by incorporating two key features into a DSGE model: We introduce land as a collateral asset in firms' credit constraints, and we identify a shock that drives most of the observed fluctuations in land prices. Our estimates imply that these two features combine to generate an empirically important mechanism that amplifies and propagates macroeconomic fluctuations through the joint dynamics of land prices and business investment.</p> </abstract>
<abstract> <p>This paper is concerned with robust estimation under moment restrictions. A moment restriction model is semiparametric and distribution-free; therefore it imposes mild assumptions. Yet it is reasonable to expect that the probability law of observations may have some deviations from the ideal distribution being modeled, due to various factors such as measurement errors. It is then sensible to seek an estimation procedure that is robust against slight perturbation in the probability measure that generates observations. This paper considers local deviations within shrinking topological neighborhoods to develop its large sample theory, so that both bias and variance matter asymptotically. The main result shows that there exists a computationally convenient estimator that achieves optimal minimax robust properties. It is semiparametrically efficient when the model assumption holds, and, at the same time, it enjoys desirable robust properties when it does not.</p> </abstract>
<abstract> <p>This paper proposes two new estimators for determining the number of factors (r) in static approximate factor models. We exploit the well-known fact that the r largest eigenvalues of the variance matrix of N response variables grow unboundedly as N increases, while the other eigenvalues remain bounded. The new estimators are obtained simply by maximizing the ratio of two adjacent eigenvalues. Our simulation results provide promising evidence for the two estimators.</p> </abstract>
<abstract> <p>If voter preferences depend on a noisy state variable, under what conditions do large elections deliver outcomes "as if" the state were common knowledge? While the existing literature models elections using the jury metaphor where a change in information regarding the state induces all voters to switch in favor of only one alternative, we allow for more general preferences where a change in information can induce a switch in favor of either alternative. We show that information is aggregated for any voting rule if, for a randomly chosen voter, the probability of switching in favor of one alternative is strictly greater than the probability of switching away from that alternative for any given change in belief over states. If the preference distribution violates this condition, there exist equilibria that produce outcomes different from the full information outcome with high probability for large classes of voting rules. In other words, unless preferences closely conform to the jury metaphor, information aggregation is not guaranteed to obtain.</p> </abstract>
<abstract> <p>We analyze games of incomplete information and offer equilibrium predictions that are valid for, and in this sense robust to, all possible private information structures that the agents may have. The set of outcomes that can arise in equilibrium for some information structure is equal to the set of Bayes correlated equilibria. We completely characterize the set of Bayes correlated equilibria in a class of games with quadratic payoffs and normally distributed uncertainty in terms of restrictions on the first and second moments of the equilibrium action—state distribution. We derive exact bounds on how prior knowledge about the private information refines the set of equilibrium predictions. We consider information sharing among firms under demand uncertainty and find new optimal information policies via the Bayes correlated equilibria. We also reverse the perspective and investigate the identification problem under concerns for robustness to private information. The presence of private information leads to set rather than point identification of the structural parameters of the game.</p> </abstract>
<abstract> <p>We study European banks' demand for short-term funds (liquidity) during the summer 2007 subprime market crisis. We use bidding data from the European Central Bank's auctions for one-week loans, their main channel of monetary policy implementation. Our analysis provides a high-frequency, disaggregated perspective on the 2007 crisis, which was previously studied through comparisons of collateralized and uncollateralized interbank money market rates which do not capture the heterogeneous impact of the crisis on individual banks. Through a model of bidding, we show that banks' bids reflect their cost of obtaining short-term funds elsewhere (e.g., in the interbank market) as well as a strategic response to other bidders. The strategic response is empirically important: while a naïve interpretation of the raw bidding data may suggest that virtually all banks suffered an increase in the cost of short-term funding, we find that, for about one third of the banks, the change in bidding behavior was simply a strategic response. We also find considerable heterogeneity in the short-term funding costs among banks: for over one third of the bidders, funding costs increased by more than 20 basis points, and funding costs vary widely with respect to the country-of-origin. The funding costs we estimate using bidding data are also predictive of market- and accounting-based measures of bank performance, reinforcing the usefulness of "revealed preference" information contained in bids.</p> </abstract>
<abstract> <p>The impact of R&amp;D on growth through spillovers has been a major topic of economic research over the last thirty years. A central problem in the literature is that firm performance is affected by two countervailing "spillovers" : a positive effect from technology (knowledge) spillovers and a negative business stealing effects from product market rivals. We develop a general framework incorporating these two types of spillovers and implement this model using measures of a firm's position in technology space and product market space. Using panel data on U.S. firms, we show that technology spillovers quantitatively dominate, so that the gross social returns to R&amp;D are at least twice as high as the private returns. We identify the causal effect of R&amp;D spillovers by using changes in federal and state tax incentives for R&amp;D. We also find that smaller firms generate lower social returns to R&amp;D because they operate more in technological niches. Finally, we detail the desirable properties of an ideal spillover measure and how existing approaches, including our new Mahalanobis measure, compare to these criteria.</p> </abstract>
<abstract> <p>In this paper, we use indirect inference to estimate a joint model of earnings, employment, job changes, wage rates, and work hours over a career. We use the model to address a number of important questions in labor economics, including the source of the experience profile of wages, the response of job changes to outside wage offers, and the effects of seniority on job changes. We also study the dynamic response of wage rates, hours, and earnings to various shocks, and measure the relative contributions of the shocks to the variance of earnings in a given year and over a lifetime. We find that human capital accounts for most of the growth of earnings over a career, although job seniority and job mobility also play significant roles. Unemployment shocks have a large impact on earnings in the short run, as well as a substantial long-term effect that operates through the wage rate. Shocks associated with job changes and unemployment make a large contribution to the variance of career earnings and operate mostly through the job-specific error components of wages and hours.</p> </abstract>
<abstract> <p>Information costs, which comprise costs of gathering and processing information about stock values and costs of deciding how to respond to this information, induce a consumer to remain inattentive to the stock market for finite intervals of time. Whether, and how much, a consumer transfers assets between accounts depends on the costs of undertaking such transactions. In general, optimal behavior by a consumer facing both information costs and transactions costs is state-dependent, with the timing of observations and the timing and size of transactions depending on the state. Surprisingly, if the fixed component of the transactions cost is sufficiently small, then eventually, with probability 1, a time-dependent rule emerges: the interval between observations is constant and on each observation date, the consumer converts enough assets to liquid assets to finance consumption until the next observation. If the fixed component of transactions costs is large, the optimal rule remains state-dependent indefinitely.</p> </abstract>
<abstract> <p>We model a dynamic, competitive market, where in every period, risk-neutral traders trade a one-period bond against an infinitely lived asset, with limited short-selling of the long-term asset. Traders lack structural knowledge and use different "incomplete theories," all of which give statistically correct beliefs about next period's market price of the long-term asset. The more theories there are in the market, the higher is the equilibrium price of the long-term asset. Investors with more complete theories do not necessarily earn higher returns than those with less complete ones, who can earn above the risk-free rate. We provide two necessary conditions for a trader to earn above the risk-free rate.</p> </abstract>
<abstract> <p>We propose a bubble game that involves sequential trading of an asset commonly known to be valueless. Because no trader is ever sure to be last in the market sequence, the game allows for a bubble at the Nash equilibrium when there is no cap on the maximum price. We run experiments both with and without a price cap. Structural estimation of behavioral game theory models suggests that quantal responses and analogy-based expectations are important drivers of speculation.</p> </abstract>
<abstract> <p>We consider a general representation of the delegation problem, with and without money burning, and provide sufficient and necessary conditions under which an interval allocation is optimal. We also apply our results to the theory of trade agreements among privately informed governments. For both perfect and monopolistic competition settings, we provide conditions under which tariff caps are optimal.</p> </abstract>
<abstract> <p>We study the role of incomplete information and outside options in determining bargaining postures and surplus division in repeated bargaining between a long-run player and a sequence of short-run players. The outside option is not only a disagreement point, but reveals information privately held by the long-run player. In equilibrium, the uninformed short-run players' offers do not always respond to changes in reputation and the informed long-run player's payoffs are discontinuous. The long-run player invokes inefficient random outside options repeatedly to build reputation to a level where the subsequent short-run players succumb to his extraction of a larger payoff, but he also runs the risk of losing reputation and relinquishing bargaining power. We investigate equilibrium properties when the discount factor goes to 1 and when the informativeness of outside options diffuses. In both cases, bargaining outcomes become more inefficient and the limit reputation-building probabilities are interior.</p> </abstract>
<abstract> <p>We develop an asymptotic theory for the pre-averaging estimator when asset price jumps are weakly identified, here modeled as local to zero. The theory unifies the conventional asymptotic theory for continuous and discontinuous semimartingales as two polar cases with a continuum of local asymptotics, and explains the breakdown of the conventional procedures under weak identification. We propose simple bias-corrected estimators for jump power variations, and construct robust confidence sets with valid asymptotic size in a uniform sense. The method is also robust to certain forms of microstructure noise.</p> </abstract>
<abstract> <p>Branch selection is a key decision in a cadet's military career. Cadets at USMA can increase their branch priorities at a fraction of slots by extending their service agreement. This real-life matching problem fills an important gap in the market design literature, providing strong empirical legitimacy to a series of elegant theoretical works on matching with contracts. Although priorities fail a key substitutes condition, the agent-optimal stable mechanism is well defined, and in contrast to the current USMA mechanism it is fair, stable, strategy-proof, and respects improvements in cadet priorities. Adoption of this mechanism benefits cadets and the Army. This new application shows that the matching with contracts model is practically relevant beyond traditional domains that satisfy the substitutes condition.</p> </abstract>
<abstract> <p>We introduce and apply a new nonparametric approach to identification and inference on data from ascending auctions. We exploit variation in the number of bidders across auctions to nonparametrically identify useful bounds on seller profit and bidder surplus using a general model of correlated private values that nests the standard independent private values (IPV) model. We also translate our identified bounds into closed form and asymptotically valid confidence intervals for several economic measures of interest. Applying our methods to much studied U.S. Forest Service timber auctions, we find evidence of correlation among values after controlling for a rich vector of relevant auction covariates; this correlation causes expected profit, the profit-maximizing reserve price, and bidder surplus to be substantially lower than conventional (IPV) analysis of the data would suggest.</p> </abstract>
<abstract> <p>Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time-homogeneity conditions that are like "time is randomly assigned" or "time is an instrument." Partial-identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial-identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of T, the number of time periods, is shown by deriving shrinkage rates for the identified set as T grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.</p> </abstract>
<abstract> <p>This paper considers random coefficients binary choice models. The main goal is to estimate the density of the random coefficients nonparametrically. This is an ill-posed inverse problem characterized by an integral transform. A new density estimator for the random coefficients is developed, utilizing Fourier—Laplace series on spheres. This approach offers a clear insight on the identification problem. More importantly, it leads to a closed form estimator formula that yields a simple plug-in procedure requiring no numerical optimization. The new estimator, therefore, is easy to implement in empirical applications, while being flexible about the treatment of unobserved heterogeneity. Extensions including treatments of nonrandom coefficients and models with endogeneity are discussed.</p> </abstract>
<abstract> <p>In this paper, we propose an instrumental variable approach to constructing confidence sets (CS's) for the true parameter in models defined by conditional moment inequalities/equalities. We show that by properly choosing instrument functions, one can transform conditional moment inequalities/equalities into unconditional ones without losing identification power. Based on the unconditional moment inequalities/equalities, we construct CS's by inverting Cramér—von Mises-type or Kolmogorov—Smirnov-type tests. Critical values are obtained using generalized moment selection (GMS) procedures. We show that the proposed CS's have correct uniform asymptotic coverage probabilities. New methods are required to establish these results because an infinite-dimensional nuisance parameter affects the asymptotic distributions. We show that the tests considered are consistent against all fixed alternatives and typically have power against n -1/2 -local alternatives to some, but not all, sequences of distributions in the null hypothesis. Monte Carlo simulations for five different models show that the methods perform well in finite samples.</p> </abstract>
<abstract> <p>We develop a practical and novel method for inference on intersection bounds, namely bounds defined by either the infimum or supremum of a parametric or nonparametric function, or, equivalently, the value of a linear programming problem with a potentially infinite constraint set. We show that many bounds characterizations in econometrics, for instance bounds on parameters under conditional moment inequalities, can be formulated as intersection bounds. Our approach is especially convenient for models comprised of a continuum of inequalities that are separable in parameters, and also applies to models with inequalities that are nonseparable in parameters. Since analog estimators for intersection bounds can be severely biased in finite samples, routinely underestimating the size of the identified set, we also offer a medianbias-corrected estimator of such bounds as a by-product of our inferential procedures. We develop theory for large sample inference based on the strong approximation of a sequence of series or kernel-based empirical processes by a sequence of "penultimate" Gaussian processes. These penultimate processes are generally not weakly convergent, and thus are non-Donsker. Our theoretical results establish that we can nonetheless perform asymptotically valid inference based on these processes. Our construction also provides new adaptive inequality/moment selection methods. We provide conditions for the use of nonparametric kernel and series estimators, including a novel result that establishes strong approximation for any general series estimator admitting linearization, which may be of independent interest.</p> </abstract>
<abstract> <p>This paper develops a new theory of fluctuations—one that helps accommodate the notions of "animal spirits" and "market sentiment" in unique-equilibrium, rational-expectations, macroeconomic models. To this goal, we limit the communication that is embedded in a neoclassical economy by allowing trading to be random and decentralized. We then show that the business cycle may be driven by a certain type of extrinsic shocks which we call sentiments. These shocks formalize shifts in expectations of economic activity without shifts in the underlying preferences and technologies; they are akin to sunspots, but operate in unique-equilibrium models. We further show how communication may help propagate these shocks in a way that resembles the spread of fads and rumors and that gives rise to boom-and-bust phenomena. We finally illustrate the quantitative potential of our insights within a variant of the RBC model.</p> </abstract>
<abstract> <p>Different people use language in different ways. We capture this by making language competence—the set of messages an agent can use and understand—private information. Our primary focus is on common-interest games. Communication generally remains possible; it may be severely impaired even with common knowledge that language competence is adequate; and, indeterminacy of meaning, the confounding of payoff-relevant information with information about language competence, is optimal.</p> </abstract>
<abstract> <p>In this note, we prove an equilibrium existence theorem for games with discontinuous payoffs and convex and compact strategy spaces. It generalizes the classical result of Reny (1999), as well as the recent paper of McLennan, Monteiro, and Tourky (2011). Our conditions are simple and easy to verify. Importantly, examples of spatial location models show that our conditions allow for economically meaningful payoff discontinuities, that are not covered by other conditions in the literature.</p> </abstract>
<abstract> <p>We study whether priors that admit full surplus extraction (FSE) are generic, an issue that becomes a gauge to evaluate the validity of the current mechanism design paradigm. We consider the space of priors on the universal type space, and thereby relax the assumption of a fixed finite number of types made by Crémer and McLean (1988). We show that FSE priors are topologically generic, contrary to the result of Heifetz and Neeman (2006) that FSE is generically impossible, both geometrically and measure-theoretically. Instead of using the BDP approach or convex combinations of priors adopted in Heifetz and Neeman (2006), we prove our genericity results by showing a robustness property of Crémer—McLean mechanisms.</p> </abstract>
<abstract> <p>Across a wide set of nongroup insurance markets, applicants are rejected based on observable, often high-risk, characteristics. This paper argues that private information, held by the potential applicant pool, explains rejections. I formulate this argument by developing and testing a model in which agents may have private information about their risk. I first derive a new no-trade result that theoretically explains how private information could cause rejections. I then develop a new empirical methodology to test whether this no-trade condition can explain rejections. The methodology uses subjective probability elicitations as noisy measures of agents' beliefs. I apply this approach to three nongroup markets: long-term care, disability, and life insurance. Consistent with the predictions of the theory, in all three settings I find significant amounts of private information held by those who would be rejected; I find generally more private information for those who would be rejected relative to those who can purchase insurance, and I show it is enough private information to explain a complete absence of trade for those who would be rejected. The results suggest that private information prevents the existence of large segments of these three major insurance markets.</p> </abstract>
<abstract> <p>This article predicts how radio station formats would change if, as was recently proposed, music stations were made to pay fees for musical performance rights. It does so by estimating and solving, using parametric approximations to firms' value functions, a dynamic model that captures important features of the industry such as vertical and horizontal product differentiation, demographic variation in programming tastes, and multi-station ownership. The estimated model predicts that high fees would cause the number of music stations to fall significantly and quite quickly. For example, a fee equal to 10% of revenues would cause a 4.6% drop in the number of music stations within $2\frac{1}{2}$ years, and a 9.4% drop in the long run. The size of the change is limited, however, by the fact that many listeners, particularly in demographics that are valued by advertisers, have strong preferences for music programming.</p> </abstract>
<abstract> <p>It is well known that, in misspecified parametric models, the maximum likelihood estimator (MLE) is consistent for the pseudo-true value and has an asymptotically normal sampling distribution with "sandwich" covariance matrix. Also, posteriors are asymptotically centered at the MLE, normal, and of asymptotic variance that is, in general, different than the sandwich matrix. It is shown that due to this discrepancy, Bayesian inference about the pseudo-true parameter value is, in general, of lower asymptotic frequentist risk when the original posterior is substituted by an artificial normal posterior centered at the MLE with sandwich covariance matrix. An algorithm is suggested that allows the implementation of this artificial posterior also in models with high dimensional nuisance parameters which cannot reasonably be estimated by maximizing the likelihood.</p> </abstract>
<abstract> <p>This paper derives optimal inheritance tax formulas that capture the key equity-efficiency trade-off, are expressed in terms of estimable sufficient statistics, and are robust to the underlying structure of preferences. We consider dynamic stochastic models with general and heterogeneous bequest tastes and labor productivities. We limit ourselves to simple but realistic linear or two-bracket tax structures to obtain tractable formulas. We show that long-run optimal inheritance tax rates can always be expressed in terms of aggregate earnings and bequest elasticities with respect to tax rates, distributional parameters, and social preferences for redistribution. Those results carry over with tractable modifications to (a) the case with social discounting (instead of steady-state welfare maximization), (b) the case with partly accidental bequests, (c) the standard Barro—Becker dynastic model. The optimal tax rate is positive and quantitatively large if the elasticity of bequests to the tax rate is low, bequest concentration is high, and society cares mostly about those receiving little inheritance. We propose a calibration using micro-data for France and the United States. We find that, for realistic parameters, the optimal inheritance tax rate might be as large as 50%—60%—or even higher for top bequests, in line with historical experience.</p> </abstract>
<abstract> <p>We study repeated Bayesian games with communication and observable actions in which the players' privately known payoffs evolve according to an irreducible Markov chain whose transitions are independent across players. Our main result implies that, generically, any Pareto-efficient payoff vector above a stationary minmax value can be approximated arbitrarily closely in a perfect Bayesian equilibrium as the discount factor goes to 1. As an intermediate step, we construct an approximately efficient dynamic mechanism for long finite horizons without assuming transferable utility.</p> </abstract>
<abstract> <p>This paper studies a dynamic agency problem which includes limited liability, moral hazard, and adverse selection. The paper develops a robust approach to dynamic contracting based on calibrating the incentive properties of simple benchmark contracts that are attractive but infeasible, due to limited liability constraints. The resulting dynamic contracts are detail-free and satisfy robust performance bounds independently of the underlying process for returns, which need not be i.i.d. or even ergodic.</p> </abstract>
<abstract> <p>We present two examples of discounted stochastic games, each with a continuum of states, finitely many players, and actions, that possess no stationary equilibria. The first example has deterministic transitions—an assumption undertaken in most of the early applications of dynamics games in economics—and perfect information, and does not possess even stationary approximate equilibria or Markovian equilibria. The second example satisfies, in addition to stronger regularity assumptions, that all transitions are absolutely continuous with respect to a fixed measure—an assumption that has been widely used in more recent economic applications. This assumption has been undertaken in several positive results on the existence of stationary equilibria in special cases, and in particular, guarantees the existence of stationary approximate equilibria.</p> </abstract>
<abstract> <p>This paper proposes a symmetry-breaking model of trade with a (large but) finite number of (ex ante) identical countries and a continuum of tradeable goods, which differ in their dependence on local differentiated producer services. Productivity differences across countries arise endogenously through free entry to the local service sector in each country. In any stable equilibrium, the countries sort themselves into specializing in different sets of tradeable goods, and a strict ranking of countries in per capita income, TFP, and the capital-labor ratio emerges endogenously. Furthermore, the distribution of country shares, the Lorenz curve, is unique and analytically solvable in the limit, as the number of countries grows unbounded. Using this limit as an approximation allows us to study what determines the shape of distribution, to perform various comparative statics, and to evaluate the welfare effects of trade.</p> </abstract>
<abstract> <p>Using centuries of Nile flood data, I document that during deviant Nile floods, Egypt's highest-ranking religious authority was less likely to be replaced and relative allocations to religious structures increased. These findings are consistent with historical evidence that Nile shocks increased this authority's political influence by raising the probability he could coordinate a revolt. I find that the available data provide support for this interpretation and weigh against some of the most plausible alternatives. For example, I show that while Nile shocks increased historical references to social unrest, deviant floods did not increase a proxy for popular religiosity. Together, the results suggest an increase in the political power of religious leaders during periods of economic downturn.</p> </abstract>
<abstract> <p>This paper presents a theoretical and empirical analysis of the role of life expectancy for optimal schooling and lifetime labor supply. The results of a simple prototype Ben-Porath model with age-specific survival rates show that an increase in lifetime labor supply is not a necessary, or a sufficient, condition for greater life expectancy to increase optimal schooling. The observed increase in survival rates during working ages that follows from the "rectangularization" of the survival function is crucial for schooling and labor supply. The empirical results suggest that the relative benefits of schooling have been increasing across cohorts of U.S. men born between 1840 and 1930. A simple quantitative analysis shows that a realistic shift in the survival function can lead to an increase in schooling and a reduction in lifetime labor hours.</p> </abstract>
<abstract> <p>We consider the invertibility (injectivity) of a nonparametric nonseparable demand system. Invertibility of demand is important in several contexts, including identification of demand, estimation of demand, testing of revealed preference, and economic theory exploiting existence of an inverse demand function or (in an exchange economy) uniqueness of Walrasian equilibrium prices. We introduce the notion of "connected substitutes" and show that this structure is sufficient for invertibility. The connected substitutes conditions require weak substitution between all goods and sufficient strict substitution to necessitate treating them in a single demand system. The connected substitutes conditions have transparent economic interpretation, are easily checked, and are satisfied in many standard models. They need only hold under some transformation of demand and can accommodate many models in which goods are complements. They allow one to show invertibility without strict gross substitutes, functional form restrictions, smoothness assumptions, or strong domain restrictions. When the restriction to weak substitutes is maintained, our sufficient conditions are also "nearly necessary" for even local invertibility.</p> </abstract>
<abstract> <p>This comment corrects two results in the 2006 Econometrica paper by Amador, Werning, and Angeletos (AWA), that features a model in which individuals face a trade-off between flexibility and commitment. First, in contrast to Proposition 1 in AWA, we show that money-burning can be part of the ex ante optimal contract when there are two states. Second, in contrast to Proposition 2 in AWA, we show that money-burning can be imposed at the top (in the highest liquidity shock state), even when there is a continuum of states. We provide corrected versions of the above results.</p> </abstract>
<abstract> <p>We develop a property-rights model of the firm in which production entails a continuum of uniquely sequenced stages. In each stage, a final-good producer contracts with a distinct supplier for the procurement of a customized stage-specific component. Our model yields a sharp characterization for the optimal allocation of ownership rights along the value chain. We show that the incentive to integrate suppliers varies system-atically with the relative position (upstream versus downstream) at which the supplier enters the production line. Furthermore, the nature of the relationship between integration and "downstreamness" depends crucially on the elasticity of demand faced by the final-good producer. Our model readily accommodates various sources of asymmetry across final-good producers and across suppliers within a production line, and we show how it can be taken to the data with international trade statistics. Combining data from the U.S. Census Bureau's Related Party Trade database and estimates of U.S. import demand elasticities from Broda and Weinstein (2006), we find empirical evidence broadly supportive of our key predictions. In the the process, we develop two novel measures of the average position of an industry in the value chain, which we construct using U.S. Input—Output Tables.</p> </abstract>
<abstract> <p>Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article, we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios, we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States. As a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the entire conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.</p> </abstract>
<abstract> <p>What preferences will prevail in a society of rational individuals when preference evolution is driven by the resulting payoffs? We show that when individuals' preferences are their private information, a convex combination of selfishness and morality stands out as evolutionarily stable. We call individuals with such preferences homo moralis. At one end of the spectrum is homo oeconomicus, who acts so as to maximize his or her own payoff. At the opposite end is homo kantiensis, who does what would be "the right thing to do," in terms of payoffs, if all others would do likewise. We show that the stable degree of morality—the weight placed on the moral goal—is determined by the degree of assortativity in the process whereby individuals are matched to interact.</p> </abstract>
<abstract> <p>This paper proposes a new approach to equilibrium selection in repeated games with transfers, supposing that in each period the players bargain over how to play. Although the bargaining phase is cheap talk (following a generalized alternating-offer protocol), sharp predictions arise from three axioms. Two axioms allow the players to meaningfully discuss whether to deviate from their plan; the third embodies a "theory of disagreement"—that play under disagreement should not vary with the manner in which bargaining broke down. Equilibria that satisfy these axioms exist for all discount factors and are simple to construct; all equilibria generate the same welfare. Optimal play under agreement generally requires suboptimal play under disagreement. Whether patient players attain efficiency depends on both the stage game and the bargaining protocol. The theory extends naturally to games with imperfect public monitoring and heterogeneous discount factors, and yields new insights into classic relational contracting questions.</p> </abstract>
<abstract> <p>The aim of this paper is to develop revealed preference tests for Cournot equilibrium. The tests are akin to the widely used revealed preference tests for consumption, but have to take into account the presence of strategic interaction in a game-theoretic setting. The tests take the form of linear programs, the solutions to which also allow us to recover cost information on the firms. To check that these nonparametric tests are sufficiently discriminating to reject real data, we apply them to the market for crude oil.</p> </abstract>
<abstract> <p>We propose a model of firm reputation in which a firm can invest or disinvest in product quality and the firm's reputation is defined as the market's belief about this quality. We analyze the relationship between a firm's reputation and its investment incentives, and derive implications for reputational dynamics. Reputational incentives depend on the specification of market learning. When consumers learn about quality through perfect good news signals, incentives decrease in reputation and there is a unique work—shirk equilibrium with ergodic dynamics. When learning is through perfect bad news signals, incentives increase in reputation and there is a continuum of shirk—work equilibria with path-dependent dynamics. For a class of imperfect Poisson learning processes and low investment costs, we show that there exists a work—shirk equilibrium with ergodic dynamics. For a subclass of these learning processes, any equilibrium must feature working at all low and intermediate levels of reputation and shirking at the top.</p> </abstract>
<abstract> <p>This paper constructs an efficient, budget-balanced, Bayesian incentive-compatible mechanism for a general dynamic environment with quasilinear payoffs in which agents observe private information and decisions are made over countably many periods. First, under the assumption of "private values" (other agents' private information does not directly affect an agent's payoffs), we construct an efficient, ex post incentive-compatible mechanism, which is not budget-balanced. Second, under the assumption of "independent types" (the distribution of each agent's private information is not directly affected by other agents' private information), we show how the budget can be balanced without compromising agents' incentives. Finally, we show that the mechanism can be made self-enforcing when agents are sufficiently patient and the induced stochastic process over types is an ergodic finite Markov chain.</p> </abstract>
<abstract> <p>We develop a network-flow approach for characterizing interim-allocation rules that can be implemented by ex post allocations. Our method can be used to characterize feasible interim allocations in general multi-unit auctions where agents face capacity constraints, both ceilings and floors. Applications include a variety of settings of practical interest, ranging from individual and group-specific capacity constraints, set-aside sale, partnership dissolution, and government license reallocation.</p> </abstract>
<abstract> <p>A choice function is backwards-induction rationalizable if there exists a finite perfect-information extensive-form game such that for each subset of alternatives, the backwards-induction outcome of the restriction of the game to that subset of alternatives coincides with the choice from that subset. We prove that every choice function is backwards-induction rationalizable.</p> </abstract>
<abstract> <p>This paper examines three distinct hypothesis testing problems that arise in the context of identification of some nonparametric models with endogeneity. The first hypothesis testing problem we study concerns testing necessary conditions for identification in some nonparametric models with endogeneity involving mean independence restrictions. These conditions are typically referred to as completeness conditions. The second and third hypothesis testing problems we examine concern testing for identification directly in some nonparametric models with endogeneity involving quantile independence restrictions. For each of these hypothesis testing problems, we provide conditions under which any test will have power no greater than size against any alternative. In this sense, we conclude that no nontrivial tests for these hypothesis testing problems exist.</p> </abstract>
<abstract> <p>This paper proposes a test for common conditionally heteroskedastic (CH) features in asset returns. Following Engle and Kozicki (1993), the common CH features property is expressed in terms of testable overidentifying moment restrictions. However, as we show, these moment conditions have a degenerate Jacobian matrix at the true parameter value and therefore the standard asymptotic results of Hansen (1982) do not apply. We show in this context that Hansen's (1982) J-test statistic is asymptotically distributed as the minimum of the limit of a certain random process with a markedly nonstandard distribution. If two assets are considered, this asymptotic distribution is a fifty—fifty mixture of ${\mathrm{\chi }}_{\mathrm{H}-1}^{2}$ and ${\mathrm{\chi }}_{\mathrm{H}}^{2}$ , where H is the number of moment conditions, as opposed to a ${\mathrm{\chi }}_{\mathrm{H}-1}^{2}$ . With more than two assets, this distribution lies between the ${\mathrm{\chi }}_{\mathrm{H}-\mathrm{p}}^{2}$ and ${\mathrm{\chi }}_{\mathrm{H}}^{2}$ (p denotes the number of parameters). These results show that ignoring the lack of first-order identification of the moment condition model leads to oversized tests with a possibly increasing overrejection rate with the number of assets. A Monte Carlo study illustrates these findings.</p> </abstract>
<abstract> <p>We introduce and analyze expected uncertain utility (EUU) theory. A prior and an interval utility characterize an EUU decision maker. The decision maker transforms each uncertain prospect into an interval-valued prospect that assigns an interval of prizes to each state. She then ranks prospects according to their expected interval utilities. We define uncertainty aversion for EUU, use the EUU model to address the Ellsberg Paradox and other ambiguity evidence, and relate EUU theory to existing models.</p> </abstract>
<abstract> <p>We analyze a dynamic stochastic general-equilibrium (DSGE) model with an externality—through climate change—from using fossil energy. Our central result is a simple formula for the marginal externality damage of emissions (or, equivalently, for the optimal carbon tax). This formula, which holds under quite plausible assumptions, reveals that the damage is proportional to current GDP, with the proportion depending only on three factors: (i) discounting, (ii) the expected damage elasticity (how many percent of the output flow is lost from an extra unit of carbon in the atmosphere), and (iii) the structure of carbon depreciation in the atmosphere. Thus, the stochastic values of future output, consumption, and the atmospheric CO 2 concentration, as well as the paths of technology (whether endogenous or exogenous) and population, and so on, all disappear from the formula. We find that the optimal tax should be a bit higher than the median, or most well-known, estimates in the literature. We also formulate a parsimonious yet comprehensive and easily solved model allowing us to compute the optimal and market paths for the use of different sources of energy and the corresponding climate change. We find coal—rather than oil—to be the main threat to economic welfare, largely due to its abundance. We also find that the costs of inaction are particularly sensitive to the assumptions regarding the substitutability of different energy sources and technological progress.</p> </abstract>
<abstract> <p>We model the decisions of a multiproduct firm that faces a fixed "menu" cost: once it is paid, the firm can adjust the price of all its products. We characterize analytically the steady state firm's decisions in terms of the structural parameters: the variability of the flexible prices, the curvature of the profit function, the size of the menu cost, and the number of products sold. We provide expressions for the steady state frequency of adjustment, the hazard rate of price adjustments, and the size distribution of price changes, all in terms of the structural parameters. We study analytically the impulse response of aggregate prices and output to a monetary shock. The size of the output response and its duration both increase with the number of products; they more than double as the number of products goes from 1 to 10, quickly converging to the response of Taylor's staggered price model.</p> </abstract>
<abstract> <p>Parents gauge school quality in part by the level of student achievement and a school's racial and socioeconomic mix. The importance of school characteristics in the housing market can be seen in the jump in house prices at school district boundaries where peer characteristics change. The question of whether schools with more attractive peers are really better in a value-added sense remains open, however. This paper uses a fuzzy regression-discontinuity design to evaluate the causal effects of peer characteristics. Our design exploits admissions cutoffs at Boston and New York City's heavily over-subscribed exam schools. Successful applicants near admissions cutoffs for the least selective of these schools move from schools with scores near the bottom of the state SAT score distribution to schools with scores near the median. Successful applicants near admissions cutoffs for the most selective of these schools move from above-average schools to schools with students whose scores fall in the extreme upper tail. Exam school students can also expect to study with fewer nonwhite classmates than unsuccessful applicants. Our estimates suggest that the marked changes in peer characteristics at exam school admissions cutoffs have little causal effect on test scores or college quality.</p> </abstract>
<abstract> <p>Short-run subsidies for health products are common in poor countries. How do they affect long-run adoption? A common fear among development practitioners is that one-off subsidies may negatively affect long-run adoption through reference-dependence: People might anchor around the subsidized price and be unwilling to pay more for the product later. But for experience goods, one-off subsidies could also boost long-run adoption through learning. This paper uses data from a two-stage randomized pricing experiment in Kenya to estimate the relative importance of these effects for a new, improved antimalarial bed net. Reduced form estimates show that a one-time subsidy has a positive impact on willingness to pay a year later inherit. To separately identify the learning and anchoring effects, we estimate a parsimonious experience-good model. Estimation results show a large, positive learning effect but no anchoring. We black then discuss the types of products and the contexts inherit for which these results may apply.</p> </abstract>
<abstract> <p>Does Islamic political control affect women's empowerment? Several countries have recently experienced Islamic parties coming to power through democratic elections. Due to strong support among religious conservatives, constituencies with Islamic rule often tend to exhibit poor women's rights. Whether this reflects a causal relationship or a spurious one has so far gone unexplored. I provide the first piece of evidence using a new and unique data set of Turkish municipalities. In 1994, an Islamic party won multiple municipal mayor seats across the country. Using a regression discontinuity (RD) design, I compare municipalities where this Islamic party barely won or lost elections. Despite negative raw correlations, the RD results reveal that, over a period of six years, Islamic rule increased female secular high school education. Corresponding effects for men are systematically smaller and less precise. In the longer run, the effect on female education remained persistent up to 17 years after, and also reduced adolescent marriages. An analysis of long-run political effects of Islamic rule shows increased female political participation and an overall decrease in Islamic political preferences. The results are consistent with an explanation that emphasizes the Islamic party's effectiveness in overcoming barriers to female entry for the poor and pious.</p> </abstract>
<abstract> <p>We define the class of two-player zero-sum games with payoffs having mild discontinuities, which in applications typically stem from how ties are resolved. For such games, we establish sufficient conditions for existence of a value of the game, maximin and minimax strategies for the players, and a Nash equilibrium. If all discontinuities favor one player, then a value exists and that player has a maximin strategy. A property called payoff approachability implies existence of an equilibrium, and that the resulting value is invariant: games with the same payoffs at points of continuity have the same value and ε-equilibria. For voting games in which two candidates propose policies and a candidate wins election if a weighted majority of voters prefer his proposed policy, we provide tie-breaking rules and assumptions about voters' preferences sufficient to imply payoff approachability. These assumptions are satisfied by generic preferences if the dimension of the space of policies exceeds the number of voters; or with no dimensional restriction, if the electorate is sufficiently large. Each Colonel Blotto game is a special case in which each candidate allocates a resource among several constituencies and a candidate gets votes from those allocated more than his opponent offers; in this case, for simple-majority rule we prove existence of an equilibrium with zero probability of ties.</p> </abstract>
<abstract> <p>This paper considers nonparametric identification of a two-stage entry and bidding game we call the Affiliated-Signal (AS) model. This model assumes that potential bidders have private values, observe signals of their values prior to entry, and then choose whether to undertake a costly entry process, but imposes only minimal structure on the relationship between signals and values. It thereby nests a wide range of entry processes, including in particular the Samuelson (1985) and Levin and Smith (1994) models as special cases. Working within the AS model, we map variation in factors affecting entry behavior (potential competition or entry costs) into identified bounds on model fundamentals. These bounds are constructive, collapse to point identification when available entry variation is continuous, and can readily be refined to produce the pointwise sharp identified set. We then extend our core results to accommodate non-separable unobserved auction-level heterogeneity and potential endogeneity of entry shifters, thereby establishing a formal identification framework for structural analysis of auctions with selective entry.</p> </abstract>
<abstract> <p>This paper introduces a general method to convert a model defined by moment conditions that involve both observed and unobserved variables into equivalent moment conditions that involve only observable variables. This task can be accomplished without introducing infinite-dimensional nuisance parameters using a least favorable entropy-maximizing distribution. We demonstrate, through examples and simulations, that this approach covers a wide class of latent variables models, including some game-theoretic models and models with limited dependent variables, interval-valued data, errors-in-variables, or combinations thereof. Both point- and set-identified models are transparently covered. In the latter case, the method also complements the recent literature on generic set-inference methods by providing the moment conditions needed to construct a generalized method of moments-type objective function for a wide class of models. Extensions of the method that cover conditional moments, independence restrictions, and some state-space models are also given.</p> </abstract>
<abstract> <p>This paper examines the efficient estimation of partially identified models defined by moment inequalities that are convex in the parameter of interest. In such a setting, the identified set is itself convex and hence fully characterized by its support function. We provide conditions under which, despite being an infinite dimensional parameter, the support function admits √n-consistent regular estimators. A semiparametric efficiency bound is then derived for its estimation, and it is shown that any regular estimator attaining it must also minimize a wide class of asymptotic loss functions. In addition, we show that the "plug-in" estimator is efficient, and devise a consistent bootstrap procedure for estimating its limiting distribution. The setting we examine is related to an incomplete linear model studied in Beresteanu and Molinari (2008) and Bontemps, Magnac, and Maurin (2012), which further enables us to establish the semiparametric efficiency of their proposed estimators for that problem.</p> </abstract>
<abstract> <p>Say that one information structure is eventually Blackwell sufficient for another if, for every large enough n, an n-sample from the first is Blackwell sufficient (Blackwell (1951, 1954)) for an n-sample from the second. This note shows that eventual Blackwell sufficiency lies strictly between (one-shot) Blackwell sufficiency and the ordering of information structures formulated by Moscarini and Smith (2002), and thus offers a new criterion for comparing experiments. A characterization of eventual Blackwell sufficiency in terms of the one-shot experiments remains an open question.</p> </abstract>
<abstract> <p>A wide body of empirical evidence finds that approximately 25 percent of fiscal stimulus payments (e.g., tax rebates) are spent on nondurable household consumption in the quarter that they are received. To interpret this fact, we develop a structural economic model where households can hold two assets: a low-return liquid asset (e.g., cash, checking account) and a high-return illiquid asset that carries a transaction cost (e.g., housing, retirement account). The optimal life-cycle pattern of portfolio choice implies that many households in the model are "wealthy hand-to-mouth": they hold little or no liquid wealth despite owning sizable quantities of illiquid assets. Therefore, they display large propensities to consume out of additional transitory income, and small propensities to consume out of news about future income. We document the existence of such households in data from the Survey of Consumer Finances. A version of the model parameterized to the 2001 tax rebate episode yields consumption responses to fiscal stimulus payments that are in line with the evidence, and an order of magnitude larger than in the standard "one-asset" framework. The model's nonlinearities with respect to the rebate size and the prevailing aggregate economic conditions have implications for policy design.</p> </abstract>
<abstract> <p>This paper uses an unusual pay reform to test the responsiveness of investment in schooling to changes in redistribution schemes that increase the rate of return to education. We exploit an episode where different Israeli kibbutzim shifted from equal sharing to productivity-based wages in different years and find that students in kibbutzim that reformed earlier invested more in high school education and, in the long run, also in post-secondary schooling. We further show that the effect is mainly driven by students in kibbutzim that reformed to a larger degree. Our findings support the prediction that education is highly responsive to changes in the redistribution policy.</p> </abstract>
<abstract> <p>Using a high-stakes field experiment conducted with a financial brokerage, we implement a novel design to separately identify two channels of social influence in financial decisions, both widely studied theoretically. When someone purchases an asset, his peers may also want to purchase it, both because they learn from his choice ("social learning") and because his possession of the asset directly affects others' utility of owning the same asset ("social utility"). We randomize whether one member of a peer pair who chose to purchase an asset has that choice implemented, thus randomizing his ability to possess the asset. Then, we randomize whether the second member of the pair: (i) receives no information about the first member, or (ii) is informed of the first member's desire to purchase the asset and the result of the randomization that determined possession. This allows us to estimate the effects of learning plus possession, and learning alone, relative to a (no information) control group. We find that both social learning and social utility channels have statistically and economically significant effects on investment decisions. Evidence from a follow-up survey reveals that social learning effects are greatest when the first (second) investor is financially sophisticated (financially unsophisticated); investors report updating their beliefs about asset quality after learning about their peer's revealed preference; and, they report motivations consistent with "keeping up with the Joneses" when learning about their peer's possession of the asset. These results can help shed light on the mechanisms underlying herding behavior in financial markets and peer effects in consumption and investment decisions.</p> </abstract>
<abstract> <p>This paper uses a data base covering the universe of French firms for the period 1990–2007 to provide a forensic account of the role of individual firms in generating aggregate fluctuations. We set up a simple multisector model of heterogeneous firms selling to multiple markets to motivate a theoretically founded decomposition of firms' annual sales growth rate into different components. We find that the firm-specific component contributes substantially to aggregate sales volatility, mattering about as much as the components capturing shocks that are common across firms within a sector or country. We then decompose the firm-specific component to provide evidence on two mechanisms that generate aggregate fluctuations from microeconomic shocks highlighted in the recent literature: (i) when the firm size distribution is fat-tailed, idiosyncratic shocks to large firms directly contribute to aggregate fluctuations, and (ii) aggregate fluctuations can arise from idiosyncratic shocks due to input—output linkages across the economy. Firm linkages are approximately three times as important as the direct effect of firm shocks in driving aggregate fluctuations.</p> </abstract>
<abstract> <p>We evaluate the effect of land use regulation on the value of land and on welfare. Our estimates are based on a decomposition of the effects of regulation into three components: an own-lot effect, which reflects the cost of regulatory constraints to the owner of a parcel; an external effect, which reflects the value of regulatory constraints on one's neighbors; a supply effect, which reflects the effect of regulated scarcity of developable land. Using this decomposition, we arrive at a novel strategy for estimating a plausibly causal effect of land use regulation on land value and welfare. This strategy exploits cross-border changes in development, prices, and regulation in regions near municipal borders. Our estimates suggest large negative effects of regulation on the value of land and welfare in these regions.</p> </abstract>
<abstract> <p>We argue that the notion of Pareto dominance is not as compelling in the presence of uncertainty as it is under certainty. In particular, voluntary trade based on differences in tastes is commonly accepted as desirable, because tastes cannot be wrong. By contrast, voluntary trade based on incompatible beliefs may indicate that at least one agent entertains mistaken beliefs. We propose and characterize a weaker, No-Betting, notion of Pareto domination which requires, on top of unanimity of preference, the existence of shared beliefs that can rationalize such preference for each agent.</p> </abstract>
<abstract> <p>This paper studies the interaction between default and liquidity for corporate bonds that are traded in an over-the-counter secondary market with search frictions. Bargaining with dealers determines a bond's endogenous liquidity, which depends on both the firm fundamental and the time-to-maturity of the bond. Corporate default decisions interact with the endogenous secondary market liquidity via the rollover channel. A default-liquidity loop arises: Assuming a relative illiquid secondary bond market in default, earlier endogenous default worsens a bond's secondary market liquidity, which amplifies equity holders' rollover losses, which in turn leads to earlier endogenous default. Besides characterizing in closed form the full interdependence between liquidity and default for credit spreads, our calibrated model can jointly match empirically observed credit spreads and liquidity measures of bonds across different rating classes.</p> </abstract>
<abstract> <p>We develop a model of experimentation and learning in policymaking when control of power is temporary. We demonstrate how an early office holder who would otherwise not experiment is nonetheless induced to experiment when his hold on power is temporary. This preemptive policy experiment is profitable for the early office holder as it reveals information about the policy mapping to his successor, information that shapes future policy choices. Thus policy choices today can cast a long shadow over future choices purely through information transmission and absent any formal institutional constraints or real state variables. The model we develop utilizes a recent innovation that represents the policy mapping as the realized path of a Brownian motion. We provide a precise characterization of when preemptive experimentation emerges in equilibrium and the form it takes. We apply the model to several well known episodes of policymaking, reinterpreting the policy choices as preemptive experiments.</p> </abstract>
<abstract> <p>We study a principal-agent model in which the agent is boundedly rational in his ability to understand the principal's decision rule. The principal wishes to elicit an agent's true profile so as to determine whether or not to grant him a certain request. The principal designs a questionnaire and commits himself to accepting certain responses. In designing such a questionnaire, the principal takes into account the bounded rationality of the agent and wishes to reduce the success probability of a dishonest agent who is trying to game the system. It is shown that the principal can construct a sufficiently complex questionnaire that will allow him to respond optimally to agents who tell the truth and at the same time to almost eliminate the probability that a dishonest agent will succeed in cheating.</p> </abstract>
<abstract> <p>We identify the effects of monetary policy on credit risk-taking with an exhaustive credit register of loan applications and contracts. We separate the changes in the composition of the supply of credit from the concurrent changes in the volume of supply and quality, and the volume of demand. We employ a two-stage model that analyzes the granting of loan applications in the first stage and loan outcomes for the applications granted in the second stage, and that controls for both observed and unobserved, time-varying, firm and bank heterogeneity through time*firm and time*bank fixed effects. We find that a lower overnight interest rate induces lowly capitalized banks to grant more loan applications to ex ante risky firms and to commit larger loan volumes with fewer collateral requirements to these firms, yet with a higher ex post likelihood of default. A lower long-term interest rate and other relevant macroeconomic variables have no such effects.</p> </abstract>
<abstract> <p>In many real-life house allocation problems, rents are bounded from above by price ceilings imposed by a government or a local administration. This is known as rent control. Because some price equilibria may be disqualified given such restrictions, this paper proposes an alternative equilibrium concept, called rationing price equilibrium, tailored to capture the specific features of housing markets with rent control. An allocation rule that always selects a rationing price equilibrium is defined, and it is demonstrated to be constrained efficient and (group) non-manipulable for "almost all" preference profiles. In its bounding cases, the rule reduces to a number of well-known mechanisms from the matching literature. In this sense, the housing market with rent control investigated in this paper integrates several of the predominant matching models into a more general framework.</p> </abstract>
<abstract> <p>We formulate a notion of stable outcomes in matching problems with one-sided asymmetric information. The key conceptual problem is to formulate a notion of a blocking pair that takes account of the inferences that the uninformed agent might make. We show that the set of stable outcomes is nonempty in incomplete-information environments, and is a superset of the set of complete-information stable outcomes. We then provide sufficient conditions for incomplete-information stable matchings to be efficient. Lastly, we define a notion of price-sustainable allocations and show that the set of incomplete-information stable matchings is a subset of the set of such allocations.</p> </abstract>
<abstract> <p>We show in an environment of incomplete information that monotonicity and the Pareto property applied only when there is common knowledge of Pareto dominance imply (i) there must exist a common prior over the smallest common knowledge event, and (ii) aggregation must be ex ante and ex post utilitarian with respect to that common prior and individual von Neumann–Morgenstern utility indices.</p> </abstract>
<abstract> <p>We study mechanism design in dynamic quasilinear environments where private information arrives over time and decisions are made over multiple periods. We make three contributions. First, we provide a necessary condition for incentive compatibility that takes the form of an envelope formula for the derivative of an agent's equilibrium expected payoff with respect to his current type. It combines the familiar marginal effect of types on payoffs with novel marginal effects of the current type on future ones that are captured by "impulse response functions." The formula yields an expression for dynamic virtual surplus that is instrumental to the design of optimal mechanisms and to the study of distortions under such mechanisms. Second, we characterize the transfers that satisfy the envelope formula and establish a sense in which they are pinned down by the allocation rule ("revenue equivalence"). Third, we characterize perfect Bayesian equilibrium-implementable allocation rules in Markov environments, which yields tractable sufficient conditions that facilitate novel applications. We illustrate the results by applying them to the design of optimal mechanisms for the sale of experience goods ("bandit auctions").</p> </abstract>
<abstract> <p>We consider a decision maker who faces dynamic decision situations that involve intertemporal trade-offs, as in consumption–savings problems, and who experiences taste shocks that are transient contingent on the state of the world. We axiomatize a recursive representation of choice over state contingent infinite horizon consumption problems, where uncertainty about consumption utilities depends on the observable state and the state follows a subjective Markov process. The parameters of the representation are the subjective process that governs the evolution of beliefs over consumption utilities and the discount factor; they are uniquely identified from behavior. We characterize a natural notion of greater preference for flexibility in terms of a dilation of beliefs. An important special case of our representation is a recursive version of the Anscombe–Aumann model with parameters that include a subjective Markov process over states and state-dependent utilities, all of which are uniquely identified.</p> </abstract>
<abstract> <p>This study documents two empirical facts using matched employer–employee data for Denmark and Portugal. First, workers who are hired last, are the first to leave the firm. Second, workers' wages rise with seniority, where seniority is defined as a worker's tenure relative to the tenure of his colleagues. Controlling for tenure, the probability of a worker leaving the firm decreases with seniority. The increase in expected seniority with tenure explains a large part of the negative duration dependence of the separation hazard. Conditional on ten years of tenure, the wage differential between the 10th and the 90th percentiles of the seniority distribution is 1.1–1.4 percentage points in Denmark and 2.3–3.4 in Portugal.</p> </abstract>
<abstract> <p>Cities exist because of the productivity gains that arise from clustering production and workers, a process called agglomeration. How important is agglomeration for aggregate growth? This paper constructs a dynamic stochastic general equilibrium model of cities and uses it to estimate the effect of local agglomeration on aggregate growth. We combine aggregate time-series and city-level panel data to estimate the model's parameters via generalized method of moments. The estimates imply a statistically and economically significant impact of local agglomeration on the growth rate of per capita consumption, raising it by about 10%.</p> </abstract>
<abstract> <p>This paper proposes a class of optimal tests for the constancy of parameters in random coefficients models. Our testing procedure covers the class of Hamilton's models, where the parameters vary according to an unobservable Markov chain, but also applies to nonlinear models where the random coefficients need not be Markov. We show that the contiguous alternatives converge to the null hypothesis at a rate that is slower than the standard rate. Therefore, standard approaches do not apply. We use Bartlett-type identities for the construction of the test statistics. This has several desirable properties. First, it only requires estimating the model under the null hypothesis where the parameters are constant. Second, the proposed test is asymptotically optimal in the sense that it maximizes a weighted power function. We derive the asymptotic distribution of our test under the null and local alternatives. Asymptotically valid bootstrap critical values are also proposed.</p> </abstract>
<abstract> <p>In parametric, nonlinear structural models, a classical sufficient condition for local identification, like Fisher (1966) and Rothenberg (1971), is that the vector of moment conditions is differentiable at the true parameter with full rank derivative matrix. We derive an analogous result for the nonparametric, nonlinear structural models, establishing conditions under which an infinite dimensional analog of the full rank condition is sufficient for local identification. Importantly, we show that additional conditions are often needed in nonlinear, nonparametric models to avoid nonlinearities overwhelming linear effects. We give restrictions on a neighborhood of the true value that are sufficient for local identification. We apply these results to obtain new, primitive identification conditions in several important models, including nonseparable quantile instrumental variable (IV) models and semiparametric consumption-based asset pricing models.</p> </abstract>
<abstract> <p>We consider the identification of counterfactual distributions and treatment effects when the outcome variables and conditioning covariates are observed in separate data sets. Under the standard selection on observables assumption, the counterfactual distributions and treatment effect parameters are no longer point identified. However, applying the classical monotone rearrangement inequality, we derive sharp bounds on the counterfactual distributions and policy parameters of interest.</p> </abstract>
<abstract> <p>This paper studies the optimal level of discretion in policymaking. We consider a fiscal policy model where the government has time-inconsistent preferences with a present bias toward public spending. The government chooses a fiscal rule to trade off its desire to commit to not overspend against its desire to have flexibility to react to privately observed shocks to the value of spending. We analyze the optimal fiscal rule when the shocks are persistent. Unlike under independent and identically distributed shocks, we show that the ex ante optimal rule is not sequentially optimal, as it provides dynamic incentives. The ex ante optimal rule exhibits history dependence, with high shocks leading to an erosion of future fiscal discipline compared to low shocks, which lead to the reinstatement of discipline. The implied policy distortions oscillate over time given a sequence of high shocks, and can force the government to accumulate maximal debt and become immiserated in the long run.</p> </abstract>
<abstract> <p>In this paper, we describe a series of laboratory experiments that implement specific examples of a general network structure. Specifically, actions are either strategic substitutes or strategic complements, and participants have either complete or incomplete information about the structure of a random network. Since economic environments typically have a considerable degree of complementarity or substitutability, this framework applies to a wide variety of settings. We examine behavior and equilibrium selection. The degree of equilibrium play is striking, in particular with incomplete information. Behavior closely resembles the theoretical equilibrium whenever this is unique; when there are multiple equilibria, general features of networks, such as connectivity, clustering, and the degree of the players, help to predict informed behavior in the lab. People appear to be strongly attracted to maximizing aggregate payoffs (social efficiency), but there are forces that moderate this attraction: (1) people seem content with (in the aggregate) capturing only the lion's share of the efficient profits in exchange for reduced exposure to loss, and (2) uncertainty about the network structure makes it considerably more difficult to coordinate on a demanding, but efficient, equilibrium that is typically implemented with complete information.</p> </abstract>
<abstract> <p>Hunger during pre-harvest lean seasons is widespread in the agrarian areas of Asia and Sub-Saharan Africa. We randomly assign an $8.50 incentive to households in rural Bangladesh to temporarily out-migrate during the lean season. The incentive induces 22% of households to send a seasonal migrant, their consumption at the origin increases significantly, and treated households are 8–10 percentage points more likely to re-migrate 1 and 3 years after the incentive is removed. These facts can be explained qualitatively by a model in which migration is risky, mitigating risk requires individual-specific learning, and some migrants are sufficiently close to subsistence that failed migration is very costly. We document evidence consistent with this model using heterogeneity analysis and additional experimental variation, but calibrations with forward-looking households that can save up to migrate suggest that it is difficult for the model to quantitatively match the data. We conclude with extensions to the model that could provide a better quantitative accounting of the behavior.</p> </abstract>
<abstract> <p>We present new identification results for nonparametric models of differentiated products markets, using only market level observables. We specify a nonparametric random utility discrete choice model of demand allowing rich preference heterogeneity, product/market unobservables, and endogenous prices. Our supply model posits nonparametric cost functions, allows latent cost shocks, and nests a range of standard oligopoly models. We consider identification of demand, identification of changes in aggregate consumer welfare, identification of marginal costs, identification of firms' marginal cost functions, and discrimination between alternative models of firm conduct. We explore two complementary approaches. The first demonstrates identification under the same nonparametric instrumental variables conditions required for identification of regression models. The second treats demand and supply in a system of nonparametric simultaneous equations, leading to constructive proofs exploiting exogenous variation in demand shifters and cost shifters. We also derive testable restrictions that provide the first general formalization of Bresnahan's (1982) intuition for empirically distinguishing between alternative models of oligopoly competition. From a practical perspective, our results clarify the types of instrumental variables needed with market level data, including tradeoffs between functional form and exclusion restrictions.</p> </abstract>
<abstract> <p>This paper studies inference in models that are identified by moment restrictions. We show how instability of the moments can be used constructively to improve the identification of structural parameters that are stable over time. A leading example is macroeconomic models that are immune to the well-known (Lucas (1976)) critique in the face of policy regime shifts. This insight is used to develop novel econometric methods that extend the widely used generalized method of moments (GMM). The proposed methods yield improved inference on the parameters of the new Keynesian Phillips curve.</p> </abstract>
<abstract> <p>This paper considers mechanism design problems in environments with ambiguitysensitive individuals. The novel idea is to introduce ambiguity in mechanisms so as to exploit the ambiguity sensitivity of individuals. Deliberate engineering of ambiguity, through ambiguous mediated communication, can allow (partial) implementation of social choice functions that are not incentive compatible with respect to prior beliefs. We provide a complete characterization of social choice functions partially implementable by ambiguous mechanisms.</p> </abstract>
<abstract> <p>We develop an extension of Luce's random choice model to study violations of the weak axiom of revealed preference. We introduce the notion of a stochastic preference and show that it implies the Luce model. Then, to address well-known difficulties of the Luce model, we define the attribute rule and establish that the existence of a well-defined stochastic preference over attributes characterizes it. We prove that the set of attribute rules and random utility maximizers are essentially the same. Finally, we show that both the Luce and attribute rules have a unique consistent extension to dynamic problems.</p> </abstract>
<abstract> <p>Risk aversion (a second-order risk preference) is a time-proven concept in economic models of choice under risk. More recently, the higher order risk preferences of prudence (third-order) and temperance (fourth-order) also have been shown to be quite important. While a majority of the population seems to exhibit both risk aversion and these higher order risk preferences, a significant minority does not. We show how both risk-averse and risk-loving behaviors might be generated by a simple type of basic lottery preference for either (1) combining "good" outcomes with "bad" ones, or (2) combining "good with good" and "bad with bad," respectively. We further show that this dichotomy is fairly robust at explaining higher order risk attitudes in the laboratory. In addition to our own experimental evidence, we take a second look at the extant laboratory experiments that measure higher order risk preferences and we find a fair amount of support for this dichotomy. Our own experiment also is the first to look beyond fourth-order risk preferences, and we examine risk attitudes at even higher orders.</p> </abstract>
<abstract> <p>We axiomatize preferences that can be represented by a monotonic aggregation of subjective expected utilities generated by a utility function and some set of i.i.d. probability measures over a product state space, S ∞ . For such preferences, we define relevant measures, show that they are treated as if they were the only marginals possibly governing the state space, and connect them with the measures appearing in the aforementioned representation. These results allow us to interpret relevant measures as reflecting part of perceived ambiguity, meaning subjective uncertainty about probabilities over states. Under mild conditions, we show that increases or decreases in ambiguity aversion cannot affect the relevant measures. This property, necessary for the conclusion that these measures reflect only perceived ambiguity, distinguishes the set of relevant measures from the leading alternative in the literature. We apply our findings to a number of well-known models of ambiguity-sensitive preferences. For each model, we identify the set of relevant measures and the implications of comparative ambiguity aversion.</p> </abstract>
<abstract> <p>This paper considers the problem of testing a finite number of moment inequalities. We propose a two-step approach. In the first step, a confidence region for the moments is constructed. In the second step, this set is used to provide information about which moments are "negative." A Bonferonni-type correction is used to account for the fact that, with some probability, the moments may not lie in the confidence region. It is shown that the test controls size uniformly over a large class of distributions for the observed data. An important feature of the proposal is that it remains computationally feasible, even when the number of moments is large. The finite-sample properties of the procedure are examined via a simulation study, which demonstrates, among other things, that the proposal remains competitive with existing procedures while being computationally more attractive.</p> </abstract>
<abstract> <p>This paper estimates a structural dynamic equilibrium model of the Brazilian labor market in order to study trade-induced transitional dynamics. The model features a multi-sector economy with overlapping generations, heterogeneous workers, endogenous accumulation of sector-specific experience, and costly switching of sectors. The model's estimates yield median costs of mobility ranging from 1.4 to 2.7 times annual average wages, but a high dispersion of these costs across the population. In addition, sector-specific experience is imperfectly transferable across sectors, leading to additional barriers to mobility. Using the estimated model for counterfactual trade liberalization experiments, the main findings are: (1) there is a large labor market response following trade liberalization but the transition may take several years; (2) potential aggregate welfare gains are significantly reduced due to the delayed adjustment; (3) trade-induced welfare effects depend on initial sector of employment and on worker demographics such as age and education. The experiments also highlight the sensitivity of the transitional dynamics with respect to assumptions regarding the mobility of capital.</p> </abstract>
<abstract> <p>We formulate and solve a range of dynamic models of constrained credit/insurance that allow for moral hazard and limited commitment. We compare them to full insurance and exogenously incomplete financial regimes (autarky, saving only, borrowing and lending in a single asset). We develop computational methods based on mechanism design, linear programming, and maximum likelihood to estimate, compare, and statistically test these alternative dynamic models with financial/information constraints. Our methods can use both cross-sectional and panel data and allow for measurement error and unobserved heterogeneity. We estimate the models using data on Thai households running small businesses from two separate samples. We find that in the rural sample, the exogenously incomplete saving only and borrowing regimes provide the best fit using data on consumption, business assets, investment, and income. Family and other networks help consumption smoothing there, as in a moral hazard constrained regime. In contrast, in urban areas, we find mechanism design financial/information regimes that are decidedly less constrained, with the moral hazard model fitting best combined business and consumption data. We perform numerous robustness checks in both the Thai data and in Monte Carlo simulations and compare our maximum likelihood criterion with results from other metrics and data not used in the estimation. A prototypical counterfactual policy evaluation exercise using the estimation results is also featured.</p> </abstract>
<abstract> <p>In this paper, we compare how two different types of political regimes—direct versus representative democracy—redistribute income toward the relatively poor segments of society after the introduction of universal and equal suffrage. Swedish local governments are used as a testing ground since this setting offers a number of attractive features for a credible impact evaluation. Most importantly, we exploit the existence of a population threshold, which partly determined a local government's choice of democracy to implement a regression-discontinuity design. The results indicate that direct democracies spend 40-60 percent less on public welfare. Our interpretation is that direct democracy may be more prone to elite capture than representative democracy since the elite's potential to exercise de facto power is likely to be greater in direct democracy after democratization.</p> </abstract>
<abstract> <p>We develop and estimate a comprehensive dynamic programming (DP) model for the joint decisions of residential location, employment location, occupational choices, and labor market outcomes. We use data on immigrants from the former Soviet Union (FSU). We provide an extensive empirical evaluation of policies that have been designed to affect the residential and employment location decisions of the migrant population. The results shed new, and important, light on several issues regarding this group of immigrants. We find large regional differences in wages for the white-collar workers, but only little differences for the blue-collar workers. A careful examination of a number of policy measures indicate that a direct subsidy, in the form of a lump-sum transfer, is most effective in achieving the government stated goal of inducing people to reside in the northern region of the Galilee and southern region of the Negev. Other policies, such as rental and wage subsidies, can also be quite effective, but these are more difficult to administer.</p> </abstract>
<abstract> <p>The paper studies how asset prices are determined in a decentralized market with asymmetric information about asset values. We consider an economy in which a large number of agents trade two assets in bilateral meetings. A fraction of the agents has private information about the asset values. We show that, over time, uninformed agents can elicit information from their trading partners by making small offers. This form of experimentation allows the uninformed agents to acquire information as long as there are potential gains from trade in the economy. As a consequence, the economy converges to a Pareto efficient allocation.</p> </abstract>
<abstract> <p>This article asks when communication with certifiable information leads to complete information revelation. We consider Bayesian games augmented by a pre-play communication phase in which announcements are made publicly. We first characterize the augmented games in which there exists a fully revealing sequential equilibrium with extremal beliefs (i.e., any deviation is attributed to a single type of the deviator). Next, we define a class of games for which existence of a fully revealing equilibrium is equivalent to a richness property of the evidence structure. This characterization enables us to provide different sets of sufficient conditions for full information disclosure that encompass and extend all known results in the literature, and are easily applicable. We use these conditions to obtain new insights in games with strategic complementarities, voting with deliberation, and persuasion games with multidimensional types.</p> </abstract>
<abstract> <p>We test the portability of level-0 assumptions in level-k theory in an experimental investigation of behavior in Coordination, Discoordination, and Hide and Seek games with common, non-neutral frames. Assuming that level-0 behavior depends only on the frame, we derive hypotheses that are independent of prior assumptions about salience. Those hypotheses are not confirmed. Our findings contrast with previous research which has fitted parameterized level-k models to Hide and Seek data. We show that, as a criterion of successful explanation, the existence of a plausible model that replicates the main patterns in these data has a high probability of false positives.</p> </abstract>
<abstract> <p>We model a boundedly rational agent who suffers from limited attention. The agent considers each feasible alternative with a given (unobservable) probability, the attention parameter, and then chooses the alternative that maximizes a preference relation within the set of considered alternatives. We show that this random choice rule is the only one for which the impact of removing an alternative on the choice probability of any other alternative is asymmetric and menu independent. Both the preference relation and the attention parameters are identified uniquely by stochastic choice data.</p> </abstract>
<abstract> <p>Local to unity limit theory is used in applications to construct confidence intervals (CIs) for autoregressive roots through inversion of a unit root test (Stock (1991)). Such CIs are asymptotically valid when the true model has an autoregressive root that is local to unity (ρ = 1 + $\frac{\mathrm{c}}{\mathrm{n}}$ ), but are shown here to be invalid at the limits of the domain of definition of the localizing coefficient c because of a failure in tightness and the escape of probability mass. Failure at the boundary implies that these CIs have zero asymptotic coverage probability in the stationary case and vicinities of unity that are wider than O(n -1/3 ). The inversion methods of Hansen (1999) and Mikusheva (2007) are asymptotically valid in such cases. Implications of these results for predictive regression tests are explored. When the predictive regressor is stationary, the popular Campbell and Yogo (2006) CIs for the regression coefficient have zero coverage probability asymptotically, and their predictive test statistic Q erroneously indicates predictability with probability approaching unity when the null of no predictability holds. These results have obvious cautionary implications for the use of the procedures in empirical practice.</p> </abstract>
<abstract> <p>I discuss the failure of the canonical search and matching model to match the cyclical volatility in the job finding rate. I show that job creation in the model is influenced by wages in new matches. I summarize microeconometric evidence and find that wages in new matches are volatile and consistent with the model's key predictions. Therefore, explanations of the unemployment volatility puzzle have to preserve the cyclical volatility of wages. I discuss a modification of the model, based on fixed matching costs, that can increase cyclical unemployment volatility and is consistent with wage flexibility in new matches.</p> </abstract>
<abstract> <p>I study individuals who use frequentist models to draw uniform inferences from independent and identically distributed data. The main contribution of this paper is to show that distinct models may be consistent with empirical evidence, even in the limit when data increases without bound. Decision makers may then hold different beliefs and interpret their environment differently even though they know each other's model and base their inferences on the same evidence. The behavior modeled here is that of rational individuals confronting an environment in which learning is hard, rather than individuals beset by cognitive limitations or behavioral biases.</p> </abstract>
<abstract> <p>The econometric literature of high frequency data often relies on moment estimators which are derived from assuming local constancy of volatility and related quantities. We here study this local-constancy approximation as a general approach to estimation in such data. We show that the technique yields asymptotic properties (consistency, normality) that are correct subject to an ex post adjustment involving asymptotic likelihood ratios. These adjustments are derived and documented. Several examples of estimation are provided: powers of volatility, leverage effect, and integrated betas. The first order approximations based on local constancy can be over the period of one observation or over blocks of successive observations. It has the advantage of gaining in transparency in defining and analyzing estimators. The theory relies heavily on the interplay between stable convergence and measure change, and on asymptotic expansions for martingales.</p> </abstract>
<abstract> <p>In this paper we study high-dimensional time series that have the generalized dynamic factor structure. We develop a test of the null of k₀ factors against the alternative that the number of factors is larger than k₀ but no larger than k₁ &gt; k₀. Our test statistic equals ${\rm max}_{k_{0}&lt;k\leq k_{1}}(\gamma _{k}-\gamma _{k+1})/(\gamma _{k+1}-\gamma _{k+2})$ , where $\gamma _{i}$ is the ith largest eigenvalue of the smoothed periodogram estimate of the spectral density matrix of data at a prespecified frequency. We describe the asymptotic distribution of the statistic, as the dimensionality and the number of observations rise, as a function of the Tracy—Widom distribution and tabulate the critical values of the test. As an application, we test different hypotheses about the number of dynamic factors in macroeconomic time series and about the number of dynamic factors driving excess stock returns.</p> </abstract>
<abstract> <p>This paper uses control variables to indentify and estimate models with nonseparable, multidimensional disturbances. Triangular simultaneous equations models are considered, with instruments and disturbances that are independent and a reduced form that is strictly monotonic in a scalar disturbance. Here it is shown that the conditional cumulative distribution function of the endogenous variable given the instruments is a control variable. Also, for any control variable, identification results are given for quantile, average, and policy effects. Bounds are given when a common support assumption is not satisfied. Estimators of identified objects and bounds are provided, and a demand analysis empirical example is given.</p> </abstract>
<abstract> <p>We solve for the equilibrium dynamics of information sharing in a large population. Each agent is endowed with signals regarding the likely outcome of a random variable of common concern. Individuals choose the effort with which they search for others from whom they can gather additional information. When two agents meet, they share their information. The information gathered is further shared at subsequent meetings, and so on. Equilibria exist in which agents search maximally until they acquire sufficient information precision and then search minimally. A tax whose proceeds are used to subsidize the costs of search improves information sharing and can, in some cases, increase welfare. On the other hand, endowing agents with public signals reduces information sharing and can, in some cases, decrease welfare.</p> </abstract>
<abstract> <p>Anscombe and Aumann (1963) wrote a classic characterization of subjective expected utility theory. This paper employs the same domain for preference and a closely related (but weaker) set of axioms to characterize preferences that use second-order beliefs (beliefs over probability measures). Such preferences are of interest because they accommodate Ellsberg-type behavior.</p> </abstract>
<abstract> <p>A norm of 50-50 division appears to have considerable force in a wide range of economic environments, both in the real world and in the laboratory. Even in settings where one party unilaterally determines the allocation of a prize (the dictator game), many subjects voluntarily cede exactly half to another individual. The hypothesis that people care about fairness does not by itself account for key experimental patterns. We consider an alternative explanation, which adds the hypothesis that people like to be perceived as fair. The properties of equilibria for the resulting signaling game correspond closely to laboratory observations. The theory has additional testable implications, the validity of which we confirm through new experiments.</p> </abstract>
<abstract> <p>We use a controlled experiment to explore whether there are gender differences in selecting into competitive environments across two distinct societies: the Maasai in Tanzania and the Khasi in India. One unique aspect of these societies is that the Maasai represent a textbook example of a patriarchal society, whereas the Khasi are matrilineal. Similar to the extant evidence drawn from experiments executed in Western cultures, Maasai men opt to compete at roughly twice the rate as Maasai women. Interestingly, this result is reversed among the Khasi, where women choose the competitive environment more often than Khasi men, and even choose to compete weakly more often than Maasai men. These results provide insights into the underpinnings of the factors hypothesized to be determinants of the observed gender differences in selecting into competitive environments.</p> </abstract>
<abstract> <p>This paper develops a method for inference in dynamic discrete choice models with serially correlated unobserved state variables. Estimation of these models involves computing high-dimensional integrals that are present in the solution to the dynamic program and in the likelihood function. First, the paper proposes a Bayesian Markov chain Monte Carlo estimation procedure that can handle the problem of multidimensional integration in the likelihood function. Second, the paper presents an efficient algorithm for solving the dynamic program suitable for use in conjunction with the proposed estimation procedure.</p> </abstract>
<abstract> <p>This paper develops asymptotic optimality theory for statistical treatment rules in smooth parametric and semiparametric models. Manski (2000, 2002, 2004) and Dehejia (2005) have argued that the problem of choosing treatments to maximize social welfare is distinct from the point estimation and hypothesis testing problems usually considered in the treatment effects literature, and advocate formal analysis of decision procedures that map empirical data into treatment choices. We develop large-sample approximations to statistical treatment assignment problems using the limits of experiments framework. We then consider some different loss functions and derive treatment assignment rules that are asymptotically optimal under average and minmax risk criteria.</p> </abstract>
<abstract> <p>I study asset prices in a two-agent macroeconomic model with two key features: limited stock market participation and heterogeneity in the elasticity of intertemporal substitution in consumption (EIS). The model is consistent with some prominent features of asset prices, such as a high equity premium, relatively smooth interest rates, procyclical stock prices, and countercyclical variation in the equity premium, its volatility, and in the Sharpe ratio. In this model, the risk-free asset market plays a central role by allowing non-stockholders (with low EIS) to smooth the fluctuations in their labor income. This process concentrates non-stockholders' labor income risk among a small group of stockholders, who then demand a high premium for bearing the aggregate equity risk. Furthermore, this mechanism is consistent with the very small share of aggregate wealth held by non-stockholders in the U.S. data, which has proved problematic for previous models with limited participation. I show that this large wealth inequality is also important for the model's ability to generate a countercyclical equity premium. When it comes to business cycle performance, the model's progress has been more limited: consumption is still too volatile compared to the data, whereas investment is still too smooth. These are important areas for potential improvement in this framework.</p> </abstract>
<abstract> <p>In this paper, we build a model where the presence of liquidity constraints tends to magnify the economy's response to aggregate shocks. We consider a decentralized model of trade, where agents may use money or credit to buy goods. When agents do not have access to credit and the real value of money balances is low, agents are more likely to be liquidity constrained. This makes them more concerned about their short-term earning prospects when making their consumption decisions and about their short-term spending opportunities when making their production decisions. This generates a coordination element in spending and production which leads to greater aggregate volatility and greater comovement across producers.</p> </abstract>
<abstract> <p>We provide a practical method to estimate the payoff functions of players in complete information, static, discrete games. With respect to the empirical literature on entry games originated by Bresnahan and Reiss (1990) and Berry (1992), the main novelty of our framework is to allow for general forms of heterogeneity across players without making equilibrium selection assumptions. We allow the effects that the entry of each individual airline has on the profits of its competitors, its "competitive effects," to differ across airlines. The identified features of the model are sets of parameters (partial identification) such that the choice probabilities predicted by the econometric model are consistent with the empirical choice probabilities estimated from the data. We apply this methodology to investigate the empirical importance of firm heterogeneity as a determinant of market structure in the U.S. airline industry. We find evidence of heterogeneity across airlines in their profit functions. The competitive effects of large airlines (American, Delta, United) are different from those of low cost carriers and Southwest. Also, the competitive effect of an airline is increasing in its airport presence, which is an important measure of observable heterogeneity in the airline industry. Then we develop a policy experiment to estimate the effect of repealing the Wright Amendment on competition in markets out of the Dallas airports. We find that repealing the Wright Amendment would increase the number of markets served out of Dallas Love.</p> </abstract>
<abstract> <p>Conventional wisdom suggests that increased life expectancy had a key role in causing a rise in investment in human capital. I incorporate the retirement decision into a version of Ben-Porath's (1967) model and find that a necessary condition for this causal relationship to hold is that increased life expectancy will also increase lifetime labor supply. I then show that this condition does not hold for American men born between 1840 and 1970 and for the American population born between 1890 and 1970. The data suggest similar patterns in Western Europe. I end by discussing the implications of my findings for the debate on the fundamental causes of long-run growth.</p> </abstract>
<abstract> <p>We propose a new methodology for structural estimation of infinite horizon dynamic discrete choice models. We combine the dynamic programming (DP) solution algorithm with the Bayesian Markov chain Monte Carlo algorithm into a single algorithm that solves the DP problem and estimates the parameters simultaneously. As a result, the computational burden of estimating a dynamic model becomes comparable to that of a static model. Another feature of our algorithm is that even though the number of grid points on the state variable is small per solution-estimation iteration, the number of effective grid points increases with the number of estimation iterations. This is how we help ease the "curse of dimensionality." We simulate and estimate several versions of a simple model of entry and exit to illustrate our methodology. We also prove that under standard conditions, the parameters converge in probability to the true posterior distribution, regardless of the starting values.</p> </abstract>
<abstract> <p>Nonparametric estimation of a structural cointegrating regression model is studied. As in the standard linear cointegrating regression model, the regressor and the dependent variable are jointly dependent and contemporaneously correlated. In nonparametric estimation problems, joint dependence is known to be a major complication that affects identification, induces bias in conventional kernel estimates, and frequently leads to ill-posed inverse problems. In functional cointegrating regressions where the regressor is an integrated or near-integrated time series, it is shown here that inverse and ill-posed inverse problems do not arise. Instead, simple nonparametric kernel estimation of a structural nonparametric cointegrating regression is consistent and the limit distribution theory is mixed normal, giving straightforward asymptotics that are useable in practical work. It is further shown that use of augmented regression, as is common in linear cointegration modeling to address endogeneity, does not lead to bias reduction in nonparametric regression, but there is an asymptotic gain in variance reduction. The results provide a convenient basis for inference in structural nonparametric regression with nonstationary time series when there is a single integrated or near-integrated regressor. The methods may be applied to a range of empirical models where functional estimation of cointegrating relations is required.</p> </abstract>
<abstract> <p>We identify a new way to order functions, called the interval dominance order, that generalizes both the single crossing property and a standard condition used in statistical decision theory. This allows us to provide a unified treatment of the major theorems on monotone comparative statics with and without uncertainty, the comparison of signal informativeness, and a non-Bayesian theorem on the completeness of increasing decision rules. We illustrate the concept and results with various applications, including an application to optimal stopping time problems where the single crossing property is typically violated.</p> </abstract>
<abstract> <p>Information asymmetries are important in theory but difficult to identify in practice. We estimate the presence and importance of hidden information and hidden action problems in a consumer credit market using a new field experiment methodology. We randomized 58,000 direct mail offers to former clients of a major South African lender along three dimensions: (i) an initial "offer interest rate" featured on a direct mail solicitation; (ii) a "contract interest rate" that was revealed only after a borrower agreed to the initial offer rate; and (ii) a dynamic repayment incentive that was also a surprise and extended preferential pricing on future loans to borrowers who remained in good standing. These three randomizations, combined with complete knowledge of the lender's information set, permit identification of specific types of private information problems. Our setup distinguishes hidden information effects from selection on the offer rate (via unobservable risk and anticipated effort), from hidden action effects (via moral hazard in effort) induced by actual contract terms. We find strong evidence of moral hazard and weaker evidence of hidden information problems. A rough estimate suggests that perhaps 13% to 21% of default is due to moral hazard. Asymmetric information thus may help explain the prevalence of credit constraints even in a market that specializes in financing high-risk borrowers.</p> </abstract>
<abstract> <p>We show by counterexample that Proposition 2 in Fernández-Villaverde, Rubio-Ramírez, and Santos (Econometrica (2006), 74, 93—119) is false. We also show that even if their Proposition 2 were corrected, it would be irrelevant for parameter estimates. As a more constructive contribution, we consider the effects of approximation error on parameter estimation, and conclude that second order approximation errors in the policy function have at most second order effects on parameter estimates.</p> </abstract>
<abstract> <p>The absence of state capacities to raise revenue and to support markets is a key factor in explaining the persistence of weak states. This paper reports on an ongoing project to investigate the incentive to invest in such capacities. The paper sets out a simple analytical structure in which state capacities are modeled as forward looking investments by government. The approach highlights some determinants of state building including the risk of external or internal conflict, the degree of political instability, and dependence on natural resources. Throughout, we link these state capacity investments to patterns of development and growth.</p> </abstract>
<abstract> <p>We construct a new index of media slant that measures the similarity of a news outlet's language to that of a congressional Republican or Democrat. We estimate a model of newspaper demand that incorporates slant explicitly, estimate the slant that would be chosen if newspapers independently maximized their own profits, and compare these profit-maximizing points with firms' actual choices. We find that readers have an economically significant preference for like-minded news. Firms respond strongly to consumer preferences, which account for roughly 20 percent of the variation in measured slant in our sample. By contrast, the identity of a newspaper's owner explains far less of the variation in slant.</p> </abstract>
<abstract> <p>We study a continuous-time principal—agent model in which a risk-neutral agent with limited liability must exert unobservable effort to reduce the likelihood of large but relatively infrequent losses. Firm size can be decreased at no cost or increased subject to adjustment costs. In the optimal contract, investment takes place only if a long enough period of time elapses with no losses occurring. Then, if good performance continues, the agent is paid. As soon as a loss occurs, payments to the agent are suspended, and so is investment if further losses occur. Accumulated bad performance leads to downsizing. We derive explicit formulae for the dynamics of firm size and its asymptotic growth rate, and we provide conditions under which firm size eventually goes to zero or grows without bounds.</p> </abstract>
<abstract> <p>The topic of this paper is inference in models in which parameters are defined by moment inequalities and/or equalities. The parameters may or may not be identified. This paper introduces a new class of confidence sets and tests based on generalized moment selection (GMS). GMS procedures are shown to have correct asymptotic size in a uniform sense and are shown not to be asymptotically conservative. The power of GMS tests is compared to that of subsampling, m out of n bootstrap, and "plug-in asymptotic" (PA) tests. The latter three procedures are the only general procedures in the literature that have been shown to have correct asymptotic size (in a uniform sense) for the moment inequality/equality model. GMS tests are shown to have asymptotic power that dominates that of subsampling, m out of n bootstrap, and PA tests. Subsampling and m out of n bootstrap tests are shown to have asymptotic power that dominates that of PA tests.</p> </abstract>
<abstract> <p>This paper considers a panel data model for predicting a binary outcome. The conditional probability of a positive response is obtained by evaluating a given distribution function (F) at a linear combination of the predictor variables. One of the predictor variables is unobserved. It is a random effect that varies across individuals but is constant over time. The semiparametric aspect is that the conditional distribution of the random effect, given the predictor variables, is unrestricted. This paper has two results. If the support of the observed predictor variables is bounded, then identification is possible only in the logistic case. Even if the support is unbounded, so that (from Manski (1987)) identification holds quite generally, the information bound is zero unless F is logistic. Hence consistent estimation at the standard pn rate is possible only in the logistic case.</p> </abstract>
<abstract> <p>This paper provides computationally intensive, yet feasible methods for inference in a very general class of partially identified econometric models. Let P denote the distribution of the observed data. The class of models we consider is defined by a population objective function Q(θ, P) for θ ∈ Θ. The point of departure from the classical extremum estimation framework is that it is not assumed that Q(θ, P) has a unique minimizer in the parameter space Θ. The goal may be either to draw inferences about some unknown point in the set of minimizers of the population objective function or to draw inferences about the set of minimizers itself. In this paper, the object of interest is $\Theta _{0}(P)={\rm arg}\ {\rm min}_{\theta \in \Theta }Q(\theta ,P)$ , and so we seek random sets that contain this set with at least some prespecified probability asymptotically. We also consider situations where the object of interest is the image of Θ₀(P) under a known function. Random sets that satisfy the desired coverage property are constructed under weak assumptions. Conditions are provided under which the confidence regions are asymptotically valid not only pointwise in P, but also uniformly in P. We illustrate the use of our methods with an empirical study of the impact of top-coding outcomes on inferences about the parameters of a linear regression. Finally, a modest simulation study sheds some light on the finite-sample behavior of our procedure.</p> </abstract>
<abstract> <p>We present a model in which a principal delegates the choice of project to an agent with different preferences. The principal determines the set of projects from which the agent may choose. The principal can verify the characteristics of the project chosen by the agent, but does not know which other projects were available to the agent. We consider situations where the collection of available projects is exogenous to the agent but uncertain, where the agent must invest effort to discover a project, where the principal can pay the agent to choose a desirable project, and where the principal can adopt more complex schemes than simple permission sets.</p> </abstract>
<abstract> <p>We consider a model of strategic trading with asymmetric information of an asset whose value follows a Brownian motion. An insider continuously observes a signal that tracks the evolution of the asset's fundamental value. The value of the asset is publicly revealed at a random time. The equilibrium has two regimes separated by an endogenously determined time T. In [0, T), the insider gradually transfers her information to the market. By time T, all her information has been transferred and the price agrees with the market value of the asset. In the interval [T, ∞), the insider trades large volumes and reveals her information immediately, so market prices track the market value perfectly. Despite this market efficiency, the insider is able to collect strictly positive rents after T.</p> </abstract>
<abstract> <p>This paper studies partnerships that employ a mediator to improve their contractual ability. Intuitively, profitable deviations must be attributable, that is, there must be some group behavior such that an individual can be statistically identified as innocent, to provide incentives in partnerships. Mediated partnerships add value by effectively using different behavior to attribute different deviations. As a result, mediated partnerships are necessary to provide the right incentives in a wide range of economic environments.</p> </abstract>
<abstract> <p>We prove the generic existence of a recursive equilibrium for overlapping-generations economies with uncertainty. "Generic" here means in a residual set of utilities and endowments. The result holds provided there is a sufficient number of potentially different individuals within each cohort.</p> </abstract>
<abstract> <p>We use a preference-over-menus framework to model a decision maker who is affected by multiple temptations. Our two main axioms on preference—exclusion and inclusion—identify when the agent would want to restrict his choice set and when he would want to expand his choice set. An agent who is tempted would want to restrict his choice set by excluding the normatively worst alternative of that choice set. Simultaneously, he would want to expand his choice set by including a normatively superior alternative. Our representation identifies the agent's normative preference and temptations, and suggests the agent is uncertain which of these temptations will affect him. We provide examples to illustrate how our model improves on those of Gul and Pesendorfer (2001) and Dekel, Lipman, and Rustichini (2009).</p> </abstract>
<abstract> <p>This paper develops methods for evaluating marginal policy changes. We characterize how the effects of marginal policy changes depend on the direction of the policy change, and show that marginal policy effects are fundamentally easier to identify and to estimate than conventional treatment parameters. We develop the connection between marginal policy effects and the average effect of treatment for persons on the margin of indifference between participation in treatment and nonparticipation, and use this connection to analyze both parameters. We apply our analysis to estimate the effect of marginal changes in tuition on the return to going to college.</p> </abstract>
<abstract> <p>An emerging literature in time series econometrics concerns the modeling of potentially nonlinear temporal dependence in stationary Markov chains using copula functions. We obtain sufficient conditions for a geometric rate of mixing in models of this kind. Geometric β-mixing is established under a rather strong sufficient condition that rules out asymmetry and tail dependence in the copula function. Geometric ρ-mixing is obtained under a weaker condition that permits both asymmetry and tail dependence. We verify one or both of these conditions for a range of parametric copula functions that are popular in applied work.</p> </abstract>
<abstract> <p>The paper studies the implementation problem, first analyzed by Maskin and Moore (1999), in which two agents observe an unverifiable state of nature and may renegotiate inefficient outcomes following play of the mechanism. We develop a first-order approach to characterizing the set of implementable utility mappings in this problem, paralleling Mirrlees's (1971) first-order analysis of standard mechanism design problems. We use this characterization to study optimal contracting in hold-up and risk-sharing models. In particular, we examine when the contracting parties can optimally restrict attention to simple contracts, such as noncontingent contracts and option contracts (where only one agent sends a message).</p> </abstract>
<abstract> <p>This paper estimates a structural model of optimal life-cycle consumption expenditures in the presence of realistic labor income uncertainty. We employ synthetic cohort techniques and Consumer Expenditure Survey data to construct average age-profiles of consumption and income over the working lives of typical households across different education and occupation groups. The model fits the profiles quite well. In addition to providing reasonable estimates of the discount rate and risk aversion, we find that consumer behavior changes strikingly over the life cycle. Young consumers behave as buffer-stock agents. Around age 40, the typical household starts accumulating liquid assets for retirement and its behavior mimics more closely that of a certainty equivalent consumer. Our methodology provides a natural decomposition of saving and wealth into its precautionary and life-cycle components.</p> </abstract>
<abstract> <p>This paper reports estimates of the effects of JTPA training programs on the distribution of earnings. The estimation uses a new instrumental variable (IV) method that measures program impacts on quantiles. The quantile treatment effects (QTE) estimator reduces to quantile regression when selection for treatment is exogenously determined. QTE can be computed as the solution to a convex linear programming problem, although this requires first-step estimation of a nuisance function. We develop distribution theory for the case where the first step is estimated nonparametrically. For women, the empirical results show that the JTPA program had the largest proportional impact at low quantiles. Perhaps surprisingly, however, JTPA training raised the quantiles of earnings for men only in the upper half of the trainee earnings distribution.</p> </abstract>
<abstract> <p>This paper establishes the higher-order equivalence of the k-step bootstrap, introduced recently by Davidson and MacKinnon (1999), and the standard bootstrap. The k-step bootstrap is a very attractive alternative computationally to the standard bootstrap for statistics based on nonlinear extremum estimators, such as generalized method of moment and maximum likelihood estimators. The paper also extends results of Hall and Horowitz (1996) to provide new results regarding the higher-order improvements of the standard bootstrap and the k-step bootstrap for extremum estimators (compared to procedures based on first-order asymptotics). The results of the paper apply to Newton-Raphson (NR), default NR, line-search NR, and Gauss-Newton k-step bootstrap procedures. The results apply to the nonparametric iid bootstrap and nonoverlapping and overlapping block bootstraps. The results cover symmetric and equal-tailed two-sided t tests and confidence intervals, one-sided t tests and confidence intervals, Wald tests and confidence regions, and J tests of over-identifying restrictions.</p> </abstract>
<abstract> <p>We develop a new specification test for IV estimators adopting a particular second order approximation of Bekker. The new specification test compares the difference of the forward (conventional) 2SLS estimator of the coefficient of the right-hand side endogenous variable with the reverse 2SLS estimator of the same unknown parameter when the normalization is changed. Under the null hypothesis that conventional first order asymptotics provide a reliable guide to inference, the two estimates should be very similar. Our test sees whether the resulting difference in the two estimates satisfies the results of second order asymptotic theory. Essentially the same idea is applied to develop another new specification test using second-order unbiased estimators of the type first proposed by Nagar. If the forward and reverse Nagar-type estimators are not significantly different we recommend estimation by LIML, which we demonstrate is the optimal linear combination of the Nagar-type estimators (to second order). We also demonstrate the high degree of similarity for k-class estimators between the approach of Bekker and the Edgeworth expansion approach of Rothenberg. An empirical example and Monte Carlo evidence demonstrate the operation of the new specification test.</p> </abstract>
<abstract> <p>In this paper we develop some econometric theory for factor models of large dimensions. The focus is the determination of the number of factors (r), which is an unresolved issue in the rapidly growing literature on multifactor models. We first establish the convergence rate for the factor estimates that will allow for consistent estimation of r. We then propose some panel criteria and show that the number of factors can be consistently estimated using the criteria. The theory is developed under the framework of large cross-sections (N) and large time dimensions (T). No restriction is imposed on the relation between N and T. Simulations show that the proposed criteria have good finite sample properties in many configurations of the panel data encountered in practice.</p> </abstract>
<abstract> <p>When a continuous-time diffusion is observed only at discrete dates, in most cases the transition distribution and hence the likelihood function of the observations is not explicitly computable. Using Hermite polynomials, I construct an explicit sequence of closed-form functions and show that it converges to the true (but unknown) likelihood function. I document that the approximation is very accurate and prove that maximizing the sequence results in an estimator that converges to the true maximum likelihood estimator and shares its asymptotic properties. Monte Carlo evidence reveals that this method outperforms other approximation schemes in situations relevant for financial models.</p> </abstract>
<abstract> <p>We present an axiomatic model depicting the choice behavior of a self-interest seeking moral individual over random allocation procedures. Individual preferences are decomposed into a self-interest component and a component representing the individual's moral value judgment. Each component has a distinct utility representation, and the preference relation depicting the choice behavior is representable by a real-valued function defined on the components utilities. The utility representing the self-interest component is linear and the utility representing the individual's moral value judgment is quasi-concave. The addition of a hexagon condition implies that the utility representing the individual's preference is additively separable in the components utilities.</p> </abstract>
<abstract> <p> This paper analyzes the complexity of the contraction fixed point problem: compute an ε-approximation to the fixed point V* = Γ(V*) of a contraction mapping Γ that maps a Banach space B&lt;sub&gt;d&lt;/sub&gt; of continuous functions of d variables into itself. We focus on quasi linear contractions where Γ is a nonlinear functional of a finite number of conditional expectation operators. This class includes contractive Fredholm integral equations that arise in asset pricing applications and the contractive Bellman equation from dynamic programming. In the absence of further restrictions on the domain of Γ, the quasi linear fixed point problem is subject to the curse of dimensionality, i.e., in the worst case the minimal number of function evaluations and arithmetic operations required to compute an ε-approximation to a fixed point &lt;tex-math&gt;$V^{\ast}\in B_{d}$&lt;/tex-math&gt; increases exponentially in d. We show that the curse of dimensionality disappears if the domain of Γ has additional special structure. We identify a particular type of special structure for which the problem is strongly tractable even in the worst case, i.e., the number of function evaluations and arithmetic operations needed to compute an ε-approximation of V* is bounded by &lt;tex-math&gt;$C\varepsilon ^{-p}$&lt;/tex-math&gt; where C and p are constants independent of d. We present examples of economic problems that have this type of special structure including a class of rational expectations asset pricing problems for which the optimal exponent p = 1 is nearly achieved. </p> </abstract>
<abstract> <p>The paper first develops an economic analysis of the concept of shareholder value, describes its approach, and discusses some open questions. It emphasizes the relationship between pledgeable income, monitoring, and control rights using a unifying and simple framework. The paper then provides a first 2and preliminary analysis of the concept of the stakeholder society. It investigates whether the managerial incentives and the control structure described in the first part can be modified so as to promote the stakeholder society. It shows that the implementation of the stakeholder society strikes three rocks: dearth of pledgeable income, deadlocks in decision-making, and lack of clear mission for management. While it fares better than the stakeholder society on those three grounds, shareholder value generates biased decision-making; the paper analyzes the costs and benefits of various methods of protecting noncontrolling stakeholders: covenants, exit options, flat claims, enlarged fiduciary duty.</p> </abstract>
<abstract> <p>We consider discriminatory and uniform price auctions for multiple identical units of a good. Players have private values, possibly asymmetrically distributed and for multiple units. Our setting allows for aggregate uncertainty about demand and supply. In this setting, equlibria generally will be inefficient. Despite this, we show that such auctions become arbitrarily close to efficient if they are "large," and use this to derive an asymptotic characterization of revenue and bidding behavior.</p> </abstract>
<abstract> <p>The fiscal theory says that the price level is determined by the ratio of nominal debt to the present value of real primary surpluses. I analyze long-term debt and optimal policy in the fiscal theory. I find that the maturity structure of the debt matters. For example, it determines whether news of future deficits implies current inflation or future inflation. When long-term debt is present, the government can trade current inflation for future inflation by debt operations; this tradeoff is not present if the government rolls over short-term debt. The maturity structure of outstanding debt acts as a "budget constraint" determining which periods' price levels the government can affect by debt variation alone. In addition, debt policy-the expected pattern of future state-contingent debt sales, repurchases and redemptions-matters crucially for the effects of a debt operation. I solve for optimal debt policies to minimize the variance of inflation. I find cases in which long-term debt helps to stabilize inflation. I also find that the optimal policy produces time series that are similar to U.S. surplus and debt time series. To understand the data, I must assume that debt policy offsets the inflationary impact of cyclical surplus shocks, rather than causing price level disturbances by policy-induced shocks. Shifting the objective from price level variance to inflation variance, the optimal policy produces much less volatile inflation at the cost of a unit root in the price level; this is consistent with the stabilization of U.S. inflation after the gold standard was abandoned.</p> </abstract>
<abstract> <p>An asymptotic theory is developed for nonlinear regression with integrated processes. The models allow for nonlinear effects from unit root time series and therefore deal with the case of parametric nonlinear cointegration. The theory covers integrable and asymptotically homogeneous functions. Sufficient conditions for weak consistency are given and a limit distribution theory is provided. The rates of convergence depend on the properties of the nonlinear regression function, and are shown to be as slow as n&lt;sup&gt;1/4&lt;/sup&gt; for integrable functions, and to be generally polynomial in n&lt;sup&gt;1/2&lt;/sup&gt; for homogeneous functions. For regressions with integrable functions, the limiting distribution theory is mixed normal with mixing variates that depend on the sojourn time of the limiting Brownian motion of the integrated process.</p> </abstract>
<abstract> <p>We study a coordination game with randomly changing payoffs and small frictions in changing actions. Using only backwards induction, we find that players must coordinate on the risk-dominant equilibrium. More precisely, a continuum of fully rational players are randomly matched to play a symmetric 2 × 2 game. The payoff matrix changes according to a random walk. Players observe these payoffs and the population distribution of actions as they evolve. The game has frictions: opportunities to change strategies arrive from independent random processes, so that the players are locked into their actions for some time. As the frictions disappear, each player ignores what the others are doing and switches at her first opportunity to the risk-dominant action. History dependence emerges in some cases when frictions remain positive.</p> </abstract>
<abstract> <p>We prove a Folk Theorem for asynchronously repeated games in which the set of players who can move in period t, denoted by I&lt;sub&gt;t&lt;/sub&gt;, is a random variable whose distribution is a function of the past action choices of the players and the past realizations of &lt;tex-math&gt;$I_{\tau}\text{'}{\rm s},\tau =1,2,\ldots ,t-1$&lt;/tex-math&gt;. We impose a condition, the finite periods of inaction (FPI) condition, which requires that the number of periods in which every player has at least one opportunity to move is bounded. Given the FPI condition together with the standard nonequivalent utilities (NEU) condition, we show that every feasible and strictly individually rational payoff vector can be supported as a subgame perfect equilibrium outcome of an asynchronously repeated game.</p> </abstract>
<abstract> <p> This paper proposes a new framework for determining whether a given relationship is nonlinear, what the nonlinearity looks like, and whether it is adequately described by a particular parametric model. The paper studies a regression or forecasting model of the form &lt;tex-math&gt;$y_{t}=\mu ({\bf x}_{t})+\varepsilon _{t}$&lt;/tex-math&gt; where the functional form of μ(·) is unknown. We propose viewing μ(·) itself as the outcome of a random process. The paper introduces a new stationary random field m(·) that generalizes finite-differenced Brownian motion to a vector field and whose realizations could represent a broad class of possible forms for μ(·). We view the parameters that characterize the relation between a given realization of m(·) and the particular value of μ(·) for a given sample as population parameters to be estimated by maximum likelihood or Bayesian methods. We show that the resulting inference about the functional relation also yields consistent estimates for a broad class of deterministic functions μ(·). The paper further develops a new test of the null hypothesis of linearity based on the Lagrange multiplier principle and small-sample confidence intervals based on numerical Bayesian methods. An empirical application suggests that properly accounting for the nonlinearity of the inflation-unemployment trade-off may explain the previously reported uneven empirical success of the Phillips Curve. </p> </abstract>
<abstract> <p>This paper compares two different models in a common environment. The first model has liquidity constraints in that consumers save a single asset that they cannot sell short. The second model has debt constraints in that consumers cannot borrow so much that they would want to default, but is otherwise a standard complete markets model. Both models share the features that individuals are unable to completely insure against idiosyncratic shocks and that interest rates are lower than subjective discount rates. In a stochastic environment, the two models have quite different dynamic properties, with the debt constrained model exhibiting simple stochastic steady states, while the liquidity constrained model has greater persistence of shocks.</p> </abstract>
<abstract> <p>We develop a new test of a parametric model of a conditional mean function against a nonparametric alternative. The test adapts to the unknown smoothness of the alternative model and is uniformly consistent against alternatives whose distance from the parametric model converges to zero at the fastest possible rate. This rate is slower than n&lt;sup&gt;-1/2&lt;/sup&gt;. Some existing tests have nontrivial power against restricted classes of alternatives whose distance from the parametric model decreases at the rate n&lt;sup&gt;-1/2&lt;/sup&gt;. There are, however, sequences of alternatives against which these tests are inconsistent and ours is consistent. As a consequence, there are alternative models for which the finite-sample power of our test greatly exceeds that of existing tests. This conclusion is illustrated by the results of some Monte Carlo experiments.</p> </abstract>
<abstract> <p>We study the implications of imperfect information for term structures of credit spreads on corporate bonds. We suppose that bond investors cannot observe the issuer's assets directly, and receive instead only periodic and imperfect accounting reports. For a setting in which the assets of the firm are a geometric Brownian motion until informed equityholders optimally liquidate, we derive the conditional distribution of the assets, given accounting data and survivorship. Contrary to the perfect-information case, there exists a default-arrival intensity process. That intensity is calculated in terms of the conditional distribution of assets. Credit yield spreads are characterized in terms of accounting information. Generalizations are provided.</p> </abstract>
<abstract> <p>Regulation requiring insiders to publicly disclose their stock trades after the fact complicates the trading decisions of informed, rent-seeking insiders. Given this requirement, we present an insider's equilibrium trading strategy in a multiperiod rational expectations framework. Relative to Kyle (1985), price discovery is accelerated and insider profits are lower. The strategy balances immediate profits from informed trades against the reduction in future profits following trade disclosure and, hence, revelation of some of the insider's information. Our results offer a novel rationale for contrarian trading: dissimulation, a phenomenon distinct from manipulation, may underlie insiders' trading decisions.</p> </abstract>
<abstract> <p>This paper considers testing problems where several of the standard regularity conditions fail to hold. We consider the case where (i) parameter vectors in the null hypothesis may lie on the boundary of the maintained hypothesis and (ii) there may be a nuisance parameter that appears under the alternative hypothesis, but not under the null. The paper establishes the asymptotic null and local alternative distributions of quasi-likelihood ratio, rescaled quasi-likelihood ratio, Wald, and score tests in this case. The results apply to tests based on a wide variety of extremum estimators and apply to a wide variety of models. Examples treated in the paper are: (i) tests of the null hypothesis of no conditional heteroskedasticity in a GARCH(1, 1) regression model and (ii) tests of the null hypothesis that some random coefficients have variances equal to zero in a random coefficients regression model with (possibly) correlated random coefficients.</p> </abstract>
<abstract> <p>This paper reviews a set of recent studies that have attempted to measure the causal effect of education on labor market earnings by using institutional features of the supply side of the education system as exogenous determinants of schooling outcomes. A simple theoretical model that highlights the role of comparative advantage in the optimal schooling decision is presented and used to motivate an extended discussion of econometric issues, including the properties of ordinary least squares and instrumental variables estimators. A review of studies that have used compulsory schooling laws, differences in the accessibility of schools, and similar features as instrumental variables for completed education, reveals that the resulting estimates of the return to schooling are typically as big or bigger than the corresponding ordinary least squares estimates. One interpretation of this finding is that marginal returns to education among the low-education subgroups typically affected by supply-side innovations tend to be relatively high, reflecting their high marginal costs of schooling, rather than low ability that limits their return to education.</p> </abstract>
<abstract> <p>Properties of instrumental variable estimators are sensitive to the choice of valid instruments, even in large cross-section applications. In this paper we address this problem by deriving simple mean-square error criteria that can be minimized to choose the instrument set. We develop these criteria for two-stage least squares (2SLS), limited information maximum likelihood (LIML), and a bias adjusted version of 2SLS (B2SLS). We give a theoretical derivation of the mean-square error and show optimality. In Monte Carlo experiments we find that the instrument choice generally yields an improvement in performance. Also, in the Angrist and Krueger (1991) returns to education application, when the instrument set is chosen in the way we consider, it turns out that both 2SLS and LIML give similar (large) returns to education.</p> </abstract>
<abstract> <p>This paper reports experiments designed to study strategic sophistication, the extent to which behavior in games reflects attempts to predict others' decisions, taking their incentives into account. We study subjects' initial responses to normal-form games with various patterns of iterated dominance and unique pure-strategy equilibria without dominance, using a computer interface that allowed them to search for hidden payoff information, while recording their searches. Monitoring subjects' information searches along with their decisions allows us to better understand how their decisions are determined, and subjects' deviations from the search patterns suggested by equilibrium analysis help to predict their deviations from equilibrium decisions.</p> </abstract>
<abstract> <p>We study efficient, Bayes-Nash incentive compatible mechanisms in a social choice setting that allows for informational and allocative externalities. We show that such mechanisms exist only if a congruence condition relating private and social rates of information substitution is satisfied. If signals are multi-dimensional, the congruence condition is determined by an integrability constraint, and it can hold only in nongeneric cases where values are private or a certain symmetry assumption holds. If signals areone-dimensional, the congruence condition reduces to a monotonicity constraint and it can be generically satisfied. We apply the results to the study ofmulti-object auctions, and we discuss why such auctions cannot be reduced to one-dimensional models without loss of generality.</p> </abstract>
<abstract> <p>This paper introduces a stochastic algorithm for computing symmetric Markov perfect equilibria. The algorithm computes equilibrium policy and value functions, and generates a transition kernel for the (stochastic) evolution of the state of the system. It has two features that together imply that it need not be subject to the curse of dimensionality. First, the integral that determines continuation values is never calculated; rather it is approximated by a simple average of returns from past outcomes of the algorithm, an approximation whose computational burden is not tied to the dimension of the state space. Second, iterations of the algorithm update value and policy functions at a single (rather than at all possible) points in the state space. Random draws from a distribution set by the updated policies determine the location of the next iteration's updates. This selection only repeatedly hits the recurrent class of points, a subset whose cardinality is not directly tied to that of the state space. Numerical results for industrial organization problems show that our algorithm can increase speed and decrease memory requirements by several orders of magnitude.</p> </abstract>
<abstract> <p>A new method is proposed for constructing confidence intervals in autoregressive models with linear time trend. Interest focuses on the sum of the autoregressive coefficients because this parameter provides a useful scalar measure of the long-run persistence properties of an economic time series. Since the type of the limiting distribution of the corresponding OLS estimator, as well as the rate of its convergence, depend in a discontinuous fashion upon whether the true parameter is less than one or equal to one (that is, trend-stationary case or unit root case), the construction of confidence intervals is notoriously difficult. The crux of our method is to recompute the OLS estimator on smaller blocks of the observed data, according to the general subsampling idea of Politis and Romano (1994a), although some extensions of the standard theory are needed. The method is more general than previous approaches in that it works for arbitrary parameter values, but also because it allows the innovations to be a martingale difference sequence rather than i.i.d. Some simulation studies examine the finite sample performance.</p> </abstract>
<abstract> <p>When individual statistics are aggregated through a strictly monotone function to an aggregate statistic, common knowledge of the value of the aggregate statistic does not imply, in general, that the individual statistics are either equal or constant. This paper discusses circumstances where constancy and equality both hold. The first case arises when partitions are independently drawn, and each individual's information is determined by their own partition and some public signal. In this case common knowledge of the value of the aggregator function implies (with probability one) that the individual statistics are constant, so that in the case where the individual statistics have the same expected value, they must all be equal. The second circumstance is where private statistics are related: affiliation of individual statistics and a lattice condition imply that the individual statistics are equal when the value of the aggregate statistic is common knowledge.</p> </abstract>
<abstract> <p>This paper suggests a behavioral definition of (subjective) ambiguity in an abstract setting where objects of choice are Savage-style acts. Then axioms are described that deliver probabilistic sophistication of preference on the set of unambiguous acts. In particular, both the domain and the values of the decision-maker's probability measure are derived from preference. It is argued that the noted result also provides a decision-theoretic foundation for the Knightian distinction between risk and ambiguity.</p> </abstract>
<abstract> <p>The ready-to-eat cereal industry is characterized by high concentration, high price-cost margins, large advertising-to-sales ratios, and numerous introductions of new products. Previous researchers have concluded that the ready-to-eat cereal industry is a classic example of an industry with nearly collusive pricing behavior and intense nonprice competition. This paper empirically examines this conclusion. In particular, I estimate price-cost margins, but more importantly I am able empirically to separate these margins into three sources: (i) that which is due to product differentiation; (ii) that which is due to multi-product firm pricing; and (iii) that due to potential price collusion. The results suggest that given the demand for different brands of cereal, the first two effects explain most of the observed price-cost margins. I conclude that prices in the industry are consistent with noncollusive pricing behavior, despite the high price-cost margins. Leading firms are able to maintain a portfolio of differentiated products and influence the perceived product quality. It is these two factors that lead to high price-cost margins.</p> </abstract>
<abstract> <p>We study a two-person bargaining problem in which the buyer may invest and increase his valuation of the object before bargaining. We show that if all offers are made by the seller and the time between offers is small, then the buyer invests efficiently and the seller extracts all of the surplus. Hence, bargaining with frequently repeated offers remedies the hold-up problem even when the agent who makes the relation-specific investment has no bargaining power and contracting is not possible. We consider alternative formulations with uncertain gains from trade or two-sided investment.</p> </abstract>
<abstract> <p>We identify the inefficiencies that arise when negotiation between two parties takes place in the presence of transaction costs. First, for some values of these costs it is efficient to reach an agreement but the unique equilibrium outcome is one in which agreement is never reached. Secondly, even when there are equilibria in which an agreement is reached, we find that the model always has an equilibrium in which agreement is never reached, as well as equilibria in which agreement is delayed for an arbitrary length of time. Finally, the only way in which the parties can reach an agreement in equilibrium is by using inefficient punishments for (some of) the opponent's deviations. We argue that this implies that, when the parties are given the opportunity to renegotiate out of these inefficiencies, the only equilibrium outcome that survives is the one in which agreement is never reached, regardless of the value of the transaction costs.</p> </abstract>
<abstract> <p>We experimentally investigate the sensitivity of bidders demanding multiple units of a homogeneous commodity to the demand reduction incentives inherent in uniform price auctions. There is substantial demand reduction in both sealed bid and ascending price clock auctions with feedback regarding rivals' drop-out prices. Although both auctions have the same normal form representation, bidding is much closer to equilibrium in the ascending price auctions. We explore the behavioral process underlying these differences along with dynamic Vickrey auctions designed to eliminate the inefficiencies resulting from demand reduction in the uniform price auctions.</p> </abstract>
<abstract> <p> Consider nonempty finite pure strategy sets &lt;tex-math&gt;$S_{1},\ldots ,S_{n}$&lt;/tex-math&gt;, let &lt;tex-math&gt;$S=S_{1}\times \cdots \times S_{n}$&lt;/tex-math&gt;, let Ω be a finite space of "outcomes," let Δ(Ω) be the set of probability distributions on Ω, and let θ: S → Δ(Ω) be a function. We study the conjecture that for any utility in a generic set of n-tuples of utilities on Ω there are finitely many distributions on Ω induced by the Nash equilibria of the game given by the induced utilities on S. We give a counterexample refuting the conjecture for n ≥ 3. Several special cases of the conjecture follow from well known theorems, and we provide some generalizations of these results. </p> </abstract>
<abstract> <p>The PPP puzzle is based on empirical evidence that international price differences for individual goods (LOOP) or baskets of goods (PPP) appear highly persistent or even nonstationary. The present consensus is these price differences have a half-life that is of the order of five years at best, and infinity at worst. This seems unreasonable in a world where transportation and transaction costs appear so low as to encourage arbitrage and the convergence of price gaps over much shorter horizons, typically days or weeks. However, current empirics rely on a particular choice of methodology, involving (i) relatively low-frequency monthly, quarterly, or annual data, and (ii) a linear model specification. In fact, these methodological choices are not innocent, and they can be shown to bias analysis towards findings of slow convergence and a random walk. Intuitively, if we suspect that the actual adjustment horizon is of the order of days, then monthly and annual data cannot be expected to reveal it. If we suspect arbitrage costs are high enough to produce a substantial "band of inaction," then a linear model will fail to support convergence if the process spends considerable time random-walking in that band. Thus, when testing for PPP or LOOP, model specification and data sampling should not proceed without consideration of the actual institutional context and logistical framework of markets.</p> </abstract>
<abstract> <p>We report the results of an experiment designed to study the role of speculation in the formation of bubbles and crashes in laboratory asset markets. In a setting in which speculation is not possible, bubbles and crashes are observed. The results suggest that the departures from fundamental values are not caused by the lack of common knowledge of rationality leading to speculation, but rather by behavior that itself exhibits elements of irrationality. Much of the trading activity that accompanies bubble formation, in markets where speculation is possible, is due to the fact that there is no other activity available for participants in the experiment.</p> </abstract>
<abstract> <p>This paper analyzes a class of games of incomplete information where each agent has private information about her own type, and the types are drawn from an atomless joint probability distribution. The main result establishes existence of pure strategy Nash equilibria (PSNE) under a condition we call the single crossing condition (SCC), roughly described as follows: whenever each opponent uses a nondecreasing strategy (in the sense that higher types choose higher actions), a player's best response strategy is also nondecreasing. When the SCC holds, a PSNE exists in every finite-action game. Further, for games with continuous payoffs and a continuum of actions, there exists a sequence of PSNE to finite-action games that converges to a PSNE of the continuum-action game. These convergence and existence results also extend to some classes of games with discontinuous payoffs, such as first-price auctions, where bidders may be heterogeneous and reserve prices are permitted. Finally, the paper characterizes the SCC based on properties of utility functions and probability distributions over types. Applications include first-price, multi-unit, and all-pay auctions; pricing games with incomplete information about costs; and noisy signaling games.</p> </abstract>
<abstract> <p>We extend Kreps' (1979) analysis of preference for flexibility, reinterpreted by Kreps (1992) as a model of unforeseen contingencies. We enrich the choice set, consequently obtaining uniqueness results that were not possible in Kreps' model. We consider several representations and allow the agent to prefer commitment in some contingencies. In the representations, the agent acts as if she had coherent beliefs about a set of possible future (ex post) preferences, each of which is an expected-utility preference. We show that this set of ex post preferences, called the subjective state space, is essentially unique given the restriction that all ex post preferences are expected-utility preferences and is minimal even without this restriction. Because the subjective state space is identified, the way ex post utilities are aggregated into an ex ante ranking is also essentially unique. Hence when a representation that is additive across states exists, the additivity is meaningful in the sense that all representations are intrinsically additive. Uniqueness enables us to show that the size of the subjective state space provides a measure of the agent's uncertainty about future contingencies and that the way the states are aggregated indicates whether these contingencies lead to a desire for flexibility or commitment.</p> </abstract>
<abstract> <p>Laboratory and field studies of time preference find that discount rates are much greater in the short-run than in the long-run. Hyperbolic discount functions capture this property. This paper solves the decision problem of a hyperbolic consumer who faces stochastic income and a borrowing constraint. The paper uses the bounded variation calculus to derive the Hyperbolic Euler Relation, a natural generalization of the standard Exponential Euler Relation. The Hyperbolic Euler Relation implies that consumers act as if they have endogenous rates of time preference that rise and fall with the future marginal propensity to consume (e.g., discount rates that endogenously range from 5% to 41% for the example discussed in the paper).</p> </abstract>
<abstract> <p>This paper is concerned with the Bayesian estimation of nonlinear stochastic differential equations when observations are discretely sampled. The estimation framework relies on the introduction of latent auxiliary data to complete the missing diffusion between each pair of measurements. Tuned Markov chain Monte Carlo (MCMC) methods based on the Metropolis-Hastings algorithm, in conjunction with the Euler-Maruyama discretization scheme, are used to sample the posterior distribution of the latent data and the model parameters. Techniques for computing the likelihood function, the marginal likelihood, and diagnostic measures (all based on the MCMC output) are developed. Examples using simulated and real data are presented and discussed in detail.</p> </abstract>
<abstract> <p>This paper studies necessity of transversality conditions for the continuous time, reduced form model. By generalizing Benveniste and Scheinkman's (1982) "envelope" condition and Michel's (1990) version of the squeezing argument, we show a generalization of Michel's (1990, Theorem 1) necessity result that does not assume concavity. The generalization enables us to generalize Ekeland and Scheinkman's (1986) result as well as to establish a new result that does not require the objective functional to be finite. The new result implies that homogeneity of the return function alone is sufficient for the necessity of the most standard transversality condition. Our results are also applied to a nonstationary version of the one-sector growth model. It is shown that bubbles never arise in an equilibrium asset pricing model with a nonlinear constraint.</p> </abstract>
<abstract> <p>We study the incentives of candidates to strategically affect the outcome of a voting procedure. We show that the outcomes of every nondictatorial voting procedure that satisfies unanimity will be affected by the incentives of noncontending candidates (i.e., who cannot win the election) to influence the outcome by entering or exiting the election.</p> </abstract>
<abstract> <p>In expected utility theory, risk attitudes are modeled entirely in terms of utility. In the rank-dependent theories, a new dimension is added: chance attitude, modeled in terms of nonadditive measures or nonlinear probability transformations that are independent of utility. Most empirical studies of chance attitude assume probabilities given and adopt parametric fitting for estimating the probability transformation. Only a few qualitative conditions have been proposed or tested as yet, usually quasi-concavity or quasi-convexity in the case of given probabilities. This paper presents a general method of studying qualitative properties of chance attitude such as optimism, pessimism, and the "inverse-S shape" pattern, both for risk and for uncertainty. These qualitative properties can be characterized by permitting appropriate, relatively simple, violations of the sure-thing principle. In particular, this paper solves a hitherto open problem: the preference axiomatization of convex ("pessimistic" or "uncertainty averse") nonadditive measures under uncertainty. The axioms of this paper preserve the central feature of rank-dependent theories, i.e. the separation of chance attitude and utility.</p> </abstract>
<abstract> <p>We show that, in repeated common interest games without discounting, strong 'perturbation implies efficiency' results require that the perturbations must include strategies that are 'draconian' in the sense that they are prepared to punish to the maximum extent possible. Moreover, there is a draconian strategy whose presence in the perturbations guarantees that any equilibrium is efficient. We also argue that the results of Anderlini and Sabourian (1995) using perturbation strategies that are cooperative (and hence nondraconian) are not due to computability per se but to the further restrictions they impose on allowable beliefs.</p> </abstract>
<abstract> <p>This paper extends the revelation principle to environments in which the mechanism designer cannot fully commit to the outcome induced by the mechanism. We show that he may optimally use a direct mechanism under which truthful revelation is an optimal strategy for the agent. In contrast with the conventional revelation principle, however, the agent may not use this strategy with probability one. Our results apply to contracting problems between a principal and a single agent. By reducing such problems to well-defined programming problems they provide a basic tool for studying imperfect commitment.</p> </abstract>
<abstract> <p>We study a two-period model where ex ante inferior choice may tempt the decision-maker in the second period. Individuals have preferences over sets of alternatives that represent second period choices. Our axioms yield a representation that identifies the individual's commitment ranking, temptation ranking, and cost of self-control. An agent has a preference for commitment if she strictly prefers a subset of alternatives to the set itself. An agent has self-control if she resists temptation and chooses an option with higher ex ante utility. We introduce comparative measures of preference for commitment and self-control and relate them to our representations.</p> </abstract>
<abstract> <p>The goal of this paper is to provide a comprehensive empirical analysis of majority rule and Tiebout sorting within a system of local jurisdictions. The idea behind the estimation procedure is to investigate whether observed levels of public expenditures satisfy necessary conditions implied by majority rule in a general equilibrium model of residential choice. The estimator controls for observed and unobserved heterogeneity among households, observed and unobserved characteristics of communities, and the potential endogeneity of prices and expenditures, as well as the self-selection of households into communities of their choice. We estimate the structural parameters of the model using data from the Boston Metropolitan Area. The empirical findings reject myopic voting models. More sophisticated voting models based on utility-taking provide a potential explanation of the main empirical regularities.</p> </abstract>
<abstract> <p>This paper develops a continuous-time equilibrium model of a two-country exchange economy with heterogeneous agents and nontraded goods. Nontraded goods play the role of state variables that shift the marginal utility of traded goods. This affects prices and generates dynamic hedging demands that explain the well documented home bias puzzle in international equity portfolios. When calibrated to both consumption and production data, the model is able to generate significative home bias in equity portfolios. A new methodology, based on Malliavin calculus, is presented to solve for the portfolio policies along the equilibrium path. This methodology allows one to reduce the determination of equilibrium portfolio holdings to the solution of a linear algebraic system, rather than a partial differential equation.</p> </abstract>
<abstract> <p>This paper presents a full characterization of the equilibrium value set of a Ramsey tax model. More generally, it develops a dynamic programming method for a class of policy games between the government and a continuum of households. By selectively incorporating Euler conditions into a strategic dynamic programming framework, we wed two technologies that are usually considered competing alternatives, resulting in a substantial simplification of the problem.</p> </abstract>
<abstract> <p>It is widely known that when there are errors with a moving-average root close to -1, a high order augmented autoregression is necessary for unit root tests to have good size, but that information criteria such as the AIC and the BIC tend to select a truncation lag (k) that is very small. We consider a class of Modified Information Criteria (MIC) with a penalty factor that is sample dependent. It takes into account the fact that the bias in the sum of the autoregressive coefficients is highly dependent on k and adapts to the type of deterministic components present. We use a local asymptotic framework in which the moving-average root is local to -1 to document how the MIC performs better in selecting appropriate values of k. In Monte-Carlo experiments, the MIC is found to yield huge size improvements to the DF&lt;sup&gt;GLS&lt;/sup&gt; and the feasible point optimal P&lt;sub&gt;T&lt;/sub&gt; test developed in Elliott, Rothenberg, and Stock (1996). We also extend the M tests developed in Perron and Ng (1996) to allow for GLS detrending of the data. The MIC along with GLS detrended data yield a set of tests with desirable size and power properties.</p> </abstract>
<abstract> <p>This paper develops an asymptotic theory of inference for an unrestricted two-regime threshold autoregressive (TAR) model with an autoregressive unit root. We find that the asymptotic null distribution of Wald tests for a threshold are nonstandard and different from the stationary case, and suggest basing inference on a bootstrap approximation. We also study the asymptotic null distributions of tests for an autoregressive unit root, and find that they are nonstandard and dependent on the presence of a threshold effect. We propose both asymptotic and bootstrap-based tests. These tests and distribution theory allow for the joint consideration of nonlinearity (thresholds) and nonstationary (unit roots). Our limit theory is based on a new set of tools that combine unit root asymptotics with empirical process methods. We work with a particular two-parameter empirical process that converges weakly to a two-parameter Brownian motion. Our limit distributions involve stochastic integrals with respect to this two-parameter process. This theory is entirely new and may find applications in other contexts. We illustrate the methods with an application to the U.S. monthly unemployment rate. We find strong evidence of a threshold effect. The point estimates suggest that the threshold effect is in theshort-run dynamics, rather than in the dominate root. While the conventional ADF test for a unit root is insignificant, our TAR unit root tests are arguably significant. The evidence is quite strong that the unemployment rate is not a unit root process, and there is considerable evidence that the series is a stationary TAR process.</p> </abstract>
<abstract> <p>This paper evaluates the effectiveness of four econometric approaches intended to identify the learning rules being used by subjects in experiments with normal form games. This is done by simulating experimental data and then estimating the econometric models on the simulated data to determine if they can correctly identify the rule that was used to generate the data. The results show that all of the models examined possess difficulties in accurately distinguishing between the data generating processes.</p> </abstract>
<abstract> <p>The paper examines within a unified methodology expectational coordination in a series of economic models. The methodology views the predictions associated with the Rational Expectations Hypothesis as reasonable whenever they can be derived from the more basic Common Knowledge Hypothesis. The paper successively considers a simple non-noisy N-dimensional model, standard models with "intrinsic" uncertainty, and reference intertemporal models with infinite horizon. It reviews existing results and suggests new ones. It translates the formal results into looser but economically intuitive statements, whose robustness, in the present state of knowledge, is tentatively ascertained.</p> </abstract>
<abstract> <p>This paper studies the effects of progressive income taxes and education finance in a dynamic heterogeneous-agent economy. Such redistributive policies entail distortions to labor supply and savings, but also serve as partial substitutes for missing credit and insurance markets. The resulting tradeoffs for growth and efficiency are explored, both theoretically and quantitatively, in a model that yields complete analytical solutions. Progressive education finance always leads to higher income growth than taxes and transfers, but at the cost of lower insurance. Overall efficiency is assessed using a new measure that properly reflects aggregate resources and idiosyncratic risks but, unlike a standard social welfare function, does not reward equality per se. Simulations using empirical parameter estimates show that the efficiency costs and benefits of redistribution are generally of the same order of magnitude, resulting in plausible values for the optimal rates. Aggregate income and aggregate welfare provide only crude lower and upper bounds around the true efficiency tradeoff.</p> </abstract>
<abstract> <p>This paper examines inference on regressions when interval data are available on one variable, the other variables being measured precisely. Let a population be characterized by a distribution &lt;tex-math&gt;$P(y,x,v,v_{0},v_{1})$&lt;/tex-math&gt;, where &lt;tex-math&gt;$y\in R^{1},x\in R^{k}$&lt;/tex-math&gt;, and the real variables &lt;tex-math&gt;$(v,v_{0},v_{1})$&lt;/tex-math&gt; satisfy &lt;tex-math&gt;$v_{0}\leq v\leq v_{1}$&lt;/tex-math&gt;. Let a random sample be drawn from P and the realizations of &lt;tex-math&gt;$(y,x,v_{0},v_{1})$&lt;/tex-math&gt; be observed, but not those of v. The problem of interest may be to infer E(y/x, v) or E(v/x). This analysis maintains Interval (I), Monotonicity (M), and Mean Independence (MI) assumptions: (I) &lt;tex-math&gt;$P(v_{0}\leq v\leq v_{1})=1$&lt;/tex-math&gt;; (M) E(y/x, v) is monotone in v; (MI) &lt;tex-math&gt;$E(y|x,v,v_{0},v_{1})$&lt;/tex-math&gt; = E(y/x, v). No restrictions are imposed on the distribution of the unobserved values of v within the observed intervals &lt;tex-math&gt;$[v_{0},v_{1}]$&lt;/tex-math&gt;. It is found that the IMMI Assumptions alone imply simple nonparametric bounds on E(y/x, v) and E(v/x). These assumptions invoked when y is binary and combined with a semiparametric binary regression model yield an identification region for the parameters that may be estimated consistently by a modified maximum score (MMS) method. The IMMI assumptions combined with a parametric model for E(y/x, v) or E(v/x) yield an identification region that may be estimated consistently by a modified minimum-distance (MMD) method. Monte Carlo methods are used to characterize the finite-sample performance of these estimators. Empirical case studies are performed using interval wealth data in the Health and Retirement Study and interval income data in the Current Population Survey.</p> </abstract>
<abstract> <p>A principal and an agent enter into a sequence of agreements. The principal faces an interim participation constraint at each date, but can commit to the current agreement; in contrast, the agent has the opportunity to renege on the current agreement. We study the time structure of agreement sequences that satisfy participation and no-deviation constraints and are (constrained) efficient. We show that every such sequence must, after a finite number of dates, exhibit a continuation that maximizes the agent's payoff over all such efficient, self-enforcing sequences. Additional results are provided for situations with transferable payoffs.</p> </abstract>
<abstract> <p>The standard envelope theorems apply to choice sets with convex and topological structure, providing sufficient conditions for the value function to be differentiable in a parameter and characterizing its derivative. This paper studies optimization with arbitrary choice sets and shows that the traditional envelope formula holds at any differentiability point of the value function. We also provide conditions for the value function to be, variously, absolutely continuous, left- and right-differentiable, or fully differentiable. These results are applied to mechanism design, convex programming, continuous optimization problems, saddle-point problems, problems with parameterized constraints, and optimal stopping problems.</p> </abstract>
<abstract> <p>Evidence suggests that municipal water utility administrators in the western US price water significantly below its marginal cost and, in so doing, inefficiently exploit aquifer stocks and induce social surplus losses. This paper empirically identifies the objective function of those managers, measures the deadweight losses resulting from their price-discounting decisions, and recovers the efficient water pricing policy function from counterfactual experiments. In doing so, the estimation uses a "continuous-but-constrained-control" version of a nested fixed-point algorithm in order to measure the important intertemporal consequences of groundwater pricing decisions.</p> </abstract>
<abstract> <p>One of the central features of classical models of competitive markets is the generic determinacy of competitive equilibria. For smooth economies with a finite number of commodities and a finite number of consumers, almost all initial endowments admit only a finite number of competitive equilibria, and these equilibria vary (locally) smoothly with endowments; thus equilibrium comparative statics are locally determinate. This paper establishes parallel results for economies with finitely many consumers and infinitely many commodities. The most important new condition we introduce, quadratic concavity, rules out preferences in which goods are perfect substitutes globally, locally, or asymptotically. Our framework is sufficiently general to encompass many of the models that have proved important in the study of continuous-time trading in financial markets, trading over an infinite time horizon, and trading of finely differentiated commodities.</p> </abstract>
<abstract> <p>We develop a &lt;tex-math&gt;$\sqrt{n}\text{-}\text{consistent}$&lt;/tex-math&gt; and asymptotically normal estimator of the parameters (regression coefficients and threshold points) of a semiparametric ordered response model under the assumption of independence of errors and regressors. The independence assumption implies shift restrictions allowing identification of threshold points up to location and scale. The estimator is useful in various applications, particularly in new product demand forecasting from survey data subject to systematic misreporting. We apply the estimator to assess exaggeration bias in survey data on demand for a new telecommunications service.</p> </abstract>
<abstract> <p>This paper presents an algorithm for computing an equilibrium of an extensive two-person game with perfect recall. The method is computationally efficient by virtue of using the sequence form, whose size is proportional to the size of the game tree. The equilibrium is traced on a piecewise linear path in the sequence form strategy space from an arbitrary starting vector. If the starting vector represents a pair of completely mixed strategies, then the equilibrium is normal form perfect. Computational experiments compare the sequence form and the reduced normal form, and show that only the sequence form is tractable for larger games.</p> </abstract>
<abstract> <p>This paper uses "revealed probability trade-offs" to provide a natural foundation for probability weighting in the famous von Neumann and Morgenstern axiomatic set-up for expected utility. In particular, it shows that a rank-dependent preference functional is obtained in this set-up when the independence axiom is weakened to stochastic dominance and a probability trade-off consistency condition. In contrast with the existing axiomatizations of rank-dependent utility, the resulting axioms allow for complete flexibility regarding the outcome space. Consequently, a parameter-free test/elicitation of rank-dependent utility becomes possible. The probability-oriented approach of this paper also provides theoretical foundations for probabilistic attitudes towards risk. It is shown that the preference conditions that characterize the shape of the probability weighting function can be derived from simple probability trade-off conditions.</p> </abstract>
<abstract> <p>In this lecture, it is argued that Schumpeterian Growth Theory, in which growth is driven by a sequence of quality-improving innovations, can shed light on two important puzzles raised by the recent evolution of wage inequality in developed economies. The first puzzle concerns wage inequality between educational groups, which has substantially risen in the US and the UK during the past two decades following a sharp increase in the supply of educated labor. The second puzzle concerns wage inequality within educational groups, which accounts for a large fraction of the observed increase in wage inequality, although in contrast to between-group wage inequality it has mainly affected the temporary component of income.</p> </abstract>
<abstract> <p>Do political institutions shape economic policy? I argue that this question should naturally appeal to economists. Moreover, the answer is in the affirmative, both in theory and in practice. In particular, recent theoretical work predicts systematic effects of electoral rules and political regimes on the size and composition of government spending. Results from ongoing empirical work indicate that such effects are indeed present in the data. Some empirical results are consistent with theoretical predictions: presidential regimes have smaller governments and countries with majoritarian elections have smaller welfare-state programs and less corruption. Other results present puzzles for future research: the adjustment to economic events appears highly institution-dependent, as does the timing and nature of the electoral cycle.</p> </abstract>
<abstract> <p>Backus, Kehoe, and Kydland (1992), Baxter and Crucini (1995), and Stockman and Tesar (1995) find two major discrepancies between standard international business cycle models with complete markets and the data: In the models, cross-country correlations are much higher for consumption than for output, while in the data the opposite is true; and cross-country correlations of employment and investment are negative, while in the data they are positive. This paper introduces a friction into a standard model that helps resolve these anomalies. The friction is that international loans are imperfectly enforceable; any country can renege on its debts and suffer the consequences for future borrowing. To solve for equilibrium in this economy with endogenous incomplete markets, the methods of Marcet and Marimon (1999) are extended. Incorporating the friction helps resolve the anomalies more than does exogenously restricting the assets that can be traded.</p> </abstract>
<abstract> <p>This article concerns an infinite horizon economy where trade must occur pairwise, using a double auction mechanism, and where fiat money overcomes lack of double coincidence of wants. Traders are anonymous and lack market power. Goods are divisible and perishable, and are consumed at every date. Preferences are defined by utility-stream overtaking. Money is divisible and not subject to inventory constraints. The evolution of individual and economywide money holdings distributions is characterized. There is a welfare-ordered continuum of single price equilibria, reflecting indeterminacy of the price level rather than of relative prices.</p> </abstract>
<abstract> <p>This paper investigates belief learning. Unlike other investigators who have been forced to use observable proxies to approximate unobserved beliefs, we have, using a belief elicitation procedure (proper scoring rule), elicited subject beliefs directly. As a result we were able to perform a more direct test of the proposition that people behave in a manner consistent with belief learning. What we find is interesting. First to the extent that subjects tend to "belief learn," the beliefs they use are the stated beliefs we elicit from them and not the "empirical beliefs" posited by fictitious play or Cournot models. Second, we present evidence that the stated beliefs of our subjects differ dramatically, both quantitatively and qualitatively, from the type of empirical or historical beliefs usually used as proxies for them. Third, our belief elicitation procedures allow us to examine how far we can be led astray when we are forced to infer the value of parameters using observable proxies for variables previously thought to be unobservable. By transforming a heretofore unobservable into an observable, we can see directly how parameter estimates change when this new information is introduced. Again, we demonstrate that such differences can be dramatic. Finally, our belief learning model using stated beliefs outperforms both a reinforcement and EWA model when all three models are estimated using our data.</p> </abstract>
<abstract> <p>We consider a general mechanism design setting where each agent can acquire (covert) information before participating in the mechanism. The central question is whether a mechanism exists that provides the efficient incentives for information acquisition ex-ante and implements the efficient allocation conditional on the private information ex-post. It is shown that in every private value environment the Vickrey-Clark-Groves mechanism guarantees both ex-ante as well as ex-post efficiency. In contrast, with common values, ex-ante and ex-post efficiency cannot be reconciled in general. Sufficient conditions in terms of sub- and supermodularity are provided when (all) ex-post efficient mechanisms lead to private under- or over-acquisition of information.</p> </abstract>
<abstract> <p>Important estimation problems in econometrics like estimating the value of a spectral density at frequency zero, which appears in the econometrics literature in the guises of heteroskedasticity and autocorrelation consistent variance estimation and long run variance estimation, are shown to be "ill-posed" estimation problems. A prototypical result obtained in the paper is that the minimax risk for estimating the value of the spectral density at frequency zero is infinite regardless of sample size, and that confidence sets are close to being uninformative. In this result the maximum risk is over commonly used specifications for the set of feasible data generating processes. The consequences for inference on unit roots and cointegration are discussed. Similar results for persistence estimation and estimation of the long memory parameter are given. All these results are obtained as special cases of a more general theory developed for abstract estimation problems, which readily also allows for the treatment of other ill-posed estimation problems such as, e.g., nonparametric regression or density estimation.</p> </abstract>
<abstract> <p>Band spectral regression with both deterministic and stochastic trends is considered. It is shown that trend removal by regression in the time domain prior to band spectral regression can lead to biased and inconsistent estimates in models with frequency dependent coefficients. Both semiparametric and nonparametric regression formulations are considered, the latter including general systems of two-sided distributed lags such as those arising in lead and lag regressions. The bias problem arises through omitted variables and is avoided by careful specification of the regression equation. Trend removal in the frequency domain is shown to be a convenient option in practice. An asymptotic theory is developed and the two cases of stationary data and cointegrated nonstationary data are compared. In the latter case, a levels and differences regression formulation is shown to be useful in estimating the frequency response function at nonzero as well as zero frequencies.</p> </abstract>
<abstract> <p>We investigate the nature of price competition among firms that produce differentiated products and compete in markets that are limited in extent. We propose an instrumental variables series estimator for the matrix of cross price response coefficients, demonstrate that our estimator is consistent, and derive its asymptotic distribution. Our semiparametric approach allows us to discriminate among models of global competition, in which all products compete with all others, and local competition, in which products compete only with their neighbors. We apply our semiparametric estimator to data from U.S. wholesale gasoline markets and find that, in this market, competition is highly localized.</p> </abstract>
<abstract> <p>How can diversity be measured? What does it mean to value biodiversity? Can we assist Noah in constructing his preferences? To address these questions, we propose a multi-attribute approach under which the diversity of a set of species is the sum of the values of all attributes possessed by some species in the set. We develop the basic intuitions and requirements for a theory of diversity and show that the multi-attribute approach satisfies them in a flexible yet tractable manner. A natural starting point is to think of the diversity of a set as an aggregate of the pairwise dissimilarities between its elements. The multi-attribute framework allows one to make this program formally precise. It is shown that the program can be realized if and only if the family of relevant attributes is well-ordered ("acyclic"). Moreover, there is a unique functional form aggregating dissimilarity into diversity, the length of a minimum spanning tree. Examples are taxonomic hierarchies and lines representing uni-dimensional qualities. In multi-dimensional settings, pairwise dissimilarity information among elements is insufficient to determine their diversity. By consequence, the qualitative and quantitative behavior of diversity differs fundamentally.</p> </abstract>
<abstract> <p>The paper is a discussion of the interpretation of game theory. Game theory is viewed as an abstract inquiry into the concepts used in social reasoning when dealing with situations of conflict and not as an attempt to predict behavior. The first half of the paper deals with the notion of "strategy." Its principal claim is that the conventional interpretation of a "strategy" is not consistent with the manner in which it is applied, and that this inconsistency frequently results in confusion and misunderstanding. In order to prove this point, the term "strategy" is discussed in three contexts: extensive games in which players have to act more than once in some prespecified order, games normally analyzed using mixed strategies, and games with limited memory. The paper endorses the view that equilibrium strategy describes a player's plan of action, as well as those considerations which support the optimality of his plan rather than being merely a description of a "plan of action." Deviation from the perception of a strategy as a mere "plan of action" fits in well with the interpretation of the notion "game" which is discussed in the second half of this paper. It is argued that a good model in game theory has to be realistic in the sense that it provides a model for the perception of real life social phenomena. It should incorporate a description of the relevant factors involved, as perceived by the decision makers. These need not necessarily represent the physical rules of the world. It is not meant to be isomorphic with respect to "reality" but rather with respect to our perception of regular phenomena in reality.</p> </abstract>
<abstract> <p>We examine the effects of male and female labor supply on household demands and present a simple and robust test for the separability of commodity demands from labor supply. Using data on individual households from six years of the UK FES we estimate a demand system for seven goods which includes hours and participation dummies as conditioning variables. Allowance is made for the possible endogeneity of these conditioning labor supply variables. We find that separability is rejected. Furthermore, we present evidence that ignoring the effects of labor supply leads to bias in the parameter estimates.</p> </abstract>
<abstract> <p>In this paper I discuss economic processes that may give rise to spatial patterns in data, and explore the relative merits of alternative modeling approaches when data are spatially correlated. Specifically, I present an estimation scheme that allows for spatial random effects, and focus attention on cases in which such a framework may be preferred to the more general fixed effects framework that nests it. I use the models presented, together with information on the location of households in an Indonesian socio-economic survey, to test spatial relationships in Indonesian demand for rice.</p> </abstract>
<abstract> <p>This paper deals with error correction models (ECM's) and cointegrated systems that are formulated in continuous time. Long-run equilibrium coefficients in the continuous system are always identified in the discrete time reduced form, so that there is no aliasing problem for these parameters. The long-run relationships are also preserved under quite general data filtering. Frequency domain procedures are outlined for estimation and inference. These methods are asymptotically optimal under Gaussian assumptions and they have the advantages of simplicity of computation and generality of specification, thereby avoiding some methodological problems of dynamic specification. In addition, they facilitate the treatment of data irregularities such as mixed stock and flow data and temporally aggregated partial equilibrium formulations. Models with restricted cointegrating matrices are also considered.</p> </abstract>
<abstract> <p>This paper derives several properties unique to nonlinear model hypothesis testing problems involving linear or nonlinear inequality constraints in the null or alternative hypothesis. The paper is organized around a lemma which characterizes the set containing the least favorable parameter value for a nonlinear model inequality constraints hypothesis test. We then present two examples which illustrate several implications of this lemma. We also discuss the impact of these properties on the empirical implementation and interpretation of these test procedures.</p> </abstract>
<abstract> <p> Reformulate the classical implementation problem à la Maskin (1977) as follows. Think of a social choice correspondence as a mapping from preference profiles to lotteries over some finite set of alternative. Say that a soical choice function f is virtually implementable in Nash equilibrium if for all ε &gt; 0 there exists a game from G such that for all preference profiles θ, G has a unique equilibrium outcome x(θ), and x(θ) is ε-close to f(θ). (This definition may be directly extended to social choice correspondences.) Then (under mild domain restrictions) the following result is true: In societies with at least three individuals all social choice correspondences are virtually implementable in Nash equilibrium. This proposition should be contrasted with Maskin's (1977) classic characterization, according to which the nontrivial requirement of monotonicity is a necessary condition for exact implementation in Nash equilibrium. The two-person case needs to be considered separately. We provide a complete characterization of virtually implementable two-person social choice functions. While not all two-person social choice functions are virtually implementable, our necessary and sufficient condition is simple. This contrasts with the rather complex necessary and sufficient conditions for exact implementation. We show how our results can be extended to implementation in strict Nash equilibrium and coalition-proof Nash equilibrium, to social choice correspondences which map from cardinal preference profiles to lotteries, and to environments with a continuum of pure alternatives. </p> </abstract>
<abstract> <p>A set of n objects and an amount M of money is to be distributed among m people. Example: the objects are tasks and the money is compensation from a fixed budget. An elementary argument via constrained optimization shows that for M sufficiently large the set of efficient, envy free allocations is nonempty and has a nice structure. In particular, various criteria of justice lead to unique best fair allocations which are well behaved with respect to changes of M. This is in sharp contrast to the usual fair division theory with divisible goods.</p> </abstract>
<abstract> <p>We describe a model of general equilibrium with incomplete markets in which firms can innovate by issuing arbitrary, costly securities. When short sales are prohibited, firms behave competitively and equilibrium is efficient. When short sales are allowed, these classical properties may fail. If unlimited short sales are allowed, imperfect competition may persist, even when the number of potential innovators is large. If limited short sales are allowed, perfect competition may obtain in the limit but equilibrium can be inefficient because of the presence of an externality: the private benefits of innovation for firms differ from the social benefits.</p> </abstract>
<abstract> <p>In this paper, I examine the constrained optimal pattern of capital flows between a lender and a borrower in an environment in which there are two impediments to forming contracts. The first impediment to contracting arises from the assumption that lenders cannot observe whether borrowers invest or consume borrowed funds. This assumption leads to a moral hazard problem in investment. The second impediment arises from the assumption that the borrower, as a sovereign nation, may choose to repudiate his debts. The optimal contract is shown to specify that the borrowing country experience a capital outflow when the worst realizations of national output occur. This seemingly perverse capital outflow forms a necessary part of the optimal solution to the moral hazard problem in investment.</p> </abstract>
<abstract> <p>A state-dependent income tax is incorporated into a stochastic, discrete-time, infinite-horizon production economy. Methods are developed for establishing the existence and uniqueness of an equilibrium, and for computing this equilibrium.</p> </abstract>
<abstract> <p>It seems inconsistent to model boundedly rational action choice by assuming that the agent chooses the optimal decision procedure. This criticism is not avoided by assuming that he chooses the optimal procedure to choose a procedure to... to choose an action. I show that, properly interpreted, this regress, continued transfinitely, generates a model representing the agent's perception of all his options including every way to refine his perceptions. In this model, the agent surely must choose the perceived best option. Hence it is not inconsistent to model limited rationality by assuming that the agent uses the "optimal" decision procedure.</p> </abstract>
<abstract> <p>We consider a model of a trading world consisting of an arbitrary number of nations engaged in international trade in an arbitrary number of commodities. Starting from an arbitrary tariff-distorted initial equilibrium, we examine the possibilities of undertaking a gradual multilateral reform of tariffs (and other trade taxes and subsidies) so as to attain a strict Pareto improvement in welfare. Necessary and sufficient conditions for the existence of strict Pareto-improving multilateral (differential) tariff reforms, accompanied by international transfers of income, are obtained. These results are then applied to various concrete tariff reform proposals such as proportional reductions in tariffs and the reduction of the highest ad valorem tariff rates. Some of our theorems extend the generality of previously obtained results, and some new tariff reform proposals are also made.</p> </abstract>
<abstract> <p>This paper seeks to explain the causes of volatility clustering in exchange rates. Careful examination of intra-daily exchange rates provides a test of two hypotheses--heat waves and meteor showers. The heat wave hypothesis is that the volatility has only country-specific autocorrelation. Alternatively, the meteor shower is a phenomenon of intra-daily volatility spillovers from one market to the next. Using the GARCH model to specify the heteroskedasticity across intra-daily market segments, we find that the empirical evidence is generally against the null hypothesis of the heat wave. Using a volatility type of vector autoregression we examine the impact of news in one market on the time path of per-hour volatility in other markets.</p> </abstract>
<abstract> <p>This paper is an empirical investigation of equilibrium restrictions on household consumption and labor supply. It posits, estimates, and tests a model where the equilibrium behavior of agents sometimes leads them to locate on the boundary of their respective choice sets. The key to our framework is a simple factor structure which characterizes the effects of market forces on household choices and returns to financial assets. It can be rationalized by the two assumptions that household allocations are Pareto optimal and that the labor market is competitive. If markets were complete, then the factors would represent real wages to standardized labor, prices for the future contingent claims which are ultimately realized, and the marginal utilities of wealth to households. Our empirical work estimates household preferences and tests how well this parsimonious factor structure represents panel data on married couples and time series data on asset returns. Most of our estimates are roughly comparable to those found in previous work; we find no evidence against the simple factor representation, and cannot reject the intertemporal capital asset pricing model.</p> </abstract>
<abstract> <p>This paper proposes a characterization of optimal strategies for playing certain repeated coordination games whose players have identical preferences. Players' optimal coordination strategies reflect their uncertainty about how their partners will respond to multiple-equilibrium problems; this uncertainty constraints the statistical relationships between their strategy choices players can bring about. We show that optimality is nevertheless consistent with subgame-perfect equilibrium. Examples are analyzed in which players use precedents as focal points to achieve and maintain coordination, and in which they play dominated strategies with positive probability in early stages in the hope of generating a useful precedent.</p> </abstract>
<abstract> <p>Long-term relationships are often governed by short-term contracts; this is usually explained by referring to the costs of specifying and enforcing a complete contingent contract. We focus here on the benefits usually associated with long-term commitment, namely the efficiency costs involved when long-term contracts are not available. We prove that renegotiable short-term contracts will implement the long-term optimum in a multi-period principal-agent framework when transfers are not too limited, objectives are conflicting, and there is no relevant asymmetric information at the contracting dates. This last assumption excludes adverse selection models, but not repeated moral hazard models when technologies and preferences are time separable.</p> </abstract>
<abstract> <p>Suppose that a representative downstream firm must buy relationship-specific capital before trading with an upstream monopolist. Under reasonable conditions the monopolist has an incentive to precommit to its unit price in order to induce its customers to increase their investment, and thereby their demand for the upstream product. Now suppose that the monopolist is privately informed of its unit costs after the downstream firms invest. The monopolist now chooses among contingent contracts, in which price is conditioned on announced costs. We demonstrate that the optimal contract specifies a maximum ("list") price, which is charged for a significant subset of cost realizations, but which may be discounted (to "transactions" prices) when costs are low. The model therefore rationalizes transactions/list pricing: a mode of pricing that is prevalent in interfirm trade. We also use our results to explain why Stigler and Kindahl's medium-term price series tracked the associated Wholesale Price Indices whenever the latter were nondecreasing, but otherwise fell significantly faster.</p> </abstract>
<abstract> <p>We develop a model of alternating price competition between firms selling differentiated products to nonhomogeneous consumers. Subgame perfect equilibria exist that support quite collusive prices even though each firm's price depends only upon its rival's current price. We find two types of equilibria. One, which we call "disciplined," arise when products are close substitutes. The other, which we call "spontaneous," emerge when products are more differentiated. In disciplined equilibria the steady-state price is enforced by the implicit threat to respond to a price cut with further price cutting. In spontaneous equilibria no such threat is needed. Consumers in the smaller market tend to pay a higher price, as do consumers in the market served by the more efficient firm. The price supported by a disciplined equilibrium is greater the less differentiated are the products.</p> </abstract>
<abstract> <p>This paper considers a general equilibrium model of an economy where some firms may exhibit increasing returns to scale or more general types of nonconvexities. The firms are instructed to follow the standard marginal cost pricing rule or to fulfill the first-order necessary conditions for profit maximization. A general existence theorem of equilibria is proved in the case of an arbitrary number of firms. No assumption is made to imply the aggregate productive efficiency of equilibria, a condition that must be excluded in the nonconvex case.</p> </abstract>
<abstract> <p>It is shown that if an economy's participants cannot be separated into groups across which there are no potentially conflicting interests--i.e., if the economy is "indecomposable"--then every continuous truth-dominant allocation mechanism will attain nonoptimal allocations on an open dense set of preference profiles. Classical "Edgeworth-box" exchange economies (economies with no externalities and no production, but with arbitrary numbers of consumers and goods), as well as economies with public goods and economies with other kinds of externalities, are all shown via simple arguments to be indecomposable. The results are extended to cover nonrevelation mechanisms that have dominant-strategy equilibria.</p> </abstract>
<abstract> <p>In this paper we study a general concave discrete time infinite horizon optimal control problem. We establish necessary and sufficient conditions for optimality in the weak sense of Brock and for optimality in the strong sense of Gale. The corresponding transversality conditions are general exhaustion properties of the limit value of the optimal state variables; these properties cover and extend the well known results obtained in special cases.</p> </abstract>
<abstract> <p>An adaptive learning rule is exhibited for the Azariadis (1981) overlapping generations model of a monetary economy with multiple equilibria, under which the economy may converge to a stationary sunspot equilibrium, even if agents do not initially believe that outcomes are significantly different "sunspot" states. The type of learning rule studied is of the "stochastic approximation" form studied by Robbins and Monro (1951); methods for analyzing the convergence of this form of algorithm are presented that may be of use in many other contexts as well. Conditions are given under which convergence to a sunspot equilibrium occurs with probability one.</p> </abstract>
<abstract> <p>Although the rational expectations hypothesis is widely applied in asset pricing models with differentially informed traders, the extent to which markets actually aggregate and transmit information is an open question. In this study we report the results from laboratory asset markets which were designed to examine this question. Initially we designed our markets with the intention of giving the RE model its best chance for success. After documenting evidence in favor of the RE model we examined which features of our environment are necessary or sufficient to achieve an RE equilibrium. Within the class of treatments we consider we find that trading experience and common knowledge of dividends are jointly sufficient to achieve an RE equilibrium, but that neither is a sufficient condition by itself. We also present some stylized facts about the convergence process leading to an RE equilibrium.</p> </abstract>
<abstract> <p>This paper analyzes preference relations over two-stage lotteries, i.e., lotteries having as outcomes tickets for other, simple, lotteries. Empirical evidence indicates that decision makers do not always behave in accordance with the reduction of compound lotteries axiom, but it seems that they satisfy a compound independence axiom (also known as the certainty equivalent mechanism). It turns out that although the reduction and the compound independence axioms together with continuity imply expected utility theory, each of them by itself is compatible with all possible preference relations over simple lotteries. By using these axioms I analyze three different versions of expected utility for two-stage lotteries. The second part of the paper is devoted to possible replacements of the reduction axiom. For this I suggest several different compound dominance axioms. These axioms compare two-stage lotteries by the probability they assign to the upper and lower sets of all simple lotteries X. (For a simple lottery X, its upper (lower) set is the set of lotteries that dominate (are dominated by) X by first order stochastic dominance.) It turns out that these axioms are all strictly weaker than the reduction of compound lotteries axiom. The main theoretical results of this part are: (1) an axiomatic basis for expected utility theory that does not require the reduction axiom and (2) a new axiomatization of the anticipated utility model (also known as expected utility with rank-dependent probabilities). These representation theorems indicate that to a certain extent the rank dependent probabilities model is a natural extension of expected utility theory. Finally, I show that some paradoxes in expected utility theory can be explained, provided one is willing to use the compound independence rather than the reduction axiom.</p> </abstract>
<abstract> <p>We analyze the principal-agent relationship when the principal has private information as a three-stage noncooperative game: contract proposal, acceptance/refusal, and contract execution. We assume that the information does not directly affect the agent's payoff (private values). Equilibrium exists and is generically locally unique. Moreover, it is Pareto optimal for the different types of principal. The principal generically does strictly better than when the agent knows her information. Equilibrium allocations are the Walrasian equilibria of an "economy" where the traders are different types of principal and "exchange" the slack on the agent's individual rationality and incentive compatibility constraints.</p> </abstract>
<abstract> <p>Optimal futures hedging and equilibrium futures price bias are examined in a model characterized by two consumption goods, one of which has stochastically varying output, and where information arrives sequentially. Positive (negative) complementarity in consumer preferences promotes downward (upward) bias in the futures price viewed as a predictor of the later spot price. I demonstrate that the conclusion derived from partial equilibrium analysis--that when speculators are risk averse, risk premia are a function of hedging pressure--fails in the general equilibrium analysis, so long as there are no transaction costs. A counterexample is analyzed in which, as consumers' additive logarithmic preferences are varied, producers' hedging positions change from long to short, while the futures risk premium remains unchanged. However, hedging pressure is reinstated as a force influencing risk premia in the sense that the futures price is downward biased when hedgers take short positions and is upward biased when hedgers take long positions, provided it can be assumed (as is usually valid) that fixed setup costs of trading deter consumers more than producers from participating in the futures market.</p> </abstract>
<abstract> <p>This paper analyzes a competitive credit market where banks use imperfect and independent tests to assess the ability of a potential creditor to repay credit. The banks compete by announcing interest rates at which they will provide credit to those applicants who pass the banks' tests. The proportion of applicants who pass the test of at least one bank increases with the number of banks providing credit, so the average credit-worthiness decreases. It is then shown that in a situation where all banks charge the same interest rate, a bank always has the incentive to undercut in order to improve the average credit-worthiness of its own clientele. This feature represents the major difference from the situations in standard Bertrand and Bertrand-Edgeworth models.</p> </abstract>
<abstract> <p>This paper presents a simple estimator of the shape parameter in a Weibull duration model with unobserved heterogeneity. The estimator is consistent and asymptotically normal under mild conditions on the moments of the unobserved heterogeneity, and a consistent estimator of the asymptotic variance is available. A Monte Carlo study indicates that the asymptotic distribution of the estimator provides a good approximation to the finite sample distribution. It is illustrated that the estimation strategy can be extended to a model with regressors and to a log-logistic model with unobserved heterogeneity. The main advantages of the estimator are that it is easy to calculate and that its asymptotic distribution can be derived.</p> </abstract>
<abstract> <p>This paper proposes a general method to build exact tests and confidence sets in linear regressions with first-order autoregressive Gaussian disturbances. Because of a nuisance parameter problem, we argue that generalized bounds tests and conservative confidence sets provide natural inference procedures in such a context. Given an exact confidence set for the autocorrelation coefficient, we describe how to obtain a similar simultaneous confidence set for the autocorrelation coefficient and any subvector of regression coefficients. Conservative confidence sets for the regression coefficients are then deduced by a projection method. For any hypothesis that specifies jointly the value of the autocorrelation coefficient and any set of linear restrictions on the regression coefficients, we get exact similar tests. For testing linear hypotheses about the regression coefficients only, we suggest bounds-type procedures. Exact confidence sets for the autocorrelation coefficient are built by "inverting" autocorrelation tests. The method is illustrated with two examples.</p> </abstract>
<abstract> <p>Asymptotic distributions are derived for the ordinary least squares (OLS) estimate of a first order autoregression when the series is fractionally integrated of order 1 + d, for &lt;latex&gt;$- 1/2 &lt; d &lt; 1/2$&lt;/latex&gt;. The fractional unit root distribution is introduced to describe the limiting distribution. The unit root distribution (d = 0) is seen to be an atypical member of this family because its density is nonzero over the entire real line. For &lt;latex&gt;$- 1/2 &lt; d &lt; 0$&lt;/latex&gt; the fractional unit root distribution has nonpositive support, while if &lt;latex&gt;$0 &lt; d &lt; 1/2$&lt;/latex&gt; the fractional unit root distribution has nonnegative support. Any misspecification of the order of differencing leads to drastically different limiting distributions. Testing for unit roots is further complicated by the result that the t statistic in this model only converges when d = 0 Results are proven by means of functional limit theorems.</p> </abstract>
<abstract> <p>Problems of social choice frequently take the following form. There are n voters and a set K = {1, 2, ldots, k} of objects. The voters must choose a subset of K. We define a class of voting schemes called voting by committees. The main result of the paper is a characterization of voting by committees, which is the class of all voting schemes that satisfy voter sovereignty and nonmanipulability on the domain of separable preferences. This result is analogous to the literature on the Groves and Clarke scheme in that it characterizes all of the nonmanipulable voting schemes on an important domain.</p> </abstract>
<abstract> <p>This paper concerns moral hazard problems in multi-agent situations where cooperation is an issue. Each agent chooses his own effort, which improves stochastically the outcome of his own task. He also chooses the amount of "help" to extend to other agents, which improves their performance. By selecting appropriate compensation schemes, the principal can design a task structure: the principal may prefer an unambiguous division of labor, where each agent specializes in his own task; or the principal may prefer teamwork where each agent is motivated to help other agents. We provide a sufficient condition for teamwork to be optimal, based on its incentive effects. We also show a nonconvexity of the optimal task structure: The principal wants either an unambiguous division of labor or a substantial teamwork.</p> </abstract>
<abstract> <p>Evolutionary games are introduced as models for repeated anonymous strategic interaction. The basic idea is that actions (or behaviors) which are more "fit," given the current distribution of behaviors, tend over time to displace less fit behaviors. Simple numerical examples motivate the key concepts of fitness function and compatible dynamics, and illustrate the relation to previous biological models. Cone fields are introduced to characterize the continuous-time dynamical processes compatible with a given fitness function. The analysis focuses on dynamic steady state equilibria and their relation to the static equilibria known as NE (Nash equilibrium) and ESS (evolutionary stable state). For large classes of dynamics it is shown that all stable dynamic steady states are NE and that all NE are dynamic steady states. The biologists' ESS condition is less closely related to the dynamic equilibria. The paper concludes with a brief survey of economic applications.</p> </abstract>
<abstract> <p> An axiomatic model of preferences over lotteries is developed. It is shown that this model is consistent with the Allais Paradox, includes expected utility theory as a special case, and is only one parameter (β) richer than the expected utility model. Allais Paradox type behavior is identified with positive values of β. Preferences with positive β are said to be disappointment averse. It is shown that risk aversion implies disappointment aversion and that the Arrow-Pratt measures of risk aversion can be generalized in a straight-forward manner, to the current framework. </p> </abstract>
<abstract> <p>It seems desirable that the overall level of poverty should fall whenever poverty decreases within some subgroup of the population and is unchange outside that group. Yet this simple and attractive property, which we call "subgroup consistency," is violated by many of the poverty indices suggested in recent years. This paper characterizes the class of subgroup consistent poverty indices, and identifies the special features associated with this property.</p> </abstract>
<abstract> <p>Gorman's (1981) concept of Engel curve "rank" is extended to apply to any demand system. Rank is shown to have implications for specification, separability, and aggregation of demands. A simple nonparametric test of rank using Engel curve data is described and applied to U.S. and U.K. consumer survey data. The test employs a new general method for testing the rank of estimated matrices. The results are used to assess theoretical and empirical aggregation error in representative consumer models, and to explain a representative consumer paradox.</p> </abstract>
<abstract> <p>We derive from a model of investment with multiple capital goods a one-to-one relation between the growth rate of the capital aggregate and the stock market-based Q. We estimate the growth-Q relation using a panel of Japanese manufacturing firms taking into account the endogeneity of Q. Identification is achieved by combining the theoretical structure of the Q model and an assumed serial correlation structure of the technology shock which is the error term in the growth-Q equation. For early years of our sample, cash flow has significant explanatory power over and above Q. The significance of cash flow disappears for more recent years for heavy industry when Japanese capital markets were liberalized. The estimated Q coefficient implies that the adjustment cost is less than a half of gross profits net of the adjustment cost.</p> </abstract>
<abstract> <p>A method is presented for the estimation of nonlinear simultaneous equations and transformation models in the presence of disturbance distribution of unknown form. For a given, finite, set of equations, the instrument achieving the lower variance bound for instrumental variables estimates is a nonparametric regression function. We approximate this without use of smoothed nonparametric estimation: our feasible optimal instruments average over the unsmoothed empirical distribution of preliminary residuals. We justify the method under stationary serial dependence. Somewhat different estimates are proposed in each of three settings: independent disturbances and strongly exogenous but possibly serially dependent explanatory variables; independent disturbances and explanatory variables that include lagged response variables; parametrically autocorrelated disturbances and strongly exogenous but possibly serially dependent explanatory variables. In each case large-sample inference rules are justified. The limiting normal distribution of the estimates is found to be unaffected if the optimal instruments are based on only an arbitrarily small, vanishing fraction of the residuals. In the setting of independent observations we investigate theoretically the effect of such computational savings on the goodness of the normal approximation, obtaining a rate of uniform convergence to normality.</p> </abstract>
<abstract> <p>The information matrix (IM) test is shown to have a finite sample distribution which is poorly approximated by its asymptotic χ &lt;sup&gt;2&lt;/sup&gt; distribution in models and sample sizes commonly encountered in applied econometric research. The quality of the χ &lt;sup&gt;2&lt;/sup&gt; approximation depends upon the method chosen to compute the test. Failure to exploit restrictions on the covariance matrix of the test can lead to a test with appalling finite sample properties. Order O(n&lt;sup&gt;-1&lt;/sup&gt;) approximations to the exact distribution of an efficient form of the IM test are reported. These are developed from asymptotic expansions of the Edgeworth and Cornish-Fisher types. They are compared with Monte Carlo estimates of the finite sample distribution of the test and are found to be superior to the usual χ &lt;sup&gt;2&lt;/sup&gt; approximations in sample sizes of the magnitude found in applied micro-econometric work. The methods developed in the paper are applied to normal and exponential models and to normal regression models. Results are provided for the full IM test and for heteroskedasticity and nonnormality diagnostic tests which are special cases of the IM test. In general the quality of alternative approximations is sensitive to covariate design. However commonly used nonnormality tests are found to have distributions which, to order O(n&lt;sup&gt;-1&lt;/sup&gt;), are invariant under changes in covariate design. This leads to simple design and parameter invariant size corrections for nonnormality tests.</p> </abstract>
<abstract> <p> This paper is concerned with the estimation of covariance matrices in the presence of heteroskedasticity and autocorrelation of unknown forms. Currently available estimators that are designed for this context depend upon the choice of a lag truncation parameter and a weighting scheme. Results in the literature provide a condition on the growth rate of the lag truncation parameter as T → ∞ that is sufficient for consistency. No results are available, however, regarding the choice of lag truncation parameter for a fixed sample size, regarding data-dependent automatic lag truncation parameters, or regarding the choice of weighting scheme. In consequence, available estimators are not entirely operational and the relative merits of the estimators are unknown. This paper addresses these problems. The asymptotic truncated mean squared errors of estimators in a given class are determined and compared. Asymptotically optimal kernel/weighting scheme and bandwidth/lag truncation parameters are obtained using an asymptotic truncated mean squared error criterion. Using these results, data-dependent automatic bandwidth/lag truncation parameters are introduced. The finite sample properties of the estimators are analyzed via Monte Carlo simulation. </p> </abstract>
<abstract> <p> A celebrated result of Black (1948a) demonstrates the existence of a simple-majority winner when preferences are single-peaked. The social choice follows the preferences of the median voter: the median voter's most-preferred outcome beats any alternative. However, this conclusion does not extend to elections in which candidates differ in more than one dimension. This paper provides a multi-dimensional analog of the median voter result. We provide conditions under which the mean voter's most preferred outcome is unbeatable according to a 64%-majority rule. The conditions supporting this result represent a significant generalization of Caplin and Nalebuff (1988). The proof of our mean voter result uses a mathematical aggregation theorem due to Prékopa (1971, 1973) and Borell (1975). This theorem has broad applications in economics. An application to the distribution of income is described at the end of this paper; results on imperfect competition are presented in the companion paper, Caplin and Nalebuff (1991). </p> </abstract>
<abstract> <p> We present a new approach to the theory of imperfect competition and apply it to study price competition among differentiated products. The central result provides general conditions under which there exists a pure-strategy price equilibrium for any number of firms producing any set of products. This includes products with multi-dimensional attributes. In addition to the proof of existence, we provide conditions for uniqueness. Our analysis covers location models, the characteristics approach, and probabilistic choice together in a unified framework. To prove existence, we employ aggregation theorems due to Prékopa (1971) and Borell (1975). Our companion paper (Caplin and Nalebuff (1991)) introduces these theorems and develops the application to super-majority voting rules. </p> </abstract>
<abstract> <p>Two properties of preferences and representations for choice under uncertainty which play an important role in decision theory are: (i) admissibility, the requirement that weakly dominated actions should not be chosen; and (ii) the existence of well defined conditional probabilities, that is, given any event a conditional probability which is concentrated on that event and which corresponds to the individual's preferences. The conventional Bayesian theory of choice under uncertainty, subjective expected utility (SEU) theory, fails to satisfy these properties--weakly dominated acts may be chosen, and the usual definition of conditional probabilities applies only to non-null events. This paper develops a non-Archimedean variant of SEU where decision makers have lexicographic beliefs; that is, there are (first-order) likely events as well as (higher-order) events which are infinitely less likely but not necessarily impossible. This generalization of preferences, from those having an SEU representation to those having a representation with lexicographic beliefs, can be made to satisfy admissibility and yield well defined conditional probabilities and at the same time to allow for "null" events. The need for a synthesis of expected utility with admissibility, and to provide a ranking of null events, has often been stressed in the decision theory literature. Furthermore, lexicographic beliefs are appropriate for characterizing refinements of Nash equilibrium. In this paper we discuss: axioms on, and behavioral properties of, individual preferences which characterize lexicographic beliefs; probability-theoretic properties of the representations; and the relationships with other recent extensions of Bayesian SEU theory.</p> </abstract>
<abstract> <p>This paper develops a decision-theoretic approach to normal-form refinements of Nash equilibrium and, in particular, provides characterizations of (normal-form) perfect equilibrium (Selten (1975)) and proper equilibrium (Myerson (1978)). The approach relies on a theory of single-person decision making that is a non-Archimedean version of subjective expected utility theory. According to this theory, each player in a game possesses, in addition to a strategy space and a utility function on outcomes, a vector of probability distributions, called a lexicographic probability system (LPS), on the strategies chosen by the other players. These probability distributions can be interpreted as the player's first-order and higher order theories as to how the game will be played, and are used lexicographically in determining an optimal strategy. We define an equilibirum concept, called lexicographic Nash equilibrium, that extends the notion of Nash equilibrium in that it dictates not only a strategy for each player but also an LPS on the strategies chosen by the other players. Perfect and proper equilibria are described as lexicogrpahic Nash equilibria by placing various restrictions on the LPS's possessed by the players.</p> </abstract>
<abstract> <p>Consider an economy with one public enterprise that is subject to a budgetary constraint and charges prices according to the first order necessary conditions suggested by Boiteux (1956) or, in another context, by Ramsey (1927). We state (restrictive) conditions sufficient to ensure that the resulting allocation is second best.</p> </abstract>
<abstract> <p>Examples of well-behaved sequences of economies without monotonic preferences are constructed. These economies have core allocations that cannot be decentralized by prices, even in a weak sense. Relaxing the monotonicity assumption results in core allocations that are not uniformly integrable, breaking the connection between the continuum and the large finite model. If in addition preferences are nonconvex, even sequences of economies with uniformly bounded core allocations may fail to exhibit equivalence properties. Sufficient conditions to restore convergence are provided.</p> </abstract>
<abstract> <p>The independence axiom of expected utility theory has recently been weakened to the betweenness axiom. In this paper an even weaker axiom, called mixture symmetry, is presented. The corresponding functional structure is such that utility is a betweenness functional on part of its domain and quadratic in probabilities elsewhere. The experimental evidence against betweenness provides one motivation for the more general theory presented here. Another advantage of the mixture symmetric class of utility functions is that it is sufficiently flexible to permit the disentangling of attitudes towards risk and towards randomization.</p> </abstract>
<abstract> <p>We develop an integrated microeconomic model derived from a labor supply model in a four-sector labor market with explicit demand constraints. This method allows us to test several assumptions on labor market equilibria: strongly or weakly competitive or segmented. We show that more important a feature of labor markets than segmentation is the presence of comparative advantages for individuals between the various economic sectors. The model is applied to data on women's labor force participation in the main towns of Colombia in 1980. It uses multivariate Probit and Tobit techniques.</p> </abstract>
<abstract> <p> We estimate a dynamic programming model of job exit behavior and retirement using the method of simulated moments. The model and estimation method allow for both unobserved individual effects and unobserved job-specific "match" effects. The model is estimated using two different assumptions about individual discount factors. First, a static model, with the discount factor β equal to zero, is estimated. Then a dynamic model, with β = .95, is estimated. In both models, it is found that bad health, age, and lack of education increase the probability of retirement. The dynamic model performs better than the static model and has different implications for retirement behavior. The job-specific effects are an important source of unobserved heterogeneity. </p> </abstract>
<abstract> <p> Consider the first-order autoregressive process &lt;tex-math&gt;$y_{t}=\alpha y_{t-1}+e_{t}$&lt;/tex-math&gt;, y&lt;sub&gt;0&lt;/sub&gt; a fixed constant, e&lt;sub&gt;t&lt;/sub&gt; ∼ i.i.d. (0, σ &lt;sup&gt;2&lt;/sup&gt;), and let α̂ be the least-squares estimator of α based on a sample of size (T + 1) sampled at frequency h. Consider also the continuous time Ornstein-Uhlenbeck process &lt;tex-math&gt;$dy_{t}=\theta y_{t}\ dt+\sigma \ dw_{t}$&lt;/tex-math&gt; where w&lt;sub&gt;t&lt;/sub&gt; is a Wiener process and let θ̂ be the continuous time maximum likelihood (conditional upon y&lt;sub&gt;0&lt;/sub&gt;) estimator of θ based upon a single path of data of length N. We first show that the exact distribution of &lt;tex-math&gt;$N(\hat{\theta}-\theta)$&lt;/tex-math&gt; is the same as the asymptotic distribution of &lt;tex-math&gt;$T(\hat{\alpha}-\alpha)$&lt;/tex-math&gt; as the sampling interval converges to zero. This asymptotic distribution permits explicit consideration of the effect of the initial condition y&lt;sub&gt;0&lt;/sub&gt; upon the distribution of α̂. We use this fact to provide an approximation to the finite sample distribution of α̂ for arbitrary fixed y&lt;sub&gt;0&lt;/sub&gt;. The moment-generating function of &lt;tex-math&gt;$N(\hat{\theta}-\theta)$&lt;/tex-math&gt; is derived and used to tabulate the distribution and probability density functions. We also consider the moment of θ̂ and the power function of test statistics associated with it. In each case, the adequacy of the approximation to the finite sample distribution of α̂ is assessed for values of α in the vicinity of one. The approximations are, in general, found to be excellent. </p> </abstract>
<abstract> <p>This paper studies the properties of maximum likelihood estimates of cointegrated systems. Alternative formulations of such models are considered including a new triangular system error correction mechanism. It is shown that full system maximum likelihood brings the problem of inference within the family that is covered by the locally asymptotically mixed normal asymptotic theory provided that all unit roots in the system have been eliminated by specification and data transformation. This result has far reaching consequences. It means that cointegrating coefficient estimates are symmetrically distributed and median unbiased asymptotically, that an optimal asymptotic theory of inference applies, and that hypothesis tests may be conducted using standard asymptotic chi-squared tests. In short, this solves problems of specification and inference in cointegrated systems that have recently troubled many investigators. Methodological issues are also addressed and these provide the major focus of the paper. Our results favor the use of full system estimation in error correction mechanisms or subsystem methods that are asymptotically equivalent. They also point to disadvantages in the use of unrestricted VAR's that are formulated in levels and in certain single equation approaches to the estimation of error correction mechanisms. Unrestricted VAR's implicitly estimate unit roots that are present in the system and the relevant asymptotic theory for the VAR estimates of the cointegrating subspace inevitably involves unit root asymptotics. Single equation error correction mechanisms generally suffer from similar disadvantages through the neglect of additional equations in the system. Both examples point to the importance of the proper use of information in the estimation of cointegrated systems. In classical estimation theory the neglect of information typically results in a loss of statistical efficiency. In cointegrated systems deeper consequences occur. Single equation and VAR approaches sacrifice asymptotic median unbiasedness as well as optimality and they run into inferential difficulties through the presence of nuisance parameters in the limit distributions. The advantages of the use of fully specified systems techniques are shown to be all the more compelling in the light of these alternatives. Attention is also given to the information content that is necessary to achieve optimal estimation of the cointegrating coefficients. It is shown that optimal estimation of the latter does not require simultaneous estimation of the transient dynamics even when the parameters of the transient dynamics are functionally dependent on the parameters of the cointegrating relationship. All that is required is consistent estimation of the long run covariance matrix of the system residuals and this covariance matrix estimate can be utilized in regression formulae of the generalized least squares type. Thus, optimal estimation can be achieved without a detailed specification of the system's transient responses and thus, in practice, without the use of eigenvalue routines such as those employed in the Johansen (1988) procedure.</p> </abstract>
<abstract> <p>This paper establishes the asymptotic normality of series estimators for nonparametric regression models. Polynomial series estimators, trigonometric series estimators, and Gallant's Fourier flexible form estimators are prime examples of the estimators covered by the results. The results apply to a wide variety of estimands in the regression model under consideration, including derivatives and integrals of the regression function. The errors in the model may be homoskedastic or heteroskedastic. The paper also considers series estimators for additive interactive regression (AIR), partially linear regression, and semiparametric index regression models and shows them to be consistent and asymptotically normal. All of the consistency and asymptotic normality results in the paper follow from one set of general results for series estimators.</p> </abstract>
<abstract> <p>GARCH models have been applied in modelling the relation between conditional variance and asset risk premia. These models, however, have at least three major drawbacks in asset pricing applications: (i) Researchers beginning with Black (1976) have found a negative correlation between current returns and future returns volatility. GARCH models rule this out by assumption. (ii) GARCH models impose parameter restrictions that are often violated by estimated coefficients and that may unduly restrict the dynamics of the conditional variance process. (iii) Interpreting whether shocks to conditional variance "persist" or not is difficult in GARCH models, because the usual norms measuring persistence often do not agree. A new form of ARCH is proposed that meets these objections. The method is used to estimate a model of the risk premium on the CRSP Value-Weighted Market Index from 1962 to 1987.</p> </abstract>
<abstract> <p>The paper develops a discrete state space solution method for a class of nonlinear rational expectations models. The method works by using numerical quadrature rules to approximate the integral operators that arise in stochastic intertemporal models. The method is particularly useful for approximating asset pricing models and has potential applications in other problems as well. An empirical application uses the method to study the relationship between the risk premium and the conditional variability of the equity return under an ARCH endowment process.</p> </abstract>
<abstract> <p>Measured aggregate U.S. consumption does not behave like a martingale. This paper develops and tests two variants of the permanent income model that are consistent with this fact. In both variants, we assume agents make decisions on a continuous time basis. According to the first variant, the martingale hypothesis holds in continuous time and serial persistence in measured consumption reflects only the effects of time aggregation. We investigate this variant using both structural and atheoretical econometric models. The evidence against these models is far from overwhelming. This suggests that the martingale hypothesis may yet be a useful way to conceptualize the relationship between aggregate quarterly U.S. consumption and income. According to the second variant of the permanent income model, serial persistence in measured consumption reflects the effects of exogenous technology shocks and time aggregation. In this model, continuous time consumption does not behave like a martingale. We find little evidence against this variant of the permanent income model. It is difficult, on the basis of aggregate quarterly U.S. data, to convincingly distinguish between the different continuous time models considered in the paper.</p> </abstract>
<abstract> <p>The preference reversal phenomenon is usually interpreted as evidence of nontransitivity of preference, but has also been explained as the result of: the difference between individuals' responses to choice and valuation problems; the devices used by experimenters to elicit valuations; and the "random lottery selection" incentive system. This paper reports an experiment designed so that none of these factors could generate systematic nontransitivities; yet systematic violations of transitivity were still found. The pattern of violation was analogous with that found in previous preference reversal experiments and is consistent with regret theory.</p> </abstract>
<abstract> <p> We demonstrate existence of a perfect foresight equilibrium under borrowing constraints in a one-sector model with infinitely-lived heterogeneous agents. The class of admissible preferences includes, but is not limited to, recursive preferences. Existence is proven using a tâtonnement argument under appropriate conditions on preferences and technology. A new measure of discounting, the norm of marginal impatience, is used to determine which technologies are admissible. Depending on the norm of marginal impatience, the admissible technology may either allow for permanent growth, or have a maximum sustainable stock. </p> </abstract>
<abstract> <p>The subject of this paper is the decentralization of decision making when agents have information which is incomplete and possibly exclusive. The first theorem states that in economic environments with three or more individuals, there exists a mechanism whose Bayesian equilibria coincide with a desired collection of social choice functions if and only if closure, incentive compatibility, and Bayesian monotonicity conditions are satisfied. With regards to the previous literature, this closes the gap between necessary and sufficient conditions by using a slightly stronger definition of Bayesian monotonicity, and extends the definition of economic environments to permit externalities. The second theorem extends the analysis to noneconomic environments. It states that closure, incentive compatibility, and a combination of monotonicity and no-veto conditions, are sufficient for implementation, when there are at least three individuals. An example shows that in a Bayesian setting, the second theorem does not hold for separate monotonicity and no-veto style conditions.</p> </abstract>
<abstract> <p>We study the problem of implementing social choice correspondences using the concept of undominated Nash equilibrium, i.e. Nash equilibrium in which no one uses a weakly dominated strategy. We show that this mild refinement of Nash equilibrium has a dramatic impact on the set of implementable correspondences. Our main result is that if there are at least three agents in the society, then any correspondence which satisfies the usual no veto power condition is implementable unless some agents are completely indifferent over all possible outcomes. Many common welfare criteria, such as the Pareto correspondence, and several familiar voting rules, such as majority and plurality rules, satisfy our conditions. This possibility result stands in sharp contrast to the more restrictive findings with implementation in either Nash equilibrium or subgame perfect equilibrium. We present several examples to illustrate the difference between undominated Nash implementation and implementation with alternative solution concepts.</p> </abstract>
<abstract> <p>A sufficient condition for market demand to satisfy the Law of Demand is that the mean of all households' income effect matrices be positive definite. We show how this mean income effect matrix can be estimated from cross section data under metonymy, an assumption about the distribution of households' characteristics. The estimation procedure uses the nonparametric method of average derivatives. Income effect matrices estimated this way from U.K. family expenditure data are in fact positive definite. This result can be explained by a special form of heteroskedasticity in the data: households' demands are more dispersed at higher income levels.</p> </abstract>
<abstract> <p>The purpose of this paper is to present the likelihood methods for the analysis of cointegration in VAR models with Gaussian errors, seasonal dummies, and constant terms. We discuss likelihood ratio tests of cointegration rank and find the asymptotic distribution of the test statistics. We characterize the maximum likelihood estimator of the cointegrating relations and formulate tests of structural hypotheses about these relations. We show that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian. Once a certain eigenvalue problem is solved and the eigenvectors and eigenvalues calculated, one can conduct inference on the cointegrating rank using some nonstandard distributions, and test hypotheses about cointegrating relations using the χ &lt;sup&gt;2&lt;/sup&gt; distribution.</p> </abstract>
<abstract> <p>A Bayesian analysis of the linear regression model with only parts of the prior distribution specified or a robust Bayesian analysis lead to sets of posterior distributions. The so-called feasible ellipsoid of Leamer (1978) describes the region of posterior means if for conjugate priors the prior covariance matrix is varying in the set of symmetric positive-definite matrices. As an extension to Bayesian confidence sets (HPD regions, i.e. regions of highest posterior probability) we introduce the concept of HiFi (high fiduciary) regions. The HiFi region is a union of HPD regions, and is a tool for describing the dependence of the posterior distribution on the prior covariance. In this paper we assume that the prior covariance matrix varies in an interval of positive definite matrices.</p> </abstract>
<abstract> <p>While technically p-values should not be interpreted as probablities, they often are, and their usual asymptotic equivalence to Bayesian posterior tail probabilities provides an approximate justification for doing so. In inference about possibly nonstationary dynamic models the usual asymptotic equivalence fails, however. We show with three-dimensional graphs how it is possible that in autoregressive models the distribution of the estimator is skewed asymptotically, while the likelihood and hence the posterior pdf remains symmetric. We show that no single prior can rationalize treating p-values as probabilities in these models, and we display examples of the sample-dependent "priors" that would do so. We argue that these results imply at a minimum that the usual test statistics and covariance matrices for autoregressions, which characterize the likelihood shape in dynamic models just as in static regression models, should be reported without any corrections for the special unit root distribution theory, even if the corrected classical p-values are reported as well.</p> </abstract>
<abstract> <p> The invariance properties of some well known asymptotic tests are studied. Three types of invariance are considered: invariance to the representation of the null hypothesis, invariance to one-to-one transformations of the parameter space (reparametrizations), and invariance to one-to-one transformations of the model variables such as changes in measurement units. Tests that are not invariant include the Wald test and generalized versions of it, a widely used variant of the Lagrane multiplier test, Neyman's C(α) test, and a generalized version of the latter. For all these tests, we show that simply changing measurement units can lead to vastly different answers even when equivalent null hypotheses are tested. This problem is illustrated by considering regression models with Box-Cox transformations on the variables. We observe, in particular, that various consistent estimators of the information matrix lead to test procedures with different invariance properties. General sufficient conditions are then established, under which the generalized C(α) test becomes invariant to reformulations of the null hypothesis and/or to one-to-one transformations of the parameter space as well as to transformations of the variables. In many practical cases where Wald-type tests lack invariance, we find that special formulations of the generalized C(α) test are invariant and hardly more costly to compute than Wald tests. This computational simplicity stands in contrast with other invariant tests such as the likelihood ratio test. We conclude that noninvariant asymptotic tests should be avoided or used with great care. Further, in many situations, the suggested implementation of the generalized C(α) test often yields an attractive substitute to the Wald test (which is not invariant) and to other invariant tests (which are more costly to perform). </p> </abstract>
<abstract> <p>This paper demonstrates that an optimizing model of a monetary economy can produce perfect foresight equilibria, in which the price level fluctuates forever. Cyclically or chaotically fluctuating equilibria are more likely to exist when the rate of money supply growth is high. Furthermore, the set of equilibrium prices may have a complicated topological structure, which poses a more serious problem concerning the validity of comparative statics method than any sort of indeterminacy previously discussed in the literature.</p> </abstract>
<abstract> <p>This paper analyzes asset prices in an exchange economy in which the preferences of the representative agent exhibit habit formation. For a general class of utility indices and endowment processes we (i) characterize the optimal demand for consumption, and (ii) derive closed form solutions for the interest rate and the excess return on risky assets. Our analysis demonstrates that consumption smoothness may obtain even when the interest rate is stochastic. In economies with habit formation the consumption CAPM need not hold when the coefficients of the endowment process are stochastic processes; asset risk premia are larger under mild assumptions. The interest rate depends on the growth in the standard of living. Historical consumption as well as possible future consumption paths are determinants of the current interest rate. The analysis provides new testable restrictions on the behavior of financial assets. A new methodology, Malliavin calculus, is introduced to compute equilibrium asset risk premia.</p> </abstract>
<abstract> <p>In this paper we provide a framework to study the aggregate dynamic behavior of an economy where individual units follow (S, s) policies. We characterize structural and stochastic heterogeneities that ensure convergence of the economy's aggregate to that of its frictionless counterpart, determine the speed at which convergence takes place, and describe the transitional dynamics of this economy.</p> </abstract>
<abstract> <p>Recent work demonstrates that dynastic assumptions guarantee the irrelevance of all redistributional policies, distortionary taxes, and prices--the neutrality of fiscal policy (Ricardian equivalence) is only the "tip of the iceberg." In this paper, we investigate the possibility of reinstating approximate Ricardian equivalence by introducing a small amount of friction in intergenerational links. If Ricardian equivalence depends upon significantly shorter chains of links than do these stronger neutrality results, then friction may dissipate the effects that generate strong neutrality, without significantly affecting the Ricardian result. Although this intuition turns out to be essentially correct, we show that models with small amounts of friction have other untenable implications. We conclude that the theoretical case for Ricardian equivalence remains tenuous.</p> </abstract>
<abstract> <p>In a repeated partnership game with imperfect monitoring, we distinguish among the effects of (1) reducing the interest rate, (2) shortening the period over which actions are held fixed, and (3) shortening the lag with which accumulated information is reported. All three changes are equivalent in games with perfect monitoring. With imperfect monitoring, reducing the interest rate always increases the possibilities for cooperation, but the other two changes always have the reverse effect when the interest rate is small.</p> </abstract>
<abstract> <p>We examine the effects of renegotiation in an agency relationship. We show how renegotiation affects: (i) the set of actions the principal can induce the agent to take; and (ii) the cost of implementing a given action. We show that, when the principal receives an unverifiable signal of the agent's action, renegotiation can improve welfare. This result stands in contrast to Fudenberg and Tirole's (1990) finding that renegotiation lowers welfare when the principal receives no signal about the agent's action prior to renegotiation.</p> </abstract>
<abstract> <p>We consider the problem of fair allocation in economies with indivisible goods. Our main objective is to identify appealing subsolutions of the no-envy solution. We formulate desirable properties of solutions, and look for solutions satisfying them: given an allocation chosen by a solution for some economy and given a subgroup of the agents, consider the problem of fairly distributing among its members the resources that this group has collectively received. The solution is consistent if it recommends that the same bundle be attributed to each of these agents as initially. We show that there is no proper subsolution of the no-envy solution that satisfies consistency. However, many subsolutions satisfy bilateral consistency or the converse of consistency. But again, there is no proper subsolution satisfying these two properties together.</p> </abstract>
<abstract> <p>This paper describes a new approach to normative economics, combining the theory of social choice with econometric modeling of aggregate consumer behavior. We first derive a system of aggregate demand functions by exact aggregation over individual demand functions. We then construct measures of individual welfare from systems of individual demand functions. Finally, we incorporate these measures into a social welfare function, introducing ethical assumptions based on horizontal and vertical equity. To illustrate the application of this approach, we consider the U.S. standard of living and its cost over the period 1947-1985.</p> </abstract>
<abstract> <p>This paper investigates pure strategy sequential equilibria of repeated games with imperfect monitoring. The approach emphasizes the equilibrium value set and the static optimization problems embedded in extremal equilibria. A succession of propositions, central among which is "self-generation," allow properties of constrained efficient supergame equilibria to be deduced from the solutions of the static problems. We show that the latter include solutions having a "bang-bang" property; this affords a significant simplification of the equilibria that need be considered. These results apply to a broad class of asymmetric games, thereby generalizing our earlier work on optimal cartel equilibria. The bang-bang theorem is strengthened to a necessity result: under certain conditions, efficient sequential equilibria have the property that after every history, the value to players of the remainder of the equilibrium must be an extreme point of the equilibrium value set. General implications of the self-generation and bang-bang propositions include a proof of the monotonicity of the equilibrium average value set in the discount factor, and an iterative procedure for computing the value set.</p> </abstract>
<abstract> <p>In this paper, we re-examine the data from O'Neill's experiment involving a repeated, two-person, constant-sum game. We find that there is less evidence in support of the minimax hypothesis than indicated by O'Neill. There is strong evidence of serial correlation in players' choices, with several players displaying statistically significant dependence on the past moves of their opponents. We interpret this finding as evidence that the players themselves rejected minimax play as the appropriate model for their opponents' behavior. We find no evidence that players' behavior approached minimax behavior as players became more experienced.</p> </abstract>
<abstract> <p>We extend Maskin's results on Nash implementation. First, we establish a condition which is both necessary and sufficient for Nash implementability if there are three or more agents (this was the case covered by Maskin's sufficiency result). Second--and more important--we examine the two-agent case (for which there existed no general sufficiency results). The two-agent model is of course the leading case for applications to contracting and bargaining. For this case, too, we establish a condition which is both necessary and sufficient for Nash implementability. We use our theorems to derive simpler sufficiency conditions that are applicable in a wide variety of economic environments.</p> </abstract>
<abstract> <p>Using the Eaton and Lipsey model, we show that a hierarchical system of central places is socially optimal: firms having less frequent purchases are clustered with firms having more frequent purchases in any configuration minimizing total transport and production costs.</p> </abstract>
<abstract> <p>This paper clarifies and extends the classical Roy model of self selection and earnings inequality. The original Roy model, based on the assumption that log skills are normally distributed, is shown to imply that pursuit of comparative advantage in a free market reduces earnings inequality compared to the earnings distribution that would result if workers were randomly assigned to sectors. Aggregate log earnings are right skewed even if one sectoral distribution is left skewed. Most major implications of the log normal Roy model survive if differences in skills are log concave. However few implications of the model survive if skills are generated from more general distributions. We consider the identifiability of the Roy model from data on earnings distributions. The normal theory version is identifiable without regressors or exclusion restrictions. Sectoral distributions can be identified knowing only the aggregate earnings distribution. For general skill distributions, the model is not identified and has no empirical content. With sufficient price variation, the model can be identified from multimarket data. Cross-sectional variation in regressors can substitute for price variation in restoring empirical content to the Roy model.</p> </abstract>
<abstract> <p>The effects of firm pension plan provisions on the retirement decisions of older employees are analyzed. The empirical results are based on data from a large firm, with a typical defined benefit pension plan. The "option value" of continued work is the central feature of the analysis. Estimation relies on a retirement decision rule that is close in spirit to the dynamic programming rule but is considerably less complex than a comprehensive implementation of that rule, thus greatly facilitating the numerical analysis.</p> </abstract>
<abstract> <p>In the general structural equation model only the direction of the vector of coefficients of the endogenous variables is determined. The traditional normalization rule defines the coefficients that are of interest but should not be embodied in the estimation procedure: we show that the properties of the traditionally defined ordinary least squares and two stage least squares estimators are distorted by their dependence on the normalization rule. Symmetrically normalized analogues of these estimators are defined and are shown to have essentially similar properties to those of the limited information maximum likelihood estimator.</p> </abstract>
<abstract> <p> Following Jurečková (1981) we introduce a finite-sample measure of performance of regression estimators based on tail behavior. The least squares estimator is studied in detail, and we find that it achieves good tail performance under strictly Gaussian conditions. However, the tail performance of the least-squares estimator is found to be extremely poor in the case of heavy-tailed error distributions. Further analysis of the least-squares estimator with light-tailed errors indicates the strong influence of the design matrix in determining tail performance. Turning to the tail behavior of various robust estimators of the parameters of the linear model, we focus on tail performance under heavy (algebraic) tailed errors. The &lt;tex-math&gt;$l_{1}\text{-estimator}$&lt;/tex-math&gt; is seen to be a leading case: we find a simple characterization of its tail behavior in terms of the design configuration and show that a broad class of M-estimators have the same performance. Perhaps most significantly, it is shown that our finite-sample measure of tail performance is, for heavy tailed error distributions, essentially the same as the finite sample concept of breakdown point introduced by Donoho and Huber (1983). This finding provides an important probabilistic interpretation of the breakdown point and clarifies the role of tail behavior as a quantitative measure of robustness. This link is further explored for high-breakdown regression estimators including Rousseeuw's (1984) least-median-of-squares estimator. </p> </abstract>
<abstract> <p>This paper develops asymptotic prediction functions that approximate the shape of the density of future observations and correct for parameter uncertainty. The functions are based on extensions to a definition of predictive likelihood originally suggested by Lauritzen and Hinkley. The prediction function is shown to possess efficiency properties based on the Kullback-Leibler measure of information loss. Examples of the application of the prediction function and the derivation of relative efficiency are shown for linear normal models, nonnormal models, and ARCH models.</p> </abstract>
<abstract> <p>We study a rich class of noncooperative games that includes models of oligopoly competition, macroeconomic coordination failures, arms races, bank runs, technology adoption and diffusion, R&amp;D competition, pretrial bargaining, coordination in teams, and many others. For all these games, the sets of pure strategy Nash equilibria, correlated equilibria, and rationalizable strategies have identical bounds. Also, for a class of models of dynamic adaptive choice behavior that encompasses both best-response dynamics and Bayesian learning, the players' choices lie eventually within the same bounds. These bounds are shown to vary monotonically with certain exogenous parameters.</p> </abstract>
<abstract> <p>Previous analyses of principal-agent problems with moral hazard assume the parties can commit to a contract that will not be renegotiated. We allow the contract to be renegotiated after the agent's choice of action and before the observation of the action's consequences. Here, the principal cannot induce the agent to take a high level of effort with probability one, since the principal would renegotiate the contract to shield the agent from risk, and in equilibrium the agent randomizes over effort levels. The optimal contract gives the agent a menu of compensation schemes: safe ones intended for low-effort workers, and risky ones for those whose effort is high. The optimal contract may give the agent a positive rent, in contrast to the case without renegotiation.</p> </abstract>
<abstract> <p>This paper reports on an experimental study of the way in which individuals make inferences from publicly available information. We compare the predictions of a theoretical model of a common knowledge inference process with actual behavior. In the theoretical model, "perfect Bayesians," starting with private information, take actions; an aggregate statistic is made publicly available; the individuals do optimal Bayesian updating and take new actions; and the process continues until there is a common knowledge equilibrium with complete information pooling. We find that the theoretical model roughly predicts the observed behavior, but the actual inference process is clearly less efficient than the standard of the theoretical model, and while there is some pooling, it is incomplete.</p> </abstract>
<abstract> <p>A scheme of plain conversation is constructed, which is a universal mechanism for all noncooperative games with incomplete information with at least four players, in the following sense: every solution which can be achieved by means of an arbitrary communication mechanism is a correlated equilibrium payoff of the game extended by the scheme of plain conversation; the corresponding equilibrium strategies use only finite sets of messages. By a property of the correlated equilibrium, a similar result holds also with the Nash equilibrium solution concept. The scheme of plain conversation is universal because it does not depend on the specification of the game, nor on the solution to achieve and it is easily implemented in any institutional context. The main proposition of this paper states that it can be used without any loss of efficiency. Some results are also available for three and two person games.</p> </abstract>
<abstract> <p>The concept of strategically stable equilibria introduced by Kohlberg and Mertens (1986) is motivated by a number of requirements which Kohlberg and Mertens argue a satisfactory solution concept for noncooperative games should satisfy. They defined a number of different solution concepts of the following general form: a stable set of equilibria is a minimal closed set of equilibria such that all small perturbations of the game have equilibria close to the stable set. Their definitions varied with the definition of a small perturbation. Unfortunately none of their definitions satisfied all of their requirements. We show that by changing the definition of a perturbation it is possible to define a solution concept that does satisfy all of the requirements. In the main definition of this paper we look not at perturbations of the payoffs or the strategy space but directly at perturbations of the best reply correspondence. With the appropriate topology on this space of perturbations the resulting definition does satisfy all of the requirements. We also show that one does not have much freedom in the topology one uses.</p> </abstract>
<abstract> <p>This paper endogenizes the frequency of major discoveries and the extent of their refinement. Four axioms deliver a one-parameter family of beliefs that guide exploratory effort. Such effort trades off the prospect of major new discovery against the chance of successfully refining discoveries made in the past. The only other parameter is the cost of making new discoveries relative to the cost of refining old ones. The paper derives time-series properties of inventive activity as they relate to the two parameters, and it discusses several specific inventions and their subsequent refinement. In doing so, the paper arguably enhances our understanding of the process of discovery.</p> </abstract>
<abstract> <p>This paper estimates semiparametric reduced-form neoclassical models of life-cycle fertility in Sweden. Rising female wages delay times to all conceptions and reduce total conceptions. This result is robust across a variety of empirical specifications. In the best fitting models in which marital status is excluded, male incomes--defined to be zero for unmarried women without a cohabiting male partner--reduce times to conceptions and increase total conceptions. The results on female wages are robust across a variety of empirical specifications, but those on male incomes are not robust to the introduction of marital status and cohabitational status variables, even though models with such variables can be rejected by our model specification tests. We find a particular neoclassical model that predicts fertility attained at different ages as well as the aggregate time series of birth rates. A model that excludes wages and incomes predicts fertility attained at different ages but fails to predict the aggregate time series and is dominated by the neoclassical model in terms of non-nested test criteria. Cohort drift found in estimated parameters is consistent with the expansion of pronatal social programs. The estimated neoclassical model produces strong short-run responses of birth rates to wages and incomes of the sort that have been found in the time-series literature on fertility while generating the relatively weak long-run responses to economic variables found in the cross-sectional literature on completed fertility.</p> </abstract>
<abstract> <p>In this paper it will be shown that any conditional moment test of functional form of nonlinear regression models can be converted into a chi-square test that is consistent against all deviations from the null hypothesis that the model represents the conditional expectation of the dependent variable relative to the vector of regressors.</p> </abstract>
<abstract> <p>This paper presents a simple pairwise meetings model of trade. The new feature is that agents have asymmetric information about the true state of the world. The focus is on the transmission of the information through the process of trade. The qualitative question: to what extent is the information revealed to uninformed agents through the trading process, when the market is in some sense frictionless? In particular: does the decentralized process give rise to full revelation results as derived by the literature on rational expectations for centralized and competitive environments? In the context of the model of this paper, it turns out that the information is not fully revealed to uninformed agents, even when the market is in some sense approximately frictionless.</p> </abstract>
<abstract> <p>We analyze a model of optimal consumption and portfolio selection in which consumption services are generated by holding a durable good. The durable good is illiquid in that a transaction cost must be paid when the good is sold. It is shown that optimal consumption is not a smooth function of wealth; it is optimal for the consumer to wait until a large change in wealth occurs before adjusting his consumption. As a consequence, the consumption based capital asset pricing model fails to hold. Nevertheless, it is shown that the standard, one factor, market portfolio based capital asset pricing model does hold in this environment. It is shown that the optimal durable level is characterized by three numbers (not random variables), say x, y, and z (where &lt;latex&gt;$x &lt; y &lt; z$&lt;/latex&gt;). The consumer views the ratio of consumption to wealth (c/W) as his state variable. If this ratio is between x and z, then he does not sell the durable. If c/W is less than x or greater than z, then he sells his durable and buys a new durable of size S so that S/W = y. Thus y is his "target" level of c/W. If the stock market moves up enough so that c/W falls below x, then he sells his small durable to buy a larger durable. However, there will be many changes in the value of his wealth for which c/W stays between x and z, and thus consumption does not change. Numerical simulations show that small transactions costs can make consumption changes occur very infrequently. Further, the effect of consumption transactions costs on the demand for risky assets is substantial.</p> </abstract>
<abstract> <p> The theory of precautionary saving is shown to be isomorphic to the Arrow-Pratt theory of risk aversion, making possible the application of a large body of knowledge about risk aversion to precautionary saving--and more generally, to the theory of optimal choice under risk. In particular, a measure of the strength of the precautionary saving motive analogous to the Arrow-Pratt measure of risk aversion is used to establish a number of new propositions about precautionary saving, and to give a new interpretation of the Drèze-Modigliani substitution effect. </p> </abstract>
<abstract> <p>We present a method for calculating cost-of-living index numbers for arbitrary base period income levels without using heavy econometric estimation methods. The second order approximation formulas contain parameters which can easily be estimated by a differential demand system. Using a suitable specification, it is possible to work at a low level of commodity aggregation. The method is demonstrated on Netherlands data for the period 1952-1981.</p> </abstract>
<abstract> <p>We introduce in this paper a method for solving nonlinear-quadratic Pareto problems. The method provides the investigator with a set of time series realizations for the variables in the economy. By obtaining a large number of these realizations, we can approximate the empirical distributions of a variety of statistics, which will give a detailed description of the model's properties. In particular, those statistics can be compared with the similar ones obtained from actual data, and different criteria for goodness of fit can be defined on the basis of these comparisons.</p> </abstract>
<abstract> <p>This paper considers estimation and hypothesis testing in linear time series models when some or all of the variables have unit roots. Our motivating example is a vector autoregression with some unit roots in the companion matrix, which might include polynomials in time as regressors. In the general formulation, the variable might be integrated or cointegrated of arbitrary orders, and might have drifts as well. We show that parameters that can be written as coefficients on mean zero, nonintegrated regressors have jointly normal asymptotic distributions, converging at the rate T&lt;sup&gt;1/2&lt;/sup&gt;. In general, the other coefficients (including the coefficients on polynomials in time) will have nonnormal asymptotic distributions. The results provide a formal characterization of which t or F tests--such as Granger causality tests--will be asymptotically valid, and which will have nonstandard limiting distributions.</p> </abstract>
<abstract> <p>We consider the time series regression model where the error term follows a nonstable autoregressive process, and present a general approach for deriving the limiting distribution of a normalized estimator for the autoregressive parameter. The present approach is quite straightforward and leads us to an accurate evaluation of the distribution function, unlike the other approaches suggested in the literature. Our methodology is illustrated and percent points are tabulated. The present approach produces a good approximation method for the finite sample distribution and also provides an accurate evaluation of the limiting powers of some unit root tests under a sequence of local alternatives.</p> </abstract>
<abstract> <p>This paper develops an asymptotic theory for residual based tests for cointegration. These tests involve procedures that are designed to detect the presence of a unit root in the residuals of (cointegrating) regressions among the levels of economic time series. Attention is given to the augmented Dickey-Fuller (ADF) test that is recommended by Engle-Granger (1987) and the Z&lt;sub&gt;α&lt;/sub&gt; and Z&lt;sub&gt;t&lt;/sub&gt; unit root tests recently proposed by Phillips (1987). Two new tests are also introduced, one of which is invariant to the normalization of the cointegrating regression. All of these tests are shown to be asymptotically similar and simple representations of their limiting distributions are given in terms of standard Brownian motion. The ADF and Z&lt;sub&gt;t&lt;/sub&gt; tests are asymptotically equivalent. Power properties of the tests are also studied. The analysis shows that all the tests are consistent if suitably constructed but that the ADF and Z&lt;sub&gt;t&lt;/sub&gt; tests have slower rates of divergence under cointegration than the other tests. This indicates that, at least in large samples, the Z&lt;sub&gt;α&lt;/sub&gt; test should have superior power properties. The paper concludes by addressing the larger issue of test formulation. Some major pitfalls are discovered in procedures that are designed to test a null of cointegration (rather than no cointegration). These defects provide strong arguments against the indiscriminate use of such test formulations and support the continuing use of residual based unit root tests. A full set of critical values for residual based tests is included. These allow for demeaned and detrended data and cointegrating regressions with up to five variables.</p> </abstract>
<abstract> <p>This paper tests the effects of the level and length of unemployment insurance (UI) benefits on unemployment durations. The paper particularly studies individual behavior during the weeks just prior to when benefits lapse. Higher UI benefits are found to have a strong negative effect on the probability of leaving unemployment. However, the probability of leaving unemployment rises dramatically just prior to when benefits lapse. When the length of benefits is extended, the probability of a spell ending is high in the week benefits were previously expected to lapse. Individual data are used with accurate information on spell durations, and the level and length of benefits. Semiparametric estimation techniques are used and compared to alternative approaches. The semiparametric approach yields more plausible estimates and provides useful diagnostics.</p> </abstract>
<abstract> <p>In this paper we demonstrate the feasibility of estimating a Nash labor market equilibrium model using only information on workers. The equilibrium model is adapted from Albrecht and Axell (1984) and is based on workers who are homogeneous in terms of market productivity and heterogeneous in terms of nonmarket productivity, and on firms which are heterogeneous in terms of productive efficiency. The equilibrium model is contrasted in terms of its fit to the data with an unrestricted version of the model which is based on a mixture of negative binomial distributions. The equilibrium model fails to conform to the data in exactly the dimension of its major focus, namely it implies that measurement error accounts for almost all of the dispersion in observed wages. The equilibrium model also does not do well in fitting the unemployment duration distribution compared to the unrestricted model. The problem is that the duration distribution itself does not support the existence of significant heterogeneity, as evidenced by the estimates of the unrestricted model. The paper also illustrates the use of such models for policy analysis by simulating the welfare effects of a minimum wage.</p> </abstract>
<abstract> <p>This paper considers asymptotically efficient instrumental variables estimation of nonlinear models in an i.i.d. environment. The class of models includes nonlinear simultaneous equations models and other models of interest. A problem in constructing efficient instrumental variables estimators for such models is that the optimal instruments involve a conditional expectation, calculation of which can require functional form assumptions for the conditional distribution of endogenous variables, as well as integration. Nonparametric methods provide a way of avoiding this difficulty. Here it is shown that nonparametric estimates of the optimal instruments can give asymptotically efficient instrumental variables estimators. Also, ways of choosing the nonparametric estimate in applications are discussed. Two types of nonparametric estimates of the optimal instruments are considered. Each involves nonparametric regression, one by nearest neighbor and the other by series approximation. The finite sample properties of the estimators are considered in a small sampling experiment involving an endogenous dummy variable model.</p> </abstract>
<abstract> <p>We build a theory of the choice of techniques in joint production, at a given profit rate, considering a market algorithm. Partial results are extended by means of an abstract notion of technology, where techniques meet demand and satisfy local properties. Global results on the existence, uniqueness, and convergence towards an equilibrium technique are obtained. We thus characterize cases where a square technique is reached, which provides an answer to an old debate, initiated by Jevons, between classical and neo-classical economists. But the framework allows for a more general interpretation in terms of two-level planning procedures.</p> </abstract>
<abstract> <p>This paper proposes a new approach to the study of economic problems that have hitherto been modelled as games with discontinuous payoffs. Typically, the discontinuities arise from indeterminacies in the underlying problem. Our point of departure from the conventional approach is to view the sharing rules which resolve these indeterminacies as part of the solution rather than as part of the description of the model. A solution to our model is a sharing rule, together with a profile of (mixed) strategies that satisfies the usual (Nash) best response criterion. Our main result is that such a solution always exists.</p> </abstract>
<abstract> <p>Individuals have or observe partly private information. They independently choose acts, possibly including messages. The center may also act. Individuals' utilities may depend on all acts and information, including others' private information. Are there incentives depending only on public information that make desired behavior a Bayesian equilibrium? Assume incentive payments are separable and fully transferable. Appropriate incentives exist either if the center's information--perhaps solely messages--depends stochastically, however slightly, on all relevant private information, or if individuals' relative valuations of acts, however divergent, are not too dissimilarly affected by different states of nature. More generally, we give necessary and sufficient conditions for existence whenever the strategy profile asks agents to reveal all private knowledge relevant to their beliefs about the center's information. We also develop equivalences on the possible values of private information--concepts of similarity of agent types--that are key to resolving existence questions without such responsiveness or requiring budget balance.</p> </abstract>
<abstract> <p>In this article we compare two methods for a monopolist to sell information. In a direct sale buyers observe the seller's signal (or noisy versions of it) and subsequently use what they observe to make investment decisions. In an indirect sale the seller creates a portfolio based on his private information and then sells shares to the traders. Indirect sale allows the seller to control more effectively the buyers' reactions to the information, but may not allow as much surplus to be extracted as is possible with direct sale. We show how the optimal selling method depends on the extent to which information is revealed by assets' equilibrium prices.</p> </abstract>
<abstract> <p>We analyze steady state equilibrium of the circular flow of money and goods in a continuous time model with infinitely lived agents. Workers sell their endowments of labor on a Walrasian labor market. Labor is used in production, the output of which flows into inventories. Capitalists consume directly out of their own inventories and use remaining inventories in a retail market. The retail market is characterized by random sequential search with prices set by capitalists on a take-it or leave-it basis. Workers thus receive a continuous flow of wages and spend money in discrete jumps at times set by a Poisson process. The equilibrium distribution of money holdings is the asymptotic steady state of this stochastic process. The economy has a unique uniform price steady state equilibrium. The more rapid the search process in the retail market the higher the absolute retail price and wage, and the higher the real wage. The instantaneous effect of an equal per capita infusion of money is to raise the price, wage, real wage, and transactions rate. The immediate post-infusion price and wage can overshoot their new asymptotic values.</p> </abstract>
<abstract> <p>This paper is concerned with the theory of saving when consumers are not permitted to borrow, and with the ability of such a theory to account for some of the stylized facts of saving behavior. When consumers are relatively impatient, and when labor income is independently and identically distributed over time, assets act like a buffer stock, protecting consumption against bad draws of income. The precautionary demand for saving interacts with the borrowing constraints to provide a motive for holding assets. If the income process is positively autocorrelated, but stationary, assets are still used to buffer consumption, but do so less effectively and at a greater cost in terms of foregone consumption. In the limit, when labor income is a random walk, it is optimal for impatient liquidity constrained consumers simply to consume their incomes. As a consequence, a liquidity constrained representative agent cannot generate aggregate U.S. saving behavior if that agent receives aggregate labor income. Either there is no saving, when income is a random walk, or saving is contracyclical over the business cycle, when income changes are positively autocorrelated. However, in reality, microeconomic income processes do not resemble their average, and it is possible to construct a model of microeconomic saving under liquidity constraints which, at the aggregate level, reproduces many of the stylized facts in the actual data. While it is clear that many households are not liquidity constrained, and do not behave as described here, the models presented in the paper seem to account for important aspects of reality that are not explained by traditional life-cycle models.</p> </abstract>
<abstract> <p> Using Monte Carlo methodology, this paper investigates the effect of dynamics and simultaneity on the finite sample properties of instrumental variables statistics for testing nested and non-nested hypotheses. Simple numerical-analytical formulae (response surfaces) are obtained which closely approximate the statistics' unknown size and power functions for a dynamic simultaneous equations model. The analysis illustrates the value and limitations of asymptotic theory in interpreting finite sample properties. Two practical results arise. The F form of the Wald statistic is favored over its χ &lt;sup&gt;2&lt;/sup&gt; form, and "large-σ" and small "effective" sample size strongly affect the test of over-identifying restrictions and the Cox-type test. </p> </abstract>
<abstract> <p>A test for long-run memory that is robust to short-range dependence is developed. It is an extension of the "range over standard deviation" or R/S statistic, for which the relevant asymptotic sampling theory is derived via functional central limit theory. This test is applied to daily and monthly stock returns indexes over several time periods and, contrary to previous findings, there is no evidence of long-range dependence in any of the indexes over any sample period or sub-period once short-range dependence is taken into account. Illustrative Monte Carlo experiments indicate that the modified R/S test has power against at least two specific models of long-run memory, suggesting that stochastic models of short-range dependence may adequately capture the time series behavior of stock returns.</p> </abstract>
<abstract> <p>This paper introduces a semiparametric estimation method for polychotomous choice models. The method does not require a parametric structure for the systematic subutility of observable exogenous variables. The distribution of the random terms is assumed to be known up to a finite-dimensional parameter vector. In contrast, previous semiparametric methods of estimating discrete choice models have concentrated on relaxing parametric assumptions on the distribution of the random terms while leaving the systematic subutility parametrically specified. The systematic subutility is assumed to possess properties, such as monotonicity and concavity, that are typically assumed in microeconomic theory. The estimator for the systematic subutility and the parameter vector of the distribution is shown to be strongly consistent. A computational technique to calculate the estimators is developed.</p> </abstract>
<abstract> <p>This paper discusses frequency domain methods of statistical inference for time series, in which the degree of smoothing in nonparametric spectral density estimation is determined from the data. A result on uniform convergence of spectral density estimates is established and applied in developing robust large-sample inference procedures based on instrumental variables estimates of a fairly general linear system with disturbances that have serial correlation of unknown form. Optimal instruments depend on the spectral density of the disturbances and on the frequency response function, and the latter is also nonparametric when the system is incomplete. The system may be parameterized over only a proper subset of the sampling frequencies. We adapt to these nonparametric features, justifying feasible, asymptotically optimal parameter estimates. The above results all assume covariance stationarity of the data, except for the special case of a multivariate linear regression model for which certain trending explanatory variables are permitted. In all these results, the degree of smoothing is allowed to depend on the data in a quite general way. The major technical achievement of the paper is to justify under primitive conditions the consistency of a particular, cross-validation, method of automatically determining a desirable degree of smoothing, which we apply to a multiple regression model. A Monte Carlo study illustrates our methodology and examines its performance in small and moderate samples.</p> </abstract>
<abstract> <p>The theory applying to dynamic programming has furnished a useful set of techniques for the analysis of many types of sequential models. This theory, however, has not yielded heretofore much information about the differentiability properties of optimal solutions. This aspect is of particular interest as regards the qualitative analysis of optimal paths, where differentiable methods are often called into play. This paper shows roughly that if the objective is twice continuously differentiable and strongly concave, then any interior optimal path is continuously differentiable with respect to the initial state.</p> </abstract>
<abstract> <p>It is shown that an increasing policy function that is the solution of a C&lt;sup&gt;2&lt;/sup&gt; dynamic programming problem is always C&lt;sup&gt;1&lt;/sup&gt;. This implies that the value function is C&lt;sup&gt;2&lt;/sup&gt;. Examples are given to show that the policy function might not be twice differentiable and therefore the value might not be three times differentiable, even if the program is C&lt;sup&gt;3&lt;/sup&gt;.</p> </abstract>
<abstract> <p>The following characterization of Walrasian allocations is proved: an allocation for an exchange economy with C&lt;sup&gt;1&lt;/sup&gt; preferences is Walrasian if there is a set of net trades that (i) contains all sums of elements of itself, (ii) contains the negation of any net trade that would improve some agent in the final position, and (iii) is such that the bundles in the allocation are weakly preferred to those obtainable from the initial endowments by means of the given set of net trades. These conditions are similar to ones studied by Schmeidler and Vind (1972) and Vind (1978), but here they are thought of as characterizing the set of net trades available in steady state equilibria of market games like those studied by Douglas Gale (1984, 1985, 1986a, 1986b, and 1986c). The characterization result is used as a key step in the proof of results like Gale's: the allocations induced by steady state equilibria are Walrasian for the economy given by the (constant) flow of new agents into the market. Our approach generalizes the one followed in Gale (1986c) and allows us to dispense with assumptions made in previous treatments. For example Gale's (1986a) assumption of dispersed characteristics is dropped. We also demonstrate that such a result depends on the assumption that agents cannot observe the past behavior of agents with whom they trade.</p> </abstract>
<abstract> <p>A matching technology is proposed which permits the analysis of decentralized matching problems in which sellers can publicly commit to a trading price that differs from the price at which buyers expect to trade elsewhere in the market. The equilibrium path with this form of ex ante pricing is compared with the path for trading prices that prevails when prices are determined ex post by bargaining. It is shown that when demand and supply are nearly equal, the equilibrium ex ante price offer will lie below the Nash bargaining split, while this relationship will be reversed when the level of excess demand is large. It is shown that sellers will have an incentive to make ex ante offers when prices elsewhere are determined by Nash bargaining. This can be interpreted to mean that Nash bargaining is an unstable pricing institution.</p> </abstract>
<abstract> <p>This paper analyzes a model of a dynamic monopolist who produces at constant unit cost. Each period a new cohort of consumers enters the market. Each entering cohort is identical. Consumers within a cohort have different tastes for the good. My main results are: If players are sufficiently patient, any positive average profit less than the maximum feasible level can be attained in a subgame-perfect equilibrium; in the subset of subgame-perfect equilibria in which players use stationary strategies, the seller cannot make sales at prices significantly greater than the lowest willingness to pay when period length goes to zero; and the seller attains the maximum profit when commitment is feasible by charging the same (static monopoly) price in every period.</p> </abstract>
<abstract> <p>Many researchers have employed ARCH models to estimate conditional variances and covariances. How successfully can ARCH models carry out this estimation when they are misspecified? This paper employs continuous record asymptotics to approximate the distribution of the measurement error. This allows us to (a) compare the efficiency of various ARCH models, (b) characterize the impact of different kinds of misspecification (e.g., "fat-tailed" errors, misspecified conditional means) on efficiency, and (c) characterize asymptotically optimal ARCH conditional variance estimates. We apply our results to derive optimal ARCH filters for three diffusion models, and to examine in detail the filtering properties of GARCH(1, 1), AR(1) EGARCH, and the model of Taylor (1986) and Schwert (1989).</p> </abstract>
<abstract> <p>This paper provides a general framework for proving the &lt;tex-math&gt;$\sqrt{T}\text{-consistency}$&lt;/tex-math&gt; and asymptotic normality of a wide variety of semiparametric estimators. The class of estimators considered consists of estimators that can be defined as the solution to a minimization problem based on a criterion function that may depend on a preliminary infinite dimensional nuisance parameter estimator. The method of proof exploits results concerning the stochastic equicontinuity of stochastic processes. The results are applied to the problem of semiparametric weighted least squares estimation of partially parametric regression models. Primitive conditions are given for &lt;tex-math&gt;$\sqrt{T}\text{-consistency}$&lt;/tex-math&gt; and asymptotic normality of this estimator.</p> </abstract>
<abstract> <p>This paper derives some exact finite sample distributions and characterizes the tail behavior of maximum likelihood estimators of the cointegrating coefficients in error correction models. It is shown that the reduced rank regression estimator has a distribution with Cauchy-like tails and no finite moments of integer order. The maximum likelihood estimator of the coefficients in a particular triangular system representation is studied and shown to have matrix t-distribution tails with finite integer moments to order T - n + r where T is the sample size, n is the total number of variables in the system, and r is the dimension of the cointegration space. These results help to explain some recent simulation studies where extreme outliers are found to occur more frequently for the reduced rank regression estimator than for alternative asymptotically efficient procedures that are based on the triangular representation. In a simple triangular system, the Wald statistic for testing linear hypotheses about the columns of the cointegrating matrix is shown to have an F distribution, analogous to Hotelling's T&lt;sup&gt;2&lt;/sup&gt; distribution in multivariate linear regression.</p> </abstract>
<abstract> <p>In this paper I develop a practical extension of McFadden's method of simulated moments estimator for limited dependent variable models to the panel data case. The method is based on a factorization of the MSM first order condition into transition probabilities, along with the development of a new highly accurate method for simulating these transition probabilities. A series of Monte-Carlo tests show that this MSM estimator performs quite well relative to quadrature-based ML estimators, even when large numbers of quadrature points are employed. The estimator also performs well relative to simulated ML, even when a highly accurate method is used to simulate the choice probabilities. In terms of computational speed, complex panel data models involving random effects and ARMA errors may be estimated via MSM in times similar to those necessary for estimation of simple random effects models via ML-quadrature.</p> </abstract>
<abstract> <p>This paper describes and analyzes movements of older men among labor force states using quarterly observations derived from the Retirement History Survey (RHS). A comparison of transition rates using these quarterly records with the more typical biannual records from the RHS and other sources reveals substantial undercounts in the biannual data, indicating that the prevalence of labor force movements at older ages has been underestimated previously. The quarterly data reveal a much sharper spike in the exit rate at exact age 65 than is found in data measured at less frequent intervals. Estimates are presented of the parameters of a reduced form model of labor force transitions. A novel feature of the model is the exploration of dynamic aspects of labor force behavior at older ages. The results show that labor force dynamics at older ages are important, including duration and spell occurrence dependence, and work experience effects. These effects are robust to nonparametric controls for unobserved heterogeneity. The estimates indicate that social security benefits have strong effects on the timing of labor force transitions at older ages, but that changes in social security benefit levels over time have not contributed much to the trend toward earlier labor force exit.</p> </abstract>
<abstract> <p>We derive a necessary and sufficient condition for the solution set of an optimization problem to be monotonic in the parameters of the problem. In addition, we develop practical methods for checking the condition and demonstrate its applications to the classical theories of the competitive firm, the monopolist, the Bertrand oligopolist, consumer and growth theory, game theory, and general equilibrium analysis.</p> </abstract>
<abstract> <p>A global game is an incomplete information game where the actual payoff structure is determined by a random draw from a given class of games and where each player makes a noisy observation of the selected game. For 2 × 2 games, it is shown that, when the noise vanishes, iterated elimination of dominated strategies in the global game forces the players to conform to Harsanyi and Selten's risk dominance criterion.</p> </abstract>
<abstract> <p>Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information.</p> </abstract>
<abstract> <p>This paper discusses the dynamic implications of learning in a large population coordination game, focusing on the structure of the matching process which describes how players meet. As in Kandori, Mailath, and Rob (1993) a combination of experimentation and myopia creates "evolutionary" forces which lead players to coordinate on the risk dominant equilibrium. To describe play with finite time horizons it is necessary to consider the rates at which the dynamic systems converge. In large populations with uniform matching, play is determined largely by historical factors. In contrast, when players interact with small sets of neighbors it is more reasonable to assume that evolutionary forces may determine the outcome.</p> </abstract>
<abstract> <p>We design and study an OLG experimental economy where the government finances a fixed real deficit through seigniorage. The economy has continua of nonstationary rational expectations equilibria and two stationary rational expectations equilibria. We do not observe nonstationary rational expectations paths. Observed paths tend to converge close to, or somewhat below, the low inflation stationary state (low ISS). The adaptive learning hypothesis is consistent with our data in selecting the low ISS rational expectations equilibrium as a long-run stationary equilibrium. nevertheless, simple adaptive learning models do not capture the market uncertainty or the biases observed in the data.</p> </abstract>
<abstract> <p>In a decision-theoretic model of a firm, I represent managers as information processors of limited capacity; efficiency is measured in terms of the number of processors and the delay between the receipt of information by the organization and the implementation of the decision. I characterize efficient networks for both one-shot and repeated regimes, as well as the corresponding "production function" relating the number of items processed to the number of processors and the delay. I sketch some applications to common decision paradigms, and implications for decentralization and organizational returns to scale.</p> </abstract>
<abstract> <p>It is shown that any informationally decentralized mechanism that realizes fair allocations over the class of classical pure exchange environments has a message space of dimension no smaller than the number of agents times the number of commodities. Since the equal income Walrasian mechanism, in which all agents take prices parametrically and maximize utility subject to the average income constraint, realizes fair outcomes over the class of classical pure exchange environments and has a message space of that dimension it is informationally efficient. Further, it is shown that it is the unique informationally efficient mechanism realizing fair allocations.</p> </abstract>
<abstract> <p>This paper reports estimates of the structural parameters of a stochastic control model that describes the labor decisions of small farmers in Burkina Faso, West Africa. The focus of the estimation is on measuring flexibility in production and intertemporal substitutability in consumption. Full information maximum likelihood estimates of the primitive parameters of the model are computed even though optimal labor decision rules cannot be derived analytically. Vuong's non-nested model specification test shows that this method yields parameter estimates that are superior to those derived by assuming that farmers solve a deterministic control problem. The low levels of agricultural labor effort commonly observed in the survey area are shown to be a consequence both of the low productivity of labor in archaic rainfed agriculture and of farmers' awareness that, in the absence of a labor market, overly ambitious production plans lead to seasonal manpower constraints. To meet rainfed farmers' concerns, agricultural research institutes and extension services should factor flexibility into their research agendas.</p> </abstract>
<abstract> <p>Weighted average derivatives are useful parameters for semiparametric index models and nonparametric demand analysis. This paper gives efficiency results for average derivative estimators, including formulating estimators that have high efficiency. Our analysis is carried out in three steps. First, we derive the efficiency bound for weighted average derivatives of conditional location functionals, such as the conditional mean and median. Second, we derive the efficiency bound for semiparametric index models, where the location measure depends only on indices, or linear combinations of the regressors. Third, we compare the bound for index models with the asymptotic variance of weighted average derivative estimators of the index coefficients. We characterize the form of the optimal weight function when the distribution of the regressors is elliptically symmetric. In more general cases, we discuss how to combine estimators with different weight functions to achieve efficiency. We derive a general condition for approximate efficiency of pooled (minimum chi square) estimators for index model coefficients, based on weighted average derivatives. Finally, we discuss ways of selecting the type and number of weighting functions to achieve high efficiency.</p> </abstract>
<abstract> <p>We study repeated games in which players observe a public outcome that imperfectly signals the actions played. We provide conditions guaranteeing that any feasible, individually rational payoff vector of the stage game can arise as a perfect equilibrium of the repeated game with sufficiently little discounting. The central condition requires that there exist action profiles with the property that, for any two players, no two deviations--one by each player--give rise to the same probability distribution over public outcomes. The results apply to principal-agent, partnership, oligopoly, and mechanism-design models, and to one-shot games with transferable utilities.</p> </abstract>
<abstract> <p>A model of trade with m buyers and m sellers is considered in which price is set to equate revealed demand and supply. In a Bayesian Nash equilibrium, each trader acts not as a price-taker, but instead misrepresents his true demand/supply to influence price in his favor. This causes inefficiency. We show that in any equilibrium the amount by which a trader misreports is O(1/m) and the corresponding inefficiency is O(1/m&lt;sup&gt;2&lt;/sup&gt;). The indeterminacy and the inefficiency that is caused by the traders' bargaining behavior in small markets thus rapidly vanishes as the market increases in size.</p> </abstract>
<abstract> <p>We model investment as an N-player game with a pure informational externality. Each player's payoff depends only on his own action and the state of nature. However, because a player's action reveals his private information, players wait to see what other players will do. Equilibrium is inefficient because delay is costly and information is imperfectly revealed. We characterize the unique symmetric perfect Bayesian equilibrium and study the robustness of delay, which turns out to be sensitive to the reaction speed and the number of players. We establish the following results. (i) When the period length is very short, the game ends very quickly and there is a form of herding or informational cascade which results in a collapse of investment. (ii) As the period length increases, the possibility of herding disappears. (iii) As the number of players increases, the rate of investment and the information flow are eventually independent of the number of players; adding more players simply increases the number who delay. (iv) In the limit, the time-profile of investment is extreme, a period of low investment followed either by an investment surge or a collapse.</p> </abstract>
<abstract> <p>We provide a convergence theory for adaptive learning algorithms useful for the study of learning by economic agents. Our results extend the framework of Ljung (1977) previously utilized by Marcet-Sargent (1989a, b) and Woodford (1990), by permitting nonlinear laws of motion driven by stochastic processes that may exhibit moderate dependence, such as mixing and mixingale processes. We draw on previous work by Kushner and Clark (1978) to provide readily verifiable and/or interpretable conditions ensuring algorithm convergence, chosen for their suitability in the context of adaptive learning.</p> </abstract>
<abstract> <p>Strategic implications of the learning curve hypothesis are analyzed in a model of a price-setting, differentiated duopoly selling to a sequence of heterogeneous buyers with uncertain demands. A unique and symmetric Markov perfect equilibrium is characterized, and two concepts of self-reinforcing market dominance investigated. One is increasing dominance (ID), whereby the leading firm has a greater probability of winning the next sale; the other is increasing increasing dominance (IID), whereby a firm's probability of winning the next sale increases with the length of its lead. Sufficient conditions for IID (and thus for ID) are that the discount factor is sufficiently low or sufficiently high. Other sufficient conditions for ID and IID are given in the case of two-step learning, in which a firm reaches the bottom of its learning curve after just two sales. However, examples are also constructed for the two-step learning case in which neither ID nor IID holds. It is also shown that, in equilibrium, IID implies that learning is privately disadvantageous, although it is socially advantageous. Finally, introducing avoidable fixed costs and possible exit into the model yields a new theory of predatory pricing based on the learning curve hypothesis.</p> </abstract>
<abstract> <p>The author proves an equivalence between large games with effective small groups of players and games generated by markets. Small groups are effective if all or almost all gains to collective activities can be achieved by groups bounded in size. A market is an exchange economy where all participants have concave, quasi-linear payoff functions. The market approximating a game is socially homogeneous--all participants have the same monotonic nondecreasing, and 1-homogeneous payoff function. Our results imply that any market (more generally, any economy with effective small groups) can be approximated by a socially homogeneous market.</p> </abstract>
<abstract> <p>This paper introduces and characterizes a new class of solutions to cooperative bargaining problems that can be rationalized by generalized Gini orderings defined on the agents' utility gains. Generalized Ginis are orderings which can be represented by quasi-concave, nondecreasing functions that are linear in rank-ordered subspaces of Euclidean n-space. Our characterization of (multi-valued) generalized Gini bargaining solutions is based on a linear invariance requirement in addition to some standard conditions. Linear invariance requires that if the feasible set is changed by adding a constant to one agent's component of each vector of the feasible set (without changing the agent's rank order), the solution responds by adding the same constant to the corresponding agent's utility in the outcome of the problem. Weak linear invariance requires the solution to change in a parallel way if a constant is added to all components of each vector of the feasible set. In the two-person case, the generalized Gini bargaining solutions can be characterized by imposing weak linear invariance, whereas, for n ⩾ 3 agents, linear invariance is required. As a by-product of our main result, we show that the egalitarian bargaining solution is characterized if single-valuedness is required together with some of our axioms. The main focus of cooperative bargaining theory has been the characterization of single-valued solutions. The results of this paper demonstrate that relaxing this assumption enlarges the class of solutions considerably. Hence, single-valuedness is not merely an assumption of convenience but, rather, an assumption of substance.</p> </abstract>
<abstract> <p>The Generalized Extreme Value Model was developed by McFadden for the case with discrete choice sets. The present paper extends this model to cases with both discrete and continuous choice sets and choice sets that are unobservable by the analyst. We also propose behavioral assumptions that justify random utility functions (processes) that have a max-stable structure, i.e., utility processes where the finite-dimensional distributions are of the multivariate extreme value type. Finally we derive nonparametrically testable implications for the choice probabilities in the continuous case.</p> </abstract>
<abstract> <p>This paper establishes a correspondence in large samples between classical hypothesis tests and Bayesian posterior odds tests for models without trends. More specifically, tests of point null hypotheses and one- or two-sided alternatives are considered (where nuisance parameters may be present under both hypotheses). It is shown that for certain priors the Bayesian posterior odds test is equivalent in large samples to classical Wald, Lagrange multiplier, and likelihood ratio tests for some significance level and vice versa. The priors considered under the alternative hypothesis are taken to shrink to the null hypothesis at rate n&lt;sup&gt;-1/2&lt;/sup&gt; as the sample size n increases.</p> </abstract>
<abstract> <p>This paper presents a new approach to organizing universal health insurance. First the government divides the entire population into many large groups. Then, the government creates a Federal Health Insurance System (HealthFed), modeled on the Federal Reserve System, to fill the role now played by the benefits office of a large firm. The HealthFed would create a short menu of alternatives, solicit bids for insuring the entire group, and price alternatives. There would be redistribution between groups and pricing of alternatives to reflect optimal social insurance principles. There would be no connection between health insurance and employment.</p> </abstract>
<abstract> <p>This paper provides an extension of Savage's subjective expected utility theory for decisions under uncertainty. It includes in the set of events both unambiguous events for which probabilities are additive and ambiguous events for which probabilities are permitted to be nonadditive. The main axiom is cumulative dominance, which adapts stochastic dominance to decision making under uncertainty. We derive a Choquet expected utility representation and show that a modification of cumulative dominance leads to the classical expected utility representation. The relationship of our approach with that of Schmeidler, who uses a two-stage formulation to derive Choquet expected utility, is also explored. Our work may be viewed as a unification of Schmeidler (1989) and Gilboa (1987).</p> </abstract>
<abstract> <p>We provide conditions on an exchange economy with asymmetric information that guarantee that when the economy is replicated sufficiently often, there will be an allocation which is incentive compatible, individually rational, and nearly efficient. The main theorem covers both the case in which aggregate uncertainty remains when the economy is replicated and the case in which replication eliminates aggregate uncertainty. In addition, we demonstrate how our theorem does or does not apply to standard asymmetric information problems such as the buyer's bid double auction problem, Akerlof's lemons problem, and insurance with asymmetric information.</p> </abstract>
<abstract> <p>Using a standard model of nonuniform pricing, properties are derived for both free entry and limited entry Bertrand equilibrium in nonuniform strategies. These properties follow from the feasibility of Bertrand undercutting with nonuniform prices. When there is free entry, zero-profit minimum average cost production occurs in equilibrium. If, in addition, average cost attains its minimum at any quantity below market output, all prices collapse to a uniform price equal to minimum average cost. A necessary condition for existence of this equilibrium is derived and compared to similar conditions from uniform price theory. Analogies to uniform price theory depend on the rationing capabilities of nonuniform prices and the rationing assumptions employed in uniform price models. Without free entry, prices may not collapse to a uniform price in equilibrium. Also, positive profit may occur but all firms must earn equal profit and incur equal marginal cost, while consumers pay average outlay no greater than marginal cost.</p> </abstract>
<abstract> <p>We consider the fair division of unproduced goods when monetary compensations are feasible and utilities are quasi-linear. Under efficiency, the four axioms: individual rationality, resource monotonicity, population solidarity, and stand alone test, show little compatibility. However when the goods have enough substitutability in everyone's preferences, the Shapley value of the surplus sharing game satisfies all four axioms. An example is the optimal assignment of indivisible goods when every agent consumes only one good.</p> </abstract>
<abstract> <p>This paper develops a two-sector overlapping-generations model. It characterizes the dynamical system globally and establishes sufficient conditions for the existence of a globally unique perfect-foresight equilibrium. The paper provides therefore a useful framework for global dynamic analysis of phenomena whose modeling requires a multi-dimensional commodity space. The analysis demonstrates that gross substitutability in consumption is not sufficient to ensure the determinacy of equilibrium in this production economy. However, if in addition the investment good is capital intensive and second period consumption of two-period-lived individuals is a normal good, then the perfect-foresight equilibrium is globally unique. If the consumption good is capital intensive, global uniqueness does not hold despite the existence of gross substitutability and normality in consumption. The paper establishes in this case sufficient conditions for local determinacy, local indeterminacy, and global indeterminacy.</p> </abstract>
<abstract> <p>The existence and stability of invariant distributions for stochastically monotone processes is studied. The Knaster-Tarski fixed point theorem is applied to establish existence of fixed points of mappings on compact sets of measures that are increasing with respect to a stochastic ordering. Global convergence of a monotone Markov process to its unique invariant distribution is established under an easily verified assumption. Topkis' theory of supermodular functions is applied to stochastic dynamic optimization, providing conditions under which optimal stationary decisions are monotone functions of the state and induce a monotone Markov process. Applications of these results to investment theory, stochastic growth, and industry equilibrium dynamics are given.</p> </abstract>
<abstract> <p>This paper reports the results of a systematic experimental comparison of the effect of alternative arbitration systems on dispute rates. By using a common underlying distribution of arbitrator "fair" awards in the different arbitration systems we are able to compare dispute rates across different arbitration procedures while holding fixed the amount of objective uncertainty the bargainers face. The three main findings indicate that (i) dispute rates are inversely related to the monetary costs of disputes, (ii) the dispute rate in a final-offer arbitration system is at least as high as the dispute rate in a comparable conventional arbitration system, and (iii) dispute rates are inversely related to the uncertainty costs of disputes, indicating that some bargainers behave as if they were risk averse.</p> </abstract>
<abstract> <p> We investigate the implementation of social choice functions in complete information environments. We consider social choice functions (scf's) which map from a finite set of preference profiles to lotteries over alternatives, and require virtual implementation in iteratively undominated strategies. An scf x is virtually implementable in iteratively undominated strategies if for all ε &gt; 0, there exists an scf y which is ε-close to x (that is, for all preference profiles, x and y map to lotteries which are within ε of one another) and which is (exactly) implementable in iteratively undominated strategies. Under very weak domain restrictions we show that if there are three or more players, any scf is virtually implementable in iteratively undominated strategies. A noteworthy feature of our constructions is that we only employ finite mechanisms. As a corollary, we obtain virtual implementation in (pure and) mixed strategy Nash equilibrium using well-behaved (in particular, finite) mechanisms. The literature on implementation in Nash equilibrium and its refinements is compromised by its reliance on game forms with unnatural features (for example, "integer" games), or "modulo" constructions with mixed strategies arbitrarily excluded. In contrast, our results allow for mixed strategies and do not rely on mechanisms with obviously suspicious features. </p> </abstract>
<abstract> <p>A fixed group of n agents share a one input, one output technology with decreasing returns. We propose the following cost sharing formula. Agent 1 with the lowest demand of output q&lt;sub&gt;1&lt;/sub&gt; pays (1/n)th of the cost of nq&lt;sub&gt;1&lt;/sub&gt;. Agent 2, with the next lowest demand q&lt;sub&gt;2&lt;/sub&gt; pays agent 1's cost share plus 1/(n - 1)th of the incremental cost from nq&lt;sub&gt;1&lt;/sub&gt; to (n - 1)q&lt;sub&gt;2&lt;/sub&gt; + q&lt;sub&gt;1&lt;/sub&gt;. Agent 3, with the next lowest demand q&lt;sub&gt;3&lt;/sub&gt; pays agent 2's cost share, plus 1/(n - 2)th of the incremental cost from (n - 1)q&lt;sub&gt;2&lt;/sub&gt; + q&lt;sub&gt;1&lt;/sub&gt; to (n - 2)q&lt;sub&gt;3&lt;/sub&gt; + q&lt;sub&gt;2&lt;/sub&gt; + q&lt;sub&gt;1&lt;/sub&gt;. And so on. Among agents endowed with convex and monotonic preferences, serial cost sharing is dominance solvable and its unique equilibrium is also robust to coalitional deviations. We show that no other smooth cost sharing mechanism yields a unique Nash equilibrium at all preference profiles.</p> </abstract>
<abstract> <p>For each two-player game, a linear-programming algorithm finds a component of the Nash equilibria and a subset of its perfect equilibria that are simply stable: there are nearby equilibria for each nearby game that perturbs one strategy's probability or payoff more than others.</p> </abstract>
<abstract> <p>This paper studies the class of denumerable-armed (i.e. finite- or countably infinite-armed) bandit problems with independent arms and geometric discounting over an infinite horizon, in which each arm generates rewards according to one of a finite number of distributions, or "types." The number of types in the support of an arm, as also the types themselves, are allowed to vary across the arms. We derive certain continuity and curvature properties of the dynamic allocation (or Gittins) index of Gittins and Jones (1974), and provide necessary and sufficient conditions under which the Gittins-Jones result identifying all optimal strategies for finite-armed bandits may be extended to infinite-armed bandits. We then establish our central result: at each point in time, the arm selected by an optimal strategy will, with strictly positive probability, remain an optimal selection forever. More specifically, for every such arm, there exists (at least) one type of that arm such that, when conditioned on that type being the arm's "true" type, the arm will survive forever and continuously with nonzero probability. When the reward distributions of an arm satisfy the monotone likelihood ratio property (MLRP), the survival prospects of an arm improve when conditioned on types generating higher expected rewards; however, we show how this need not be the case in the absence of MLRP. Implications of these results are derived for the theories of job search and matching, as well as other applications of the bandit paradigm.</p> </abstract>
<abstract> <p>The paper investigates the existence of stationary sunspot equilibria (SSE) in the vicinity of a steady state in a general, one-step forward looking economic model of dimension n. It is shown that, whenever the steady state is indeterminate, for the associated deterministic dynamics--i.e., there exists a continuum of perfect foresight paths converging to the steady state--then there exists a continuum of SSE of finite order in any neighborhood of the steady state. The proof relies upon bifurcation theory; it provides a characterization of the random processes for which SSE may appear and a description of the location of the support of the SSE close to the bifurcation. The results apply in particular to linear models.</p> </abstract>
<abstract> <p>This paper develops and analyzes a dynamic stochastic model for a competitive industry which endogenously determines processes for entry and exit and for individual firms' output and employment. The concept of stationary equilibrium is introduced, extending long run industry equilibrium theory to account for entry, exit, and heterogeneity in the size and growth rate of firms. Conditions under which there will be entry and exit in the stationary equilibrium are given. Cross-sectional properties--across size and age cohorts--are analyzed and compared to the data. Implications for the equilibrium distributions of profits and the value of firms are analyzed. The effect of changes in the parameters describing the technological and market conditions of the industry on the equilibrium size distribution and turnover rates are also analyzed.</p> </abstract>
<abstract> <p>We experimentally test some qualitative predictions of the Kreps-Wilson (1982b) model of reputation building in a version of a borrower-lender game first used by Camerer-Weigelt (1988). Our results do not differ substantially from those of Camerer-Weigelt--when we use the same parameters they used. However, we find that the theory fails to account for observed behavioral variations to parameter changes. There is a systematic response to changes in the payoff function of the borrowers, but this response is not in the direction predicted by the theory. This cannot be reconciled with the theory by an appeal to "homemade" priors of the type specified by Camerer-Weigelt. Furthermore, the estimated magnitude of "homemade" priors in the Camerer-Weigelt experiment is inconsistent with estimates derived from behavior observed in our own experiment.</p> </abstract>
<abstract> <p> The paper reexamines the foundations of the axiomatic Nash bargaining theory. More specifically it questions the interpretation of the Nash bargaining solution and extends it to a family of non-expected utility preferences. A bargaining problem is presented as &lt;tex-math&gt;$\langle X,D,\geq _{1},\geq _{2}\rangle $&lt;/tex-math&gt; where X is a set of feasible agreements (described in physical terms), D is the disagreement event and &lt;tex-math&gt;$\geq _{1}$&lt;/tex-math&gt; and &lt;tex-math&gt;$\geq _{2}$&lt;/tex-math&gt; are preferences defined on the space of lotteries in which the prizes are the elements in X and D. The (ordinal)-Nash bargaining solution is defined as an agreement y* satisfying for all p ϵ [0, 1] and for all x ϵ X: if &lt;latex&gt;$px&gt;_{1}y^{\ast}$&lt;/latex&gt; then &lt;tex-math&gt;$py^{\ast}\geq _{2}x$&lt;/tex-math&gt; and if &lt;latex&gt;$px&gt;_{2}y^{\ast}$&lt;/latex&gt; then &lt;tex-math&gt;$py^{\ast}\geq _{1}x$&lt;/tex-math&gt; where px is the lottery which gives x with probability p and D with probability 1 - p. Revisions of the Pareto, Symmetry, and IIA Axioms characterize the (ordinal)-Nash bargaining solution. In the expected utility case this definition is equivalent to that of the Nash bargaining solution. However, this definition is to be preferred since it allows a statement of the Nash bargaining solution in everyday language and makes possible its natural extension to a wider set of preferences. It also reveals the logic behind some of the more interesting results of the Nash bargaining solution such as the comparative statics of risk aversion and the connection between the Nash bargaining solution and strategic models. </p> </abstract>
<abstract> <p>In this paper a new estimator is proposed for discrete choice models with choice-based sampling. The estimator is efficient and can incorporate information on the marginal choice probabilities in a straightforward manner and for that case leads to a procedure that is computationally and intuitively more appealing than the estimators that have been proposed before. The idea is to start with a flexible parameterization of the distribution of the explanatory variables and then rewrite the estimator to remove dependence on these parametric assumptions.</p> </abstract>
<abstract> <p>This paper considers a class of statistics that can be written as the ratio of the sample variance of a filtered time series to the sample variance of the original series. Any such statistic is shown to be optimal under normality for testing a null of white noise against some class of serially dependent alternatives. A simple characterization of the class of alternative models is provided in terms of the filter upon which the statistic is based. These results are applied to demonstrate that a variance ratio test for mean reversion is an optimal test for mean reversion and to illustrate the forms of mean reversion it is best at detecting.</p> </abstract>
<abstract> <p>This lecture surveys recent models of growth and trade in search of descriptions of technologies that are consistent with episodes of very rapid income growth. Emphasis is placed on the on-the-job accumulation of human capital: learning by doing. Possible connections between learning rates and international trade are discussed.</p> </abstract>
<abstract> <p>Different extensive form games with the same reduced normal form can have different information sets and subgames. This generates a tension between a belief in the strategic relevance of information sets and subgames and a belief in the sufficiency of the reduced normal form. We identify a property of extensive form information sets and subgames which we term strategic independence. Strategic independence is captured by the reduced normal form, and can be used to define normal form information sets and subgames. We prove a close relationship between these normal form structures and their extensive form namesakes. Using these structures, we are able to motivate and implement solution concepts corresponding to subgame perfection, sequential equilibrium, and forward induction entirely in the reduced normal form, and show close relations between their implications in the normal and extensive form.</p> </abstract>
<abstract> <p>Perfect equilibria of finitely repeated games may be vulnerable to the possibility of renegotiation among players. We study the limiting properties of the set of payoffs from equilibria that are immune to renegotiation. Our main result is that the limit of the set of payoffs from renegotiation proof equilibria is either a singleton or a connected subset of the Pareto efficient frontier. A simple sufficient condition for the latter to occur is also provided.</p> </abstract>
<abstract> <p>A two-person game is of conflicting interests if the strategy to which player one would most like to commit herself holds player two down to his minimax payoff. Suppose there is a positive prior probability that player one is a "commitment type" who will always play this strategy. Then player one will get at least her commitment payoff in any Nash equilibrium of the repeated game if her discount factor approaches one. This result is robust against further perturbations of the informational structure and in striking contrast to the message of the Folk Theorem for games with incomplete information.</p> </abstract>
<abstract> <p>This paper specifies and empirically analyzes a continuous-time, linear-quadratic, representative consumer model with time-nonseparable preferences of several forms. Within this framework I show how time aggregation and time nonseparabilities in preferences over consumption streams can interact. The behavior of both seasonally adjusted and unadjusted consumption data is consistent with time-nonseparable preferences if consumption goods are durable and if individuals develop habit over the flow of services from the good. The data do not support a version of the model that ignores time nonseparabilities in preferences and focuses solely upon time aggregation.</p> </abstract>
<abstract> <p>This paper proposes an estimator for discrete choice models that makes no assumption concerning the functional form of the choice probability function, where this function can be characterized by an index. The estimator is shown to be consistent, asymptotically normally distributed, and to achieve the semiparametric efficiency bound. Monte-Carlo evidence indicates that there may be only modest efficiency losses relative to maximum likelihood estimation when the distribution of the disturbances is known, and that the small-sample behavior of the estimator in other cases is good.</p> </abstract>
<abstract> <p>The goal of choice-theoretic derivations of subjective probability is to separate a decision maker's underlying beliefs (subjective probabilities of events) from their preferences (attitudes toward risk). Classical derivations have all relied upon some form of the Marschak-Samuelson "Independence Axiom" or the Savage "Sure-Thing Principle," which imply that preferences over lotteries conform to the expected utility hypothesis. This paper presents a choice-theoretic derivation of subjective probability, in a Savage-type setting of purely subjective uncertainty, which neither assumes nor implies that the decision maker's preferences over lotteries necessarily conform to the expected utility hypothesis.</p> </abstract>
<abstract> <p> We propose a family of topologies on the space of consumption patterns in continuous time under uncertainty. Preferences continuous in any of the proposed topologies treat consumptions at nearby dates as almost perfect substitutes except possibly at information surprises. The topological duals of the family of proposed topologies essentially contain processes that are the sums of processes of absolutely continuous paths and martingales. Thus if equilibrium prices for consumption come from the duals, consumptions at nearly adjacent dates in a state of nature have almost equal prices except possibly at information surprises. In particular, if the information structure is generated by a Brownian motion, the duals are composed of Itô processes. We investigate some implications of our topologies on standard models of choice in continuous time as well as on recent models of non-time-separable representations of preferences. We also discuss the properties of prices of long-lived assets in economies populated with agents whose preferences are continuous in our topologies when there are no arbitrage opportunities. </p> </abstract>
<abstract> <p>We report on an experiment in which individuals play a version of the centipede game. In this game, two players alternately get a chance to take the larger portion of a continually escalating pile of money. As soon as one person takes, the game ends with that player getting the larger portion of the pile, and the other player getting the smaller portion. If one views the experiment as a complete information game, all standard game theoretic equilibrium concepts predict the first mover should take the large pile on the first round. The experimental results show that this does not occur. An alternative explanation for the data can be given if we reconsider the game as a game of incomplete information in which there is some uncertainty over the payoff functions of the players. In particular, if the subjects believe there is some small likelihood that the opponent is an altruist, then in the equilibrium of this incomplete information game, players adopt mixed strategies in the early rounds of the experiment, with the probability of taking increasing as the pile gets larger. We investigate how well a version of this model explains the data observed in the centipede experiments.</p> </abstract>
<abstract> <p>This paper supposes an individual cares about his/her own wealth not only directly but also via the relative standing that this wealth induces. The implications for risk-taking are investigated in particular. Such a model provides a natural explanation of the "concave-convex-concave" utility described by Friedman and Savage. However, there are a number of key differences between the present model and any model based on own wealth alone. For example, an equilibrium wealth distribution here may have a middle class. Further, the status interaction involves an externality and an equilibrium wealth distribution may be Pareto inefficient.</p> </abstract>
<abstract> <p>While there are now many results on the existence of marginal cost pricing equilibrium, we argue that they are unsatisfactory in one important respect. They typically make a survival assumption which is stated as a condition on the production equilibria of the economy. The primary objective of this paper is to provide an existence result which replaces such an assumption with one on the primitive data of the economy. Our main assumption is that no firm faces unbounded increasing returns in the sense that if it uses some input then the rate at which this input can be substituted into an output is finite. The proof is relatively elementary and allows for a straightforward extension to the case of more general pricing rules.</p> </abstract>
<abstract> <p>Suppose two agents play a game, each using a computable algorithm to decide what to do, these algorithms being common knowledge. We show that it is possible to act rationally provided we limit our attention to a natural subset of solvable games and to opponents who use rational algorithms; the outcome is a Nash equilibrium. Going further we show that rationality is possible on many domains of games and opposing algorithms but each domain requires a particular solution algorithm; no one algorithm is rational on all possible domains.</p> </abstract>
<abstract> <p>This paper considers the effect of an airline's scale of operation at an airport on the profitability of routes flown out of that airport. The empirical methodology uses the entry decisions of airlines as indicators of underlying profitability; the results extend the empirical literature on airport presence by providing a new set of estimates of the determinants of city pair profitability. The literature on empirical models of oligopoly entry is also extended, particularly via a focus on the important (and difficult) role of differences between firms.</p> </abstract>
<abstract> <p>An elicitation diagnostic forms a question that compares the information in the prior distribution with the information in the given sample. One elicitation diagnostic identifies a family of prior distributions that are so diffuse that they are practically equivalent to the "completely" diffuse prior. Another elicitation diagnostic identifies a family of prior distributions that concentrate enough mass in the neighborhood of zero that they are practically equivalent to the dogmatic prior which sets a parameter exactly equal to zero. The question that is asked is whether the subject's prior distribution falls in either of these two classes. If an affirmative answer can be given to either of these two elicitation questions then there is no need to go to the expense of a more accurate elicitation of the prior distribution.</p> </abstract>
<abstract> <p>Non-nested tests are proposed for competing models estimated by generalized method of moments. Results are presented for non-nested linear regression models with heteroskedasticity and serial correlation of unknown form and differing instrument validity assumptions. Regression forms of the statistics are also presented.</p> </abstract>
<abstract> <p>Manski (1985) has shown that the maximum score estimator of the coefficient vector of a binary response model is consistent under weak distributional assumptions. Cavanagh (1987) and Kim and Pollard (1989) have shown that N&lt;sup&gt;1/3&lt;/sup&gt; times the centered maximum score estimator converges in distribution to the random variable that maximizes a certain Gaussian process. The properties of the limiting distribution are largely unknown, and the result of Cavanagh and Kim and Pollard cannot be used for inference in applications. This paper describes a modified maximum score estimator that is obtained by maximizing a smoothed version of Manski's score function. Under distributional assumptions that are somewhat stronger than Manski's but still very weak, the centered smoothed estimator is asymptotically normal with a convergence rate that is at least N&lt;sup&gt;-2/5&lt;/sup&gt; and can be made arbitrarily close to N&lt;sup&gt;-1/2&lt;/sup&gt;, depending on the strength of certain smoothness assumptions. The estimator's rate of convergence is the fastest possible under the assumptions that are made. The parameters of the limiting distribution can be estimated consistently from data, thereby making statistical inference based on the smoothed estimator possible with samples that are sufficiently large.</p> </abstract>
<abstract> <p>This paper considers estimation of truncated and censored regression models with fixed effects in panel data. Up until now, no estimator has been shown to be consistent as the cross section dimension increases with the time dimension fixed. Trimmed least absolute deviations (LAD) and trimmed least squares estimators are proposed for the case where the panel is of length two, and it is proven that they are consistent and asymptotically normal under suitable regularity conditions. It is not necessary to maintain parametric assumptions on the error terms to obtain this result. Because three of the four estimators are defined as minimizers of nondifferentiable functions, traditional methods cannot be used to establish asymptotic normality. Instead, the approach of Pakes and Pollard (1989) is used. A small scale Monte Carlo study demonstrates that these estimators can perform well in small samples. Despite their nonlinear nature, the estimators are easy to calculate in practice, as are consistent estimators of their asymptotic variances. Generalization of the estimators to panels of arbitrary length is briefly discussed.</p> </abstract>
<abstract> <p> The paper derives efficiency bounds for conditional moment restrictions with a nonparametric component. The data form a random sample from a distribution F. There is a given function of the data and a parameter. The restriction is that a conditional expectation of this function is zero at some point in the parameter space. The parameter has two parts: a finite-dimensional component θ and a general function h, which is evaluated at a subset of the conditioning variables. An example is a regression function that is additive in parametric and nonparametric components, as arises in sample selection models. If F is assumed to be multinomial with known (finite) support, then the problem becomes parametric, with the values of h at the mass points forming part of the parameter vector. Then an efficiency bound for θ and for linear functionals of h can be obtained from the Fisher information matrix, as in Chamberlain (1987). The bound depends only upon certain conditional moments and not upon the support of the distribution. A general F satisfying the restrictions can be approximated by a multinomial distribution satisfying the restrictions, and so the explicit form of the multinomial bound applies in general. The efficiency bound for θ is extended to a more general model in which different components of h may depend on different known functions of the variables. Although an explicit form for the bound is no longer available, a variational characterization of the bound is provided. The efficiency bound is applied to a random coefficients model for panel data, in which the conditional expectation of the random coefficients given covariates plays the role of the function h. An instrumental-variables estimator is set up within a finite-dimensional, method-of-moments framework. The bound provides guidance on the choice of instrumental variables. </p> </abstract>
<abstract> <p> This paper analyzes a class of alternating-offer bargaining games with one-sided incomplete information for the case of "no gap." If sequential equilibria are required to satisfy the additional restrictions of stationarity, monotonicity, pure strategies, and no free screening, we establish the Silence Theorem: When the time interval between successive periods is made sufficiently short, the informed party never makes any serious offers in the play of alternating-offer bargaining games. A class of parametric examples suggests that the time interval required to assure silence is not especially brief. As a byproduct of the analysis, we also prove (under the same set of assumptions) a uniform version of the Coase Conjecture: When the time interval between successive periods is made sufficiently short, the initial serious offer by either party in an alternating-offer bargaining game must be less than ε times the highest possible buyer valuation, for an entire family of distribution functions. </p> </abstract>
<abstract> <p>A weakening of Kreps and Wilson's (1982) notion of sequential rationality for extensive form games is presented. The motivation behind this weakening stems from the difficulty in justifying sequentially rational behavior in subgames reachable only through a violation of sequential rationality. Although the notion of weak sequential rationality developed here is based upon extensive form considerations, it bears a close relation to Selten's (1975) normal form perfect equilibria. Also, a class of restrictions one might impose upon out of equilibrium beliefs is suggested. The weakest of these results from the assumption that deviators are expected utility maximizers. The strongest yields backward induction outcomes in generic games with perfect information. Lastly, it is argued, with the aid of an example having imperfect information, that sequential rationality is not the consequence of equilibrium play and the absence of incredible threats.</p> </abstract>
<abstract> <p>We analyze the evolution of duopolists' prices and market shares in an infinite-period market with consumer switching costs, in which in every period new consumers arrive and a fraction of old consumers leaves. We show prices (and profits) are higher than without switching costs, and that this result does not depend importantly on our specific assumptions. We show switching costs make the market more attractive to a new entrant, even though an entrant must overcome the disadvantage that a large fraction of the market is already committed to the incumbent's product. We also examine the effects of market growth.</p> </abstract>
<abstract> <p>Predictions of the noisy rational expectations equilibrium (REE) model are found to be relatively accurate for both asset and information markets in the laboratory. When information about an asset's uncertain dividend is sold to a fixed number of highest bidders, prices, allocations, efficiency, and distribution of profit predictions of the full revelation REE model in the asset market dominate the predictions of the Walrasian model; demand for information shifts to the left and its price declines close to zero. When the price of information is fixed at a relatively high level, the number of informed agents and the informativeness of the asset market tends to adjust to permit the informed agents to recover their investment in information.</p> </abstract>
<abstract> <p>In this paper, it is shown that it is possible to identify binary threshold crossing models and binary choice models without imposing any parametric structure either on the systematic function of observable exogenous variables or on the distribution of the random term. This identification result is employed to develop a fully nonparametric maximum likelihood estimator for both the function of observable exogenous variables and the distribution of the random term. The estimator is shown to be strongly consistent, and a two step procedure for its calculation is developed. The paper also includes examples of economic models that satisfy the conditions that are necessary to apply the results.</p> </abstract>
<abstract> <p>We show that the CUSUM test of the stability over time of the coefficients of a linear regression model, which is usually based on recursive residuals, can also be applied to ordinary least squares residuals. We derive the limiting null distribution of the resulting test and compare its local power to that of the standard procedure. It turns out that neither version is uniformly superior to the other.</p> </abstract>
<abstract> <p>Complete and competitive markets imply a separation of the consumption (labor supply) and production (labor demand) decisions of the farm household. This paper tests for separation using the observation that in the absence of labor markets, household composition is an important determinant of farm labor use. The test has power when off-farm employment or hiring constraints, or differing efficiencies of family and hired labor lead to demographic variables affecting farm labor demand. An empirical model is developed to test the proposition that household labor demand (farm employment) is independent of family composition. The model is estimated on a household-farm data set from rural Java. Measurement error and endogeneity issues are addressed with instrumental variables techniques. I cannot reject the null hypothesis that farm labor allocation decisions are independent of household structure. The results are robust to different specifications of the labor demand function.</p> </abstract>
<abstract> <p>A model of endogenous growth is developed in which vertical innovations, generated by a competitive research sector, constitute the underlying source of growth. Equilibrium is determined by a forward-looking difference equation, according to which the amount of research in any period depends upon the expected amount of research next period. One source of this intertemporal relationship is creative destruction. That is, the prospect of more future research discourages current research by threatening to destroy the rents created by current research. The paper analyzes the positive and normative properties of stationary equilibria, in which research employment is constant and GNP follows a random walk with drift, although under some circumstances cyclical equilibria also exist. Both the average growth rate and the variance of the growth rate are increasing functions of the size of innovations, the size of the skilled labor force, and the productivity of research as measured by a parameter indicating the effect of research on the Poisson arrival rate of innovations; and decreasing functions of the rate of time preference of the representative individual. Under laissez faire the economy's growth rate may be more or less than optimal because, in addition to the appropriability and intertemporal spillover effects of other endogenous growth models, which tend to make growth slower than optimal, the model also has effects that work in the opposite direction. In particular, the fact that private research firms do not internalize the destruction of rents generated by their innovations introduces a business-stealing effect similar to that found in the partial-equilibrium patent race literature. When we endogenize the size of innovations we find that business stealing also makes innovations too small.</p> </abstract>
<abstract> <p>This paper presents a stochastic differential formulation of recursive utility. Sufficient conditions are given for existence, uniqueness, time consistency, monotonicity, continuity, risk aversion, concavity, and other properties. In the setting of Brownian information, recursive and intertemporal expected utility functions are observationally distinguishable. However, one cannot distinguish between a number of non-expected-utility theories of one-shot choice under uncertainty after they are suitably integrated into an intertemporal framework. In a "smooth" Markov setting, the stochastic differential utility model produces a generalization of the Hamilton-Jacobi-Bellman characterization of optimality. A companion paper explores the implications for asset prices.</p> </abstract>
<abstract> <p>In most models of asymmetric information, possession of private information leads to rents for the possessors. This tends to induce mechanism designers to distort away from efficiency. We show that this is an artifact of the presumption that information is independently distributed. Rent extraction in a large class of mechanism design games is analyzed, and a necessary and sufficient condition for arbitrarily small rents to private information is provided. In addition, the two person bargaining game is shown to have an efficient solution under first order stochastic dominance and a hazard rate condition. Similar conditions lead to full rent extraction in Milgrom-Weber auctions.</p> </abstract>
<abstract> <p>This paper considers a buyer-seller relationship with observable but unverifiable investments and/or random utility parameters. In such situations, it is known that contract renegotiation may prevent the implementation of first-best outcomes. In this paper, we show however that efficient investments and optimal risk-sharing can typically be achieved provided the initial contract is able to monitor the ex post renegotiation process. Specifically, we focus on the following two features of renegotiation design. First, default options in case renegotiation breaks down; second, the allocation of all bargaining power to either contracting party. Moreover, we show that these two features can be obtained in standard Rubinstein bargaining games through contractual provisions, such as specific-performance clauses and penalties for delay (or, equivalently, financial "hostages" refundable without interest).</p> </abstract>
<abstract> <p>In conformity with the Savage model of decision-making, modern asset pricing theory assumes that agents' beliefs about the likelihoods of future states of the world may be represented by a probability measure. As a result, no meaningful distinction is allowed between risk, where probabilities are available to guide choice, and uncertainty, where information is too imprecise to be summarized adequately by probabilities. In contrast, Knight and Keynes emphasized the distinction between risk and uncertainty and argued that uncertainty is more common in economic decision-making. Moreover, the Savage model is contradicted by evidence, such as the Ellsberg Paradox, that people prefer to act on known rather than unknown or vague probabilities. This paper provides a formal model of asset price determination in which Knightian uncertainty plays a role. Specifically, we extend the Lucas (1978) general equilibrium pure exchange economy by suitably generalizing the representation of beliefs along the lines suggested by Gilboa and Schmeidler. Two principal results are the proof of existence of equilibrium and the characterization of equilibrium prices by an "Euler inequality." A noteworthy feature of the model is that uncertainty may lead to equilibria that are indeterminate, that is, there may exist a continuum of equilibria for given fundamentals. That leaves the determination of a particular equilibrium price process to "animal spirits" and sizable volatility may result. Finally, it is argued that empirical investigation of our model is potentially fruitful.</p> </abstract>
<abstract> <p>We study the indeterminacy of equilibria in infinite horizon capital accumulation models with technological externalities. Our investigation encompasses models with bounded and unbounded accumulation paths, and models with one and two sectors of production. Under reasonable assumptions we find that equilibria are locally unique in one-sector economies. In economies with two sectors of production it is instead easy to construct examples where a positive external effect induces a two-dimensional manifold of equilibria converging to the same steady state (in the bounded case) or to the same constant growth rate (in the unbounded case). For the latter we point out that the dynamic behavior of these equilibria is quite complicated and that persistent fluctuations in their growth rates are possible.</p> </abstract>
<abstract> <p>The Bertrand-Edgeworth (BE) model describes competition among a group of price setting sellers, each of whom faces a production capacity constraint. We report on laboratory experiments that were designed so as to capture essential features of BE competition. These experiments permit us to evaluate different theories of BE competition: Competitive equilibrium (CE) pricing, Edgeworth cycles in prices, mixed strategy Nash equilibrium (NE) in prices, and tacit collusion. The experimental results indicate that while each of the theories helps to explain some aspects of the data, none of these theories are completely consistent with the data. In relative terms, the Edgeworth cycle theory provides better predictions than the other three theories. Most sellers adjusted their prices partially to their predicted Edgeworth price. The Edgeworth cycle theory is the only theory that predicts the kind of time dependence and cycling that was observed in most experiments.</p> </abstract>
<abstract> <p>This paper is concerned with the refined asymptotic properties of several tests for the admissibility of a subset of (overidentifying) instrumental variables. It derives maximum likelihood (ML) and linearized ML tests and calculates size corrections to the order 1/T. The local power function of the size-corrected tests is the same to the order 1/T, irrespective of the form of the test statistic or the limited information estimator used in its computation. Finally, it compares these tests with two previously proposed tests.</p> </abstract>
<abstract> <p>The 1980's witnessed rapid changes in the structure of wages in the United States. In this study, recently developed techniques for quantile regression are applied to every March Current Population Survey since 1964 and changes in the return to schooling and experience at different points of the wage distribution are examined. The quantile regression technique parsimoniously describes the entire conditional wage distribution. This conditional distribution is used to examine changes in within-group wage inequality as measured by the difference between conditional quantiles. Two linear models are considered: a simple one-group model, and a 16-group model. The results suggest that the returns to schooling and experience differ across quantiles of the wage distribution but their patterns of change are similar. Significant differences in wage inequality are also found across the various skill groups. The paper also presents a new imputation method for CPS data.</p> </abstract>
<abstract> <p>Many economic variables have a persistence property which may be called extended memory and the relationship between variables may well be nonlinear. This pair of properties allow for many more types of model misspecification than encountered with stationary or short-memory variables and linear relationships, and misspecifications lead to greater modelling difficulties. Examples are given using the idea of a model being balanced. Alternative definitions of extended memory are considered and a definition based on the properties of optimum forecasts is selected for later use. An important but not necessarily pervasive class of processes are those that are extended-memory but whose changes are short-memory. For this case, called I(1), standard cointegration ideas will apply. Tests of linearity are discussed in both the I(1) case, where a possible group of tests is easily found, and more generally. Similarly, methods of building nonlinear models based on familiar techniques, such as neural networks and projection pursuit, are briefly considered for I(1) and the more general case. A number of areas requiring further work in this new area are emphasized.</p> </abstract>
<abstract> <p>Robust estimation aims at developing point estimators that are not highly sensitive to errors in the data. However, the population parameters of interest are not identified under the assumptions of robust estimation, so the rationale for point estimation is not apparent. This paper shows that under error models used in robust estimation, unidentified population parameters can often be bounded. The bounds provide information that is not available in robust estimation. For example, it is possible to obtain finite bounds on the population mean under contaminated sampling. A method for estimating the bounds is given and illustrated with an application. It is argued that when the data may be contaminated or corrupted, estimating the bounds is more natural than attempting point estimation of unidentified parameters.</p> </abstract>
<abstract> <p>A statistical model of dynamic intrafamily investment behavior incorporating endowment heterogeneity is estimated to evaluate alternative estimation procedures that have exploited family and kinship data. These procedures, which place alternative restrictions on the endowment structure and on behavior, include generalized least squares, instrumental-variables, fixed-effects based on the children of sisters, fixed-effects based on siblings, and sibling fixed-effects with instrumental variables. The framework is applied to data on birth outcomes, with focus on the effects of teen-age childbearing net of other maternal behavior. The empirical results imply that the least restrictive statistical formulation, consistent with dynamic behavior and heterogeneity among siblings, fits the data best. All of the estimation procedures that control for a family-specific endowment indicate, however, that the biological effect of having a birth at younger ages is to marginally increase birthweight and to increase fetal growth.</p> </abstract>
<abstract> <p>This paper models and estimates congestion prices and capacity for large hub airports. The model includes stochastic queues, time-varying traffic rates, and endogenous, intertemporal adjustment of traffic in response to queuing delay and fees. Relative costs of queuing and schedule delays are estimated using data from Minneapolis-St. Paul. Simulations calculate equilibrium traffic patterns, queuing delays, schedule delays, congestion fees, airport revenues, airport capacity, and efficiency gains. The paper also investigates whether a dominant airline internalizes delays its aircraft impose. It tests game-theoretic specifications with atomistic, Nash-dominant, Stackelberg-dominant, and collusive-airline traffic.</p> </abstract>
<abstract> <p>We consider a k-player sequential bargaining model in which the size of the cake and the order in which players move follow a general Markov process. For games in which one agent makes an offer in each period and agreement must be unanimous, we characterize the sets of subgame perfect and stationary subgame perfect payoffs. With these characterizations, we investigate the uniqueness and efficiency of the equilibrium outcomes, the conditions under which agreement is delayed, and the advantage to proposing. Our analysis generalizes many existing results for games of sequential bargaining which build on the work of Stahl (1972), Rubinstein (1982), and Binmore (1987).</p> </abstract>
<abstract> <p>Core allocations may be defined for infinite horizon capital accumulation models. If agents cannot trust each other in the sense of Gale, then agents may renege on their commitments; their decisions appear time inconsistent. A core allocation is a recursive core allocation provided no coalition can improve upon its consumption stream at any time given its accumulation of assets up to that period. We show for every allocation of consumption in the initial core, one can find a distribution of capital stocks among the agents where no coalition of agents will break the initial core contract at any date. It follows that if the proper distribution of the streams of capital is achieved, then the allocation in the core is also in the recursive core. The latter therefore forges a link between the distribution of wealth (capital), the problem of trust, and time consistent intertemporal contracts.</p> </abstract>
<abstract> <p>The debate between the North and the South about the enforcement of intellectual property rights is examined within a dynamic general equilibrium framework in which the North invents new products and the South imitates them. A welfare evaluation of a policy of tighter intellectual property rights is provided by decomposing the welfare change into four items: (a) terms of trade; (b) production composition; (c) available products; and (d) intertemporal allocation of consumption. The paper provides a theoretical evaluation of the effect of each one of these items and their relative size. The analysis proceeds in stages. It begins with an exogenous rate of innovation in order to focus on the first two elements. The following two components are added by endogenizing the rate of innovation. Finally, the paper considers the role of foreign direct investment.</p> </abstract>
<abstract> <p>A dynamic model with many sellers and many buyers is constructed, in which buyers who fail to purchase in the current period may attempt to purchase in the future, and sellers who fail to sell may sell in the future. An equilibrium is found where sellers hold identical auctions and buyers randomize over the sellers they visit. Auctions alter the distribution of buyer types by removing high value buyers more rapidly than low value buyers, and an equilibrium distribution of buyer types is constructed. Sellers in equilibrium post an efficient reserve price equal to the sellers' value of the good, and an auction with efficient reserve is an optimal mechanism from each seller's point of view, in spite of the ability of any seller to alter the distribution of buyer types participating in the seller's mechanism by altering the mechanism.</p> </abstract>
<abstract> <p>Anecdotal and experimental evidence suggests that bargaining sessions subject to deadlines often begin with cheap talk and rejected proposals. Agreements, if they are reached at all, tend to be concluded near the deadline. We attempt to capture and explain these phenomena in a strategic bargaining model that incorporates a bargaining deadline, the possibility of strategic delay, and a lack of perfect player control over the timing of offers. Imperfect player control is generated by an exogenous uniformly-distributed random delay in offer transmission. Our model has a symmetric Markov-perfect equilibrium, unique at almost all nodes, in which players adopt strategic delay early in the game, make and reject offers later on, and reach agreements late in the game if at all. In equilibrium players miss the deadline with positive probability. The expected division of the surplus is unique and close to an even split.</p> </abstract>
<abstract> <p>In this paper we consider economies involving one public good, one private good, and convex technology and propose an informationally decentralized dynamic nontatonnement procedure that converges in general from the initial endowments to an allocation in the core of the economy. We then consider a general class of procedures and show that there exists none in the class that is locally incentive compatible and individually rational. These results show that there exists a trade-off between the requirements of local incentive compatibility and equitable cost sharing.</p> </abstract>
<abstract> <p>A social welfare function f is Arrovian if it has transitive values and satisfies Arrow's independence of irrelevant alternatives condition. For any fraction t and any Arrovian social welfare function f, either there will be some individual who dictates on a subset containing at least the fraction t of outcomes, or at least the fraction 1 - t of the ordered pairs of outcomes have their social ranking fixed independently of individual preference. If individual preferences are strong, we can say more: Associated with any Arrovian social welfare function, there is a set containing a large fraction of citizens whose preferences are not consulted in determining the social ranking of a large fraction of the pairs of alternatives. (The Pareto criterion is not assumed.)</p> </abstract>
<abstract> <p>This paper develops a limit theory for Wald tests of Granger causality in levels vector autoregressions (VAR's) and Johansen-type error correction models (ECM's), allowing for the presence of stochastic trends and cointegration. Earlier work by Sims, Stock, and Watson (1990) on trivariate VAR systems is extended to the general case, thereby formally characterizing the circumstances when these Wald tests are asymptotically valid as χ &lt;sup&gt;2&lt;/sup&gt; criteria. Our results for inference from unrestricted levels VAR are not encouraging. We show that without explicit information on the number of unit roots in the system and the rank of certain submatrices in the cointegration space it is impossible to determine the appropriate limit theory in advance; and, even when such information is available, the limit theory often involves both nuisance parameters and nonstandard distributions, a situation where there is no satisfactory statistical basis for mounting these tests. The situation with regard to the use of causality tests in ECM's is also complex but more encouraging. Granger causality tests in ECM's also suffer from nuisance parameter dependencies asymptotically and, in some cases that we make explicit, nonstandard limit theory. Both these results are somewhat surprising in the light of earlier research on the validity of asymptotic χ &lt;sup&gt;2&lt;/sup&gt; criteria in such systems. In spite of these difficulties, Johansen-type ECM's do offer a sound basis for empirical testing of the rank of the cointegration space and the rank of key submatrices that influence the asymptotics.</p> </abstract>
<abstract> <p>A system of consumer expenditure functions is estimated from Norwegian household budget data. Specific features of the approach are: (i) Panel data from individual households are used, which offer far richer opportunities for identification, estimation, and testing than cross section data. (ii) Measurement errors are carefully modelled. Total consumption expenditure is modelled as a latent variable; purchase expenditures on different goods and two income measures are used as indicators of this basic latent variable. The usual assumption of no measurement error in total expenditure is clearly rejected. (iii) The distribution of latent total expenditure across households, and its evolution over time, is estimated and important properties tested. (iv) The distribution of individual differences in preferences, represented by individual time invariant latent variables, are modelled, estimated, and tested. (v) We test the hypothesis that preferences are uncorrelated with total consumption expenditure, which is basic to virtually all cross section studies of consumer demand functions.</p> </abstract>
<abstract> <p>The new economics of regulation is an application of the principal-agent methodology to the contractual relationship between regulators and regulated firms. After a critique of the traditional paradigms of regulation from the point of view of information economics a canonical model of regulation under asymmetric information is developed. A survey of the main results obtained in the new economics of regulation is then provided, in particular concerning the implementation of optimal contracts by a menu of linear contracts, the dichotomy between pricing and cost reimbursement rules, the auctioning of incentive contracts, the dynamics of contracting under limited commitment, and the hierarchical problems in regulation. Empirical implications are then discussed and avenues of further research are described in the conclusion.</p> </abstract>
<abstract> <p>The full insurance model is tested using data from three poor, high risk villages in the semi-arid tropics of southern India. The model presented here incorporates a number of salient features of the actual village economies. Although the model is rejected statistically, it does provide a surprisingly good benchmark. Household consumptions comove with village average consumption. More clearly, household consumptions are not much influenced by contemporaneous own income, sickness, unemployment, or other idiosyncratic shocks, controlling for village consumption (i.e. for village level risk). There is evidence that the landless are less well insured than their village neighbors in one of the three villages.</p> </abstract>
<abstract> <p>This paper explores the distinction between nonatomicity and thick markets as the source of perfect competition. The focus of the paper is the construction of a model of an imperfectly competitive economy with a nonatomic continuum of traders and a continuum of differentiated commodities, for which Walrasian equilibria exist. The failure of perfect competition in this instance can be identified in two ways: individuals can affect prices and the core is strictly larger than the set of Walrasian allocations. By contrast, it is shown that, when markets are physically or economically thick (or both), then individuals cannot typically affect prices and the core always coincides with the set of Walrasian allocations.</p> </abstract>
<abstract> <p>The computation of large-scale nonlinear intertemporal optimization problems requires time-aggregation assumptions. This consists in choosing a specific gridding of the time horizon, and considering the optimal solution computed using this restricted set of dates as an approximation to the underlying continuous- or more disaggregated discrete-time formulation. A procedure generally adopted for time aggregation in applied intertemporal optimization economic models using mathematical programming techniques is shown to be inappropriate, as it introduces a dependency of the solution steady state to a specific choice of sequence of time intervals. We establish necessary and sufficient conditions to avoid this dependency, i.e., to ensure that the discretization satisfies steady-state invariance. The result is a considerable improvement in the numerical accuracy of the time-aggregated approximation. These easy-to-handle conditions are shown to apply to a broad class of models such as multidimensional intertemporal problems and endogenous growth models and may therefore prove extremely powerful in applied works such as large-scale applied general-equilibrium modeling. This conclusion is further highlighted by a comparison between this approach and a spectral-projection method using optimal orthogonal collocation as recently introduced by Judd (1992).</p> </abstract>
<abstract> <p>The paper considers the OLS, the IV, and two method-of-moments estimators, MM and MMK, of the coefficients of a single equation, where the explanatory variables are correlated with the disturbance term. The MM and MMK estimators are generalizations of the LIML and LIMLK estimators, respectively. Multivariate first-order approximations to the distributions are derived under normality, using a parameter sequence where the number of instruments increases as the number of observations increases. Numerical results show these approximations are more accurate, compared to large-sample approximations, even if the number of instruments is small. The moments of the multivariate limit distributions of the MM and MMK estimators can be consistently estimated under a variety of parameter sequences, including the large-sample sequence. The new approximate confidence regions perform well in terms of exact levels, compared to traditional ones. The IV estimator of the coefficient of a single explanatory endogenous variable is interpreted as a shrinkage estimator, which is dominated, in practical cases, by the MM and MMK estimators in terms of nearness to the true value in the sense of Pitman.</p> </abstract>
<abstract> <p>In many circumstances, a principal may have relevant private information when she proposes a contract to an agent. We analyze such a principal-agent relationship as a noncooperative game. The principal proposes a contract, which is accepted or rejected by the agent (who, for most of our analysis, has no private information). The contract is executed if accepted; otherwise, the reservation allocation takes effect. This allocation may be determined by a pre-existing contract (which the principal, by her proposal, is attempting to renegotiate), or it may simply be the no-trade point. In this paper, we assume that the principal's information directly affects the agent's payoff. Before solving the game, we discuss Pareto efficiency with asymmetric information. We define an incentive-compatible allocation to be weakly interim efficient (WIE) if there exists no alternative incentive-compatible allocation that both parties prefer for all possible beliefs that the agent might have about the principal's private information (type). We show that any WIE allocation is interim-efficient (IE) for some beliefs. The Rothschild-Stiglitz-Wilson (RSW) allocation relative to the reservation allocation &lt;tex-math&gt;$\mu _{0}^{\cdot}$&lt;/tex-math&gt; is the allocation that maximizes the payoff of each type of principal within the class of incentive-compatible allocations that guarantee the agent at least the utility he gets from &lt;tex-math&gt;$\mu _{0}^{\cdot}$&lt;/tex-math&gt; irrespective of his beliefs about the principal's type. The equilibrium set of the contract proposal game consists of the allocations that weakly Pareto dominate the RSW allocation. Thus, there is a unique equilibrium outcome if and only if the latter is IE (and the equilibrium outcome is the RSW allocation itself). After characterizing the equilibrium allocations, we study those that are renegotiation-proof, when either the principal or the agent leads the renegotiation. We then compare our contract proposal game, which is a signaling model, with its "screening" counterpart. We conclude by extending our results to the case in which the agent as well as the principal has private information under the assumption of quasi-linear preferences.</p> </abstract>
<abstract> <p>This paper is concerned with a problem of implementation of a given social choice correspondence (SCC). We introduce an essential monotonicity condition and show that any implementable SCC satisfies this condition. Conversely, in a case of three or more participants any essentially monotone SCC is implementable. In a case of two participants the essential monotonicity condition must be completed by a requirement that the SCC is close to an individually rational correspondence.</p> </abstract>
<abstract> <p>Quotas are the predominant means of protection in developed countries, with quota rents commonly shared between exporter and importer. This paper derives shadow prices appropriate to evaluating trade reform under these circumstances, and provides a number of useful sufficient conditions for welfare-improving "piecemeal" reform. In doing so, we apply the distorted (quantity-constrained) expenditure function, and use implicit separability to derive more powerful results than have previously been available.</p> </abstract>
<abstract> <p>This paper presents a unifying theory for valuing contingent claims under a stochastic term structure of interest rates. The methodology, based on the equivalent martingale measure technique, takes as given an initial forward rate curve and a family of potential stochastic processes for its subsequent movements. A no arbitrage condition restricts this family of processes yielding valuation formulae for interest rate sensitive contingent claims which do not explicitly depend on the market prices of risk. Examples are provided to illustrate the key results.</p> </abstract>
<abstract> <p>Much macroeconometric discussion has recently emphasized the economic significance of the size of the permanent component in GNP. Consequently, a large literature has developed that tries to estimate this magnitude--measured, essentially, as the spectral density of increments in GNP at frequency zero. This paper shows that unless the permanent component is a random walk this attention has been misplaced: in general, that quantity does not identify the magnitude of the permanent component. Further, by developing bounds on reasonable measures of this magnitude, the paper shows that a random walk specification is biased towards establishing the permanent component as important.</p> </abstract>
<abstract> <p>This paper develops a new procedure for statistical inference in cointegrating regressions. We introduce the concept of canonical cointegrating regressions, which are the regressions formulated with the transformed data. The required transformations involve simple adjustments of the integrated processes using stationary components in cointegrating models. Canonical cointegrating regressions therefore represent the same cointegrating relationships as the original models. They are, however, constructed in such a way that the usual least squares procedure yields asymptotically efficient estimators and chi-square tests. The methodology presented here is applicable to a very wide class of cointegrating models, including models with deterministic and singular, as well as stochastic and regular, cointegrations.</p> </abstract>
<abstract> <p>We develop a new form of the information matrix test for a wide variety of statistical models. Chesher (1984) showed that the implicit alternative of this test is a model with random parameter variation, a fact which we exploit by constructing the test against an explicit alternative of this type. The new test is computed using a double-length artificial regression, instead of the more conventional outer-product-of-the-gradient regression, which, although easy to use, is known to give test statistics with distributions very far from the asymptotic nominal distribution even in rather large samples. The new form on the other hand performs remarkably well, at least for the case of univariate regression models. Some approximate finite-sample distributions are calculated for this case, and lend support to the use of the new form of the test.</p> </abstract>
<abstract> <p>This paper proposes a new test statistic to detect the presence of heteroskedasticity. The proposed test does not require a parametric specification of the mean regression function in the first stage regression. The regression function is estimated nonparametrically by the kernel estimation method. The nonparametric residual is estimated and used as a proxy for the random disturbance term. This nonparametric residual is robust to regression function misspecification. Asymptotic normality is established using extensions of classical U-statistic theorems. When the disturbance term is heteroskedastic, nonparametric residuals will correctly identify the presence of heteroskedasticity. The test statistic is computed using the nonparametric quantities, but the resulting inference has a standard chi-square distribution.</p> </abstract>
<abstract> <p>This paper describes the U.S. offshore oil and gas lease sales conducted by the Department of the Interior since 1954. Several decisions are discussed, including bidding for leases, the government's decision whether to accept the highest bid, the incidence and timing of exploratory drilling, and the formation of bidding consortia. Equilibrium models of these decisions that emphasize informational and strategic issues and that account for institutional features of the leasing program are analyzed, and their predictions compared to outcomes in the data.</p> </abstract>
<abstract> <p>This paper examines the effect of prospective payment for hospital care on adverse medical outcomes. In 1983, the federal government replaced its previous cost-based reimbursement method with a Prospective Payment System, under which reimbursement depends only on the diagnosis of the patient. Hospitals thus lost the marginal reimbursement they formerly received for providing additional treatments. In addition, the average price each hospital received for patients with different diagnoses changed. This paper relates each of these changes to adverse outcomes, with two conclusions. First, there is a change in the timing of deaths associated with changes in average prices. In hospitals with price declines, a greater share of deaths occur in the hospital or shortly after discharge, but by one year post-discharge, mortality is no higher. Second, there is a trend increase in readmission rates caused by the elimination of marginal reimbursement. This appears to be due to accounting changes on the part of hospitals, however, rather than true changes in morbidity.</p> </abstract>
<abstract> <p>We consider the allocation of goods in exchange economies with a finite number of agents who may have private information about their preferences. In such a setting, standard allocation rules such as Walrasian equilibria or rational expectations equilibria are not compatible with individual incentives. We characterize the set of allocation rules which are incentive compatible, or in other words, the set of strategy-proof social choice functions. Social choice functions which are strategy-proof are those which can be obtained from trading according to a finite number of pre-specified proportions. The number of proportions which can be accommodated is proportional to the number of agents. Such rules are necessarily inefficient, even in the limit as the economy grows.</p> </abstract>
<abstract> <p>Different information systems are compared in terms of their relative efficiencies in an agency model. The mean preserving spread relation between the likelihood ratio distributions derived from the original information systems is found to be sufficient to rank information systems under quite general assumptions about the agent's utility function. Furthermore, it is shown that the mean preserving spread criterion can be applied to a broader set of information systems than Holmstrom's informativeness criterion and Blackwell's theorem.</p> </abstract>
<abstract> <p>This paper proposes a model of the process by which players learn to play repeated coordination games, with the goal of understanding the results of some recent experiments. In those experiments the dynamics of subjects' strategy choices and the resulting patterns of discrimination among equilibria varied systematically with the rule for determining payoffs and the size of the interacting groups, in ways that are not adequately explained by available methods of analysis. The model suggests a possible explanation by showing how the dispersion of subjects' beliefs interacts with the learning process to determine the probability distribution of its dynamics and limiting outcome.</p> </abstract>
<abstract> <p>This paper elucidates the logic behind recent papers which show that a unique equilibrium is selected in the presence of higher order uncertainty, i.e., when players lack common knowledge. We introduce two new concepts: belief potential of the information system and p-dominance of Nash-equilibria of the game, and show that a Nash-equilibrium is uniquely selected whenever its p-dominance is below the belief potential. This criterion applies to many-action games, not merely 2 × 2 games. It also applies to games without dominant strategies, where the set of equilibria is shown to be smaller and simpler than might be initially conjectured. Finally, the new concepts help understand the circumstances under which the set of equilibria varies with the amount of common knowledge among players.</p> </abstract>
<abstract> <p>If an agent's preferences over subjectively uncertain acts are consistent with his or her having a subjective probability distribution over the states of nature, then those preferences can induce consistent preferences over "objectively" risky lotteries. Such "probabilistically sophisticated" behavior thus allows us to treat decision making under situations of uncertainty in an analogous manner to those under risk. This paper first characterizes exactly what probabilistic sophistication entails for an agent's beliefs about the likelihood of states of nature. Secondly, it presents characterizations of probabilistically sophisticated individuals whose induced lottery preferences obey neither the Independence Axiom (unlike Savage (1954, 1972)) nor a monotonicity property that shares some of the nature of Independence (unlike Machina and Schmeidler (1992)).</p> </abstract>
<abstract> <p>Internal consistency of choice has been a central concept in demand theory, social choice theory, decision theory, behavioral economics, and related fields. It is argued here that this idea is essentially confused, and there is no way of determining whether a choice function is consistent or not without referring to something external to choice behavior (such as objectives, values, or norms). We have to re-examine the robustness of the standard results in this light. The main formal result presented here is an extension of Arrow's General Possibility Theorem. This drops the need to impose any condition of internal consistency of social choice, or any internal notion of "social rationality."</p> </abstract>
<abstract> <p>In a self-confirming equilibrium, each player's strategy is a best response to his beliefs about the play of his opponents, and each player's beliefs are correct along the equilibrium path of play. Thus, if a self-confirming equilibrium occurs repeatedly, no player ever observes play that contradicts his beliefs, even though beliefs about play at off-path information sets need not be correct. We characterize the ways in which self-confirming equilibria and Nash equilibria can differ, and provide conditions under which self-confirming equilibria correspond to standard solution concepts.</p> </abstract>
<abstract> <p>We study the steady states of a system in which players learn about the strategies their opponents are playing by updating their Bayesian priors in light of their observations. Players are matched at random to play a fixed extensive-form game, and each player observes the realized actions in his own matches, but not the intended off-path play of his opponents or the realized actions in other matches. Because players are assumed to live finite lives, there are steady states in which learning continually takes place. If lifetimes are long and players are very patient, the steady state distribution of actions approximates that of a Nash equilibrium.</p> </abstract>
<abstract> <p>We consider the problem of allocating a bundle of commodities among a group of agents who are collectively entitled to them. We prove: For an atomless economy with possibly satiated preferences, any solution that is efficient, equitable, and consistent must select allocations that are supported by equal-budget Walrasian equilibria with slack.</p> </abstract>
<abstract> <p>This paper introduces the concept of standard risk aversion. A von Neumann-Morgenstern utility function has standard risk aversion if every risk that has a negative interaction with a small reduction in wealth also has a negative interaction with any undesirable, independent risk. It is shown that, given monotonicity and concavity, the combination of decreasing absolute risk aversion and decreasing absolute prudence is necessary and sufficient for standard risk aversion. Standard risk aversion is shown to imply not only Pratt and Zeckhauser's "proper risk aversion" (an undesirable risk always remaining undesirable in the presence of an independent undesirable risk), but also that being forced to face an undesirable risk reduces the optimal investment in a risky security with an independent return.</p> </abstract>
<abstract> <p>Consider a sequence economy in which small agents trade event-contingent claims to a single physical good. The agents have uncommon priors, state-contingent utility functions, and asymmetric information in every period. The claims traded vary from period to period. An equilibrium is inessential if, at the equilibrium prices, agents have alternate optimal plans that clear opening markets and require no trades in later markets. Theorem 1 shows that inessentiality obtains if the claims traded in opening markets span the claims traded in later markets, each agent can insure herself against improvements in her private information, and agents' initial information satisfies a weak symmetry condition. Theorem 2 shows that inessentiality obtains if opening claims satisfy a stronger, backward-looking spanning condition. Theorem 3 shows that inessentiality obtains if opening claims span payoff-relevant events, opening and later markets satisfy a statistical relationship, and agents are risk averse. None of the theorems requires ex ante Pareto optimality or the absence of arbitrage opportunities. The theorems shed light on previous questions concerning the conditions permitting speculation and the role of price-contingent trading.</p> </abstract>
<abstract> <p>In this model, shareholders can use auditors' reports to contract with a privately-informed manager. Our imperfect audit technology allows the auditor and the manager to collude. Auditors are useful only if they have good information and if the manager's liability is high. Expected maximum deterrence is not desirable and production is suboptimal, even with unbounded punishments, risk-neutral agents, and costless auditing. Raising the manager's punishment raises the bribe he may offer the auditor, which raises the cost of preventing collusion. We also distinguish internal auditors (who are costless but may collude) from external ones (who are costly but never collude) and show that the optimal contract may specify random external audits.</p> </abstract>
<abstract> <p>The purpose of this paper is to study the role and implications of price advertising when shopping trips are costly to consumers. To do so, we introduce advertising into an optimal sequential search model. Information about prices is both gathered by consumers and disseminated by firms. Consumers search sequentially and stores advertise (with various intensity) when it is in their interest to do so. Our model has a unique equilibrium exhibiting price dispersion. The model generates predictions about the shape of the price distribution and firms' advertising behavior. We explore the effects of entry, and find that when initial advertising costs (at zero level of effort) are precisely zero, entry drives the equilibrium to the perfectly competitive outcome. However, when initial marginal advertising costs are positive, entry drives prices higher, and while the informed consumers almost surely locate competitive prices, welfare does not necessarily increase. Finally, we compare the effectiveness of the two informational channels. When advertising costs shrink, prices become competitive; however, when search costs shrink, prices remain bounded above marginal production costs.</p> </abstract>
<abstract> <p> We establish conditions which (in various settings) guarantee the existence of equilibria described by ergodic Markov processes with a Borel state space S. Let &lt;tex-math&gt;$\scr{P}(S)$&lt;/tex-math&gt; denote the probability measures on S, and let &lt;tex-math&gt;$s\mapsto G(s)\subset \scr{P}(S)$&lt;/tex-math&gt; be a (possibly empty-valued) correspondence with closed graph characterizing intertemporal consistency, as prescribed by some particular model. A nonempty measurable set &lt;tex-math&gt;$J\subset S$&lt;/tex-math&gt; is self-justified if &lt;tex-math&gt;$G(s)\cap \scr{P}(J)$&lt;/tex-math&gt; is not empty for all s ϵ J. A time-homogeneous Markov equilibrium (THME) for G is a self-justified set J and a measurable selection &lt;tex-math&gt;$\Pi \colon J\rightarrow \scr{P}(J)$&lt;/tex-math&gt; from the restriction of G to J. The paper gives sufficient conditions for existence of compact self-justified sets, and applies the theorem: If G is convex-valued and has a compact self-justified set, then G has an THME with an ergodic measure. The applications are (i) stochastic overlapping generations equilibria, (ii) an extension of the Lucas (1978) asset market equilibrium model to the case of heterogeneous agents, and (iii) equilibria for discounted stochastic games with uncountable state spaces. </p> </abstract>
<abstract> <p>Two of the most important refinements of the Nash equilibrium concept for extensive form games with perfect recall are Selten's (1975) perfect equilibrium and Kreps and Wilson's (1982) more inclusive sequential equilibrium. These two equilibrium refinements are motivated in very different ways. Nonetheless, as Kreps and Wilson (1982, Section 7) point out, the two concepts lead to similar prescriptions for equilibrium play. For each particular game form, every perfect equilibrium is sequential. Moreover, for almost all assignments of payoffs to outcomes, almost all sequential equilibrium strategy profiles are perfect equilibrium profiles, and all sequential equilibrium outcomes are perfect equilibrium outcomes. We establish a stronger result: For almost all assignments of payoffs to outcomes, the sets of sequential and perfect equilibrium strategy profiles are identical. In other words, for almost all games each strategy profile which can be supported by beliefs satisfying the rationality requirement of sequential equilibrium can actually be supported by beliefs satisfying the stronger rationality requirement of perfect equilibrium. We obtain this result by exploiting the algebraic/geometric structure of these equilibrium correspondences, following from the fact that they are semi-algebraic sets; i.e., they are defined by finite systems of polynomial inequalities. That the perfect and sequential equilibrium correspondences have this semi-algebraic structure follows from a deep result from mathematical logic, the Tarski-Seidenberg Theorem; that this structure has important game-theoretic consequences follows from deep properties of semi-algebraic sets.</p> </abstract>
<abstract> <p>A noncooperative implementation of the core is provided for games with transferable utility. The implementation obtained here is meant to reflect the standard motivation for the core as closely as possible. In the model proposed, time is continuous. This idealized treatment of time ensures that there is always time to reject a noncore proposal before it is consumated.</p> </abstract>
<abstract> <p>Suppose that a population of individuals may be grouped according to some vector of characteristics into "clusters," such that each cluster is very "similar" in terms of the attributes of its members, but different clusters have members with very "dissimilar" attributes. In that case we say that the society is polarized. Our purpose is to study polarization, and to provide a theory of its measurement. Our contention is that polarization, as conceptualized here, is closely related to the generation of social tensions, to the possibilities of revolution and revolt, and to the existence of social unrest in general. We take special care to distinguish our theory from the theory of inequality measurement. We derive measures of polarization that are easily applicable to distributions of characteristics such as income and wealth.</p> </abstract>
<abstract> <p>The model of general equilibrium with incomplete markets is a generalization of the Arrow-Debreu model which provides a rich framework for studying problems of macroeconomics. This paper shows how the model, which has so far been restricted to economies with a finite horizon, can be extended to the more natural setting of an open-ended future, thereby providing an extension of the finite horizon representative agent models of modern macroeconomics to economies with heterogeneous agents and incomplete markets. There are two natural concepts of equilibrium over an infinite horizon which prevent agents from entering into Ponzi schemes, that is, from indefinitely postponing the repayment of their debts. The first is based on debt constraints which place bounds on debt at each date-event; the second is based on transversality conditions which limit the asymptotic rate of growth of debt. The concept of an equilibrium with debt constraint is a natural concept of equilibrium for macroeconomic analysis; however the concept of an equilibrium with transversality condition is more amenable to theoretical analysis since it permits the powerful techniques of Arrow-Debreu theory to be carried over to the setting of incomplete markets. In an economy in which agents are impatient (expressed by the Mackey continuity of their preference orderings) and have a degree of impatience at each date-event which is bounded below (a concept defined in the paper), we show that the equilibria of an economy with transversality condition coincide with the equilibria with debt constraints. An equilibrium with transversality condition is shown to exist: it follows that for each economy there is an explicit bound M such that an equilibrium with explicit debt constraint M exists, in which the constraint is never binding--this latter property ensuring that the debt constraint, whose objective is to prevent Ponzi schemes, does not in itself introduce a new imperfection into the model over and above the incompleteness of the markets.</p> </abstract>
<abstract> <p>We analyze the Shapley value allocation of an economy with differential information. Since the intent of the Shapley value is to measure the sum of the expected marginal contributions made by an agent to any coalition to which he/she belongs, the value allocation of an economy with differential information provides an interesting way to measure the information advantage of an agent. This feature of the Shapley value allocation is not necessarily shared by the rational expectation equilibrium. Thus, we analyze the informational structure of an economy with differential information from a different and new viewpoint. In particular we address the following questions: How do coalitions of agents share their private information? How can one measure the information advantage or superiority of an agent? Is each agent's private information verifiable by other members of a coalition? Do coalitions of agents pool their private information? Do agents have an incentive to report their true private information? What is the correct concept of a value allocation in an economy with differential information? Do value allocations exist in an economy with differential information? We provide answers to each of these questions.</p> </abstract>
<abstract> <p>The empirical objective of this study is to account for the time-variation in the covariances between stock markets, and to assess the extent of capital market integration. Using data on sixteen national stock markets, we estimate a multivariate factor model in which the volatility of returns is induced by changing volatility in the factors. Unanticipated returns are assumed to depend both on innovations in "observable" economic variables and on "unobservable" factors. The risk premium on an asset is a linear combination of the risk premia associated with factors. We find that idiosyncratic risk is significantly priced, and that the "price of risk" is not common across countries. This either can be interpreted as evidence against the null of integrated capital markets or could reflect the failure of some other maintained assumptions. Another empirical finding is that only a small proportion of the covariances between national stock markets and their time-variation can be accounted for by "observable" economic variables. Changes in correlations between markets are driven primarily by movements in "unobservable" variables.</p> </abstract>
<abstract> <p>Many alternative theories have been proposed to explain violations of expected utility (EU) theory observed in experiments. Several recent studies test some of these alternative theories against each other. Formal tests used to judge the theories usually count the number of responses consistent with the theory, ignoring systematic variation in responses that are inconsistent. We develop a maximum-likelihood estimation method which uses all the information in the data, creates test statistics that can be aggregated across studies, and enables one to judge the predictive utility--the fit and parsimony--of utility theories. Analyses of 23 data sets, using several thousand choices, suggest a menu of theories which sacrifice the least parsimony for the biggest improvement in fit. The menu is: mixed fanning, prospect theory, EU, and expected value. Which theories are best is highly sensitive to whether gambles in a pair have the same support (EU fits better) or not (EU fits poorly). Our method may have application to other domains in which various theories predict different subsets of choices (e.g., refinements of Nash equilibrium in noncooperative games).</p> </abstract>
<abstract> <p>A number of generalizations of the expected utility preference functional are estimated using experimentally generated data involving 100 pairwise choice questions repeated on two separate occasions. Likelihood ratio tests are conducted to investigate the statistical superiority of the various generalizations, and the Akaike information criterion is used to distinguish between them. The economic superiority of the various generalizations is also explored and the paper concludes that, for many subjects, the superiority of several of the generalizations is not established.</p> </abstract>
<abstract> <p>"No trade" theorems have shown that new information will not lead to trade when agents share the same prior beliefs. This paper explores the structure of no trade theorems with heterogeneous prior beliefs. It is shown how different notions of efficiency under asymmetric information--ex ante, interim, ex post--are related to agents' prior beliefs, as well as incentive compatible and public versions of those efficiency concepts. These efficiency results are used to characterize necessary and sufficient conditions on agents' beliefs for no trade theorems in different trading environments.</p> </abstract>
<abstract> <p>The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for &lt;tex-math&gt;$\sqrt{n}\text{-consistency}$&lt;/tex-math&gt; and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.</p> </abstract>
<abstract> <p>This paper derives asymptotically optimal tests for testing problems in which a nuisance parameter exists under the alternative hypothesis but not under the null. For example, the results apply to tests of one-time structural change with unknown change-point. Several other examples are discussed in the paper. The results of the paper are of interest, because the testing problem considered is nonstandard and the classical asymptotic optimality results for the Lagrange multiplier (LM), Wald, and likelihood ratio (LR) tests do not apply. A weighted average power criterion is used here to generate optimal tests. This criterion is similar to that used by Wald (1943) to obtain the classical asymptotic optimality properties of Wald tests in "regular" testing problems. In fact, the optimal tests introduced here reduce to the standard LM, Wald, and LR tests when standard regularity conditions hold. Nevertheless, in the nonstandard cases of main interest, new optimal tests are obtained and the LR test is not found to be an optimal test.</p> </abstract>
<abstract> <p>We analyze a first-price, sealed bid auction with a random reservation price to study the federal sales of offshore oil and gas leases on drainage tracts. Our model assumes the object to be sold has an unknown common value, but one buyer has better information than the others. We permit the reservation price to be correlated with the information of the informed buyer, which reflects both his assessment of the value of the object and the probability of rejection at any bid. Assuming all random variables are affiliated, we establish the following results. (i) The percentage rate of increase in the distribution of the uninformed bid is never greater than the percentage rate of increase of the distribution of the informed bid. (ii) The distributions are identical at bids above the support of the reservation price. (iii) The informed buyer is more likely to submit low bids. We demonstrate that bid data from the federal sales of offshore drainage leases satisfy these restrictions.</p> </abstract>
<abstract> <p>This paper examines how the possibility of renegotiation affects contractual outcomes in environments in which adverse selection is a problem. The game setup is an extension of the one-shot signalling game in which an infinite number of rounds of renegotiation are permitted before contracted actions are in fact executed. The main results of the paper are (1) executed contracts may still contain distortions although players can never commit not to renegotiate, (2) the popular "efficient" separating-equilibrium outcome of one-shot signalling games is never an equilibrium outcome when an infinite number of rounds of renegotiation are permitted, (3) standard incentive-compatibility constraints can be easily generalized to incorporate situations that allow for an infinite number of rounds of renegotiation, (4) equilibrium outcomes can be separating and nevertheless depend on the uninformed player's priors as informed types pool in the first stage and use the renegotiation stages to separate, (5) renegotiation in signalling games may lead to outcomes similar to equilibrium outcomes of screening games in which multiple contract purchases are allowed.</p> </abstract>
<abstract> <p>Efficient estimators of cointegrating vectors are presented for systems involving deterministic components and variables of differing, higher orders of integration. The estimators are computed using GLS or OLS, and Wald Statistics constructed from these estimators have asymptotic χ &lt;sup&gt;2&lt;/sup&gt; distributions. These and previously proposed estimators of cointegrating vectors are used to study long-run U.S. money (M1) demand. M1 demand is found to be stable over 1900-1989; the 95% confidence intervals for the income elasticity and interest rate semielasticity are (.88, 1.06) and (-.13, -.08), respectively. Estimates based on the postwar data alone, however, are unstable, with variances which indicate substantial sampling uncertainty.</p> </abstract>
<abstract> <p>This paper considers tests for parameter instability and structural change with unknown change point. The results apply to a wide class of parametric models that are suitable for estimation by generalized method of moments procedures. The paper considers Wald, Lagrange multiplier, and likelihood ratio-like tests. Each test implicitly uses an estimate of a change point. The change point may be completely unknown or it may be known to lie in a restricted interval. Tests of both "pure" and "partial" structural change are discussed. The asymptotic distributions of the test statistics considered here are nonstandard because the change point parameter only appears under the alternative hypothesis and not under the null. The asymptotic null distributions are found to be given by the supremum of the square of a standardized tied-down Bessel process of order p ⩾ 1, as in D. L. Hawkins (1987). Tables of critical values are provided based on this asymptotic null distribution. As tests of parameter instability, the tests considered here are shown to have nontrivial asymptotic local power against all alternatives for which the parameters are nonconstant. As tests of one-time structural change, the tests are shown to have some weak asymptotic local power optimality properties for large sample size and small significance level. The tests are found to perform quite well in a Monte Carlo experiment reported elsewhere.</p> </abstract>
<abstract> <p>Formulas are derived for computing asymptotic covariance matrices of sets of impulse responses, step responses, or variance decompositions of estimated dynamic simultaneous-equations models in vector autoregressive moving-average (VARMA) form. Computed covariances would be used to test linear restrictions on sets of impulse responses, step responses, or variance decompositions. The results unify and extend previous formulas to handle any model in VARMA form, provide accurate computations based on analytic derivatives, and provide insights into the structures of the asymptotic covariances.</p> </abstract>
<abstract> <p>The paper develops an approach for analyzing the dynamics of a nonlinear time series that is represented by a nonparametric estimate of its one-step ahead conditional density. The approach entails examination of conditional moment profiles corresponding to certain shocks; a conditional moment profile is the conditional expectation evaluated at time t of a time invariant function evaluated at time t + j regarded as a function of j. Comparing the conditional moment profiles to baseline profiles is the nonlinear analog of conventional impulse-response analysis. The approach includes strategies for laying out realistic perturbation experiments in multivariate situations and for undertaking statistical inference using bootstrap methods. It also includes examination of profile bundles for evidence of damping or persistence. The empirical work investigates a bivariate series comprised of daily changes in the Standard and Poor's composite price index and daily NYSE transactions volume from 1928 to 1987. The effort uncovers evidence showing the heavily damped character of the "leverage effect" and the differential response (short-term increase, long-term decline) of trading volume to "common-knowledge" price shocks.</p> </abstract>
<abstract> <p>We derive low frequency, say weekly, models implied by high frequency, say daily, ARMA models with symmetric GARCH errors. Both stock and flow variable cases are considered. We show that low frequency models exhibit conditional heteroskedasticity of the GARCH form as well. The parameters in the conditional variance equation of the low frequency model depend upon mean, variance, and kurtosis parameters of the corresponding high frequency model. Moreover, strongly consistent estimators of the parameters in the high frequency model can be derived from low frequency data in many interesting cases. The common assumption in applications that rescaled innovations are independent is disputable, since it depends upon the available data frequency.</p> </abstract>
<abstract> <p>This paper provides a simulated moments estimator (SME) of the parameters of dynamic models in which the state vector follows a time-homogeneous Markov process. Conditions are provided for both weak and strong consistency as well as asymptotic normality. Various tradeoffs among the regularity conditions underlying the large sample properties of the SME are discussed in the context of an asset-pricing model.</p> </abstract>
<abstract> <p>Several types of rationing and queue mechanisms are compared in a framework of general equilibrium type models under gross substitutability and normality assumptions about consumers' Marshallian demand. During transition from rationing and queues to a market system, a group of low income people loses. The transition involves larger losses for this group if black markets prevail under rationing or queues, while a group of high income consumers gains. Some other comparative statics results are developed for a queue model with black markets.</p> </abstract>
<abstract> <p>We analyze an evolutionary model with a finite number of players and with noise or mutations. The expansion and contraction of strategies is linked--as usual--to their current relative success, but mutations--which perturb the system away from its deterministic evolution--are present as well. Mutations can occur in every period, so the focus is on the implications of ongoing mutations, not a one-shot mutation. The effect of these mutations is to drastically reduce the set of equilibria to what we term "long-run equilibria." For 2 × 2 symmetric games with two symmetric strict Nash equilibria the equilibrium selected satisfies (for large populations) Harsanyi and Selten's (1988) criterion of risk-dominance. In particular, if both strategies have equal security levels, the Pareto dominant Nash equilibrium is selected, even though there is another strict Nash equilibrium.</p> </abstract>
<abstract> <p>Consider an n-person game that is played repeatedly, but by different agents. In each period, n players are drawn at random from a large finite population. Each player chooses an optimal strategy based on a sample of information about what others players have done in the past. The sampling defines a stochastic process that, for a large class of games that includes coordination games and common interest games, converges almost surely to a pure strategy Nash equilibrium. Such an equilibrium can be interpreted as the "conventional" way of playing the game. If, in addition, the players sometimes experiment or make mistakes, then society occasionally switches from one convention to another. As the likelihood of mistakes goes to zero, only some conventions (equilibria) have positive probability in the limit. These are known as stochastically stable equilibria. They are essentially the same as the risk dominant equilibria in 2 × 2 games, but for general games the two concepts differ. The stochastically stable equilibria are computed by finding a path of least resistance from every equilibrium to every other, and then finding the equilibrium that has lowest overall resistance. This is a special case of a general theorem on perturbed Markov processes that characterizes their stochastically stable states graph-theoretically.</p> </abstract>
<abstract> <p>We study a model of optimal consumption and portfolio choice which captures, in two different interpretations, the notions of local substitution and irreversible purchases of durable goods. The class of preferences we consider excludes all nonlinear time-additive and nearly all the non-time-additive utility functions used in the literature. We discuss heuristically necessary conditions and provide sufficient conditions for a consumption and portfolio policy to be optimal. Furthermore, we demonstrate our general theory by solving in a closed form the optimal consumption and portfolio policy for a particular felicity function when the prices of the assets follow a geometric Brownian motion process. The optimal consumption policy in our solution consists of a possible initial "gulp" of consumption, or a period of no consumption, followed by a process of cumulative consumption with singular sample paths. In almost all states of nature, the agent consumes periodically and invests more in the risky assets than an agent with time-additive utility whose felicity function has the same curvature and the same time-discount parameter. We compute the equilibrium risk premium in a representative investor economy with a single physical production technology whose rate of return follows a Brownian motion. In addition, we provide some simulation results that demonstrate the properties of the purchase series for durable goods with different half-lives.</p> </abstract>
<abstract> <p>Han's maximum rank correlation (MRC) estimator is shown to be &lt;tex-math&gt;$\sqrt{n}\text{-consistent}$&lt;/tex-math&gt; and asymptotically normal. The proof rests on a general method for determining the asymptotic distribution of a maximization estimator, a simple U-statistic decomposition, and a uniform bound for degenerate U-processes. A consistent estimator of the asymptotic covariance matrix is provided, along with a result giving the explicit form of this matrix for any model within the scope of the MRC estimator. The latter result is applied to the binary choice model, and it is found that the MRC estimator does not achieve the semiparametric efficiency bound.</p> </abstract>
<abstract> <p> This paper is concerned with the estimation of first-order autoregressive/unit root models with independent identically distributed normal errors. The models considered include those without an intercept, those with an intercept, and those with an intercept and time trend. The autoregressive (AR) parameter α is allowed to lie in the interval (-1, 1], which includes the case of a unit root. Exactly median-unbiased estimators of the AR parameter α are proposed. Exact confidence intervals for this parameter are introduced. Corresponding exactly median-unbiased estimators and exact confidence intervals are also provided for the impulse response function, the cumulative impulse response, and the half life of a unit shock. An unbiased model selection procedure is discussed. The procedures that are introduced are applied to several data series including real exchange rates, the velocity of money, and industrial production. </p> </abstract>
<abstract> <p>Since the introduction of the autoregressive conditional heteroskedastic (ARCH) model in Engle (1982), numerous applications of this modeling strategy have already appeared. A common finding in many of these studies with high frequency financial or monetary data concerns the presence of an approximate unit root in the autoregressive polynomial in the univariate time series representation for the conditional second order moments of the process, as in the so-called integrated generalized ARCH (IGARCH) class of models proposed in Engle and Bollerslev (1986). In the IGARCH models shocks to the conditional variance are persistent, in the sense that they remain important for forecasts of all horizons. This idea is readily extended to a multivariate framework. Even though many time series may exhibit persistence in variance, it is likely that several different variables share the same common long-run component. In that situation, the variables are naturally defined to be co-persistent in variance, and the co-persistent linear combination is interpretable as a long-run relationship. Conditions for co-persistence to occur in the multivariate linear GARCH model are presented. These conditions parallel the conditions for linear co-integration in the mean, as developed by Engle and Granger (1987). The presence of co-persistence has important implications for asset pricing relationships and in optimal portfolio allocation decisions. An empirical example relating to the time series properties of nominal U.S. dollar exchange rates for the deutschemark and the British pound provides a simple illustration of the ideas.</p> </abstract>
<abstract> <p>This research explores the medical care consumption and absenteeism decisions of employed individuals with acute illnesses in an effort to better understand behavior that may contribute to the upward spiraling costs of health care. The theoretical framework models the decisions to visit a doctor and/or to miss work during an episode of acute illness as the sequential choices of individuals solving a discrete choice stochastic dynamic programming problem. Using data from the 1987 National Medical Expenditure Survey (NMES), I estimate the structural parameters of an individual's optimization problem. Structural estimation, as opposed to conventional reduced form estimation methods that are prevalent in the health care literature, allows for the introduction and evaluation of the impact of new public policy initiatives relating to health care. The estimates allow for predictions of the change in physician services use and illness-related absenteeism that arise with improvements in access to health care through more complete health insurance and sick leave coverage.</p> </abstract>
<abstract> <p>This paper considers issues related to multiple structural changes, occurring at unknown dates, in the linear regression model estimated by least squares. The main aspects are the properties of the estimators, including the estimates of the break dates, and the construction of tests that allow inference to be made about the presence of structural change and the number of breaks. We consider the general case of a partial structural change model where not all parameters are subject to shifts. We study both fixed and shrinking magnitudes of shifts and obtain the rates of convergence for the estimated break fractions. We also propose a procedure that allows one to test the null hypothesis of, say, l changes, versus the alternative hypothesis of l + 1 changes. This is particularly useful in that it allows a specific to general modeling strategy to consistently determine the appropriate number of changes present. An estimation strategy for which the location of the breaks need not be simultaneously determined is discussed. Instead, our method successively estimates each break point.</p> </abstract>
<abstract> <p>Methods are proposed to build exact tests and confidence sets in the linear first-order autoregressive distributed lag model with i.i.d. disturbances. For general linear hypotheses on the regression coefficients, inference procedures are obtained which have known level. The tests proposed are either similar (i.e., they have constant rejection probability for all data generating processes consistent with the null hypothesis) or use bounds which are free of nuisance parameters. Correspondingly the confidence sets are either similar with known size (i.e., they have constant coverage probability) or conservative. We also develop exact tests and confidence sets for various nonlinear transformations of model parameters, such as long-run multipliers and mean lags. The practical usefulness of these exact methods, which are also asymptotically valid under weak regularity conditions, is illustrated by some power comparisons and with applications to a dynamic trend model of money velocity and a model of money demand.</p> </abstract>
<abstract> <p> A simple root n consistent, asymptotically normal semiparametric estimator of the coefficient vector β in the latent variable specification y = L(β′ x + e) is constructed. The distribution of e is unknown and may be correlated with x or be conditionally heteroscedastic, e.g., x can contain measurement error. The function L can also be unknown. The identification assumption is that e is uncorrelated with instruments u and that the conditional distribution of e given x and u does not depend on one of the regressors, which has some special properties. Extensions to more general latent variable specifications are provided. </p> </abstract>
<abstract> <p>In this paper test statistics are proposed that can be used to test hypotheses about the parameters of the deterministic trend function of a univariate time series. The tests are valid in the presence of general forms of serial correlation in the errors and can be used without having to estimate the serial correlation parameters either parametrically or nonparametrically. The tests are valid for I(0) and I(1) errors. Trend functions that are permitted include general linear polynomial trend functions that may have breaks at either known or unknown locations. Asymptotic distributions are derived, and consistency of the tests is established. The general results are applied to a model with a simple linear trend. A local asymptotic analysis is used to compute asymptotic size and power of the tests for this example. Size is well controlled and is relatively unaffected by the variance of the initial condition. Asymptotic power curves are computed for the simple linear trend model and are compared to existing tests. It is shown that the new tests have nontrivial asymptotic power. A simulation study shows that the asymptotic approximations are adequate for sample sizes typically used in economics. The tests are used to construct confidence intervals for average GNP growth rates for eight industrialized countries using post-war data.</p> </abstract>
<abstract> <p>The volunteer armed forces play a major role in the American youth labor market, but little is known about the effects of voluntary military service on earnings. The effects of military service are difficult to measure because veterans are both self-selected and screened by the military. This study uses two strategies to reduce selection bias in estimates of the effects of military service on the earnings of veterans. Both approaches involve the analysis of a special match of Social Security earning records to administrative data on applicants to the armed forces. The first strategy compares applicants who enlisted with applicants who did not enlist, while controlling for most of the characteristics used by the military to select soldiers from the applicant pool. This is implemented using matching methods and regression. The second strategy uses instrumental variables that were generated by an error in the scoring of the exams that screen military applicants. Estimates from both strategies are interpreted using models with heterogeneous potential outcomes. The empirical results suggest that soldiers who served in the early 1980s were paid considerably more than comparable civilians while in the military, and that military service is associated with higher employment rates for veterans after service. In spite of this employment gain, however, military service led to only a modest long-run increase in the civilian earnings of nonwhite veterans while actually reducing the civilian earnings of white veterans.</p> </abstract>
<abstract> <p> Many non/semi-parametric time series estimates may be regarded as different forms of sieve extremum estimates. For stationary β-mixing observations, we obtain convergence rates of sieve extremum estimates and root-n asymptotic normality of "plug-in" sieve extremum estimates of smooth functionals. As applications to time series models, we give convergence rates for nonparametric ARX(p, q) regression via neural networks, splines, and wavelets; root-n asymptotic normality for partial linear additive AR(p) models, and monotone transformation AR(1) models. </p> </abstract>
<abstract> <p>In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the "dimension reduction." Efficient semiparametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessary for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.</p> </abstract>
<abstract> <p>One-step efficient GMM estimation has been developed in the recent papers of Back and Brown (1990), Imbens (1993), and Qin and Lawless (1994). These papers emphasized methods that correspond to using Owen's (1988) method of empirical likelihood to reweight the data so that the reweighted sample obeys all the moment restrictions at the parameter estimates. In this paper we consider an alternative KLIC motivated weighting and show how it and similar discrete reweightings define a class of unconstrained optimization problems which includes GMM as a special case. Such KLIC-motivated reweightings introduce M auxiliary "tilting" parameters, where M is the number of moments; parameter and overidentification hypotheses can be recast in terms of these tilting parameters. Such tests are often startlingly more effective than their conventional counterparts. These differences are not completely explained by differences in the leading terms of the asymptotic expansions of the test statistics.</p> </abstract>
<abstract> <p>Asymptotic normality of the posterior is a well understood result for dynamic as well as nondynamic models based on sets of abstract conditions whose actual applicability is hardly known especially for the case of nonstationarity. In this paper we provide a set of conditions by which we can relatively easily prove the asymptotic posterior normality under quite general situations of possible nonstationarity. This result reinforces and generalizes the point of Sims and Uhlig (1991) that inference based on the likelihood principle, explained by Berger and Wolpert (1988), will be unchanged regardless of whether the data are generated by a stationary process or by a unit root process. On the other hand, our conditions allow us to generalize the Bayesian information criterion known as the Schwarz criterion to the case of possible nonstationarity. In addition, we have shown that consistency of the maximum likelihood estimator, not the asymptotic normality of the estimator, with some minor additional assumptions is sufficient for asymptotic posterior normality.</p> </abstract>
<abstract> <p>This paper analyzes how self-interested subjects, as opposed to altruistic investigators, evaluate treatments in social experiments. We argue that the attrition behavior of subjects reveals their evaluation of treatments, and we discuss the usefulness of using such data in performing subject-based evaluation. We examine the possible causes of disagreements between investigators and subjects in their evaluation of treatments, and empirically assess the extent to which they disagree. Given that disagreements between subject evaluation and evaluation done by investigators could be due to sampling error, this paper provides an empirical framework for estimating the systematic level of disagreement in the presence of such errors. We illustrate this framework by estimating the extent of disagreement in clinical trials, and we find substantial evidence of over-approval by investigators in about one-third of the trials analyzed.</p> </abstract>
<abstract> <p>In this paper we develop a discretized version of the dynamic programming algorithm and study its convergence and stability properties. We show that the computed value function converges quadratically to the true value function and that the computed policy function converges linearly, as the mesh size of the discretization converges to zero; further, the algorithm is stable. We also discuss several aspects of the implementation of our procedures as applied to some commonly studied growth models.</p> </abstract>
<abstract> <p>This study establishes two main results in a dynamic general equilibrium model. The first is to demonstrate the dual Liapounov stability of a von Neumann facet without the restrictive assumptions on the structure of underlying technologies that are commonly adopted in the optimal growth literature. The second result is to demonstrate that a temporary change in fiscal policy has almost no effect on present and future consumption. While such inefficacy of temporary fiscal policy has been discussed in the context of the permanent income hypothesis, in this study, it is proved in the dynamic general equilibrium framework under a set of basic assumptions of general equilibrium theory.</p> </abstract>
<abstract> <p> A probability weighting function w(p) is a prominent feature of several non-expected utility theories, including prospect theory and rank-dependent models. Empirical estimates indicate that w(p) is regressive (first w(p) &gt; p, then w(p) &lt; p), s-shaped (first concave, then convex), and asymmetrical (intersecting the diagonal at about 1/3). The paper states axioms for several w(p) forms, including the compound invariant, w(p) = &lt;tex-math&gt;${\rm exp}\{-\{-{\rm ln}\ p\}^{\alpha}\}$&lt;/tex-math&gt;, 0 &lt; α &lt; 1, which is regressive, s-shaped, and with an invariant fixed point and inflection point at 1/e = .37. </p> </abstract>
<abstract> <p>We show that quasi-maximum likelihood (QML) estimators for conditional dispersion models can be severely affected by a small number of outliers such as market crashes and rallies, and we propose new estimation strategies (the two-stage Hampel estimators and two-stage S-estimators) resistant to the effects of outliers and study the properties of these estimators. We apply our methods to estimate models of the conditional volatility of the daily returns of the S&amp;P 500 Cash Index series. In contrast to QML estimators, our proposed method resists outliers, revealing an informative new picture of volatility dynamics during "typical" daily market activity.</p> </abstract>
<abstract> <p>This paper reports an experiment involving an ultimatum bargaining game, played in the Slovak Republic. Financial stakes were varied by a factor of 25, and behavior was observed both when players were inexperienced and as they gained experience. Consistent with prior results, changes in stakes had only a small effect on play for inexperienced players. But the present experimental design allows us to observe that rejections were less frequent the higher the stakes, and proposals in the high stakes conditions declined slowly as subjects gained experience. This Slovak experiment is the first to detect a lower frequency of rejection when stakes are higher and this can be explained by the added power due to multiple observations per subject in the experimental design. A model of learning suggests that the lower rejection frequency is the reason that the proposers in the higher stakes conditions of the ultimatum game learn to make lower offers.</p> </abstract>
<abstract> <p>This paper examines repeated games in which each player observes a private and imperfect signal on the actions played, and in which players are allowed to communicate using public messages. Providing incentives for players to reveal their observations generates (revelation) constraints that, combined with signal imperfections, may be a source of inefficiencies. However, by delaying the revelation of their observations, players may economize on the cost of deterring deviations, and thereby avoid these inefficiencies. Because a player would not want to trigger a sanction that would penalize him too, revelation constraints also tend to make sanctions difficult to enforce. However, with at least three players, detecting deviations may not require that all the players reveal their observations. In that case, we obtain a Nash threat version of the Folk theorem. With two players, we do not obtain a similar result. Nevertheless, we show that an efficient outcome can (almost) always be approximated.</p> </abstract>
<abstract> <p>We examine the possibility of cooperation in a long term relationship, where agents receive diverse imperfect information about each other's actions. "Secret price cutting" in the industrial organization literature is a leading example. In a differentiated product market, a firm cannot directly observe rival firms' secret price cutting, but its own sales can imperfectly indicate what is going on. Since the firms' sales levels are subject to random shocks, they may well end up having diverse expectations: firms with low sales may suspect price cutting while others may not. This causes a serious difficulty in sustaining collusion in such a market. In fact, the characterization of equilibria of this class of games-discounted repeated games where each player receives a different signal-has been an open question, despite the large body of literature on repeated games. The present paper shows that communication is a powerful way of resolving the possible confusion among the players in this class of games. In particular, we construct equilibria where players voluntarily communicate what they have observed and prove folk theorems. Our results thus provide a theoretical support for the conventional wisdom that communication plays an important role in sustaining collusion.</p> </abstract>
<abstract> <p>The paper introduces an alternative estimator for the linear censored quantile regression model. The objective function is globally convex and the estimator is a solution to a linear programming problem. Hence, a global minimizer is obtained in a finite number of simplex iterations. The suggested estimator also applies to the case where the censoring point is an unknown function of a set of regressors. It is shown that, under fairly weak conditions, the estimator has a &lt;tex-math&gt;$\sqrt{n}\ \text{-convergence}$&lt;/tex-math&gt; rate and is asymptotically normal. In the case of a fixed censoring point, its asymptotic property is nearly equivalent to that of the estimator suggested by Powell (1984, 1986a). A Monte Carlo study performed shows that the suggested estimator has very desirable small sample properties. It precisely corrects for the bias induced by censoring, even when there is a large amount of censoring, and for relatively small sample sizes.</p> </abstract>
<abstract> <p>We introduce nonnested hypotheses tests using indirect, simulation-based estimation procedures. We propose different test statistics and carefully examine the corresponding implicit null hypotheses. A notion of indirect encompassing comes up naturally, which may be compared to the usual encompassing conditions. Then we show how these methods extend standard specification and overidentification tests.</p> </abstract>
<abstract> <p>This paper surveys research on the welfare cost of inflation. New estimates are provided, based on U.S. time series for 1900-94, interpreted in a variety of ways. It is estimated that the gain from reducing the annual inflation rate from 10 percent to zero is equivalent to an increase in real income of slightly less than one percent. Using aggregate evidence only, it may not be possible to estimate reliably the gains from reducing inflation further, to a rate consistent with zero nominal interest.</p> </abstract>
<abstract> <p>As applied to the behavior of homeowners with mortgages, option theory predicts that mortgage prepayment or default will be exercised if the call or put option is "in the money" by some specific amount. Our analysis: tests the extent to which the option approach can explain default and prepayment behavior; evaluates the practical importance of modeling both options simultaneously; and models the unobserved heterogeneity of borrowers in the home mortgage market. The paper presents a unified model of the competing risks of mortgage termination by prepayment and default, considering the two hazards as dependent competing risks that are estimated jointly. It also accounts for the unobserved heterogeneity among borrowers, and estimates the unobserved heterogeneity simultaneously with the parameters and baseline hazards associated with prepayment and default functions. Our results show that the option model, in its most straightforward version, does a good job of explaining default and prepayment, but it is not enough by itself. The simultaneity of the options is very important empirically in explaining behavior. The results also show that there exists significant heterogeneity among mortgage borrowers. Ignoring this heterogeneity results in serious errors in estimating the prepayment behavior of homeowners.</p> </abstract>
<abstract> <p>In a public good environment with positively correlated types, we characterize optimal mechanisms when agents have private information and can enter collusive agreements. First, we prove a weak-collusion-proof principle according to which there is no restriction for the principal in offering weak-collusion-proof mechanisms. Second, with this principle, we characterize the set of allocations that satisfy individual and coalitional incentive constraints. The optimal weakly collusion-proof mechanism calls for distortions away from first-best efficiency obtained without collusion. Allowing collusion restores continuity between the correlated and the uncorrelated environments. When the correlation becomes almost perfect, first-best efficiency is approached. Finally, the optimal collusion-proof mechanism is strongly ratifiable.</p> </abstract>
<abstract> <p>In Becker's (1973) neoclassical marriage market model, matching is positively assortative if types are complements: i.e., match output f(x, y) is supermodular in x and y. We reprise this famous result assuming time-intensive partner search and transferable output. We prove existence of a search equilibrium with a continuum of types, and then characterize matching. After showing that Becker's conditions on match output no longer suffice for assortative matching, we find sufficient conditions valid for any search frictions and type distribution: supermodularity not only of output f, but also of log f&lt;sub&gt;x&lt;/sub&gt; and log f&lt;sub&gt;xy&lt;/sub&gt;. Symmetric submodularity conditions imply negatively assortative matching. Examples show these conditions are necessary.</p> </abstract>
<abstract> <p>This paper explores how Bayes-rational individuals learn sequentially from the discrete actions of others. Unlike earlier informational herding papers, we admit heterogeneous preferences. Not only may type-specific "herds" eventually arise, but a new robust possibility emerges: confounded learning. Beliefs may converge to a limit point where history offers no decisive lessons for anyone, and each type's actions forever nontrivially split between two actions. To verify that our identified limit outcomes do arise, we exploit the Markov-martingale character of beliefs. Learning dynamics are stochastically stable near a fixed point in many Bayesian learning models like this one.</p> </abstract>
<abstract> <p>The supply and price of skilled labor relative to unskilled labor have changed dramatically over the postwar period. The relative quantity of skilled labor has increased substantially, and the skill premium, which is the wage of skilled labor relative to that of unskilled labor, has grown significantly since 1980. Many studies have found that accounting for the increase in the skill premium on the basis of observable variables is difficult and have concluded implicitly that latent skill-biased technological change must be the main factor responsible. This paper examines that view systematically. We develop a framework that provides a simple, explicit economic mechanism for understanding skill-biased technological change in terms of observable variables, and we use the framework to evaluate the fraction of variation in the skill premium that can be accounted for by changes in observed factor quantities. We find that with capital-skill complementarity, changes in observed inputs alone can account for most of the variations in the skill premium over the last 30 years.</p> </abstract>
<abstract> <p>This paper develops asymptotic distribution theory for GMM estimators and test statistics when some or all of the parameters are weakly identified. General results are obtained and are specialized to two important cases: linear instrumental variables regression and Euler equations estimation of the CCAPM. Numerical results for the CCAPM demonstrate that weak-identification asymptotics explains the breakdown of conventional GMM procedures documented in previous Monte Carlo studies. Confidence sets immune to weak identification are proposed. We use these results to inform an empirical investigation of various CCAPM specifications; the substantive conclusions reached differ from those obtained using conventional methods.</p> </abstract>
<abstract> <p>Data snooping occurs when a given set of data is used more than once for purposes of inference or model selection. When such data reuse occurs, there is always the possibility that any satisfactory results obtained may simply be due to chance rather than to any merit inherent in the method yielding the results. This problem is practically unavoidable in the analysis of time-series data, as typically only a single history measuring a given phenomenon of interest is available for analysis. It is widely acknowledged by empirical researchers that data snooping is a dangerous practice to be avoided, but in fact it is endemic. The main problem has been a lack of sufficiently simple practical methods capable of assessing the potential dangers of data snooping in a given situation. Our purpose here is to provide such methods by specifying a straightforward procedure for testing the null hypothesis that the best model encountered in a specification search has no predictive superiority over a given benchmark model. This permits data snooping to be undertaken with some degree of confidence that one will not mistake results that could have been generated by chance for genuinely good results.</p> </abstract>
<abstract> <p>We propose a new and simple adaptive procedure for playing a game: "regret-matching." In this procedure, players may depart from their current play with probabilities that are proportional to measures of regret for not having used other strategies in the past. It is shown that our adaptive procedure guarantees that, with probability one, the empirical distributions of play converge to the set of correlated equilibria of the game.</p> </abstract>
<abstract> <p>We construct a quantitative equilibrium model with firms setting prices in a staggered fashion and use it to ask whether monetary shocks can generate business cycle fluctuations. These fluctuations include persistent movements in output along with the other defining features of business cycles, like volatile investment and smooth consumption. We assume that prices are exogenously sticky for a short time. Persistent output fluctuations require endogenous price stickiness in the sense that firms choose not to change prices much when they can do so. We find that for a wide range of parameter values, the amount of endogenous stickiness is small. Thus, we find that in a standard quantitative model, staggered price-setting, alone, does not generate business cycle fluctuations.</p> </abstract>
<abstract> <p>We present an approach to network formation based on the notion that social networks are formed by individual decisions that trade off the costs of forming and maintaining links against the potential rewards from doing so. We suppose that a link with another agent allows access, in part and in due course, to the benefits available to the latter via his own links. Thus individual links generate externalities whose value depends on the level of decay/delay associated with indirect links. A distinctive aspect of our approach is that the costs of link formation are incurred only by the person who initiates the link. This allows us to formulate the network formation process as a noncooperative game. We first provide a characterization of the architecture of equilibrium networks. We then study the dynamics of network formation. We find that individual efforts to access benefits offered by others lead, rapidly, to the emergence of an equilibrium social network, under a variety of circumstances. The limiting networks have simple architectures, e.g., the wheel, the star, or generalizations of these networks. In many cases, such networks are also socially efficient.</p> </abstract>
<abstract> <p>This paper examines Markov perfect equilibria of general, finite state stochastic games. Our main result is that the number of such equilibria is finite for a set of stochastic game payoffs with full Lebesgue measure. We further discuss extensions to lower dimensional stochastic games like the alternating move game.</p> </abstract>
<abstract> <p>This paper develops an asymptotic theory for time series binary choice models with nonstationary explanatory variables generated as integrated processes. Both logit and probit models are covered. The maximum likelihood (ML) estimator is consistent but a new phenomenon arises in its limit distribution theory. The estimator consists of a mixture of two components, one of which is parallel to and the other orthogonal to the direction of the true parameter vector, with the latter being the principal component. The ML estimator is shown to converge at a rate of n&lt;sup&gt;3/4&lt;/sup&gt; along its principal component but has the slower rate of n&lt;sup&gt;1/4&lt;/sup&gt; convergence in all other directions. This is the first instance known to the authors of multiple convergence rates in models where the regressors have the same (full rank) stochastic order and where the parameters appear in linear forms of these regressors. It is a consequence of the fact that the estimating equations involve nonlinear integrable transformations of linear forms of integrated processes as well as polynomials in these processes, and the asymptotic behavior of these elements is quite different. The limit distribution of the ML estimator is derived and is shown to be a mixture of two mixed normal distributions with mixing variates that are dependent upon Brownian local time as well as Brownian motion. It is further shown that the sample proportion of binary choices follows an arc sine law and therefore spends most of its time in the neighborhood of zero or unity. The result has implications for policy decision making that involves binary choices and where the decisions depend on economic fundamentals that involve stochastic trends. Our limit theory shows that, in such conditions, policy is likely to manifest streams of little intervention or intensive intervention.</p> </abstract>
<abstract> <p>The paper takes stock of the advances and directions for research on the incomplete contracting front. It first illustrates some of the main ideas of the incomplete contract literature through an example. It then offers methodological insights on the standard approach to modeling incomplete contracts; in particular it discusses a tension between two assumptions made in the literature, namely rationality and the existence of transaction costs. Last, it argues that, contrary to what is commonly argued, the complete contract methodology need not be unable to account for standard institutions such as authority and ownership; and it concludes with a discussion of the research agenda.</p> </abstract>
<abstract> <p>In this paper we derive a model of aggregate investment that builds from the lumpy microeconomic behavior of firms facing stochastic fixed adjustment costs. Instead of the standard sharp (S, s) bands, firms' adjustment policies take the form of a probability of adjustment (adjustment hazard) that responds smoothly to changes in firms' capacity gap. The model has appealing aggregation properties, and yields nonlinear aggregate time series processes. The passivity of normal times is, occasionally, more than offset by the brisk response to large accumulated shocks. Using within and out-of-sample criteria, we find that the model performs substantially better than the standard linear models of investment for postwar sectoral U.S. manufacturing equipment and structures investment data.</p> </abstract>
<abstract> <p> In 'experience-weighted attraction' (EWA) learning, strategies have attractions that reflect initial predispositions, are updated based on payoff experience, and determine choice probabilities according to some rule (e.g., logit). A key feature is a parameter δ that weights the strength of hypothetical reinforcement of strategies that were not chosen according to the payoff they would have yielded, relative to reinforcement of chosen strategies according to received payoffs. The other key features are two discount rates, φ and ρ, which separately discount previous attractions, and an experience weight. EWA includes reinforcement learning and weighted fictitious play (belief learning) as special cases, and hybridizes their key elements. When δ = 0 and ρ = 0, cumulative choice reinforcement results. When δ = 1 and ρ = φ, levels of reinforcement of strategies are exactly the same as expected payoffs given weighted fictitious play beliefs. Using three sets of experimental data, parameter estimates of the model were calibrated on part of the data and used to predict a holdout sample. Estimates of δ are generally around .50, φ around .8-1, and ρ varies from 0 to φ. Reinforcement and belief-learning special cases are generally rejected in favor of EWA, though belief models do better in some constant-sum games. EWA is able to combine the best features of previous approaches, allowing attractions to begin and grow flexibly as choice reinforcement does, but reinforcing unchosen strategies substantially as belief-based models implicitly do. </p> </abstract>
<abstract> <p> A probability distribution governing the evolution of a stochastic process has infinitely many Bayesian representations of the form &lt;tex-math&gt;$\mu =\int_{\Theta}\mu _{\theta }d\lambda (\theta)$&lt;/tex-math&gt;. Among these, a natural representation is one whose components &lt;tex-math&gt;$(\mu _{\theta}\text{'}{\rm s})$&lt;/tex-math&gt; are "learnable" (one can approximate μ &lt;sub&gt;θ&lt;/sub&gt; by conditioning μ on observation of the process) and "sufficient for prediction" (&lt;tex-math&gt;$\mu _{\theta}\text{'}{\rm s}$&lt;/tex-math&gt; predictions are not aided by conditioning on observation of the process). We show the existence and uniqueness of such a representation under a suitable asymptotic mixing condition on the process. This representation can be obtained by conditioning on the tail-field of the process, and any learnable representation that is sufficient for prediction is asymptotically like the tail-field representation. This result is related to the celebrated de Finetti theorem, but with exchangeability weakened to an asymptotic mixing condition, and with his conclusion of a decomposition into i.i.d. component distributions weakened to components that are learnable and sufficient for prediction. </p> </abstract>
<abstract> <p>Ultra-high-frequency data is defined to be a full record of transactions and their associated characteristics. The transaction arrival times and accompanying measures can be analyzed as marked point processes. The ACD point process developed by Engle and Russell (1998) is applied to IBM transactions arrival times to develop semiparametric hazard estimates and conditional intensities. Combining these intensities with a GARCH model of prices produces ultra-high-frequency measures of volatility. Both returns and variances are found to be negatively influenced by long durations as suggested by asymmetric information models of market micro-structure.</p> </abstract>
<abstract> <p> This paper considers the problem of choosing the number of bootstrap repetitions B for bootstrap standard errors, confidence intervals, confidence regions, hypothesis tests, p-values, and bias correction. For each of these problems, the paper provides a three-step method for choosing B to achieve a desired level of accuracy. Accuracy is measured by the percentage deviation of the bootstrap standard error estimate, confidence interval length, test's critical value, test's p-value, or bias-corrected estimate based on B bootstrap simulations from the corresponding ideal bootstrap quantities for which B = ∞. The results apply quite generally to parametric, semiparametric, and nonparametric models with independent and dependent data. The results apply to the standard nonparametric iid bootstrap, moving block bootstraps for time series data, parametric and semiparametric bootstraps, and bootstraps for regression models based on bootstrapping residuals. Monte Carlo simulations show that the proposed methods work very well. </p> </abstract>
<abstract> <p>This paper develops a new concept of separability with overlapping groups-latent separability. This is shown to provide a useful empirical and theoretical framework for investigating the grouping of goods and prices. It is a generalization of weak separability in which goods are allowed to enter more than one group and where the composition of groups is identified by the choice of group specific exclusive goods. Latent separability is shown to be equivalent to weak separability in latent rather than purchased goods and provides a relationship between separability and household production theory. For the popular class of linear, almost ideal and translog demand models and their generalizations, we provide a method for choosing the number of homothetic separable groups. A detailed method for exploring the composition of the separable groups is also presented. These methods are applied to a long time series of British individual household data on the consumption of twenty two nondurable and service goods.</p> </abstract>
<abstract> <p> The paper develops a reputation based theory of bargaining. The idea is to investigate and highlight the influence of bargaining "postures" on bargaining outcomes. A complete information bargaining model à la Rubinstein is amended to accommodate "irrational types" who are obstinate, and indeed for tractability assumed to be completely inflexible in their offers and demands. A strong "independence of procedures" result is derived: after initial postures have been adopted, the bargaining outcome is independent of the fine details of the bargaining protocol so long as both players have the opportunity to make offers frequently. The latter analysis yields a unique continuous-time limit with a war of attrition structure. In the continuous-time game, equilibrium is unique, and entails delay, consequently inefficiency. The equilibrium outcome reflects the combined influence of the rates of time preference of the players and the ex ante probabilities of different irrational types. As the probability of irrationality goes to zero, delay and inefficiency disappear; furthermore, if there is a rich set of types for both agents, the limit equilibrium payoffs are inversely proportional to their rates of time preference. </p> </abstract>
<abstract> <p>This paper analyzes choice-theoretic costly enforcement in an intertemporal contracting model with a differentially informed investor and entrepreneur. An intertemporal contract is modeled as a mechanism in which there is limited commitment to payment and enforcement decisions. The goal of the analysis is to characterize the effect of choice-theoretic costly enforcement on the structure of optimal contracts. The paper shows that simple debt is the optimal contract when commitment is limited and costly enforcement is a decision variable (Theorem 1). In contrast, stochastic contracts are optimal when agents can commit to the ex-ante optimal decisions (Theorem 2). The paper also shows that the costly state verification model can be viewed as a reduced form of an enforcement model in which agents choose payments and strategies as part of a perfect Bayesian Nash equilibrium.</p> </abstract>
<abstract> <p>Why do both left and right political parties typically propose progressive income taxation schemes in political competition? Analysis of this problem has been hindered by the two-dimensionality of the issue space. To give parties a choice over a domain that contains both progressive and regressive income tax policies requires an issue space that is at least two-dimensional. Nash equilibrium in pure strategies of the standard two-party game, whose players have complete preferences over a two-dimensional policy space, generically fails to exist. I introduce a new equilibrium concept for political games, based on the fact of factional conflict within parties. Each party is supposed to consist of reformists, militants, and opportunists: each faction has a complete preference order on policy space, but together they can only agree on a partial order. Nash equilibria of the two-party game, where the policy space consists of all quadratic income tax functions, and each party is represented by its partial order, exist, and it is shown that, in such equilibria, both parties propose progressive income taxation.</p> </abstract>
<abstract> <p>We consider the strategic options facing workers in labor markets with centralized market clearing mechanisms such as those in the entry level labor markets of a number of professions. If workers do not have detailed information about the preferences of other workers and firms, the scope of potentially profitable strategic behavior is considerably reduced, although not entirely eliminated. Specifically, we demonstrate that stating preferences that reverse the true preference order of two acceptable firms is not beneficial in a low information environment, but submitting a truncation of the true preferences may be. This gives some insight into the successful operation of these market mechanisms.</p> </abstract>
<abstract> <p> It is shown that an exponentially small departure from the common knowledge assumption on the number T of repetitions of the prisoners' dilemma already enables cooperation, More generally, with such a departure, any feasible individually rational outcome of any one-shot game can be approximated by a subgame perfect equilibrium of a finitely repeated version of that game. The sense in which the departure from common knowledge is small is as follows: (i) With probability one, the players know T with precision ±KK. (ii) With probability 1 - ε, the players know T precisely; moreover, this knowledge is mutual of order εT. (iii) The deviation of T from its finite expectation is exponentially small. </p> </abstract>
<abstract> <p>We consider the problem of the design and sale of a security backed by specified assets. Given access to higher-return investments, the issuer has an incentive to raise capital by securitizing part of these assets. At the time the security is issued, the issuer's or underwriter's private information regarding the payoff of the security may cause illiquidity, in the form of a downward-sloping demand curve for the security. The severity of this illiquidity depends upon the sensitivity of the value of the issued security to the issuer's private information. Thus, the security-design problem involves a tradeoff between the retention cost of holding cash flows not included in the security design, and the liquidity cost of including the cash flows and making the security design more sensitive to the issuer's private information. We characterize the optimal security design in several cases. We also demonstrate circumstances under which standard debt is optimal and show that the riskiness of the debt is increasing in the issuer's retention costs for assets.</p> </abstract>
<abstract> <p> We study preferences over Savage acts that map states to opportunity sets and satisfy the Savage axioms. Preferences over opportunity sets may exhibit a preference for flexibility due to an implicit uncertainty about future preferences reflecting anticipated unforeseen contingencies. The main result of this paper characterizes maximization of the expected indirect utility in terms of an "Indirect Stochastic Dominance" axiom that expresses a preference for "more opportunities in expectation." The key technical tool of the paper, a version of Möbius inversion, has been imported from the theory of nonadditive belief functions; it allows an alternative representation using Choquet integration, and yields a simple proof of Kreps' (1979) classic result. </p> </abstract>
<abstract> <p>We study economies with one private good and one pure public good, and consider the following axioms of social choice functions. Strategy-proofness says that no agent can benefit by misrepresenting his preferences, regardless of whether the other agents misrepresent or not, and whatever his preferences are. Symmetry says that if two agents have the same preference, they must be treated equally. Anonymity says that when the preferences of two agents are switched, their consumption bundles are also switched. Individual rationality says that a social choice function never assigns an allocation which makes some agent worse off than he would be by consuming no public good and paying nothing. In Theorem 1, we characterize the class of strategy-proof, budget-balancing, and symmetric social choice functions, assuming convexity of the cost function of the public good. In Theorem 2, we characterize the class of strategy-proof, budget-balancing, and anonymous social choice functions. In Theorem 3, we characterize the class of strategy-proof, budget-balancing, symmetric, and individually rational social choice functions.</p> </abstract>
<abstract> <p>The proportional hazard model with unobserved heterogeneity gives the hazard function of a random variable conditional on covariates and a second random variable representing unobserved heterogeneity. This paper shows how to estimate the baseline hazard function and the distribution of the unobserved heterogeneity nonparametrically. The baseline hazard function and heterogeneity distribution are assumed to satisfy smoothness conditions but are not assumed to belong to known, finite-dimensional, parametric families. Existing estimators assume that the baseline hazard function or heterogeneity distribution belongs to a known parametric family. Thus, the estimators presented here are more general than existing ones.</p> </abstract>
<abstract> <p>A game is better-reply secure if for every nonequilibrium strategy x* and every payoff vector limit u* resulting from strategies approaching x*, some player i has a strategy yielding a payoff strictly above u*&lt;sub&gt;i&lt;/sub&gt; even if the others deviate slightly from x*. If strategy spaces are compact and convex, payoffs are quasiconcave in the owner's strategy, and the game is better-reply secure, then a pure strategy Nash equilibrium exists. Better-reply security holds in many economic games. It also permits new results on the existence of symmetric and mixed strategy Nash equilibria.</p> </abstract>
<abstract> <p> This paper develops a regression limit theory for nonstationary panel data with large numbers of cross section (n) and time series (T) observations. The limit theory allows for both sequential limits, wherein T → ∞ followed by n → ∞, and joint limits where T, n → ∞ simultaneously; and the relationship between these multidimensional limits is explored. The panel structures considered allow for no time series cointegration, heterogeneous cointegration, homogeneous cointegration, and near-homogeneous cointegration. The paper explores the existence of long-run average relations between integrated panel vectors when there is no individual time series cointegration and when there is heterogeneous cointegration. These relations are parameterized in terms of the matrix regression coefficient of the long-run average covariance matrix. In the case of homogeneous and near homogeneous cointegrating panels, a panel fully modified regression estimator is developed and studied. The limit theory enables us to test hypotheses about the long run average parameters both within and between subgroups of the full population. </p> </abstract>
<abstract> <p>We show how correctly to extend known methods for generating error bands in reduced form VAR's to overidentified models. We argue that the conventional pointwise bands common in the literature should be supplemented with measures of shape uncertainty, and we show how to generate such measures. We focus on bands that characterize the shape of the likelihood. Such bands are not classical confidence regions. We explain that classical confidence regions mix information about parameter location with information about model fit, and hence can be misleading as summaries of the implications of the data for the location of parameters. Because classical confidence regions also present conceptual and computational problems in multivariate time series models, we suggest that likelihood-based bands, rather than approximate confidence bands based on asymptotic theory, be standard in reporting results for this type of model.</p> </abstract>
<abstract> <p>We provide an axiomatic foundation for decision making in a complex environment. We do not assume that the decision maker has complete structural knowledge of the environment. Instead the agent knows the set of actions he can take, he formulates preferences directly on the actions, and chooses according to these preferences. On the basis of experience he modifies these preferences according to a systematic procedure. Our axioms are imposed on this procedure, rather than directly on the choice itself. The axioms consist of a group of natural structural restrictions and a group of independence axioms. Our main result is an axiomatic foundation for a set of simple adaptive learning procedures that include the replicator dynamic.</p> </abstract>
<abstract> <p>This paper defines a general equilibrium model with exchange and club formation. Agents trade multiple private goods widely in the market, can belong to several clubs, and care about the characteristics of the other members of their clubs. The space of agents is a continuum, but clubs are finite. It is shown that (i) competitive equilibria exist, and (ii) the core coincides with the set of equilibrium states. The central subtlety is in modeling club memberships and expressing the notion that membership choices are consistent across the population.</p> </abstract>
<abstract> <p>Bidding is studied in first-price common value auctions where an insider is better informed than other bidders (outsiders) about the value of the item. With inexperienced bidders, having an insider does not materially reduce the severity of the winner's curse compared to auctions with a symmetric information structure (SIS). In contrast, super-experienced bidders, who have largely overcome the winner's curse, satisfy the comparative static predictions of equilibrium bidding theory: (i) average seller's revenue is larger with an insider than in SIS auctions, (ii) insiders make substantially greater profits, conditional on winning, than outsiders, and (iii) insiders increase their bids in response to more rivals. Further, changes in insiders' bids are consistent with directional learning theory (Selten and Buchta (1994)).</p> </abstract>
<abstract> <p>We introduce a new equilibrium concept and study its efficiency and asset pricing implications for the environment analyzed by Kehoe and Levine (1993) and Kocherlakota (1996). Our equilibrium concept has complete markets and endogenous solvency constraints. These solvency constraints prevent default at the cost of reducing risk sharing. We show versions of the welfare theorems. We characterize the preferences and endowments that lead to equilibria with incomplete risk sharing. We compare the resulting pricing kernel with the one for economies without participation constraints: interest rates are lower and risk premia depend on the covariance of the idiosyncratic and aggregate shocks. Additionally, we show that asset prices depend only on the valuation of agents with substantial idiosyncratic risk.</p> </abstract>
<abstract> <p>Consider strategic risk-neutral traders competing in schedules to supply liquidity to a risk-averse agent who is privately informed about the value of the asset and his hedging needs. Imperfect competition in this common value environment is analyzed as a multiprincipal game in which liquidity suppliers offer trading mechanisms in a decentralized way. Each liquidity supplier behaves as a monopolist facing a residual demand curve resulting from the maximizing behavior of the informed agent and the trading mechanisms offered by his competitors. There exists a unique equilibrium in convex schedules. It is symmetric and differentiable and exhibits typical features of market-power: Equilibrium trading volume is lower than ex ante efficiency would require. Liquidity suppliers charge positive mark-ups and make positive expected profits, but these profits decrease with the number of competitors. In the limit, as this number goes to infinity, ask (resp. bid) prices converge towards the upper (resp. lower) tail expectations obtained in Glosten (1994) and expected profits are zero.</p> </abstract>
<abstract> <p>In this paper, we consider identification and estimation in panel data discrete choice models when the explanatory variable set includes strictly exogenous variables, lags of the endogenous dependent variable as well as unobservable individual-specific effects. For the binary logit model with the dependent variable lagged only once, Chamberlain (1993) gave conditions under which the model is not identified. We present a stronger set of conditions under which the parameters of the model are identified. The identification result suggests estimators of the model, and we show that these are consistent and asymptotically normal, although their rate of convergence is slower than the inverse of the square root of the sample size. We also consider identification in the semiparametric case where the logit assumption is relaxed. We propose an estimator in the spirit of the conditional maximum score estimator (Manski (1987)), and we show that it is consistent. In addition, we discuss an extension of the identification result to multinomial discrete choice models, and to the case where the dependent variable is lagged twice. Finally, we present some Monte Carlo evidence on the small sample performance of the proposed estimators for the binary response model.</p> </abstract>
<abstract> <p> This paper examines an employment relation in which individual workers enjoy some bargaining power vis-à-vis the firm although they are not unionized. The main elements of the situations studied here are that the employment contracts are non-binding across periods of production and that the firm has opportunities to replace workers. The paper analyzes a dynamic model in which the processes of contracting and recontracting between the firm and its workers are intertwined with the dynamic evolution of the firm's workforce. The analysis of the model is somewhat complicated because the employment level is a nondegenerate state variable that evolves over time and is affected by past decisions. The main analytical results characterize certain important equilibria: the profit maximizing and stationary equilibria. The unique stationary equilibrium is markedly inefficient: it exhibits inefficient over-employment and the steady state wages coincide with the workers' reservation wage. It confirms earlier results derived by Stole and Zwiebel (1996a,b) in the context of a static model and shows that they are very robust even when the firm has nearly frictionless hiring opportunities. In contrast, in the profit maximizing equilibrium the outcome is nearly efficient and the wage exhibits a mark-up over the reservation wage. </p> </abstract>
<abstract> <p>This paper studies the monotonicity of individual and market demand with the aid of the indirect utility function. We identify sufficient (and in a sense, necessary) conditions on an agent's indirect utility which will guarantee that he has a monotonic demand function. Our conditions also point to a natural way of extending the result of Hildenbrand (1983). Hildenbrand showed that market demand is monontonic if the income distribution has a downward sloping density, even though individual agents' demand function might violate monotonicity. Using the indirect utility function, we introduce a measure of violations of individual monotonicity that allows us to identify a larger class of density functions that will generate a monotonic market demand.</p> </abstract>
<abstract> <p>A valid Edgeworth expansion is established for the limit distribution of density-weighted semiparametric averaged derivative estimates of single index models. The leading term that corrects the normal limit varies in magnitude, depending on the choice of bandwidth and kernel order. In general this term has order larger than the n&lt;sup&gt;-1/2&lt;/sup&gt; that prevails in standard parametric problems, but we find circumstances in which it is O(n&lt;sup&gt;-1/2&lt;/sup&gt;), thereby extending the achievement of an n&lt;sup&gt;-1/2&lt;/sup&gt; Berry-Esseen bound in Robinson (1995a). A valid empirical Edgeworth expansion is also established. We also provide theoretical and empirical Edgeworth expansions for a studentized statistic, where some correction terms are different from those for the unstudentized case. We report a Monte Carlo study of finite sample performance.</p> </abstract>
<abstract> <p>This paper introduces a new notion of consistency for social choice functions, called self-selectivity, which requires that a social choice function employed by a society to make a choice from a given alternative set it faces should choose itself from among other rival such functions when it is employed by the society to make this latter choice as well. A unanimous neutral social choice function turns out to be universally self-selective if and only if it is Paretian and satisfies independence of irrelevant alternatives. The neutral unanimous social choice functions whose domains consist of linear order profiles on nonempty sets of any finite cardinality induce a class of social welfare functions that inherit Paretianism and independence of irrelevant alternatives in case the social choice function with which one starts is universally self-selective. Thus, a unanimous and neutral social choice function is universally self-selective if and only if it is dictatorial. Moreover, universal self-selectivity for such functions is equivalent to the conjunction of strategy-proofness and independence of irrelevant alternatives or the conjunction of monotonicity and independence of irrelevant alternatives again.</p> </abstract>
<abstract> <p>"If empirically meaningful interpersonal comparisons have to be based on indifference maps, as we have argued, then the Independence of Irrelevant Alternatives must be violated. The information which enables us to assert that individual A prefers x to y more strongly than B prefers y to x must be based on comparisons by A and B of x and y not only to each other but also to other alternatives."</p> </abstract>
<abstract> <p>When do dynamic nonconvexities at the disaggregate level translate into dynamic nonconvexities at the aggregate level? We address this question in a framework where the production of differentiated intermediate inputs is subject to dynamic nonconvexities, and we show that the answer depends on the degree of Hicks-Allen complementarity (substitutability) between differentiated inputs. In our simplest model, a generalization of Judd (1985) and Grossman and Helpman (1991) among many others, there are dynamic nonconvexities at the aggregate level if and only if differentiated inputs are Hicks-Allen complements. We also compare dynamic equilibrium and optimal allocations in the presence of aggregate dynamic nonconvexities due to Hicks-Allen complementarities between differentiated inputs.</p> </abstract>
<abstract> <p>An overlapping generations model of social security with shocks to the productivity of labor and capital and demographic shocks is studied. We focus attention on stationary long run allocations. An allocation is interim optimal if there does not exist another feasible allocation that improves the expected welfare of all generations, computed conditionally on the state of the world when they are born. We characterize the set of interim optimal allocations and study the equilibria associated with various institutional forms of social security from the point of view of this optimality criterion. We obtain the analogs of the two traditional welfare theorems of microeconomic theory. Assume that there exists a financial asset in fixed quantity, which supports some (non null) intergenerational transfers. Then the rational expectations equilibrium allocation of this economy is interim optimal. Conversely, any stationary interim optimal allocation can be supported by such an equilibrium, with adequate lump sum transfers.</p> </abstract>
<abstract> <p>This paper considers a generalized method of moments (GMM) estimation problem in which one has a vector of moment conditions, some of which are correct and some incorrect. The paper introduces several procedures for consistently selecting the correct moment conditions. The procedures also can consistently determine whether there is a sufficient number of correct moment conditions to identify the unknown parameters of interest. The paper specifies moment selection criteria that are GMM analogues of the widely used BIC and AIC model selection criteria. (The latter is not consistent.) The paper also considers downward and upward testing procedures. All of the moment selection procedures discussed in this paper are based on the minimized values of the GMM criterion function for different vectors of moment conditions. The procedures are applicable in time-series and cross-sectional contexts. Application of the results of the paper to instrumental variables estimation problems yields consistent procedures for selecting instrumental variables.</p> </abstract>
<abstract> <p>This paper presents a simple two-step nonparametric estimator for a triangular simultaneous equation model. Our approach employs series approximations that exploit the additive structure of the model. The first step comprises the nonparametric estimation of the reduced form and the corresponding residuals. The second step is the estimation of the primary equation via nonparametric regression with the reduced form residuals included as a regressor. We derive consistency and asymptotic normality results for our estimator, including optimal convergence rates. Finally we present an empirical example, based on the relationship between the hourly wage rate and annual hours worked, which illustrates the utility of our approach.</p> </abstract>
<abstract> <p>We consider strategyproof social choice functions defined over product domains. If preferences are strict orderings and separable, then strategyproof social choice functions must be decomposable provided that the domain of preferences is rich. We provide several characterization results in the case where preferences are separable only with respect to the elements of some partition of the set of components and these partitions vary across individuals. We characterize the libertarian social choice function and show that no superset of the tops separable domain admits strategyproof nondictatorial social choice functions.</p> </abstract>
<abstract> <p>A dynamic search framework is developed to analyze the intertemporal labor force participation behavior of married women, using longitudinal data to allow for a rich dynamic structure. The sensitivity to alternative distributional assumptions is evaluated using linear probability and probit models. The dynamic probit models are estimated using maximum simulated likelihood (MSL) estimation, to overcome the computational difficulties inherent in maximum likelihood estimation of models with nontrivial error structures. The results find that participation decisions are characterized by significant state dependence, unobserved heterogeneity, and negative serial correlation in the error component. The hypothesis that fertility decisions are exogenous to women's participation decisions is rejected when dynamics are ignored; however, there is no evidence against this hypothesis in dynamic model specifications. Women's participation response is stronger to permanent than current nonlabor income, reflecting unobserved taste factors.</p> </abstract>
<abstract> <p>In this paper, we develop and structurally estimate a sequential model of high school attendance and work decisions. The model's estimates imply that youths who drop out of high school have different traits than those who graduate-they have lower school ability and/or motivation, they have lower expectations about the rewards from graduation, they have a comparative advantage at jobs that are done by nongraduates, and they place a higher value on leisure and have a lower consumption value of school attendance. We also found that working while in school reduces school performance. However, policy experiments based on the model's estimates indicate that even the most restrictive prohibition on working while attending high school would have only a limited impact on the high school graduation rates of white males.</p> </abstract>
<abstract> <p>This paper establishes the asymptotic distribution of an extremum estimator when the true parameter lies on the boundary of the parameter space. The boundary may be linear, curved, and/or kinked. Typically the asymptotic distribution is a function of a multivariate normal distribution in models without stochastic trends and a function of a multivariate Brownian motion in models with stochastic trends. The results apply to a wide variety of estimators and models. Examples treated in the paper are: (i) quasi-ML estimation of a random coefficients regression model with some coefficient variances equal to zero and (ii) LS estimation of an augmented Dickey-Fuller regression with unit root and time trend parameters on the boundary of the parameter space.</p> </abstract>
<abstract> <p>I provide a systematic treatment of the asymptotic properties of weighted M-estimators under variable probability stratified sampling. The characterization of the sampling scheme and representation of the objective function allow for a straightforward analysis. Simple, consistent asymptotic variance matrix estimators are proposed for a large class of problems. When stratification is based on exogenous variables, I show that the unweighted M-estimator is more efficient than the weighted estimator under a generalized conditional information matrix equality. When population frequencies are known, a more efficient weighting is possible. I also show how the results carry over to multinomial sampling.</p> </abstract>
<abstract> <p>We study a model in which two carriers choose networks to connect cities and compete for customers. We show that if carriers compete aggressively (e.g., Bertrand-like behavior), one carrier operating a single hub-spoke network is an equilibrium outcome. Competing hub-spoke networks are not an equilibrium outcome, although duopoly equilibria in nonhub networks can exist. If carriers do not compete aggressively, an equilibrium with competing hub-spoke networks exists as long as the number of cities is not too small. We provide conditions under which all equilibria consist of hub-spoke networks.</p> </abstract>
<abstract> <p>We analyze under which conditions a given vector field can be disaggregated as a linear combination of gradients. This problem is typical of aggregation theory, as illustrated by the literature on the characterization of aggregate market demand and excess demand. We argue that exterior differential calculus provides very useful tools to address these problems. In particular, we show, using these techniques, that any analytic mapping in R&lt;sup&gt;n&lt;/sup&gt; satisfying Walras Law can be locally decomposed as the sum of n individual, utility-maximizing market demand functions. In addition, we show that the result holds for arbitrary (price-dependent) income distributions, and that the decomposition can be chosen such that it varies continuously with the mapping. Finally, when income distribution can be freely chosen, then decomposition requires only n/2 agents.</p> </abstract>
<abstract> <p>This paper attempts to identify, in a framework deliberately stripped of unnecessary technicalities, some of the basic reasons why adaptive learning may or may not lead to stability and convergence to self-fulfilling expectations in large socioeconomic systems where no agent, or collection of agents, can act to manipulate macroeconomic outcomes. It is shown that if agents are somewhat uncertain about the local stability of the system, and are accordingly ready to extrapolate a large range of regularities (trends) that may show up in past small deviations from equilibrium, including divergent ones, the learning dynamics is locally divergent. On the other hand, if agents are fairly sure of the local stability of the system, and extrapolate only convergent trends out of small past deviations from equilibrium, one may get local stability. This "uncertainty principle" does show up in a wide variety of contexts: smooth or discontinuous, finite or infinite memory learning rules, error learning, recursive least squares, Bayesian learning.</p> </abstract>
<abstract> <p>We provide existence proofs and characterization results for the multidimensional version of the multiproduct monopolist problem of Mussa and Rosen (1978). These results are also directly applicable to the multidimensional nonlinear pricing problems studied by Wilson (1993) and Armstrong (1996). We establish that bunching is robust in these multidimensional screening problems, even with very regular distributions of types. This comes from a strong conflict between participation constraints and second order incentive compatibility conditions. We consequently design a new technique, the sweeping procedure, for dealing with bunching in multidimensional contexts. This technique extends the ironing procedure of Mussa and Rosen (1978) to several dimensions. We illustrate it on two examples: we first solve a linear quadratic version of the bidimensional nonlinear pricing problem, where consumers' types are exponentially distributed. The solution involves pure bundling for consumers with low demands. The second example is the bidimensional version of the Mussa and Rosen problem when consumers' types are uniformly distributed on a square. The solution is such that the seller offers a full differentiation of products in the upper part of the qualities spectrum, but only limited choice for lower qualities. This seems to be a quite general pattern for multidimensional screening problems. The sweeping procedure is potentially applicable to other multidimensional screening problems.</p> </abstract>
<abstract> <p>The 1980's tax reforms and the changing dispersion of wages offer one of the best opportunities yet to estimate labor supply effects. Nevertheless, changing sample composition, aggregate shocks, the changing composition of the tax paying population, and discontinuities in the tax system create serious identification and estimation problems. We develop grouping estimators that address these issues. Our results reveal positive and moderately sized wage elasticities. We also find negative income effects for women with children.</p> </abstract>
<abstract> <p>The method of stimulated scores (MSS) is presented for estimating limited dependent variables models (LDV) with flexible correlation structure in the unobservables. We propose simulators that are continuous in the unknown parameter vectors, and hence standard optimization methods can be used to compute the MSS estimators that employ these simulators. The first continuous method relies on recursive conditioning of the multivariate normal density through a Cholesky tiangularization of its variance-covariance matrix. The second method combines results about conditionals of the multivariate normal distribution with Gibbs resampling techniques. We establish consistency and asymptotic normality of the MSS estimators and derive suitable rates at which the number of simulations must rise if biased simulators are used.</p> </abstract>
<abstract> <p>We examine the welfare properties of surplus maximization by embedding a perfectly discriminating monopoly in an otherwise standard Arrow-dealbatus economy. Although we discover an inefficient equilibrium, we validate partial equilibria are efficient provided that the monopoly goods are costly, and (ii) that a natural monopoly can typically use personalized two-part tariffs in these equilibria. However, we find that Pareto optima are sometimes incompatible with surplus maximization, even when transfer payments are used. We provide insight into the source of this difficulty and give some instructive examples of economies where a second welfare theorem holds.</p> </abstract>
<abstract> <p>We study a longitudinal sample of over one million French workers from more than five hundred thousand employing firms. We decompose real total annual compensation per worker into components related to observable employee characteristics, personal heterogeneity, firm heterogeneity, and residual variation. Except for the residual, all components may be correlated in an arbitrary fashion. At the level of the individual, we find that person effects, especially those not related to observables like education, are a very important source of wage variation in France. Firm effects, while important, are not as important as person effects. At the level of firms, we find that enterprises that hire high-wage workers are more productive but not more profitable. They are also more capital and high-skilled employee intensive. Enterprises that pay higher wages, controlling for person effects, are more productive and more profitable. They are also more capital intensive but are not more high-skilled labor intensive. We find that person effects explain about 90% of inter-industry wage differentials and about 75% of the firm-size wage effect while firm effects explain relatively little of either differential.</p> </abstract>
<abstract> <p>The neoclassical growth model focuses on factor accumulation as an engine of growth, while the neo-Schumpetarian growth model stresses innovation. This paper argues that these two views of growth may capture different phases of a single growth experience. In the model presented below, the balanced growth path is unstable and the economy achieves sustainable growth through cycles under an empirically plausible condition, perpetually moving back and forth between two phases. One phase is characterized by higher output growth, higher investment, no innovation, and a competitive market structure. The other phase is characterized by lower output growth, lower investment, high innovation, and a more monopolistic market structure. Both investment and innovation are essential in sustaining growth indefinitely, and yet they move in an asynchronized way; only one of them appears to play a dominant role in each phase. The economy grows faster along the cycles than along the (unstable) balanced growth path.</p> </abstract>
<abstract> <p>This paper extends the classic two-armed bandit problem to a many-agent setting in which N players each face the same experimentation problem. The main change from the single-agent problem is that an agent can now learn from the current experimentation of other agents. Information is therefore a public good, and a free-rider problem in experimentation naturally arises. More interestingly, the prospect of future experimentation by others encourages agents to increase current experimentation, in order to bring forward the time at which the extra information generated by such experimentation becomes available. The paper provides an analysis of the set of stationary Markov equilibria in terms of the free-rider effect and the encouragement effect.</p> </abstract>
<abstract> <p>We derive necessary and sufficient conditions for a pair of functions to be the optimal policy function and the optimal value function of a dynamic maximization problem with convex constraints and concave objective functional. It is shown that every Lipschitz continuous function can be the solution of such a problem. If the maintained assumptions include free disposal and monotonicity, then we obtain a complete characterization of all optimal policy and optimal value functions. This is the case, e.g., in the standard aggregative optimal growth model.</p> </abstract>
<abstract> <p>When players have identical time preferences, the set of feasible repeated game payoffs coincides with the convex hull of the underlying stage-game payoffs. Moreover, all feasible and individually rational payoffs can be sustained by equilibria if the players are sufficiently patient. Neither of these facts generalizes to the case of different time preferences. First, players can mutually benefit from trading payoffs across time. Hence, the set of feasible repeated game payoffs is typically larger than the convex hull of the underlying stage-game payoffs. Second, it is not usually the case that every trade plan that guarantees individually rational payoffs can be sustained by an equilibrium, no matter how patient the players are. This paper provides a simple characterization of the sets of Nash and of subgame perfect equilibrium payoffs in two-player repeated games.</p> </abstract>
<abstract> <p>This paper proposes a general approach and a computationally convenient estimation procedure for the structural analysis of auction data. Considering first-price sealed-bid auction models within the independent private value paradigm, we show that the underlying distribution of bidders' private values is identified from observed bids and the number of actual bidders without any parametric assumptions. Using the theory of minimax, we establish the best rate of uniform convergence at which the latent density of private values can be estimated nonparametrically from available data. We then propose a two-step kernel-based estimator that converges at the optimal rate.</p> </abstract>
<abstract> <p>Threshold models have a wide variety of applications in economics. Direct applications include models of separating and multiple equilibria. Other applications include empirical sample splitting when the sample split is based on a continuously-distributed variable such as firm size. In addition, threshold models may be used as a parsimonious strategy for nonparametric function estimation. For example, the threshold autoregressive model (TAR) is popular in the nonlinear time series literature. Threshold models also emerge as special cases of more complex statistical frameworks, such as mixture models, switching models, Markov switching models, and smooth transition threshold models. It may be important to understand the statistical properties of threshold models as a preliminary step in the development of statistical tools to handle these more complicated structures. Despite the large number of potential applications, the statistical theory of threshold estimation is undeveloped. It is known that threshold estimates are super-consistent, but a distribution theory useful for testing and inference has yet to be provided. This paper develops a statistical theory for threshold estimation in the regression context. We allow for either cross-section or time series observations. Least squares estimation of the regression parameters is considered. An asymptotic distribution theory for the regression estimates (the threshold and the regression slopes) is developed. It is found that the distribution of the threshold estimate is nonstandard. A method to construct asymptotic confidence intervals is developed by inverting the likelihood ratio statistic. It is shown that this yields asymptotically conservative confidence regions. Monte Carlo simulations are presented to assess the accuracy of the asymptotic approximations. The empirical relevance of the theory is illustrated through an application to the multiple equilibria growth model of Durlauf and Johnson (1995).</p> </abstract>
<abstract> <p>This paper examines the abilities of learning models to describe subject behavior in experiments. A new experiment involving multistage asymmetric-information games is conducted, and the experimental data are compared with the predictions of Nash equilibrium and two types of learning model: a reinforcement-based model similar to that used by Roth and Erev (1995), and belief-based models similar to the "cautious fictitious play" of Fudenberg and Levine (1995, 1998). These models make predictions that are qualitatively similar-cycling around the Nash equilibrium that is much more apparent than movement toward it. While subject behavior is not adequately described by Nash equilibrium, it is consistent with the qualitative predictions of the learning models. We examine several criteria for quantitatively comparing the predictions of alternative models. According to almost all of these criteria, both types of learning model outperform Nash equilibrium. According to some criteria, the reinforcement-based model performs better than any version of the belief-based model; according to others, there exist versions of the belief-based model that outperform the reinforcement-based model. The abilities of these models are further tested with respect to the results of other published experiments. The relative performance of the two learning models depends on the experiment, and varies according to which criterion of success is used. Again, both models perform better than equilibrium in most cases.</p> </abstract>
<abstract> <p>In a rationing problem, each agent demands a quantity of a certain commodity and the available resources fall short of total demand. A rationing method solves this problem at every level of resources and individual demands. We impose three axioms: Consistency-with respect to variations of the set of agents-Upper Composition and Lower Composition-with respect to variations of the available resources. In the model where the commodity comes in indivisible units, the three axioms characterize the family of priority rules, where individual demands are met lexicographically according to an exogeneous ordering of the agents. In the (more familiar) model where the commodity is divisible, these three axioms plus Scale Invariance-independence of the measurement unit-characterize a rich family of methods. It contains exactly three symmetric methods, giving equal shares to equal demands: these are the familiar proportional, uniform gains, and uniform losses methods. The asymmetric methods in the family partition the agents into priority classes; within each class, they use either the proportional method or a weighted version of the uniform gains or uniform losses methods.</p> </abstract>
<abstract> <p>The neoclassical theory of demand applies to individuals, yet in empirical work it is usually taken as valid for households with many members. This paper explores what the theory of individuals implies for households that have more than one member. We make minimal assumptions about how the individual members of the household resolve conflicts. All we assume is that however decisions are made, outcomes are efficient. We refer to this as the collective setting. We show that in the collective setting household demands must satisfy a symmetry and rank condition on the Slutsky matrix. We also present some further results on the effects on demands of variables that do not modify preferences but that do affect how decisions are made. We apply our theory to a series of surveys of household expenditures from Canada. The tests of the usual symmetry conditions are rejected for two-person households but not for one-person households. We also show that income pooling is rejected for two-person households. We then test for our collective setting conditions on the couples data. None of the collective setting restrictions are rejected. We conclude that the collective setting is a plausible and tractable next step to take in the analysis of household behavior.</p> </abstract>
<abstract> <p>This paper is about the economic theory of biodiversity preservation. A cost-effectiveness methodology is constructed, which results in a particular formula that can be used as a criterion to rank projects. The ranking criterion is sufficiently operational to be useful in suggesting what to look at when determining actual conservation priorities among endangered species. At the same time, the formula is firmly rooted in a mathematically rigorous optimization framework, so that its theoretical underpinnings are clear. The underlying model, called the "Noah's Ark Problem," is intended to be a kind of canonical form that hones down to its analytical essence the problem of best preserving diversity under a limited budget constraint.</p> </abstract>
<abstract> <p>Some new tools for analyzing spurious regressions are presented. The theory utilizes the general representation of a stochastic process in terms of an orthonormal system and provides an extension of the Weierstrass theorem to include the approximation of continuous functions and stochastic processes by Wiener processes. The theory is applied to two classic examples of spurious regressions: regression of stochastic trends on time polynomials, and regressions among independent random walks. It is shown that such regressions reproduce in part and in whole the underlying orthonormal representations.</p> </abstract>
<abstract> <p> The least-absolute-deviations (LAD) estimator for a median-regression model does not satisfy the standard conditions for obtaining asymptotic refinements through use of the bootstrap because the LAD objective function is not smooth. This paper overcomes this problem by smoothing the objective function. The smoothed estimator is asymptotically equivalent to the standard LAD estimator. With bootstrap critical values, the rejection probabilities of symmetrical t and χ &lt;sup&gt;2&lt;/sup&gt; tests based on the smoothed estimator are correct through &lt;tex-math&gt;$O(n^{-\gamma})$&lt;/tex-math&gt; under the null hypothesis, where γ &lt; 1 but can be arbitrarily close to 1. In contrast, first-order asymptotic approximations make errors of size &lt;tex-math&gt;$O(n^{-\gamma})$&lt;/tex-math&gt;. These results also hold for symmetrical t and χ &lt;sup&gt;2&lt;/sup&gt; tests for censored median regression models. </p> </abstract>
<abstract> <p>We examine a simple bargaining setting, where heterogeneous buyers and sellers are repeatedly matched with each other. We begin by characterizing efficiency in such a dynamic setting, and discuss how it differs from efficiency in a centralized static setting. We then study the allocations which can result in equilibrium when the matched buyers and sellers bargain through some extensive game form. We take an implementation approach, characterizing the possible allocation rules which result as the extensive game form is varied. We are particularly concerned with the impact of making trade voluntary: imposing individual rationality on and off the equilibrium path. No buyer or seller consumates an agreement which leaves them worse off than the discounted expected value of their future rematching in the market. Finally, we compare and contrast the efficient allocations with those that could ever arise as the equilibria of some voluntary negotiation procedure.</p> </abstract>
<abstract> <p>We consider the problem of making asymptotically valid inference on structural parameters in instrumental variables regression with weak instruments. Using the local-to-zero asymptotics of Staiger and Stock (1997), we derive the asymptotic distributions of LR and LM type statistics for testing simple hypotheses on structural parameters based on maximum likelihood and generalized method of moments estimation methods. In contrast to the nonstandard limiting behavior of Wald statistics, the limiting distributions of certain LM and LR statistics are bounded by a chi-square distribution with degrees of freedom given by the number of instruments. Further, we show how to construct asymptotically valid confidence sets for structural parameters by inverting these statistics.</p> </abstract>
<abstract> <p>Semiparametric methods are developed to estimate the bias that arises from using nonexperimental comparison groups to evaluate social programs and to test the identifying assumptions that justify matching, selection models, and the method of difference-in-differences. Using data from an experiment on a prototypical social program and data from nonexperimental comparison groups, we reject the assumptions justifying matching and our extensions of it. The evidence supports the selection bias model and the assumptions that justify a semiparametric version of the method of difference-in-differences. We extend our analysis to consider applications of the methods to ordinary observational data.</p> </abstract>
<abstract> <p> Causality in the sense of Granger is typically defined in terms of predictibility of a vector of variables one period ahead. Recently, Lütkepohl (1993) proposed to define noncausality between two variables in terms of nonpredictibility at any number of periods ahead. When more than two vectors are considered (i.e., when the information set contains auxiliary variables), these two notions are not equivalent. In this paper, we first generalize the notion of causality by considering causality at a given (arbitrary) horizon h. Then we derive necessary and sufficient conditions for noncausality between vectors of variables (inside a larger vector) up to any given horizon h, where h can be infinite. In particular, for general possibly nonstationary processes with finite second moments, relatively simple exhaustivity and separation conditions, which are sufficient for noncausality at all horizons, are provided. To deal with cases where such conditions do not apply, we consider a more specific, although still very wide, class of vector autoregressive processes (possibly of infinite order, stationary or nonstationary), which include multivariate ARIMA processes, and we derive general parametric characterizations of noncausality at various horizons for this class (including a causality chain characterization). We also observe that the coefficients of lagged variables in forecasts at various horizons h ≥ 1 can be interpreted as generalized impulse response coefficients which yield a complete picture of linear causality properties, in contrast with usual response coefficients which can be quite misleading in this respect. </p> </abstract>
<abstract> <p>This paper proposes a new statistical model for the analysis of data which arrive at irregular intervals. The model treats the time between events as a stochastic process and proposes a new class of point processes with dependent arrival rates. The conditional intensity is developed and compared with other self-exciting processes. Because the model focuses on the expected duration between events, it is called the autoregressive conditional duration (ACD) model. Asymptotic properties of the quasi maximum likelihood estimator are developed as a corollary to ARCH model results. Strong evidence is provided for duration clustering for the financial transaction data analyzed; both deterministic time-of-day effects and stochastic effects are important. The model is applied to the arrival times of trades and therefore is a model of transaction volume, and also to the arrival of other events such as price changes. Models for the volatility of prices are estimated with price-based durations, and examined from a market microstructure point of view.</p> </abstract>
<abstract> <p> In a number of econometric models, rules of large-sample inference require a consistent estimate of f(0), where f(λ) is the spectral density matrix of &lt;tex-math&gt;$y_{t}=u_{t}\otimes x_{t}$&lt;/tex-math&gt;, for covariance stationary vectors &lt;tex-math&gt;$u_{t},x_{t}$&lt;/tex-math&gt;. Typically y&lt;sub&gt;t&lt;/sub&gt; is allowed to have nonparametric autocorrelation, and smoothing is used in the estimation of f(0). We give conditions under which f(0) can be consistently estimated without smoothing. The conditions are relevant to inference on slope parameters in models with an intercept and strictly exogenous regressors, and allow regressors and disturbances to collectively have considerable stationary long memory and to satisfy only mild, in some cases minimal, moment conditions. The estimate of f(0) dominates smoothed ones in the sense that it can have mean squared error of order n&lt;sup&gt;-1&lt;/sup&gt;, where n is sample size. Under standard additional regularity conditions, we extend the estimate of f(0) to studentize asymptotically normal estimates of structural parameters in linear simultaneous equations systems. A small Monte Carlo study of finite sample behavior is included. </p> </abstract>
<abstract> <p>In structural empirical models of labor market search, the distribution of wage offers is usually assumed to be exogenous. However, because in setting their wages profit-maximizing firms should consider the reservation wages of job seekers, the wage offer distribution is essentially endogenous. We investigate whether a proposed equilibrium search model, in which the wage offer distribution is endogenous, is able to describe observed labor market histories. We find that the distributions of job and unemployment spells are consistent with the data, and that the qualitative predictions of the model for the wages set by employers are confirmed by wage regressions. The model is estimated using panel data on unemployed and employed individuals. We distinguish between separate segments of the labor market, and we show that productivity heterogeneity is important to obtain an acceptable fit to the data. The results are used to estimate the degree of monopsony power of firms. Further, the effects of changes in the mandatory minimum wage are examined.</p> </abstract>
<abstract> <p>Blume and Easley (1992) show that if agents' have the same savings rule, those who maximize the expected logarithm of next period's outcomes will eventually hold all wealth (i.e. are "most prosperous"). However, if no agent adopts this rule then the most prosperous are not necessarily those who make the most accurate predictions. Thus, agents who make inaccurate predictions need not be driven out of the market. In this paper, it is shown that, among agents who have the same intertemporal discount factor (and who choose savings endogenously), the most prosperous are those who make accurate predictions. Hence, convergence to rational expectations obtains because agents who make inaccurate predictions are driven out of the market.</p> </abstract>
<abstract> <p>In the setting of "affine" jump-diffusion state processes, this paper provides an analytical treatment of a class of transforms, including various Laplace and Fourier transforms as special cases, that allow an analytical treatment of a range of valuation and econometric problems. Example applications include fixed-income pricing models, with a role for intensity-based models of default, as well as a wide range of option-pricing applications. An illustrative example examines the implications of stochastic volatility and jumps for option valuation. This example highlights the impact on option 'smirks' of the joint distribution of jumps in volatility and jumps in the underlying asset price, through both jump amplitude as well as jump timing.</p> </abstract>
<abstract> <p>This paper is concerned with asymptotic properties on the accuracy of numerical solutions. It is shown that the approximation error of the policy function is of the same order of magnitude as the size of the Euler equation residuals. Moreover, for bounding this approximation error the most relevant parameters are the discount factor and the curvature of the return function. These findings provide theoretical foundations for the construction of tests to assess the performance of alternative computational methods.</p> </abstract>
<abstract> <p>We give a characterization of the set of group-strategyproof, Pareto-optimal, and reallocation-proof allocation rules for the assignment problem, where individuals are assigned at most one indivisible object, without any medium of exchange. Although there are no property rights in the model, the rules satisfying the above criteria imitate a trading procedure with individual endowments, in which individuals exchange objects from their hierarchically determined endowment sets in an iterative manner. In particular, these assignment rules generalize Gale's top trading cycle procedure, the classical rule for the model in which each individual owns an indivisible good.</p> </abstract>
<abstract> <p>We derive the asymptotic sampling distribution of various estimators frequently used to order distributions in terms of poverty, welfare, and inequality. This includes estimators of most of the poverty indices currently in use, as well as estimators of the curves used to infer stochastic dominance of any order. These curves can be used to determine whether poverty, inequality, or social welfare is greater in one distribution than in another for general classes of indices and for ranges of possible poverty lines. We also derive the sampling distribution of the maximal poverty lines up to which we may confidently assert that poverty is greater in one distribution than in another. The sampling distribution of convenient dual estimators for the measurement of poverty is also established. The statistical results are established for deterministic or stochastic poverty lines as well as for paired or independent samples of incomes. Our results are briefly illustrated using data for four countries drawn from the Luxembourg Income Study data bases.</p> </abstract>
<abstract> <p>This paper introduces a nonparametric Granger-causality test for covariance stationary linear processes under, possibly, the presence of long-range dependence. We show that the test is consistent and has power against contiguous alternatives converging to the parametric rate T&lt;sup&gt;-1/2&lt;/sup&gt;. Since the test is based on estimates of the parameters of the representation of a VAR model as a, possibly, two-sided infinite distributed lag model, we first show that a modification of Hannan's (1963, 1967) estimator is root-T consistent and asymptotically normal for the coefficients of such a representation. When the data are long-range dependent, this method of estimation becomes more attractive than least squares, since the latter can be neither root-T consistent nor asymptotically normal as is the case with short-range dependent data.</p> </abstract>
<abstract> <p> We investigate the effect of introducing costs of complexity in the n-person unanimity bargaining game. As is well-known, in this game every individually rational allocation is sustainable as a Nash equilibrium (also as a subgame perfect equilibrium if players are sufficiently patient and if n &gt; 2). Moreover, delays in agreement are also possible in such equilibria. By limiting ourselves to a plausible notion of complexity that captures length of memory, we find that the introduction of complexity costs (lexicographically with the standard payoffs) does not reduce the range of possible allocations but does limit the amount of delay that can occur in any agreement. In particular, we show that in any n-player game, for any allocation z, an agreement on z at any period t can be sustained as a Nash equilibrium of the game with complexity costs if and only if t ≤ n. We use the limit on delay result to establish that, in equilibrium, the strategies implement stationary behavior. Finally, we also show that "noisy Nash equilibrium" with complexity costs sustains only the unique stationary subgame perfect equilibrium allocation. </p> </abstract>
<abstract> <p>This paper presents new identification results for models of first-price, second-price, ascending (English), and descending (Dutch) auctions. We consider a general specification of the latent demand and information structure, nesting both private values and common values models, and allowing correlated types as well as ex ante asymmetry. We address identification of a series of nested models and derive testable restrictions enabling discrimination between models on the basis of observed data. The simplest model-symmetric independent private values-is nonparametrically identified even if only the transaction price from each auction is observed. For richer models, identification and testable restrictions may be obtained when additional information of one or more of the following types is available: (i) the identity of the winning bidder or other bidders; (ii) one or more bids in addition to the transaction price; (iii) exogenous variation in the number of bidders; (iv) bidder-specific covariates. While many private values (PV) models are nonparametrically identified and testable with commonly available data, identification of common values (CV) models requires stringent assumptions. Nonetheless, the PV model can be tested against the CV alternative, even when neither model is identified.</p> </abstract>
<abstract> <p>Reinforcement learning and stochastic fictitious play are apparent rivals as models of human learning. They embody quite different assumptions about the processing of information and optimization. This paper compares their properties and finds that they are far more similar than were thought. In particular, the expected motion of stochastic fictitious play and reinforcement learning with experimentation can both be written as a perturbed form of the evolutionary replicator dynamics. Therefore they will in many cases have the same asymptotic behavior. In particular, local stability of mixed equilibria under stochastic fictitious play implies local stability under perturbed reinforcement learning. The main identifiable difference between the two models is speed: stochastic fictitious play gives rise to faster learning.</p> </abstract>
<abstract> <p>The goal of this paper is to probe the validity of the fiscal theory of the price level by modelling explicitly the market structure in which households and the government make their decisions. I describe the economy as a game, and I am thus able to state precisely the consequences of actions that are out of the equilibrium path. I show that there exist government strategies that lead to a version of the fiscal theory, in which the price level is determined by fiscal variables alone. These strategies are however more complex than the simple budgetary rules usually associated with the fiscal theory, and the government budget constraint cannot be merely viewed as an equilibrium condition.</p> </abstract>
<abstract> <p>This paper investigates the design of seller-optimal auctions when winning bidders can attempt to resell the good. In that case, the optimal allocation characterized by Myerson (1981) cannot be achieved without resale. I find a sufficient and necessary condition for sincere bidding given the possibility of resale. In two-bidder cases, I prove that the Myerson allocation can be achieved under standard conditions supplemented with two assumptions. With three or more bidders, achieving the Myerson allocation is more difficult. I prove that it can be implemented in special cases. In those cases, the Myerson allocation is generated through a sequence of resale auctions, each optimally chosen by a reseller.</p> </abstract>
<abstract> <p>This paper studies the relation between discrete-time and continuous-time principal-agent models. We derive the continuous-time model as a limit of discrete-time models with ever shorter periods and show that optimal incentive schemes in the discrete-time models approximate the optimal incentive scheme in the continuous model, which is linear in accounts. Under the additional assumption that the principal observes only cumulative total profits at the end and the agent can destroy profits unnoticed, an incentive scheme that is linear in total profits is shown to be approximately optimal in the discrete-time model when the length of the period is small.</p> </abstract>
<abstract> <p>We establish global convergence results for stochastic fictitious play for four classes of games: games with an interior ESS, zero sum games, potential games, and supermodular games. We do so by appealing to techniques from stochastic approximation theory, which relate the limit behavior of a stochastic process to the limit behavior of a differential equation defined by the expected motion of the process. The key result in our analysis of supermodular games is that the relevant differential equation defines a strongly monotone dynamical system. Our analyses of the other cases combine Lyapunov function arguments with a discrete choice theory result: that the choice probabilities generated by any additive random utility model can be derived from a deterministic model based on payoff perturbations that depend nonlinearly on the vector of choice probabilities.</p> </abstract>
<abstract> <p>We construct and estimate an equilibrium search model with on-the-job-search. Firms make take-it-or-leave-it wage offers to workers conditional on their characteristics and they can respond to the outside job offers received by their employees. Unobserved worker productive heterogeneity is introduced in the form of cross-worker differences in a "competence" parameter. On the other side of the market, firms also are heterogeneous with respect to their marginal productivity of labor. The model delivers a theory of steady-state wage dispersion driven by heterogenous worker abilities and firm productivities, as well as by matching frictions. The structural model is estimated using matched employer and employee French panel data. The exogenous distributions of worker and firm heterogeneity components are nonparametrically estimated. We use this structural estimation to provide a decomposition of cross-employee wage variance. We find that the share of the cross-sectional wage variance that is explained by person effects varies across skill groups. Specifically, this share lies close to 40% for high-skilled white collars, and quickly decreases to 0% as the observed skill level decreases. The contribution of market imperfections to wage dispersion is typically around 50%.</p> </abstract>
<abstract> <p>An unresolved problem in Bayesian decision theory is how to value and price information. This paper resolves both problems assuming inexpensive information. Building on Large Deviation Theory, we produce a generically complete asymptotic order on samples of i.i.d. signals in finite-state, finite-action models. Computing the marginal value of an additional signal, we find it is eventually exponentially falling in quantity, and higher for lower quality signals. We provide a precise formula for the information demand, valid at low prices: asymptotically a constant times the log price, and falling in the signal quality for a given price.</p> </abstract>
<abstract> <p>Self-selected migration presents one potential explanation for why observed returns to a college education in local labor markets vary widely even though U.S. workers are highly mobile. To assess the impact of self-selection on estimated returns, this paper first develops a Roy model of mobility and earnings where workers choose in which of the 50 states (plus the District of Columbia) to live and work. Available estimation methods are either infeasible for a selection model with so many alternatives or place potentially severe restrictions on earnings and the selection process. This paper develops an alternative econometric methodology that combines Lee's (1983) parametric maximum order statistic approach to reduce the dimensionality of the error terms with more recent work on semiparametric estimation of selection models (e.g., Ahn and Powell (1993)). The resulting semiparametric correction is easy to implement and can be adapted to a variety of other polychotomous choice problems. The empirical work, which uses 1990 U.S. Census data, confirms the role of comparative advantage in mobility decisions. The results suggest that self-selection of higher educated individuals to states with higher returns to education generally leads to upward biases in OLS estimates of the returns to education in state-specific labor markets. While the estimated returns to a college education are significantly biased, correcting for the bias does not narrow the range of returns across states. Consistent with the finding that the corrected return to a college education differs across the U.S., the relative state-to-state migration flows of college- versus high school-educated individuals respond strongly to differences in the return to education and amenities across states.</p> </abstract>
<abstract> <p>We examine a general equilibrium model with asymmetrically informed agents. The presence of asymmetric information generally presents a conflict between incentive compatibility and Pareto efficiency. We present a notion of informational size and show that the conflict between incentive compatibility and efficiency can be made arbitrarily small if agents are of sufficiently small informational size.</p> </abstract>
<abstract> <p>Choice models with nonlinear budget sets provide a precise way of accounting for the nonlinear tax structures present in many applications. In this paper we propose a nonparametric approach to estimation of these models. The basic idea is to think of the choice, in our case hours of labor supply, as being a function of the entire budget set. Then we can do nonparametric regression where the variable in the regression is the budget set. We reduce the dimensionality of this problem by exploiting structure implied by utility maximization with piecewise linear convex budget sets. This structure leads to estimators where the number of segments can differ across observations and does not affect accuracy. We give consistency and asymptotic normality results for these estimators. The usefulness of the estimator is demonstrated in an empirical example, where we find it has a large impact on estimated effects of the Swedish tax reform.</p> </abstract>
<abstract> <p>Liberalization of infrastructure industries presents classic economic issues about how organization and procedure affect market performance. These issues are examined in wholesale power markets. The perspective from game theory complements standard economic theory to examine effects on efficiency and incentives.</p> </abstract>
<abstract> <p>Economists have lately been called upon not only to analyze markets, but to design them. Market design involves a responsibility for detail, a need to deal with all of a market's complications, not just its principle features. Designers therefore cannot work only with the simple conceptual models used for theoretical insights into the general working of markets. Instead, market design calls for an engineering approach. Drawing primarily on the design of the entry level labor market for American doctors (the National Resident Matching Program), and of the auctions of radio spectrum conducted by the Federal Communications Commission, this paper makes the case that experimental and computational economics are natural complements to game theory in the work of design. The paper also argues that some of the challenges facing both markets involve dealing with related kinds of complementarities, and that this suggests an agenda for future theoretical research.</p> </abstract>
<abstract> <p>In previous work on cheap talk, uncertainty has almost always been modeled using a single-dimensional state variable. In this paper we prove that the dimensionality of the uncertain variable has an important qualitative impact on results and yields interesting insights into the "mechanics" of information transmission. Contrary to the unidimensional case, if there is more than one sender, full revelation of information in all states of nature is generically possible, even when the conflict of interest is arbitrarily large. What really matters in transmission of information is the local behavior of senders' indifference curves at the ideal point of the receiver, not the proximity of players' ideal point.</p> </abstract>
<abstract> <p>Models of utility in stochastic continuous-time settings typically assume that beliefs are represented by a probability measure, hence ruling out a priori any concern with ambiguity. This paper formulates a continuous-time intertemporal version of multiple-priors utility, where aversion to ambiguity is admissible. In a representative agent asset market setting, the model delivers restrictions on excess returns that admit interpretations reflecting a premium for risk and a separate premium for ambiguity.</p> </abstract>
<abstract> <p>We prove the existence of a symmetric equilibrium in a circular city in which businesses and housing can both be located anywhere in the city. In this equilibrium, firms balance the external benefits from locating near other producers against the costs of longer commutes for workers. An equilibrium city need not take the form of a central business district surrounded by a residential area. We propose a general algorithm for constructing equilibria, and use it to study the way land use is affected by changes in the model's underlying parameters.</p> </abstract>
<abstract> <p>The presence of obstinate types in bargaining has been shown to alter dramatically the bargaining equilibrium strategies and outcomes. This paper shows that outside options may cancel out the effect of obstinacy in bargaining. When parties have access to stationary outside options, we show that when opting out is preferable to accepting the inflexible demand of the other party, there is a unique Perfect Bayesian Equilibrium in which each party reveals himself as rational as soon as possible. A similar conclusion holds when outside options may only be available at a later date or when only one party has access to an outside option.</p> </abstract>
<abstract> <p> This paper proposes a new nested algorithm (NPL) for the estimation of a class of discrete Markov decision models and studies its statistical and computational properties. Our method is based on a representation of the solution of the dynamic programming problem in the space of conditional choice probabilities. When the NPL algorithm is initialized with consistent nonparametric estimates of conditional choice probabilities, successive iterations return a sequence of estimators of the structural parameters which we call K-stage policy iteration estimators. We show that the sequence includes as extreme cases a Hotz-Miller estimator (for K = 1) and Rust's nested fixed point estimator (in the limit when K → ∞). Furthermore, the asymptotic distribution of all the estimators in the sequence is the same and equal to that of the maximum likelihood estimator. We illustrate the performance of our method with several examples based on Rust's bus replacement model. Monte Carlo experiments reveal a trade-off between finite sample precision and computational cost in the sequence of policy iteration estimators. </p> </abstract>
<abstract> <p>We show that it is possible to adapt to nonparametric disturbance autocorrelation in time series regression in the presence of long memory in both regressors and disturbances by using a smoothed nonparametric spectrum estimate in frequency-domain generalized least squares. When the collective memory in regressors and disturbances is sufficiently strong, ordinary least squares is not only asymptotically inefficient but asymptotically non-normal and has a slow rate of convergence, whereas generalized least squares is asymptotically normal and Gauss-Markov efficient with standard convergence rate. Despite the anomalous behavior of nonparametric spectrum estimates near a spectral pole, we are able to justify a standard construction of frequency-domain generalized least squares, earlier considered in case of short memory disturbances. A small Monte Carlo study of finite sample performance is included.</p> </abstract>
<abstract> <p> Tests based on the quantile regression process can be formulated like the classical Kolmogorov-Smirnov and Cramér-von-Mises tests of goodness-of-fit employing the theory of Bessel processes as in Kiefer (1959). However, it is frequently desirable to formulate hypotheses involving unknown nuisance parameters, thereby jeopardizing the distribution free character of these tests. We characterize this situation as "the Durbin problem" since it was posed in Durbin (1973), for parametric empirical processes. In this paper we consider an approach to the Durbin problem involving a martingale transformation of the parametric empirical process suggested by Khmaladze (1981) and show that it can be adapted to a wide variety of inference problems involving the quantile regression process. In particular, we suggest new tests of the location shift and location-scale shift models that underlie much of classical econometric inference. The methods are illustrated with a reanalysis of data on unemployment durations from the Pennsylvania Reemployment Bonus Experiments. The Pennsylvania experiments, conducted in 1988-89, were designed to test the efficacy of cash bonuses paid for early reemployment in shortening the duration of insured unemployment spells. </p> </abstract>
<abstract> <p>Without introducing either debt constraints or transversality conditions to avoid the possibility of Ponzi schemes, we show the existence of equilibrium in an infinite horizon incomplete markets economy with a collateral structure.</p> </abstract>
<abstract> <p>This paper offers a new approach to the study of economic problems usually modeled as games of incomplete information with discontinuous payoffs. Typically, the discontinuities arise from indeterminacies (ties) in the underlying problem. The point of view taken here is that the tie-breaking rules that resolve these indeterminacies should be viewed as part of the solution rather than part of the description of the model. A solution is therefore a tie-breaking rule together with strategies satisfying the usual best-response criterion. When information is incomplete, solutions need not exist; that is, there may be no tie-breaking rule that is compatible with the existence of strategy profiles satisfying the usual bestresponse criteria. It is shown that the introduction of incentive compatible communication (cheap talk) restores existence.</p> </abstract>
<abstract> <p>We develop a Ricardian trade model that incorporates realistic geographic features into general equilibrium. It delivers simple structural equations for bilateral trade with parameters relating to absolute advantage, to comparative advantage (promoting trade), and to geographic barriers (resisting it). We estimate the parameters with data on bilateral trade in manufactures, prices, and geography from 19 OECD countries in 1990. We use the model to explore various issues such as the gains from trade, the role of trade in spreading the benefits of new technology, and the effects of tariff reduction.</p> </abstract>
<abstract> <p>We propose a novel statistic for conducting joint tests on all the structural parameters in instrumental variables regression. The statistic is straightforward to compute and equals a quadratic form of the score of the concentrated log-likelihood. It therefore attains its minimal value equal to zero at the maximum likelihood estimator. The statistic has a χ&lt;sup&gt;2&lt;/sup&gt; limiting distribution with a degrees of freedom parameter equal to the number of structural parameters. The limiting distribution does not depend on nuisance parameters. The statistic overcomes the deficiencies of the Anderson-Rubin statistic, whose limiting distribution has a degrees of freedom parameter equal to the number of instruments, and the likelihood based, Wald, likelihood ratio, and Lagrange multiplier statistics, whose limiting distributions depend on nuisance parameters. Size and power comparisons reveal that the statistic is a (asymptotic) size-corrected likelihood ratio statistic. We apply the statistic to the Angrist-Krueger (1991) data and find similar results as in Staiger and Stock (1997).</p> </abstract>
<abstract> <p>This paper argues that incompleteness of intertemporal financial markets has little effect (on welfare, prices, or consumption) in an economy with a single consumption good, provided that traders are long-lived and patient, a riskless bond is traded, shocks are transitory, and there is no aggregate risk. In an economy with aggregate risk, a similar conclusion holds, provided traders share the same CRRA utility function and the right assets are traded. Examples demonstrate that these conclusions need not hold if the wrong assets are traded or if the economy has multiple consumption goods.</p> </abstract>
<abstract> <p>Strategic behavior in a finite market can cause inefficiency in the allocation, and market mechanisms differ in how successfully they limit this inefficiency. A method for ranking algorithms in computer science is adapted here to rank market mechanisms according to how quickly inefficiency diminishes as the size of the market increases. It is shown that trade at a single market-clearing price in the k-double auction is worst-case asymptotic optimal among all plausible mechanisms: evaluating mechanisms in their least favorable trading environments for each possible size of the market, the k-double auction is shown to force the worst-case inefficiency to zero at the fastest possible rate.</p> </abstract>
<abstract> <p>In a differential information economy with quasi-linear utilities, monetary transfers facilitate the fulfillment of incentive compatibility constraints: the associated ex ante core is generically nonempty. However, we exhibit a well-behaved exchange economy in which this core is empty, even if goods are allocated through random mechanisms.</p> </abstract>
<abstract> <p>The main contribution of this paper is the development and application of cryptographic techniques to the design of strategic communication mechanisms. One of the main assumptions in cryptography is the limitation of the computational power available to agents. We introduce the concept of limited computational complexity, and by borrowing results from cryptography, we construct a communication protocol to establish that every correlated equilibrium of a two-person game with rational payoffs can be achieved by means of computationally restricted unmediated communication. This result provides an example in game theory where limitations of computational abilities of players are helpful in solving implementation problems. More specifically, it is possible to construct mechanisms with the property that profitable deviations are too complicated to compute.</p> </abstract>
<abstract> <p>With the cointegration formulation of economic long-run relations the test for cointegrating rank has become a useful econometric tool. The limit distribution of the test is often a poor approximation to the finite sample distribution and it is therefore relevant to derive an approximation to the expectation of the likelihood ratio test for cointegration in the vector autoregressive model in order to improve the finite sample properties. The correction factor depends on moments of functions of the random walk, which are tabulated by simulation, and functions of the parameters, which are estimated. From this approximation we propose a correction factor with the purpose of improving the small sample performance of the test. The correction is found explicitly in a number of simple models and its usefulness is illustrated by some simulation experiments.</p> </abstract>
<abstract> <p>This paper presents a new test for fractionally integrated (FI) processes. In particular, we propose a testing procedure in the time domain that extends the well-known Dickey-Fuller approach, originally designed for the I(1) versus I(0) case, to the more general setup of FI(d&lt;sub&gt;0&lt;/sub&gt;) versus FI(d&lt;sub&gt;1&lt;/sub&gt;), with &lt;latex&gt;$d_1 &lt; d_0$&lt;/latex&gt;. When d&lt;sub&gt;0&lt;/sub&gt; = 1, the proposed test statistics are based on the OLS estimator, or its t-ratio, of the coefficient on Δ&lt;sup&gt;d&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;y&lt;sub&gt;t-1&lt;/sub&gt; in a regression of Δ y&lt;sub&gt;t&lt;/sub&gt; on Δ&lt;sup&gt;d&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;y&lt;sub&gt;t-1&lt;/sub&gt; and, possibly, some lags of Δ y&lt;sub&gt;t&lt;/sub&gt;. When d&lt;sub&gt;1&lt;/sub&gt; is not taken to be known a priori, a pre-estimation of d&lt;sub&gt;1&lt;/sub&gt; is needed to implement the test. We show that the choice of any T&lt;sup&gt;1/2&lt;/sup&gt;-consistent estimator of d&lt;sub&gt;1&lt;/sub&gt; ∈ [0,1) suffices to make the test feasible, while achieving asymptotic normality. Monte-Carlo simulations support the analytical results derived in the paper and show that proposed tests fare very well, both in terms of power and size, when compared with others available in the literature. The paper ends with two empirical applications.</p> </abstract>
<abstract> <p>Iterated elimination of strictly dominated strategies is an order dependent procedure. It can also generate spurious Nash equilibria, fail to converge in countable steps, or converge to empty strategy sets. If best replies are well-defined, then spurious Nash equilibria cannot appear; if strategy spaces are compact and payoff functions are uppersemicontinuous in own strategies, then order does not matter; if strategy sets are compact and payoff functions are continuous in all strategies, then a unique and nonempty maximal reduction exists. These positive results extend neither to the better-reply secure games for which Reny has established the existence of a Nash equilibrium, nor to games in which (under iterated eliminations) any dominated strategy has an undominated dominator.</p> </abstract>
<abstract> <p>A predictor is asked to rank eventualities according to their plausibility, based on past cases. We assume that she can form a ranking given any memory that consists of finitely many past cases. Mild consistency requirements on these rankings imply that they have a numerical representation via a matrix assigning numbers to eventuality-case pairs, as follows. Given a memory, each eventuality is ranked according to the sum of the numbers in its row, over cases in memory. The number attached to an eventuality-case pair can be interpreted as the degree of support that the past case lends to the plausibility of the eventuality. Special instances of this result may be viewed as axiomatizing kernel methods for estimation of densities and for classification problems. Interpreting the same result for rankings of theories or hypotheses, rather than of specific eventualities, it is shown that one may ascribe to the predictor subjective conditional probabilities of cases given theories, such that her rankings of theories agree with rankings by the likelihood functions.</p> </abstract>
<abstract> <p>In this paper we estimate a bargaining model of government formation in parliamentary democracies. We use the estimated structural model to conduct constitutional experiments aimed at evaluating the impact of institutional features of the political environment on the duration of the government formation process, the type of coalitions that form, and their relative stability.</p> </abstract>
<abstract> <p>Methods are proposed for testing stochastic dominance of any pre-specified order, with primary interest in the distributions of income. We consider consistent tests, that are similar to Kolmogorov-Smirnov tests, of the complete set of restrictions that relate to the various forms of stochastic dominance. For such tests, in the case of tests for stochastic dominance beyond first order, we propose and justify a variety of approaches to inference based on simulation and the bootstrap. We compare these approaches to one another and to alternative approaches based on multiple comparisons in the context of a Monte Carlo experiment and an empirical example.</p> </abstract>
<abstract> <p>Public information in financial markets often arrives through the disclosures of interested parties who have a material interest in the reactions of the market to the new information. When the strategic interaction between the sender and the receiver is formalized as a disclosure game with verifiable reports, equilibrium prices can be given a simple characterization in terms of the concatenation of binomial pricing trees. There are a number of empirical implications. The theory predicts that the return variance following a poor disclosed outcome is higher than it would have been if the disclosed outcome were good. Also, when investors are risk averse, this leads to negative serial correlation of asset returns. Other points of contact with the empirical literature are discussed.</p> </abstract>
<abstract> <p>This paper develops an inferential theory for factor models of large dimensions. The principal components estimator is considered because it is easy to compute and is asymptotically equivalent to the maximum likelihood estimator (if normality is assumed). We derive the rate of convergence and the limiting distributions of the estimated factors, factor loadings, and common components. The theory is developed within the framework of large cross sections (N) and a large time dimension (T), to which classical factor analysis does not apply. We show that the estimated common components are asymptotically normal with a convergence rate equal to the minimum of the square roots of N and T. The estimated factors and their loadings are generally normal, although not always so. The convergence rate of the estimated factors and factor loadings can be faster than that of the estimated common components. These results are obtained under general conditions that allow for correlations and heteroskedasticities in both dimensions. Stronger results are obtained when the idiosyncratic errors are serially uncorrelated and homoskedastic. A necessary and sufficient condition for consistency is derived for large N but fixed T.</p> </abstract>
<abstract> <p>We present a model in which an asset bubble can persist despite the presence of rational arbitrageurs. The resilience of the bubble stems from the inability of arbitrageurs to temporarily coordinate their selling strategies. This synchronization problem together with the individual incentive to time the market results in the persistence of bubbles over a substantial period. Since the derived trading equilibrium is unique, our model rationalizes the existence of bubbles in a strong sense. The model also provides a natural setting in which news events, by enabling synchronization, can have a disproportionate impact relative to their intrinsic informational content.</p> </abstract>
<abstract> <p>This paper applies revealed preference theory to the nonparametric statistical analysis of consumer demand. Knowledge of expansion paths is shown to improve the power of nonparametric tests of revealed preference. The tightest bounds on indifference surfaces and welfare measures are derived using an algorithm for which revealed preference conditions are shown to guarantee convergence. Nonparametric Engel curves are used to estimate expansion paths and provide a stochastic structure within which to examine the consistency of household level data and revealed preference theory. An application is made to a long time series of repeated cross-sections from the Family Expenditure Survey for Britain. The consistency of these data with revealed preference theory is examined. For periods of consistency with revealed preference, tight bounds are placed on true cost of living indices.</p> </abstract>
<abstract> <p>We propose a functional estimation procedure for homogeneous stochastic differential equations based on a discrete sample of observations and with minimal requirements on the data generating process. We show how to identify the drift and diffusion function in situations where one or the other function is considered a nuisance parameter. The asymptotic behavior of the estimators is examined as the observation frequency increases and as the time span lengthens. We prove almost sure consistency and weak convergence to mixtures of normal laws, where the mixing variates depend on the chronological local time of the underlying diffusion process, that is the random time spent by the process in the vicinity of a generic spatial point. The estimation method and asymptotic results apply to both stationary and nonstationary recurrent processes.</p> </abstract>
<abstract> <p>ARCH and GARCH models directly address the dependency of conditional second moments, and have proved particularly valuable in modelling processes where a relatively large degree of fluctuation is present. These include financial time series, which can be particularly heavy tailed. However, little is known about properties of ARCH or GARCH models in the heavy-tailed setting, and no methods are available for approximating the distributions of parameter estimators there. In this paper we show that, for heavy-tailed errors, the asymptotic distributions of quasi-maximum likelihood parameter estimators in ARCH and GARCH models are nonnormal, and are particularly difficult to estimate directly using standard parametric methods. Standard bootstrap methods also fail to produce consistent estimators. To overcome these problems we develop percentile-t, subsample bootstrap approximations to estimator distributions. Studentizing is employed to approximate scale, and the subsample bootstrap is used to estimate shape. The good performance of this approach is demonstrated both theoretically and numerically.</p> </abstract>
<abstract> <p>The paper proposes a new and normative approach for adjusting households' incomes in order to account for the heterogeneity of needs across income recipients when measuring inequality and welfare. We derive the implications for the structure of the adjustment method of two conditions concerned with the way the ranking of situations is modified by a change in the reference household type and by more equally distributed living standards across households. Our results suggest that concern for greater equality in living standards conflicts with the basic welfarist principle of symmetrical treatment of individuals that is at the core of the standard equivalence scale approach.</p> </abstract>
<abstract> <p>I consider transactions involving asymmetric prisoners' dilemmas between pairs of players randomly selected from two large populations. Games are played repeatedly, but information about cheating is not adequate to sustain cooperation, and there is no official legal system of contract enforcement. I examine how profit-maximizing private intermediation can supply the information and enforcement. I obtain conditions under which private governance can improve upon no governance, and examine why it fails to achieve social optimality.</p> </abstract>
<abstract> <p>High-frequency financial data are not only discretely sampled in time but the time separating successive observations is often random. We analyze the consequences of this dual feature of the data when estimating a continuous-time model. In particular, we measure the additional effects of the randomness of the sampling intervals over and beyond those due to the discreteness of the data. We also examine the effect of simply ignoring the sampling randomness. We find that in many situations the randomness of the sampling has a larger impact than the discreteness of the data.</p> </abstract>
<abstract> <p>Is the stock market boom a result of the baby boom? This paper develops an overlapping generations model in which a baby boom is modeled as a high realization of a random birth rate, and the price of capital is determined endogenously by a convex cost of adjustment. A baby boom increases national saving and investment and thus causes an increase in the price of capital. The price of capital is mean-reverting so the initial increase in the price of capital is followed by a decrease. Social Security can potentially affect national saving and investment, though in the long run, it does not affect the price of capital.</p> </abstract>
<abstract> <p>We provide a framework for integration of high-frequency intraday data into the measurement, modeling, and forecasting of daily and lower frequency return volatilities and return distributions. Building on the theory of continuous-time arbitrage-free price processes and the theory of quadratic variation, we develop formal links between realized volatility and the conditional covariance matrix. Next, using continuously recorded observations for the Deutschemark/Dollar and Yen/Dollar spot exchange rates, we find that forecasts from a simple long-memory Gaussian vector autoregression for the logarithmic daily realized volatilities perform admirably. Moreover, the vector autoregressive volatility forecast, coupled with a parametric lognormal-normal mixture distribution produces well-calibrated density forecasts of future returns, and correspondingly accurate quantile predictions. Our results hold promise for practical modeling and forecasting of the large covariance matrices relevant in asset pricing, asset allocation, and financial risk management applications.</p> </abstract>
<abstract> <p>This paper characterizes empirically achievable limits for time series econometric modeling and forecasting. The approach involves the concept of minimal information loss in time series regression and the paper shows how to derive bounds that delimit the proximity of empirical measures to the true probability measure (the DGP) in models that are of econometric interest. The approach utilizes joint probability measures over the combined space of parameters and observables and the results apply for models with stationary, integrated, and cointegrated data. A theorem due to Rissanen is extended so that it applies directly to probabilities about the relative likelihood (rather than averages), a new way of proving results of the Rissanen type is demonstrated, and the Rissanen theory is extended to nonstationary time series with unit roots, near unit roots, and cointegration of unknown order. The corresponding bound for the minimal information loss in empirical work is shown not to be a constant, in general, but to be proportional to the logarithm of the determinant of the (possibility stochastic) Fisher-information matrix. In fact, the bound that determines proximity to the DGP is generally path dependent, and it depends specifically on the type as well as the number of regressors. For practical purposes, the proximity bound has the asymptotic form (K/2) log n, where K is a new dimensionality factor that depends on the nature of the data as well as the number of parameters in the model. When 'good' model selection principles are employed in modeling time series data, we are able to show that our proximity bound quantifies empirical limits even in situations where the models may be incorrectly specified. One of the main implications of the new result is that time trends are more costly than stochastic trends, which are more costly in turn than stationary regressors in achieving proximity to the true density. Thus, in a very real sense and quantifiable manner, the DGP is more elusive when there is nonstationarity in the data. The implications for prediction are explored and a second proximity theorem is given, which provides a bound that measures how close feasible predictors can come to the optimal predictor. Again, the bound has the asymptotic form (K/2) log n, showing that forecasting trends is fundamentally more difficult than forecasting stationary time series, even when the correct form of the model for the trends is known.</p> </abstract>
<abstract> <p>In this paper, we propose a simple bias-reduced log-periodogram regression estimator, &lt;tex-math&gt;$\hat d_r$&lt;/tex-math&gt;, of the long-memory parameter, d, that eliminates the first- and higher-order biases of the Geweke and Porter-Hudak (1983) (GPH) estimator. The bias-reduced estimator is the same as the GPH estimator except that one includes frequencies to the power 2k for k = 1,..., r, for some positive integer r, as additional regressors in the pseudo-regression model that yields the GPH estimator. The reduction in bias is obtained using assumptions on the spectrum only in a neighborhood of the zero frequency. Following the work of Robinson (1995b) and Hurvich, Deo, and Brodsky (1998), we establish the asymptotic bias, variance, and mean-squared error (MSE) of &lt;tex-math&gt;$\hat d_r$&lt;/tex-math&gt;, determine the asymptotic MSE optimal choice of the number of frequencies, m, to include in the regression, and establish the asymptotic normality of &lt;tex-math&gt;$\hat d_r$&lt;/tex-math&gt;. These results show that the bias of &lt;tex-math&gt;$\hat d_r$&lt;/tex-math&gt; goes to zero at a faster rate than that of the GPH estimator when the normalized spectrum at zero is sufficiently smooth, but that its variance only is increased by a multiplicative constant. We show that the bias-reduced estimator &lt;tex-math&gt;$\hat d_r$&lt;/tex-math&gt; attains the optimal rate of convergence for a class of spectral densities that includes those that are smooth of order s ≥ 1 at zero when r ≥ (s-2)/2 and m is chosen appropriately. For s &gt; 2, the GPH estimator does not attain this rate. The proof uses results of Giraitis, Robinson, and Samarov (1997). We specify a data-dependent plug-in method for selecting the number of frequencies m to minimize asymptotic MSE for a given value of r. Some Monte Carlo simulation results for stationary Gaussian ARFIMA(1, d, 1) and (2, d, 0) models show that the bias-reduced estimators perform well relative to the standard log-periodogram regression estimator.</p> </abstract>
<abstract> <p>This note proposes a necessary and sufficient condition on a utility function to guarantee that it generates a demand function satisfying the law of demand. This condition can be interpreted in terms of an agent's attitude towards lotteries in commodity space. As an application, we show that when an agent has an expected utility function, her demand for securities satisfies the law of demand if her coefficient of relative risk aversion does not vary by more than 4.</p> </abstract>
<abstract> <p>In the Self Sufficiency Project (SSP) welfare demonstration, members of a randomly assigned treatment group could receive a subsidy for full-time work. The subsidy was available for 3 years, but only to people who began working full time within 12 months of random assignment. A simple optimizing model suggests that the eligibility rules created an "establishment" incentive to find a job and leave welfare within a year of random assignment, and an "entitlement" incentive to choose work over welfare once eligibility was established. Building on this insight, we develop an econometric model of welfare participation that allows us to separate the two effects and estimate the impact of the earnings subsidy on welfare entry and exit rates among those who achieved eligibility. The combination of the two incentives explains the time profile of the experimental impacts, which peaked 15 months after random assignment and faded relatively quickly. Our findings suggest that about half of the peak impact of SSP was attributable to the establishment incentive. Despite the extra work effort generated by SSP, the program had no lasting impact on wages and little or no long-run effect on welfare participation.</p> </abstract>
<abstract> <p>The mechanism design literature assumes too much common knowledge of the environment among the players and planner. We relax this assumption by studying mechanism design on richer type spaces. We ask when ex post implementation is equivalent to interim (or Bayesian) implementation for all possible type spaces. The equivalence holds in the case of separable environments; examples of separable environments arise (1) when the planner is implementing a social choice function (not correspondence) and (2) in a quasilinear environment with no restrictions on transfers. The equivalence fails in general, including in some quasilinear environments with budget balance. In private value environments, ex post implementation is equivalent to dominant strategies implementation. The private value versions of our results offer new insights into the relationship between dominant strategy implementation and Bayesian implementation.</p> </abstract>
<abstract> <p>We study how intermediation and asset prices in over-the-counter markets are affected by illiquidity associated with search and bargaining. We compute explicitly the prices at which investors trade with each other, as well as marketmakers' bid and ask prices, in a dynamic model with strategic agents. Bid-ask spreads are lower if investors can more easily find other investors or have easier access to multiple marketmakers. With a monopolistic marketmaker, bid-ask spreads are higher if investors have easier access to the marketmaker. We characterize endogenous search and welfare, and discuss empirical implications.</p> </abstract>
<abstract> <p> We propose and characterize a model of preferences over acts such that the decision maker prefers act f to act g if and only if<tex-math>${\Bbb E}_{\mu}\phi ({\Bbb E}_{\pi }u\circ f)\geq {\Bbb E}_{\mu}\phi ({\Bbb E}_{\pi }u\circ g)$</tex-math>, where E is the expectation operator, u is a von Neumann-Morgenstern utility function, φ is an increasing transformation, and μ is a subjective probability over the set Π of probability measures π that the decision maker thinks are relevant given his subjective information. A key feature of our model is that it achieves a separation between ambiguity, identified as a characteristic of the decision maker's subjective beliefs, and ambiguity attitude, a characteristic of the decision maker's tastes. We show that attitudes toward pure risk are characterized by the shape of u, as usual, while attitudes toward ambiguity are characterized by the shape of φ. Ambiguity itself is defined behaviorally and is shown to be characterized by properties of the subjective set of measures Π. One advantage of this model is that the well-developed machinery for dealing with risk attitudes can be applied as well to ambiguity attitudes. The model is also distinct from many in the literature on ambiguity in that it allows smooth, rather than kinked, indifference curves. This leads to different behavior and improved tractability, while still sharing the main features (e.g., Ellsberg's paradox). The maxmin expected utility model (e.g., Gilboa and Schmeidler (1989)) with a given set of measures may be seen as a limiting case of our model with infinite ambiguity aversion. Two illustrative portfolio choice examples are offered. </p> </abstract>
<abstract> <p>Alternating-offer and demand bargaining models of legislative bargaining make very different predictions in terms of both ex ante and ex post distribution of payoffs, as well as in the role of the order of play. The experiment shows that actual bargaining behavior is not as sensitive to the different bargaining rules as the theoretical point predictions, whereas the comparative statics are in line with both models. We compare our results to studies that attempt to distinguish between these two approaches using field data, finding strong similarities between the laboratory and field data regardless of the underlying bargaining process.</p> </abstract>
<abstract> <p>This paper is concerned with accuracy properties of simulations of approximate solutions for stochastic dynamic models. Our analysis rests upon a continuity property of invariant distributions and a generalized law of large numbers. We then show that the statistics generated by any sufficiently good numerical approximation are arbitrarily close to the set of expected values of the model's invariant distributions. Also, under a contractivity condition on the dynamics, we establish error bounds. These results are of further interest for the comparative study of stationary solutions and the estimation of structural dynamic models.</p> </abstract>
<abstract> <p>We derive a lower bound for the volatility of the permanent component of investors' marginal utility of wealth or, more generally, asset pricing kernels. The bound is based on return properties of long-term zero-coupon bonds, risk-free bonds, and other risky securities. We find the permanent component of the pricing kernel to be very volatile; its volatility is about at least as large as the volatility of the stochastic discount factor. A related measure for the transitory component suggest it to be considerably less important. We also show that, for many cases where the pricing kernel is a function of consumption, innovations to consumption need to have permanent effects.</p> </abstract>
<abstract> <p>This paper investigates the driving forces behind informal sanctions in cooperation games and the extent to which theories of fairness and reciprocity capture these forces. We find that cooperators' punishment is almost exclusively targeted toward the defectors, but the latter also impose a considerable amount of spiteful punishment on the cooperators. However, spiteful punishment vanishes if the punishers can no longer affect the payoff differences between themselves and the punished individual, whereas the cooperators even increase the resources devoted to punishment in this case. Our data also discriminate between different fairness principles. Fairness theories that are based on the assumption that players compare their own payoff to the group's average or the group's total payoff cannot explain the fact that cooperators target their punishment at the defectors. Fairness theories that assume that players aim to minimize payoff inequalities cannot explain the fact that cooperators punish defectors even if payoff inequalities cannot be reduced. Therefore, retaliation, i.e., the desire to harm those who committed unfair acts, seems to be the most important motive behind fairness-driven informal sanctions.</p> </abstract>
<abstract> <p>With many semi-anonymous players, the equilibria of simultaneous-move games are "extensively robust." This means that the equilibria survive even if the simultaneous-play assumption is relaxed to allow for a large variety of extensive modifications. Such modifications include sequential play with partial and differential revelation of information, commitments, multiple revisions of choices, cheap talk announcements, and more.</p> </abstract>
<abstract> <p>This paper proposes an asymptotically efficient method for estimating models with conditional moment restrictions. Our estimator generalizes the maximum empirical likelihood estimator (MELE) of Qin and Lawless (1994). Using a kernel smoothing method, we efficiently incorporate the information implied by the conditional moment restrictions into our empirical likelihood-based procedure. This yields a one-step estimator which avoids estimating optimal instruments. Our likelihood ratio-type statistic for parametric restrictions does not require the estimation of variance, and achieves asymptotic pivotalness implicitly. The estimation and testing procedures we propose are normalization invariant. Simulation results suggest that our new estimator works remarkably well in finite samples.</p> </abstract>
<abstract> <p>A speaker wishes to persuade a listener to accept a certain request. The conditions under which the request is justified, from the listener's point of view, depend on the values of two aspects. The values of the aspects are known only to the speaker and the listener can check the value of at most one. A mechanism specifies a set of messages that the speaker can send and a rule that determines the listener's response, namely, which aspect he checks and whether he accepts or rejects the speaker's request. We study mechanisms that maximize the probability that the listener accepts the request when it is justified and rejects the request when it is unjustified, given that the speaker maximizes the probability that his request is accepted. We show that a simple optimal mechanism exists and can be found by solving a linear programming problem in which the set of constraints is derived from what we call the L-principle.</p> </abstract>
<abstract> <p>We develop the measurement theory of polarization for the case in which income distributions can be described using density functions. The main theorem uniquely characterizes a class of polarization measures that fits into what we call the "identity-alienation" framework, and simultanously satisfies a set of axioms. Second, we provide sample estimators of population polarization indices that can be used to compare polarization across time or entities. Distribution-free statistical inference results are also used in order to ensure that the orderings of polarization across entities are not simply due to sampling noise. An illustration of the use of these tools using data from 21 countries shows that polarization and inequality orderings can often differ in practice.</p> </abstract>
<abstract> <p>We introduce a family of generalized-method-of-moments estimators of the parameters of a continuous-time Markov process observed at random time intervals. The results include strong consistency, asymptotic normality, and a characterization of standard errors. Sampling is at an arrival intensity that is allowed to depend on the underlying Markov process and on the parameter vector to be estimated. We focus on financial applications, including tick-based sampling, allowing for jump diffusions, regime-switching diffusions, and reflected diffusions.</p> </abstract>
<abstract> <p>In this paper we investigate methods for testing the existence of a cointegration relationship among the components of a nonstationary fractionally integrated (NFI) vector time series. Our framework generalizes previous studies restricted to unit root integrated processes and permits simultaneous analysis of spurious and cointegrated NFI vectors. We propose a modified F-statistic, based on a particular studentization, which converges weakly under both hypotheses, despite the fact that OLS estimates are only consistent under cointegration. This statistic leads to a Wald-type test of cointegration when combined with a narrow band GLS-type estimate. Our semiparametric methodology allows consistent testing of the spurious regression hypothesis against the alternative of fractional cointegration without prior knowledge on the memory of the original series, their short run properties, the cointegrating vector, or the degree of cointegration. This semiparametric aspect of the modelization does not lead to an asymptotic loss of power, permitting the Wald statistic to diverge faster under the alternative of cointegration than when testing for a hypothesized cointegration vector. In our simulations we show that the method has comparable power to customary procedures under the unit root cointegration setup, and maintains good properties in a general framework where other methods may fail. We illustrate our method testing the cointegration hypothesis of nominal GNP and simple-sum (M1, M2, M3) monetary aggregates.</p> </abstract>
<abstract> <p>Recently a growing body of research has studied inference in settings where parameters of interest are partially identified. In many cases the parameter is real-valued and the identification region is an interval whose lower and upper bounds may be estimated from sample data. For this case confidence intervals (CIs) have been proposed that cover the entire identification region with fixed probability. Here, we introduce a conceptually different type of confidence interval. Rather than cover the entire identification region with fixed probability, we propose CIs that asymptotically cover the true value of the parameter with this probability. However, the exact coverage probabilities of the simplest version of our new CIs do not converge to their nominal values uniformly across different values for the width of the identification region. To avoid the problems associated with this, we modify the proposed CI to ensure that its exact coverage probabilities do converge uniformly to their nominal values. We motivate this modified CI through exact results for the Gaussian case.</p> </abstract>
<abstract> <p>This paper extends the conditional logit approach (Rasch, Andersen, Chamberlain) used in panel data models of binary variables with correlated fixed effects and strictly exogenous regressors. In a two-period two-state model, necessary and sufficient conditions on the joint distribution function of the individual-and-period specific shocks are given such that the sum of individual binary variables across time is a sufficient statistic for the individual effect. By extending a result of Chamberlain, it is shown that root-n consistent regular estimators can be constructed in panel binary models if and only if the property of sufficiency holds. In applied work, the estimation method amounts to quasi-differencing the binary variables as if they were continuous variables and transforming a panel data model into a cross-section model. Semiparametric approaches can then be readily applied.</p> </abstract>
<abstract> <p>Conditional moment restrictions can be combined through GMM estimation to construct more efficient semiparametric estimators. This paper is about attainable efficiency for such estimators. We define and use a moment tangent set, the directions of departure from the truth allowed by the moments, to characterize when the semiparametric efficiency bound can be attained. The efficiency condition is that the moment tangent set equals the model tangent set. We apply these results to transformed, censored, and truncated regression models, e.g., finding that the conditional moment restrictions from Powell's (1986) censored regression quantile estimators can be combined to approximate efficiency when the disturbance is independent of regressors.</p> </abstract>
<abstract> <p> This paper investigates asymptotic properties of the maximum likelihood estimator and the quasi-maximum likelihood estimator for the spatial autoregressive model. The rates of convergence of those estimators may depend on some general features of the spatial weights matrix of the model. It is important to make the distinction with different spatial scenarios. Under the scenario that each unit will be influenced by only a few neighboring units, the estimators may have √n-rate of convergence and be asymptotically normal. When each unit can be influenced by many neighbors, irregularity of the information matrix may occur and various components of the estimators may have different rates of convergence. </p> </abstract>
<abstract> <p>A complex financial system comprises both financial markets and financial intermediaries. We distinguish financial intermediaries according to whether they issue complete contingent contracts or incomplete contracts. Intermediaries such as banks that issue incomplete contracts, e.g., demand deposits, are subject to runs, but this does not imply a market failure. A sophisticated financial system--a system with complete markets for aggregate risk and limited market participation--is incentive-efficient, if the intermediaries issue complete contingent contracts, or else constrained-efficient, if they issue incomplete contracts. We argue that there may be a role for regulating liquidity provision in an economy in which markets for aggregate risks are incomplete.</p> </abstract>
<abstract> <p>The holdup problem arises when parties negotiate to divide the surplus generated by their relationship specific investments. We study this problem in a dynamic model of bargaining and investment which, unlike the stylized static model, allows the parties to continue to invest until they agree on the terms of trade. The investment dynamics overturns the conventional wisdom dramatically. First, the holdup problem need not entail underinvestment when the parties are sufficiently patient. Second, inefficiencies can arise unambiguously in some cases, but they are not caused by the sharing of surplus per se but rather by a failure of an individual rationality constraint.</p> </abstract>
<abstract> <p>We establish the existence of pure strategy equilibria in monotone bidding functions in first-price auctions with asymmetric bidders, interdependent values, and affiliated one-dimensional signals. By extending a monotonicity result due to Milgrom and Weber (1982), we show that single crossing can fail only when ties occur at winning bids or when bids are individually irrational. We avoid these problems by considering limits of ever finer finite bid sets such that no two bidders have a common serious bid, and by recalling that single crossing is needed only at individually rational bids. Two examples suggest that our results cannot be extended to multidimensional signals or to second-price auctions.</p> </abstract>
<abstract> <p>This paper develops a new methodology that makes use of the factor structure of large dimensional panels to understand the nature of nonstationarity in the data. We refer to it as PANIC-Panel Analysis of Nonstationarity in Idiosyncratic and Common components. PANIC can detect whether the nonstationarity in a series is pervasive, or variable-specific, or both. It can determine the number of independent stochastic trends driving the common factors. PANIC also permits valid pooling of individual statistics and thus panel tests can be constructed. A distinctive feature of PANIC is that it tests the unobserved components of the data instead of the observed series. The key to PANIC is consistent estimation of the space spanned by the unobserved common factors and the idiosyncratic errors without knowing a priori whether these are stationary or integrated processes. We provide a rigorous theory for estimation and inference and show that the tests have good finite sample properties.</p> </abstract>
<abstract> <p>In a one-principal two-agent model with adverse selection and collusion among agents, we show that delegating to one agent the right to subcontract with the other agent always earns lower profit for the principal compared with centralized contracting. Delegation to an intermediary is also not in the principal's interest if the agents supply substitutes. It can be beneficial if the agents produce complements and the intermediary is well informed.</p> </abstract>
<abstract> <p>An important objective of empirical research on treatment response is to provide decision makers with information useful in choosing treatments. This paper studies minimax-regret treatment choice using the sample data generated by a classical randomized experiment. Consider a utilitarian social planner who must choose among the feasible statistical treatment rules, these being functions that map the sample data and observed covariates of population members into a treatment allocation. If the planner knew the population distribution of treatment response, the optimal treatment rule would maximize mean welfare conditional on all observed covariates. The appropriate use of covariate information is a more subtle matter when only sample data on treatment response are available. I consider the class of conditional empirical success rules; that is, rules assigning persons to treatments that yield the best experimental outcomes conditional on alternative subsets of the observed covariates. I derive a closed-form bound on the maximum regret of any such rule. Comparison of the bounds for rules that condition on smaller and larger subsets of the covariates yields sufficient sample sizes for productive use of covariate information. When the available sample size exceeds the sufficiency boundary, a planner can be certain that conditioning treatment choice on more covariates is preferable (in terms of minimax regret) to conditioning on fewer covariates.</p> </abstract>
<abstract> <p>In an environment where trading volume affects security prices and where prices are uncertain when trades are submitted, quasi-arbitrage is the availability of a series of trades that generate infinite expected profits with an infinite Sharpe ratio. We show that when the price impact of trades is permanent and time-independent, only linear price-impact functions rule out quasi-arbitrage and thus support viable market prices. When trades have also a temporary price impact, only the permanent price impact must be linear while the temporary one can be of a more general form. We also extend the analysis to a time-dependent framework.</p> </abstract>
<abstract> <p>An asymptotically efficient likelihood-based semiparametric estimator is derived for the censored regression (tobit) model, based on a new approach for estimating the density function of the residuals in a partially observed regression. Smoothing the self-consistency equation for the nonparametric maximum likelihood estimator of the distribution of the residuals yields an integral equation, which in some cases can be solved explicitly. The resulting estimated density is smooth enough to be used in a practical implementation of the profile likelihood estimator, but is sufficiently close to the nonparametric maximum likelihood estimator to allow estimation of the semiparametric efficient score. The parameter estimates obtained by solving the estimated score equations are then asymptotically efficient. A summary of analogous results for truncated regression is also given.</p> </abstract>
<abstract> <p>Fixed effects estimators of panel models can be severely biased because of the well-known incidental parameters problem. We show that this bias can be reduced by using a panel jackknife or an analytical bias correction motivated by large T. We give bias corrections for averages over the fixed effects, as well as model parameters. We find large bias reductions from using these approaches in examples. We consider asymptotics where T grows with n, as an approximation to the properties of the estimators in econometric applications. We show that if T grows at the same rate as n, the fixed effects estimator is asymptotically biased, so that asymptotic confidence intervals are incorrect, but that they are correct for the panel jackknife. We show T growing faster than n&lt;sup&gt;1/3&lt;/sup&gt; suffices for correctness of the analytic correction, a property we also conjecture for the jackknife.</p> </abstract>
<abstract> <p>The paper studies the optimal tax-subsidy schedules in an economy where the only decision of the agents is to work, or not, with an application to the case of France. Given an income guarantee provided by the welfare state, the tax schedule that maximizes government revenue provides a benchmark, the Laffer bound, above which it is inefficient to tax. In fact, under mild conditions, a feasible allocation is second best optimal if and only if the associated taxes are lower than the Laffer bound. The only restriction that efficiency puts on the shape of the tax scheme is this upper Laffer bound. The Laffer tax scheme itself can be computed from the joint distribution of the agents' productivities and work opportunity costs. Depending on the economy, it can take widely different forms, and exhibit, for instance, negative marginal tax rates. After estimating the joint distribution of productivities and work opportunity costs on French data, I compute the Laffer bound for two subpopulations, single women and married women with two children or more. Quite surprisingly, the actual incentives to work appear to be very close to the bound.</p> </abstract>
<abstract> <p>We introduce a class of strategies that generalizes examples constructed in two-player games under imperfect private monitoring. A sequential equilibrium is belief-free if, after every private history, each player's continuation strategy is optimal independently of his belief about his opponents' private histories. We provide a simple and sharp characterization of equilibrium payoffs using those strategies. While such strategies support a large set of payoffs, they are not rich enough to generate a folk theorem in most games besides the prisoner's dilemma, even when noise vanishes.</p> </abstract>
<abstract> <p>This paper disentangles the impact of schools and teachers in influencing achievement with special attention given to the potential problems of omitted or mismeasured variables and of student and school selection. Unique matched panel data from the UTD Texas Schools Project permit the identification of teacher quality based on student performance along with the impact of specific, measured components of teachers and schools. Semiparametric lower bound estimates of the variance in teacher quality based entirely on within-school heterogeneity indicate that teachers have powerful effects on reading and mathematics achievement, though little of the variation in teacher quality is explained by observable characteristics such as education or experience. The results suggest that the effects of a costly ten student reduction in class size are smaller than the benefit of moving one standard deviation up the teacher quality distribution, highlighting the importance of teacher effectiveness in the determination of school quality.</p> </abstract>
<abstract> <p>Consider a two-player discounted infinitely repeated game. A player's belief is a probability distribution over the opponent's repeated game strategies. This paper shows that, for a large class of repeated games, there are no beliefs that satisfy three properties: learnability, a diversity of belief condition called CSP, and consistency. Loosely, if players learn to forecast the path of play whenever each plays a strategy that the other anticipates (in the sense of being in the support of that player's belief) and if the sets of anticipated strategies are sufficiently rich, then neither anticipates any of his opponent's best responses. This generalizes results in Nachbar (1997).</p> </abstract>
<abstract> <p> This paper brings together the microeconomic-labor and the macroeconomic-equilibrium views of matching in labor markets. We nest a job matching model à la Jovanovic (1984) into a Mortensen and Pissarides (1994)-type equilibrium search environment. The resulting framework preserves the implications of job matching theory for worker turnover and wage dynamics, and it also allows for aggregation and general equilibrium analysis. We obtain two new equilibrium implications of job matching and search frictions for wage inequality. First, learning about match quality and worker turnover map Gaussian output noise into an ergodic wage distribution of empirically accurate shape: unimodal, skewed, with a Paretian right tail. Second, high idiosyncratic productivity risk hinders learning and sorting, and reduces wage inequality. The equilibrium solutions for the wage distribution and for the aggregate worker flows-quits to unemployment and to other jobs, displacements, hires--provide the likelihood function of the model in closed form. </p> </abstract>
<abstract> <p> In many trading environments, any incentive compatible and individually rational market mechanism will be either inefficient or will run a deficit. We prove that as the market size m gets large, for any fixed surplus (or deficit) x, m times the minimal absolute inefficiency converges to c(x) where c(·) is essentially a quadratic function of x. We introduce a new mechanism, the double auction with a fixed transaction fee. By choosing the size of the fee appropriately, any level of deficit or surplus can be implemented and the resulting mechanisms achieve the above bound. Corollaries include: an asymptotic version of the Myerson-Satterthwaite Impossibility Theorem; a description of the minimal subsidy required to implement the efficient trading rule; a characterization of the minimal inefficiency obtainable with budget-balanced market mechanisms; recommendations on the optimal organization of trade; and insights on the effects of taxation. </p> </abstract>
<abstract> <p>We investigate the effect of employer-provided health insurance on job mobility rates and economic welfare using a search, matching, and bargaining framework. In our model, health insurance coverage decisions are made in a cooperative manner that recognizes the productivity effects of health insurance as well as its nonpecuniary value to the employee. The resulting equilibrium is one in which not all employment matches are covered by health insurance, wages at jobs providing health insurance are larger (in a stochastic sense) than those at jobs without health insurance, and workers at jobs with health insurance are less likely to leave those jobs, even after conditioning on the wage rate. We estimate the model using the 1996 panel of the Survey of Income and Program Participation, and find that the employer-provided health insurance system does not lead to any serious inefficiencies in mobility decisions.</p> </abstract>
<abstract> <p>Repeated games with unknown payoff distributions are analogous to a single decision maker's "multi-armed bandit" problem. Each state of the world corresponds to a different payoff matrix of a stage game. When monitoring is perfect, information about the state is public, and players are sufficiently patient, the following result holds: For any function that maps each state to a payoff vector that is feasible and individually rational in that state, there is a sequential equilibrium in which players experiment to learn the realized state and achieve a payoff close to the one specified for that state.</p> </abstract>
<abstract> <p>We consider an exchange economy in which there are infinitely many consumers and some commodities are bads, that is, cause disutility to consumers. We give an example of such an economy for which there is no competitive equilibrium or its variants (quasi- or pseudo-equilibrium), and an example of the failure of the so-called uniform integrability condition of equilibrium allocations of increasingly populous finite economies.</p> </abstract>
<abstract> <p> Most applications of Nash bargaining over wages ignore between-employer competition for labor services and attribute all of the workers' rent to their bargaining power. In this paper, we write and estimate an equilibrium model with strategic wage bargaining and on-the-job search and use it to take another look at the determinants of wages in France. There are three essential determinants of wages in our model: productivity, competition between employers resulting from on-the-job search, and the workers' bargaining power. We find that between-firm competition matters a lot in the determination of wages, because it is quantitatively more important than wage bargaining à la Nash in raising wages above the workers' "reservation wages," defined as out-of-work income. In particular, we detect no significant bargaining power for intermediate- and low-skilled workers, and a modestly positive bargaining power for high-skilled workers. </p> </abstract>
<abstract> <p>We study the optimal trade-off between commitment and flexibility in a consumption-savings model. Individuals expect to receive relevant information regarding tastes and thus they value the flexibility provided by larger choice sets. On the other hand, they also expect to suffer from temptation, with or without self-control, and thus they value the commitment afforded by smaller choice sets. The optimal commitment problem we study is to find the best subset of the individual's budget set. This problem leads to a principal-agent formulation. We find that imposing a minimum level of savings is always a feature of the solution. Necessary and sufficient conditions are derived for minimum-savings policies to completely characterize the solution. We also discuss other applications, such as the design of fiscal constitutions, the problem faced by a paternalist, and externalities.</p> </abstract>
<abstract> <p>The Lemke-Howson algorithm is the classical method for finding one Nash equilibrium of a bimatrix game. This paper presents a class of square bimatrix games for which this algorithm takes, even in the best case, an exponential number of steps in the dimension d of the game. Using polytope theory, the games are constructed using pairs of dual cyclic polytopes with 2d suitably labeled facets in d-space. The construction is extended to nonsquare games where, in addition to exponentially long Lemke-Howson computations, finding an equilibrium by support enumeration takes on average exponential time.</p> </abstract>
<abstract> <p>This paper develops a generalization of the widely used difference-in-differences method for evaluating the effects of policy changes. We propose a model that allows the control and treatment groups to have different average benefits from the treatment. The assumptions of the proposed model are invariant to the scaling of the outcome. We provide conditions under which the model is nonparametrically identified and propose an estimator that can be applied using either repeated cross section or panel data. Our approach provides an estimate of the entire counterfactual distribution of outcomes that would have been experienced by the treatment group in the absence of the treatment and likewise for the untreated group in the presence of the treatment. Thus, it enables the evaluation of policy interventions according to criteria such as a mean-variance trade-off. We also propose methods for inference, showing that our estimator for the average treatment effect is root-N consistent and asymptotically normal. We consider extensions to allow for covariates, discrete dependent variables, and multiple groups and time periods.</p> </abstract>
<abstract> <p>Most theoretical or applied research on repeated games with imperfect monitoring has focused on public strategies: strategies that depend solely on the history of publicly observable signals. This paper sheds light on the role of private strategies: strategies that depend not only on public signals, but also on players' own actions in the past. Our main finding is that players can sometimes make better use of information by using private strategies and that efficiency in repeated games can be improved. Our equilibrium private strategy for repeated prisoners' dilemma games consists of two states and has the property that each player's optimal strategy is independent of the other player's state.</p> </abstract>
<abstract> <p>This paper is concerned with inference about a function g that is identified by a conditional moment restriction involving instrumental variables. The paper presents a test of the hypothesis that g belongs to a finite-dimensional parametric family against a nonparametric alternative. The test does not require nonparametric estimation of g and is not subject to the ill-posed inverse problem of nonparametric instrumental variables estimation. Under mild conditions, the test is consistent against any alternative model. In large samples, its power is arbitrarily close to 1 uniformly over a class of alternatives whose distance from the null hypothesis is O(n<sup>-1/2</sup>), where n is the sample size. In Monte Carlo simulations, the finite-sample power of the new test exceeds that of existing tests.</p> </abstract>
<abstract> <p>Quantile regression (QR) fits a linear model for conditional quantiles just as ordinary least squares (OLS) fits a linear model for conditional means. An attractive feature of OLS is that it gives the minimum mean-squared error linear approximation to the conditional expectation function even when the linear model is misspecified. Empirical research using quantile regression with discrete covariates suggests that QR may have a similar property, but the exact nature of the linear approximation has remained elusive. In this paper, we show that QR minimizes a weighted mean-squared error loss function for specification error. The weighting function is an average density of the dependent variable near the true conditional quantile. The weighted least squares interpretation of QR is used to derive an omitted variables bias formula and a partial quantile regression concept, similar to the relationship between partial regression and OLS. We also present asymptotic theory for the QR process under misspecification of the conditional quantile function. The approximation properties of QR are illustrated using wage data from the U.S. census. These results point to major changes in inequality from 1990 to 2000.</p> </abstract>
<abstract> <p>A key argument in Caplin and Leahy (1997) states that the correlation between monetary shocks and output is falling in the variance of the money supply. We demonstrate that this conclusion depends on solving for the correlation in the nonstationary state of the model. In the stationary state, that correlation is initially rising.</p> </abstract>
<abstract> <p>We propose two new methods for estimating models with nonseparable errors and endogenous regressors. The first method estimates a local average response. One estimates the response of the conditional mean of the dependent variable to a change in the explanatory variable while conditioning on an external variable and then undoes the conditioning. The second method estimates the nonseparable function and the joint distribution of the observable and unobservable explanatory variables. An external variable is used to impose an equality restriction, at two points of support, on the conditional distribution of the unobservable random term given the regressor and the external variable. Our methods apply to cross sections, but our lead examples involve panel data cases in which the choice of the external variable is guided by the assumption that the distribution of the unobservable variables is exchangeable in the values of the endogenous variable for members of a group.</p> </abstract>
<abstract> <p>We propose a generalized method of moments (GMM) Lagrange multiplier statistic, i.e., the K statistic, that uses a Jacobian estimator based on the continuous updating estimator that is asymptotically uncorrelated with the sample average of the moments. Its asymptotic χ<sup>2</sup>distribution therefore holds under a wider set of circumstances, like weak instruments, than the standard full rank case for the expected Jacobian under which the asymptotic χ<sup>2</sup>distributions of the traditional statistics are valid. The behavior of the K statistic can be spurious around inflection points and maxima of the objective function. This inadequacy is overcome by combining the K statistic with a statistic that tests the validity of the moment equations and by an extension of Moreira's (2003) conditional likelihood ratio statistic toward GMM. We conduct a power comparison to test for the risk aversion parameter in a stochastic discount factor model and construct its confidence set for observed consumption growth and asset return series.</p> </abstract>
<abstract> <p>A decision maker is asked to express her beliefs by assigning probabilities to certain possible states. We focus on the relationship between her database and her beliefs. We show that if beliefs given a union of two databases are a convex combination of beliefs given each of the databases, the belief formation process follows a simple formula: beliefs are a similarity-weighted average of the beliefs induced by each past case.</p> </abstract>
<abstract> <p>Exploiting a rich panel data set on anti-ulcer drug prescriptions, we measure the effects of uncertainty and learning in the demand for pharmaceutical drugs. We estimate a dynamic matching model of demand under uncertainty in which patients learn from prescription experience about the effectiveness of alternative drugs. Unlike previous models, we allow drugs to have distinct symptomatic and curative effects, and endogenize treatment length by allowing drug choices to affect patients' underlying probability of recovery. We find that drugs' rankings along these dimensions differ, with high symptomatic effects for drugs with the highest market shares and high curative effects for drugs with the greatest medical efficacy. Our results also indicate that while there is substantial heterogeneity in drug efficacy across patients, learning enables patients and their doctors to dramatically reduce the costs of uncertainty in pharmaceutical markets.</p> </abstract>
<abstract> <p>This paper shows that the bootstrap does not consistently estimate the asymptotic distribution of the maximum score estimator. The theory developed also applies to other estimators within a cube-root convergence class. For some single-parameter estimators in this class, the results suggest a simple method for inference based upon the bootstrap.</p> </abstract>
<abstract> <p>This paper develops theoretical foundations for an error analysis of approximate equilibria in dynamic stochastic general equilibrium models with heterogeneous agents and incomplete financial markets. While there are several algorithms that compute prices and allocations for which agents' first-order conditions are approximately satisfied ("approximate equilibria"), there are few results on how to interpret the errors in these candidate solutions and how to relate the computed allocations and prices to exact equilibrium allocations and prices. We give a simple example to illustrate that approximate equilibria might be very far from exact equilibria. We then interpret approximate equilibria as equilibria for close-by economies; that is, for economies with close-by individual endowments and preferences. We present an error analysis for two models that are commonly used in applications, an overlapping generations (OLG) model with stochastic production and an asset pricing model with infinitely lived agents. We provide sufficient conditions that ensure that approximate equilibria are close to exact equilibria of close-by economies. Numerical examples illustrate the analysis.</p> </abstract>
<abstract> <p>In econometric applications, often several hypothesis tests are carried out at once. The problem then becomes how to decide which hypotheses to reject, accounting for the multitude of tests. This paper suggests a stepwise multiple testing procedure that asymptotically controls the familywise error rate. Compared to related single-step methods, the procedure is more powerful and often will reject more false hypotheses. In addition, we advocate the use of studentization when feasible. Unlike some stepwise methods, the method implicitly captures the joint dependence structure of the test statistics, which results in increased ability to detect false hypotheses. The methodology is presented in the context of comparing several strategies to a common benchmark. However, our ideas can easily be extended to other contexts where multiple tests occur. Some simulation studies show the improvements of our methods over previous proposals. We also provide an application to a set of real data.</p> </abstract>
<abstract> <p> We consider semiparametric estimation of the memory parameter in a model that includes as special cases both long-memory stochastic volatility and fractionally integrated exponential GARCH (FIEGARCH) models. Under our general model the logarithms of the squared returns can be decomposed into the sum of a long-memory signal and a white noise. We consider periodogram-based estimators using a local Whittle criterion function. We allow the optional inclusion of an additional term to account for possible correlation between the signal and noise processes, as would occur in the FIEGARCH model. We also allow for potential nonstationarity in volatility by allowing the signal process to have a memory parameter<tex-math>$d^{\ast}\geq 1/2$</tex-math>. We show that the local Whittle estimator is consistent for<tex-math>$d^{\ast}\in (0,1)$</tex-math>. We also show that the local Whittle estimator is asymptotically normal for<tex-math>$d^{\ast}\in (0,3/4)$</tex-math>and essentially recovers the optimal semiparametric rate of convergence for this problem. In particular, if the spectral density of the short-memory component of the signal is sufficiently smooth, a convergence rate of<tex-math>$n^{2/5-\delta}$</tex-math>for<tex-math>$d^{\ast}\in (0,3/4)$</tex-math>can be attained, where n is the sample size and δ &gt; 0 is arbitrarily small. This represents a strong improvement over the performance of existing semiparametric estimators of persistence in volatility. We also prove that the standard Gaussian semiparametric estimator is asymptotically normal if<tex-math>$d^{\ast}=0$</tex-math>. This yields a test for long memory in volatility. </p> </abstract>
<abstract> <p>This paper proposes a model for multilateral contracting, where contracts are written and renegotiated over time, and where contracts may impose externalities on other agents. Equilibria always exist and the equilibrium value function is linear and monotonically increasing on the contracts. If the grand coalition, or contracting among all agents, is inefficient, we show that bargaining delays arise in positive-externality games and equilibrium inefficiency may remain bounded away from zero even as bargaining frictions converge to zero. Otherwise, if the grand coalition is efficient, there are no bargaining delays, convergence to the grand coalition occurs in a finite number of contracting rounds, and the outcome becomes efficient as players become more patient.</p> </abstract>
<abstract> <p>It is well known that standard asymptotic theory is not applicable or is very unreliable in models with identification problems or weak instruments. One possible way out consists of using a variant of the Anderson-Rubin ((1949), AR) procedure. The latter allows one to build exact tests and confidence sets only for the full vector of the coefficients of the endogenous explanatory variables in a structural equation, but not for individual coefficients. This problem may in principle be overcome by using projection methods (Dufour (1997), Dufour and Jasiak (2001)). At first sight, however, this technique requires the application of costly numerical algorithms. In this paper, we give a general necessary and sufficient condition that allows one to check whether an AR-type confidence set is bounded. Furthermore, we provide an analytic solution to the problem of building projection-based confidence sets from AR-type confidence sets. The latter involves the geometric properties of "quadrics" and can be viewed as an extension of usual confidence intervals and ellipsoids. Only least squares techniques are needed to build the confidence intervals.</p> </abstract>
<abstract> <p>We apply recent econometric advances to study the distribution of commuters' preferences for speedy and reliable highway travel. Our analysis applies mixed logit to combined revealed and stated preference data on commuter choices of whether to pay a toll for congestion-free express travel. We find that motorists exhibit high values of travel time and reliability, and substantial heterogeneity in those values. We suggest that road pricing policies designed to cater to such varying preferences can improve efficiency and reduce the disparity of welfare impacts compared with recent pricing experiments.</p> </abstract>
<abstract> <p>The asymptotic refinements attributable to the block bootstrap for time series are not as large as those of the nonparametric iid bootstrap or the parametric bootstrap. One reason is that the independence between the blocks in the block bootstrap sample does not mimic the dependence structure of the original sample. This is the join-point problem. In this paper, we propose a method of solving this problem. The idea is not to alter the block bootstrap. Instead, we alter the original sample statistics to which the block bootstrap is applied. We introduce block statistics that possess join-point features that are similar to those of the block bootstrap versions of these statistics. We refer to the application of the block bootstrap to block statistics as the block-block bootstrap. The asymptotic refinements of the block-block bootstrap are shown to be greater than those obtained with the block bootstrap and close to those obtained with the nonparametric iid bootstrap and parametric bootstrap.</p> </abstract>
<abstract> <p>This paper studies the impact of credit markets on optimal contracting, when the agent's "interim preference" over upcoming contracts is private information because personal financial decisions affect it via the wealth effect. The main result is a severe loss of incentive provision: equilibrium contracts invariably cause the agent to shirk (i.e., exert minimal effort) if the agent's private financial decision precedes moral hazard contracting. The basic intuition is that committing on another private variable, other than the effort level, exposes the parties to further exploitation of efficient risksharing by relaxing the incentive constraint that was binding ex ante, unless the risksharing was fully efficient to begin with.</p> </abstract>
<abstract> <p>We provide evidence that long-term relationships between trading parties emerge endogenously in the absence of third party enforcement of contracts and are associated with a fundamental change in the nature of market interactions. Without third party enforcement, the vast majority of trades are initiated with private offers and the parties share the gains from trade equally. Low effort or bad quality is penalized by the termination of the relationship, wielding a powerful effect on contract enforcement. Successful long-term relations exhibit generous rent sharing and high effort (quality) from the very beginning of the relationship. In the absence of third-party enforcement, markets resemble a collection of bilateral trading islands rather than a competitive market. If contracts are third party enforceable, rent sharing and long-term relations are absent and the vast majority of trades are initiated with public offers. Most trades take place in one-shot transactions and the contracting parties are indifferent with regard to the identity of their trading partner.</p> </abstract>
<abstract> <p>Different people may use different strategies, or decision rules, when solving complex decision problems. We provide a new Bayesian procedure for drawing inferences about the nature and number of decision rules present in a population, and use it to analyze the behaviors of laboratory subjects confronted with a difficult dynamic stochastic decision problem. Subjects practiced before playing for money. Based on money round decisions, our procedure classifies subjects into three types, which we label "Near Rational," "Fatalist," and "Confused." There is clear evidence of continuity in subjects' behaviors between the practice and money rounds: types who performed best in practice also tended to perform best when playing for money. However, the agreement between practice and money play is far from perfect. The divergences appear to be well explained by a combination of type switching (due to learning and/or increased effort in money play) and errors in our probabilistic type assignments.</p> </abstract>
<abstract> <p>We investigate two-player infinitely repeated games where the discount factor is less than but close to unity. Monitoring is private and players cannot communicate. We require no condition concerning the accuracy of players' monitoring technology. We show the folk theorem for the prisoners' dilemma with conditional independence. We also investigate more general games where players' private signals are correlated only through an unobservable macro shock. We show that efficiency is sustainable for generic private signal structures when the size of the set of private signals is sufficiently large. Finally, we show that cartel collusion is sustainable in price-setting duopoly.</p> </abstract>
<abstract> <p>This paper investigates the effects of financial market globalization on the inequality of nations. The world economy consists of inherently identical countries, which differ only in their levels of capital stock. Each country is represented by the standard overlapping generations model, modified only to incorporate credit market imperfection. An integration of financial markets affects the set of stable steady states, as it changes the balance between the equalizing force of the diminishing returns technology and the unequalizing force of the wealth-dependent borrowing constraint. The model is tractable enough to allow for a complete characterization of the stable steady states. In the absence of the international financial market, the world economy has a unique steady state, which is symmetric and stable. In the presence of the international financial market, symmetry-breaking occurs under some conditions. That is, the symmetric steady state loses its stability and stable asymmetric steady states come to exist. In the stable asymmetric steady states, the world economy is endogenously divided into the rich and poor countries; the borrowing constraints are binding in the poor but not in the rich; the world output is smaller, the rich are richer and the poor are poorer in any of the stable asymmetric steady states than in the (unstable) symmetric steady state.</p> </abstract>
<abstract> <p>This paper analyses multivariate high frequency financial data using realized covariation. We provide a new asymptotic distribution theory for standard methods such as regression, correlation analysis, and covariance. It will be based on a fixed interval of time (e.g., a day or week), allowing the number of high frequency returns during this period to go to infinity. Our analysis allows us to study how high frequency correlations, regressions, and covariances change through time. In particular we provide confidence intervals for each of these quantities.</p> </abstract>
<abstract> <p>An asymmetric information model of a finite horizon "nth order" rational asset price bubble is presented, where &lt;tex-math&gt;$(\text{all agents know that})^{n}$&lt;/tex-math&gt; the asset is worthless. Also, the model has only two agents, so the first order version of the bubble is simpler than other first order bubbles in the literature.</p> </abstract>
<abstract> <p> A new class of autocorrelation robust test statistics is introduced. The class of tests generalizes the Kiefer, Vogelsang, and Bunzel (2000) test in a manner analogous to Anderson and Darling's (1952) generalization of the Cramér-von Mises goodness of fit test. In a Gaussian location model, the error in rejection probability of the new tests is found to be &lt;tex-math&gt;$O(T^{-1}\ \text{log}\ T)$&lt;/tex-math&gt;, where T denotes the sample size. </p> </abstract>
<abstract> <p>The purpose of this note is to show how semiparametric estimators with a small bias property can be constructed. The small bias property (SBP) of a semiparametric estimator is that its bias converges to zero faster than the pointwise and integrated bias of the nonparametric estimator on which it is based. We show that semiparametric estimators based on twicing kernels have the SBP. We also show that semiparametric estimators where nonparametric kernel estimation does not affect the asymptotic variance have the SBP. In addition we discuss an interpretation of series and sieve estimators as idempotent transformations of the empirical distribution that helps explain the known result that they lead to the SBP. In Monte Carlo experiments we find that estimators with the SBP have mean-square error that is smaller and less sensitive to bandwidth than those that do not have the SBP.</p> </abstract>
<abstract> <p>This paper examines the problem of measuring intellectual influence based on data on citations between scholarly publications. We follow an axiomatic approach and find that the properties of invariance to reference intensity, weak homogeneity, weak consistency, and invariance to splitting of journals characterize a unique ranking method. This method is different from those regularly used in economics and other social sciences.</p> </abstract>
<abstract> <p>Recent theoretical work has shown the importance of measuring microeconomic uncertainty for models of both general and partial equilibrium under imperfect insurance. In this paper the assumption of i.i.d. income innovations used in previous empirical studies is removed and the focus of the analysis is placed on models for the conditional variance of income shocks, which is related to the measure of risk emphasized by the theory. We first discriminate amongst various models of earnings determination that separate income shocks into idiosyncratic transitory and permanent components. We allow for education- and time-specific differences in the stochastic process for earnings and for measurement error. The conditional variance of the income shocks is modelled as a parsimonious ARCH process with both observable and unobserved heterogeneity. The empirical analysis is conducted on data drawn from the 1967-1992 Panel Study of Income Dynamics. We find strong evidence of sizeable ARCH effects as well as evidence of unobserved heterogeneity in the variances.</p> </abstract>
<abstract> <p>This paper presents a solution to an important econometric problem, namely the root n consistent estimation of nonlinear models with measurement errors in the explanatory variables, when one repeated observation of each mismeasured regressor is available. While a root n consistent estimator has been derived for polynomial specifications (see Hausman, Ichimura, Newey, and Powell (1991)), such an estimator for general nonlinear specifications has so far not been available. Using the additional information provided by the repeated observation, the suggested estimator separates the measurement error from the "true" value of the regressors thanks to a useful property of the Fourier transform: The Fourier transform converts the integral equations that relate the distribution of the unobserved "true" variables to the observed variables measured with error into algebraic equations. The solution to these equations yields enough information to identify arbitrary moments of the "true," unobserved variables. The value of these moments can then be used to construct any estimator that can be written in terms of moments, including traditional linear and nonlinear least squares estimators, or general extremum estimators. The proposed estimator is shown to admit a representation in terms of an influence function, thus establishing its root n consistency and asymptotic normality. Monte Carlo evidence and an application to Engel curve estimation illustrate the usefulness of this new approach.</p> </abstract>
<abstract> <p>We analyze bidding behavior in auctions when risk-averse buyers bid for a good whose value is risky. We show that when the risk in the valuations increases, DARA bidders will reduce their bids by more than the appropriate increase in the risk premium. Ceteris paribus, buyers will be better off bidding for a more risky object in first price, second price, and English auctions with affiliated common (interdependent) values. This "precautionary bidding" effect arises because the expected marginal utility of income increases with risk, so buyers are reluctant to bid so highly. We also show that precautionary bidding behavior can make DARA bidders prefer bidding in a common values setting to bidding in a private values one when risk-neutral or CARA bidders would be indifferent. Thus the potential for a "winner's curse" can be a blessing for rational DARA bidders.</p> </abstract>
<abstract> <p>Preferences exhibit relative consumption effects if a person's satisfaction with their own consumption appears to depend upon how much others are consuming. This paper examines a model of an evolutionary environment in which Nature optimally builds relative consumption effects into preferences in order to compensate for incomplete environmental information.</p> </abstract>
<abstract> <p>To study the behavior of agents who are susceptible to temptation in infinite horizon consumption problems under uncertainty, we define and characterize dynamic self-control (DSC) preferences. DSC preferences are recursive and separable. In economies with DSC agents, equilibria exist but may be inefficient; in such equilibria, steady state consumption is independent of initial endowments and increases in self-control. Increasing the preference for commitment while keeping self-control constant increases the equity premium. Removing nonbinding constraints changes equilibrium allocations and prices. Debt contracts can be sustained even if the only feasible punishment for default is the termination of the contract.</p> </abstract>
<abstract> <p>Intestinal helminths-including hookworm, roundworm, whipworm, and schistosomiasis--infect more than one-quarter of the world's population. Studies in which medical treatment is randomized at the individual level potentially doubly underestimate the benefits of treatment, missing externality benefits to the comparison group from reduced disease transmission, and therefore also underestimating benefits for the treatment group. We evaluate a Kenyan project in which school-based mass treatment with deworming drugs was randomly phased into schools, rather than to individuals, allowing estimation of overall program effects. The program reduced school absenteeism in treatment schools by one-quarter, and was far cheaper than alternative ways of boosting school participation. Deworming substantially improved health and school participation among untreated children in both treatment schools and neighboring schools, and these externalities are large enough to justify fully subsidizing treatment. Yet we do not find evidence that deworming improved academic test scores.</p> </abstract>
<abstract> <p>In an effort to improve the small sample properties of generalized method of moments (GMM) estimators, a number of alternative estimators have been suggested. These include empirical likelihood (EL), continuous updating, and exponential tilting estimators. We show that these estimators share a common structure, being members of a class of generalized empirical likelihood (GEL) estimators. We use this structure to compare their higher order asymptotic properties. We find that GEL has no asymptotic bias due to correlation of the moment functions with their Jacobian, eliminating an important source of bias for GMM in models with endogeneity. We also find that EL has no asymptotic bias from estimating the optimal weight matrix, eliminating a further important source of bias for GMM in panel data models. We give bias corrected GMM and GEL estimators. We also show that bias corrected EL inherits the higher order property of maximum likelihood, that it is higher order asymptotically efficient relative to the other bias corrected estimators.</p> </abstract>
<abstract> <p>We consider bilateral matching problems where each person views those on the other side of the market as either acceptable or unacceptable: an acceptable mate is preferred to remaining single, and the latter to an unacceptable mate; all acceptable mates are welfare-wise identical. Using randomization, many efficient and fair matching methods define strategyproof revelation mechanisms. Randomly selecting a priority ordering of the participants is a simple example. Equalizing as much as possible the probability of getting an acceptable mate across all participants stands out for its normative and incentives properties: the profile of probabilities is Lorenz dominant, and the revelation mechanism is group-strategyproof for each side of the market. Our results apply to the random assignment problem as well.</p> </abstract>
<abstract> <p>The Amsterdam auction has been used to sell real estate in the Dutch capital for centuries. By awarding a premium to the highest losing bidder, the Amsterdam auction favors weak bidders without having the implementation difficulties of Myerson's (1981) optimal auction. In a series of experiments, we compare the standard first-price and English auctions, the optimal auction, and two variants of the Amsterdam auction. With strongly asymmetric bidders, the second-price Amsterdam auction raises substantially more revenues than standard formats and only slightly less than the optimal auction.</p> </abstract>
<abstract> <p>In this paper we propose a new estimator for a model with one endogenous regressor and many instrumental variables. Our motivation comes from the recent literature on the poor properties of standard instrumental variables estimators when the instrumental variables are weakly correlated with the endogenous regressor. Our proposed estimator puts a random coefficients structure on the relation between the endogenous regressor and the instruments. The variance of the random coefficients is modelled as an unknown parameter. In addition to proposing a new estimator, our analysis yields new insights into the properties of the standard two-stage least squares (TSLS) and limited-information maximum likelihood (LIML) estimators in the case with many weak instruments. We show that in some interesting cases, TSLS and LIML can be approximated by maximizing the random effects likelihood subject to particular constraints. We show that statistics based on comparisons of the unconstrained estimates of these parameters to the implicit TSLS and LIML restrictions can be used to identify settings when standard large sample approximations to the distributions of TSLS and LIML are likely to perform poorly. We also show that with many weak instruments, LIML confidence intervals are likely to have under-coverage, even though its finite sample distribution is approximately centered at the true value of the parameter. In an application with real data and simulations around this data set, the proposed estimator performs markedly better than TSLS and LIML, both in terms of coverage rate and in terms of risk.</p> </abstract>
<abstract> <p>This paper uses the marginal treatment effect (MTE) to unify the nonparametric literature on treatment effects with the econometric literature on structural estimation using a nonparametric analog of a policy invariant parameter; to generate a variety of treatment effects from a common semiparametric functional form; to organize the literature on alternative estimators; and to explore what policy questions commonly used estimators in the treatment effect literature answer. A fundamental asymmetry intrinsic to the method of instrumental variables (IV) is noted. Recent advances in IV estimation allow for heterogeneity in responses but not in choices, and the method breaks down when both choice and response equations are heterogeneous in a general way.</p> </abstract>
<abstract> <p>Extensive-form market games typically have a large number of noncompetitive equilibria. In this paper, we argue that the complexity of noncompetitive behavior provides a justification for competitive equilibrium in the sense that if rational agents have an aversion to complexity (at the margin), then maximizing behavior will result in simple behavioral rules and hence in a competitive outcome. For this purpose, we use a class of extensive-form dynamic matching and bargaining games with a finite number of agents. In particular, we consider markets with heterogeneous buyers and sellers and deterministic, exogenous, sequential matching rules, although the results can be extended to other matching processes. If the complexity costs of implementing strategies enter players' preferences lexicographically with the standard payoff, then every equilibrium strategy profile induces a competitive outcome.</p> </abstract>
<abstract> <p> We investigate a class of semiparametric ARCH(∞) models that includes as a special case the partially nonparametric (PNP) model introduced by Engle and Ng (1993) and which allows for both flexible dynamics and flexible function form with regard to the "news impact" function. We show that the functional part of the model satisfies a type II linear integral equation and give simple conditions under which there is a unique solution. We propose an estimation method that is based on kernel smoothing and profiled likelihood. We establish the distribution theory of the parametric components and the pointwise distribution of the nonparametric component of the model. We also discuss efficiency of both the parametric part and the nonparametric part. We investigate the performance of our procedures on simulated data and on a sample of S&amp;P500 index returns. We find evidence of asymmetric news impact functions, consistent with the parametric analysis. </p> </abstract>
<abstract> <p>Entropy is a classical statistical concept with appealing properties. Establishing asymptotic distribution theory for smoothed nonparametric entropy measures of dependence has so far proved challenging. In this paper, we develop an asymptotic theory for a class of kernel-based smoothed nonparametric entropy measures of serial dependence in a time-series context. We use this theory to derive the limiting distribution of Granger and Lin's (1994) normalized entropy measure of serial dependence, which was previously not available in the literature. We also apply our theory to construct a new entropy-based test for serial dependence, providing an alternative to Robinson's (1991) approach. To obtain accurate inferences, we propose and justify a consistent smoothed bootstrap procedure. The naive bootstrap is not consistent for our test. Our test is useful in, for example, testing the random walk hypothesis, evaluating density forecasts, and identifying important lags of a time series. It is asymptotically locally more powerful than Robinson's (1991) test, as is confirmed in our simulation. An application to the daily S&amp;P 500 stock price index illustrates our approach.</p> </abstract>
<abstract> <p>In a number of semiparametric models, smoothing seems necessary in order to obtain estimates of the parametric component which are asymptotically normal and converge at parametric rate. However, smoothing can inflate the error in the normal approximation, so that refined approximations are of interest, especially in sample sizes that are not enormous. We show that a bootstrap distribution achieves a valid Edgeworth correction in the case of density-weighted averaged derivative estimates of semiparametric index models. Approaches to bias reduction are discussed. We also develop a higher-order expansion to show that the bootstrap achieves a further reduction in size distortion in the case of two-sided testing. The finite-sample performance of the methods is investigated by means of Monte Carlo simulations from a Tobit model.</p> </abstract>
<abstract> <p>Previous work on the denomination structure of currency treats as exogenous the distribution of transactions and the denominations held by people. Here, by way of a matching model, both are endogenous. In the model, trades in pairwise meetings alternate in time with the opportunity to freely choose a portfolio of denominations and there is a trade-off between the benefits of small-denomination money for transacting and the costliness of carrying a large quantity of small-denomination money. For a given denomination structure, a monetary steady state is shown to exist. The model implies that too small denominations are abandoned.</p> </abstract>
<abstract> <p>We present a new method for solving asset pricing models, which yields an analytic price-dividend function of one state variable. To illustrate our method we give a detailed analysis of Abel's asset pricing model. A function is analytic in an open interval if it can be represented as a convergent power series near every point of that interval. In addition to allowing us to solve for the exact equilibrium price-dividend function, the analyticity property also lets us assess the accuracy of any numerical solution procedure used in the asset pricing literature.</p> </abstract>
<abstract> <p>For stationary time series models with serial correlation, we consider generalized method of moments (GMM) estimators that use heteroskedasticity and autocorrelation consistent (HAC) positive definite weight matrices and generalized empirical likelihood (GEL) estimators based on smoothed moment conditions. Following the analysis of Newey and Smith (2004) for independent observations, we derive second order asymptotic biases of these estimators. The inspection of bias expressions reveals that the use of smoothed GEL, in contrast to GMM, removes the bias component associated with the correlation between the moment function and its derivative, while the bias component associated with third moments depends on the employed kernel function. We also analyze the case of no serial correlation, and find that the seemingly unnecessary smoothing and HAC estimation can reduce the bias for some of the estimators.</p> </abstract>
<abstract> <p>We exhibit a large class of simple rules of behavior, which we call adaptive heuristics, and show that they generate rational behavior in the long run. These adaptive heuristics are based on natural regret measures, and may be viewed as a bridge between rational and behavioral viewpoints. Taken together, the results presented here establish a solid connection between the dynamic approach of adaptive heuristics and the static approach of correlated equilibria.</p> </abstract>
<abstract> <p>How much discretion should the monetary authority have in setting its policy? This question is analyzed in an economy with an agreed-upon social welfare function that depends on the economy's randomly fluctuating state. The monetary authority has private information about that state. Well designed rules trade off society's desire to give the monetary authority discretion to react to its private information against society's need to prevent that authority from giving in to the temptation to stimulate the economy with unexpected inflation, the time inconsistency problem. Although this dynamic mechanism design problem seems complex, its solution is simple: legislate an inflation cap. The optimal degree of monetary policy discretion turns out to shrink as the severity of the time inconsistency problem increases relative to the importance of private information. In an economy with a severe time inconsistency problem and unimportant private information, the optimal degree of discretion is none.</p> </abstract>
<abstract> <p>This paper addresses how changing the admission and financial aid rules at colleges affects future earnings. I estimate a structural model of the following decisions by individuals: where to submit applications, which school to attend, and what field to study. The model also includes decisions by schools as to which students to accept and how much financial aid to offer. Simulating how black educational choices would change were they to face the white admission and aid rules shows that race-based advantages had little effect on earnings. However, removing race-based advantages does affect black educational outcomes. In particular, removing advantages in admissions substantially decreases the number of black students at top-tier schools, while removing advantages in financial aid causes a decrease in the number of blacks who attend college.</p> </abstract>
<abstract> <p>This paper provides weak conditions under which there is nonparametric interval identification of local features of a structural function that depends on a discrete endogenous variable and is nonseparable in latent variates. The function delivers values of a discrete or continuous outcome and instruments may be discrete valued. Application of the analog principle leads to quantile regression based interval estimators of values and partial differences of structural functions. The results are used to investigate the nonparametric identifying power of the quarter-of-birth instruments used in Angrist and Krueger's 1991 study of the returns to schooling.</p> </abstract>
<abstract> <p>This paper considers regression models for cross-section data that exhibit cross-section dependence due to common shocks, such as macroeconomic shocks. The paper analyzes the properties of least squares (LS) estimators in this context. The results of the paper allow for any form of cross-section dependence and heterogeneity across population units. The probability limits of the LS estimators are determined, and necessary and sufficient conditions are given for consistency. The asymptotic distributions of the estimators are found to be mixed normal after recentering and scaling. The t, Wald, and F statistics are found to have asymptotic standard normal, χ<sup>2</sup>, and scaled χ<sup>2</sup>distributions, respectively, under the null hypothesis when the conditions required for consistency of the parameter under test hold. However, the absolute values of t, Wald, and F statistics are found to diverge to infinity under the null hypothesis when these conditions fail. Confidence intervals exhibit similarly dichotomous behavior. Hence, common shocks are found to be innocuous in some circumstances, but quite problematic in others. Models with factor structures for errors and regressors are considered. Using the general results, conditions are determined under which consistency of the LS estimators holds and fails in models with factor structures. The results are extended to cover heterogeneous and functional factor structures in which common factors have different impacts on different population units.</p> </abstract>
<abstract> <p>In this paper, I consider a dynamic economy in which a government needs to finance a stochastic process of purchases. The agents in the economy are privately informed about their skills, which evolve stochastically over time; I impose no restriction on the stochastic evolution of skills. I construct a tax system that implements a symmetric constrained Pareto optimal allocation. The tax system is constrained to be linear in an agent's wealth, but can be arbitrarily nonlinear in his current and past labor incomes. I find that wealth taxes in a given period depend on the individual's labor income in that period and previous ones. However, in any period, the expectation of an agent's wealth tax rate in the following period is zero. As well, the government never collects any net revenue from wealth taxes.</p> </abstract>
<abstract> <p>There is evidence that people do not fully take into account how other people's actions depend on these other people's information. This paper defines and applies a new equilibrium concept in games with private information, cursed equilibrium, which assumes that each player correctly predicts the distribution of other players' actions, but underestimates the degree to which these actions are correlated with other players' information. We apply the concept to common-values auctions, where cursed equilibrium captures the widely observed phenomenon of the winner's curse, and to bilateral trade, where cursedness predicts trade in adverse-selections settings for which conventional analysis predicts no trade. We also apply cursed equilibrium to voting and signalling models. We test a single-parameter variant of our model that embeds Bayesian Nash equilibrium as a special case and find that parameter values that correspond to cursedness fit a broad range of experimental datasets better than the parameter value that corresponds to Bayesian Nash equilibrium.</p> </abstract>
<abstract> <p> This paper analyzes the conditions under which consistent estimation can be achieved in instrumental variables (IV) regression when the available instruments are weak and the number of instruments, K<sub>n</sub>, goes to infinity with the sample size. We show that consistent estimation depends importantly on the strength of the instruments as measured by r<sub>n</sub>, the rate of growth of the so-called concentration parameter, and also on K<sub>n</sub>. In particular, when K<sub>n</sub>→ ∞, the concentration parameter can grow, even if each individual instrument is only weakly correlated with the endogenous explanatory variables, and consistency of certain estimators can be established under weaker conditions than have previously been assumed in the literature. Hence, the use of many weak instruments may actually improve the performance of certain point estimators. More specifically, we find that the limited information maximum likelihood (LIML) estimator and the bias-corrected two-stage least squares (B2SLS) estimator are consistent when<tex-math>$\sqrt{K_{n}}/r_{n}$</tex-math>→ 0, while the two-stage least squares (2SLS) estimator is consistent only if<tex-math>$K_{n}/r_{n}$</tex-math>→ 0 as n → ∞. These consistency results suggest that LIML and B2SLS are more robust to instrument weakness than 2SLS. </p> </abstract>
<abstract> <p>Each agent in a finite set requests an integer quantity of an idiosyncratic good; the resulting total cost must be shared among the participating agents. The Aumann-Shapley prices are given by the Shapley value of the game where each unit of each good is regarded as a distinct player. The Aumann-Shapley cost-sharing method charges to an agent the sum of the prices attached to the units she consumes. We show that this method is characterized by the two standard axioms of Additivity and Dummy, and the property of No Merging or Splitting: agents never find it profitable to split or to merge their consumptions. We offer a variant of this result using the No Reshuffling condition: the total cost share paid by a group of agents who consume perfectly substitutable goods depends only on their aggregate consumption. We extend this characterization to the case where agents are allowed to consume bundles of goods.</p> </abstract>
<abstract> <p>To predict choice behavior, the standard practice of economists has been to infer decision processes from data on observed choices. When decision makers act with partial information, economists typically assume that persons form probabilistic expectations for unknown quantities and maximize expected utility. Observed choices may be consistent with many alternative specifications of preferences and expectations, so researchers commonly assume particular sorts of expectations. It would be better to measure expectations in the form called for by modern economic theory; that is, subjective probabilities. Data on expectations can be used to relax or validate assumptions about expectations. Since the early 1990's, economists have increasingly undertaken to elicit from survey respondents probabilistic expectations of significant personal events. This article discusses the history underlying the new literature, describes some of what has been learned thus far, and looks ahead towards making further progress.</p> </abstract>
<abstract> <p>Previous research has shown that under a suitable no-jump condition, the price of a defaultable security is equal to its risk-neutral expected discounted cash flows if a modified discount rate is introduced to account for the possibility of default. Below, we generalize this result by demonstrating that one can always value defaultable claims using expected risk-adjusted discounting provided that the expectation is taken under a slightly modified probability measure. This new probability measure puts zero probability on paths where default occurs prior to the maturity, and is thus only absolutely continuous with respect to the risk-neutral probability measure. After establishing the general result and discussing its relation with the existing literature, we investigate several examples for which the no-jump condition fails. Each example illustrates the power of our general formula by providing simple analytic solutions for the prices of defaultable securities.</p> </abstract>
<abstract> <p>This paper uses political reservations for women in India to study the impact of women's leadership on policy decisions. Since the mid-1990's, one third of Village Council head positions in India have been randomly reserved for a woman: In these councils only women could be elected to the position of head. Village Councils are responsible for the provision of many local public goods in rural areas. Using a dataset we collected on 265 Village Councils in West Bengal and Rajasthan, we compare the type of public goods provided in reserved and unreserved Village Councils. We show that the reservation of a council seat affects the types of public goods provided. Specifically, leaders invest more in infrastructure that is directly relevant to the needs of their own genders.</p> </abstract>
<abstract> <p>We study inference in structural models with a jump in the conditional density, where location and size of the jump are described by regression curves. Two prominent examples are auction models, where the bid density jumps from zero to a positive value at the lowest cost, and equilibrium job-search models, where the wage density jumps from one positive level to another at the reservation wage. General inference in such models remained a long-standing, unresolved problem, primarily due to nonregularities and computational difficulties caused by discontinuous likelihood functions. This paper develops likelihood-based estimation and inference methods for these models, focusing on optimal (Bayes) and maximum likelihood procedures. We derive convergence rates and distribution theory, and develop Bayes and Wald inference. We show that Bayes estimators and confidence intervals are attractive both theoretically and computationally, and that Bayes confidence intervals, based on posterior quantiles, provide a valid large sample inference method.</p> </abstract>
<abstract> <p>GARCH models are commonly used as latent processes in econometrics, financial economics, and macroeconomics. Yet no exact likelihood analysis of these models has been provided so far. In this paper we outline the issues and suggest a Markov chain Monte Carlo algorithm which allows the calculation of a classical estimator via the simulated EM algorithm or a Bayesian solution in O(T) computational operations, where T denotes the sample size. We assess the performance of our proposed algorithm in the context of both artificial examples and an empirical application to 26 UK sectorial stock returns, and compare it to existing approximate solutions.</p> </abstract>
<abstract> <p>Wavelet analysis is a new mathematical method developed as a unified field of science over the last decade or so. As a spatially adaptive analytic tool, wavelets are useful for capturing serial correlation where the spectrum has peaks or kinks, as can arise from persistent dependence, seasonality, and other kinds of periodicity. This paper proposes a new class of generally applicable wavelet-based tests for serial correlation of unknown form in the estimated residuals of a panel regression model, where error components can be one-way or two-way, individual and time effects can be fixed or random, and regressors may contain lagged dependent variables or deterministic/stochastic trending variables. Our tests are applicable to unbalanced heterogenous panel data. They have a convenient null limit N(0,1) distribution. No formulation of an alternative model is required, and our tests are consistent against serial correlation of unknown form even in the presence of substantial inhomogeneity in serial correlation across individuals. This is in contrast to existing serial correlation tests for panel models, which ignore inhomogeneity in serial correlation across individuals by assuming a common alternative, and thus have no power against the alternatives where the average of serial correlations among individuals is close to zero. We propose and justify a data-driven method to choose the smoothing parameter--the finest scale in wavelet spectral estimation, making the tests completely operational in practice. The data-driven finest scale automatically converges to zero under the null hypothesis of no serial correlation and diverges to infinity as the sample size increases under the alternative, ensuring the consistency of our tests. Simulation shows that our tests perform well in small and finite samples relative to some existing tests.</p> </abstract>
<abstract> <p>An extension of Condorcet's paradox by McGarvey (1953) asserts that for every asymmetric relation R on a finite set of candidates there is a strict-preferences voter profile that has the relation R as its strict simple majority relation. We prove that McGarvey's theorem can be extended to arbitrary neutral monotone social welfare functions that can be described by a strong simple game G if the voting power of each individual, measured by the Shapley-Shubik power index, is sufficiently small. Our proof is based on an extension to another classic result concerning the majority rule. Condorcet studied an election between two candidates in which the voters' choices are random and independent and the probability of a voter choosing the first candidate is p &gt; 1/2. Condorcet's jury theorem asserts that if the number of voters tends to infinity then the probability that the first candidate will be elected tends to one. We prove that this assertion extends to a sequence of arbitrary monotone strong simple games if and only if the maximum voting power for all individuals tends to zero.</p> </abstract>
<abstract> <p>The theory of global games has shown that coordination games with multiple equilibria may have a unique equilibrium if certain parameters of the payoff function are private information instead of common knowledge. We report the results of an experiment designed to test the predictions of this theory. Comparing sessions with common and private information, we observe only small differences in behavior. For common information, subjects coordinate on threshold strategies that deviate from the global game solution towards the payoff-dominant equilibrium. For private information, thresholds are closer to the global game solution than for common information. Variations in the payoff function affect behavior as predicted by comparative statics of the global game solution. Predictability of coordination points is about the same for both information conditions.</p> </abstract>
<abstract> <p>In econometrics, models stated as conditional moment restrictions are typically estimated by means of the generalized method of moments (GMM). The GMM estimation procedure can render inconsistent estimates since the number of arbitrarily chosen instruments is finite. In fact, consistency of the GMM estimators relies on additional assumptions that imply unclear restrictions on the data generating process. This article introduces a new, simple and consistent estimation procedure for these models that is directly based on the definition of the conditional moments. The main feature of our procedure is its simplicity, since its implementation does not require the selection of any user-chosen number, and statistical inference is straightforward since the proposed estimator is asymptotically normal. In addition, we suggest an asymptotically efficient estimator constructed by carrying out one Newton-Raphson step in the direction of the efficient GMM estimator.</p> </abstract>
<abstract> <p>Agents' valuations are interdependent if they depend on the signals, or types, of all agents. Under the implicit assumption that agents cannot observe their outcome-decision payoffs, previous literature has shown that with interdependent valuations and independent signals, efficient design is impossible. This paper shows that an efficient mechanism exists in an environment where first the final outcome (e.g., allocation of the goods) is determined, then the agents observe their own outcome-decision payoffs, and then final transfers are made.</p> </abstract>
<abstract> <p>This paper examines direct broadcast satellites (DBS) as a competitor to cable. We first estimate a structural consumer level demand system for satellite, basic cable, premium cable and local antenna using micro data on almost 30,000 households in 317 markets, including extensive controls for unobserved product quality and allowing the distribution of unobserved tastes to follow a fully flexible multivariate normal distribution. The estimated elasticity of expanded basic is about -1.5, with the demand for premium cable and DBS more elastic. The results identify strong correlations in the taste for different products not captured in conventional logit models. Estimates of the supply response of cable suggest that without DBS entry cable prices would be about 15 percent higher and cable quality would fall. We find a welfare gain of between $127 and $190 per year (aggregate $2.5 billion) for satellite buyers, and about $50 (aggregate $3 billion) for cable subscribers.</p> </abstract>
<abstract> <p>This paper considers learning rules for environments in which little prior and feedback information is available to the decision maker. Two properties of such learning rules are studied: absolute expediency and monotonicity. Both require that some aspect of the decision maker's performance improves from the current period to the next. The paper provides some necessary, and some sufficient conditions for these properties. It turns out that there is a large variety of learning rules that have the properties. However, all learning rules that have these properties are related to the replicator dynamics of evolutionary game theory. For the case in which there are only two actions, it is shown that one of the absolutely expedient learning rules dominates all others.</p> </abstract>
<abstract> <p>We study the long-run sustainability of reputations in games with imperfect public monitoring. It is impossible to maintain a permanent reputation for playing a strategy that does not play an equilibrium of the game without uncertainty about types. Thus, a player cannot indefinitely sustain a reputation for noncredible behavior in the presence of imperfect monitoring.</p> </abstract>
<abstract> <p>This paper analyzes models of securities markets with a single strategic informed trader and competitive market makers. In one version, uninformed trades arrive as a Brownian motion and market makers see only the order imbalance, as in Kyle (1985). In the other version, uninformed trades arrive as a Poisson process and market makers see individual trades. This is similar to the Glosten-Milgrom (1985) model, except that we allow the informed trader to optimize his times of trading. We show there is an equilibrium in the Glosten-Milgrom-type model in which the informed trader plays a mixed strategy (a point process with stochastic intensity). In this equilibrium, informed and uninformed trades arrive probabilistically, as Glosten and Milgrom assume. We study a sequence of such markets in which uninformed trades become smaller and arrive more frequently, approximating a Brownian motion. We show that the equilibria of the Glosten-Milgrom model converge to the equilibrium of the Kyle model.</p> </abstract>
<abstract> <p> This paper investigates a generalized method of moments (GMM) approach to the estimation of autoregressive roots near unity with panel data and incidental deterministic trends. Such models arise in empirical econometric studies of firm size and in dynamic panel data modeling with weak instruments. The two moment conditions in the GMM approach are obtained by constructing bias corrections to the score functions under OLS and GLS detrending, respectively. It is shown that the moment condition under GLS detrending corresponds to taking the projected score on the Bhattacharya basis, linking the approach to recent work on projected score methods for models with infinite numbers of nuisance parameters (Waterman and Lindsay (1998)). Assuming that the localizing parameter takes a nonpositive value, we establish consistency of the GMM estimator and find its limiting distribution. A notable new finding is that the GMM estimator has convergence rate n&lt;sup&gt;1/6&lt;/sup&gt;, slower than √n, when the true localizing parameter is zero (i.e., when there is a panel unit root) and the deterministic trends in the panel are linear. These results, which rely on boundary point asymptotics, point to the continued difficulty of distinguishing unit roots from local alternatives, even when there is an infinity of additional data. </p> </abstract>
<abstract> <p>We study strategic voting after weakening the notion of strategy-proofness to Ordinal Bayesian Incentive Compatibility (OBIC). Under OBIC, truth-telling is required to maximize the expected utility of every voter, expected utility being computed with respect to the voter's prior beliefs and under the assumption that everybody else is also telling the truth. We show that for a special type of priors, i.e., the uniform priors, there exists a large class of social choice functions that are OBIC. However, for priors that are generic in the set of independent beliefs, a social choice function is OBIC only if it is dictatorial. This result underlines the robustness of the Gibbard-Satterthwaite Theorem.</p> </abstract>
<abstract> <p>We show that optimal monetary and fiscal policies are time consistent for a class of economies often used in applied work, economies appealing because they are consistent with the growth facts. We establish our results in two steps. We first show that for this class of economies, the Friedman rule of setting nominal interest rates to zero is optimal under commitment. We then show that optimal policies are time consistent if the Friedman rule is optimal. For our benchmark economy in which the time consistency problem is most severe, the converse also holds: if optimal policies are time consistent, then the Friedman rule is optimal.</p> </abstract>
<abstract> <p> The local Whittle (or Gaussian semiparametric) estimator of long range dependence, proposed by Künsch (1987) and analyzed by Robinson (1995a), has a relatively slow rate of convergence and a finite sample bias that can be large. In this paper, we generalize the local Whittle estimator to circumvent these problems. Instead of approximating the short-run component of the spectrum, φ(λ), by a constant in a shrinking neighborhood of frequency zero, we approximate its logarithm by a polynomial. This leads to a "local polynomial Whittle" (LPW) estimator. We specify a data-dependent adaptive procedure that adjusts the degree of the polynomial to the smoothness of φ(λ) at zero and selects the bandwidth. The resulting "adaptive LPW" estimator is shown to achieve the optimal rate of convergence, which depends on the smoothness of φ(λ) at zero, up to a logarithmic factor. </p> </abstract>
<abstract> <p>Several experimental studies have provided evidence that suggest indifference curves have a kink around the current endowment level. These results, which clearly contradict closely held economic doctrines, have led some influential commentators to call for an entirely new economic paradigm to displace conventional neoclassical theory--e.g., prospect theory, which invokes psychological effects. This paper pits neoclassical theory against prospect theory by investigating data drawn from more than 375 subjects actively participating in a well-functioning marketplace. The pattern of results suggests that prospect theory adequately organizes behavior among inexperienced consumers, but consumers with intense market experience behave largely in accordance with neoclassical predictions. Moreover, the data are consistent with the notion that consumers learn to overcome the endowment effect in situations beyond specific problems they have previously encountered. This "transference of behavior" across domains has important implications in both a positive and normative sense.</p> </abstract>
<abstract> <p>We study fairness in economies with one private good and one partially excludable nonrival good. A social ordering function determines for each profile of preferences an ordering of all conceivable allocations. We propose the following Free Lunch Aversion condition: if the private good contributions of two agents consuming the same quantity of the nonrival good have opposite signs, reducing that gap improves social welfare. This condition, combined with the more standard requirements of Unanimous Indifference and Responsiveness, delivers a form of welfare egalitarianism in which an agent's welfare is measured by the quantity of the nonrival good that, consumed at no cost, would leave her indifferent to the bundle she is assigned.</p> </abstract>
<abstract> <p>We establish consistency and asymptotic normality of the quasi-maximum likelihood estimator in the linear ARCH model. Contrary to the existing literature, we allow the parameters to be in the region where no stationary version of the process exists. This implies that the estimator is always asymptotically normal.</p> </abstract>
<abstract> <p>A systems cointegration rank test is proposed that is applicable for vector autoregressive (VAR) processes with a structural shift at unknown time. The structural shift is modeled as a simple shift in the level of the process. It is proposed to estimate the break date first on the basis of a full unrestricted VAR model. Two alternative estimators are considered and their asymptotic properties are derived. In the next step the deterministic part of the process including the shift size is estimated and the series are adjusted by subtracting the estimated deterministic part. A Johansen type test for the cointegrating rank is applied to the adjusted series. The test statistic is shown to have a well-known asymptotic null distribution that does not depend on the break date. The performance of the procedure in small samples is investigated by simulations.</p> </abstract>
<abstract> <p>One of the most striking changes in the U.S. economy over the past 50 years has been the growth in the service sector. Between 1950 and 2000, service-sector employment grew from 57 to 75 percent of total employment. However, over this time, the real hourly wage in the service sector grew only slightly faster than in the goods sector. In this paper, we assess whether or not the essential constancy of the relative wage implies that individuals face small costs of switching sectors, and we quantify the relative importance of labor supply and demand factors in the growth of the service sector. We specify and estimate a two-sector labor market equilibrium model that allows us to address these empirical issues in a unified framework. Our estimates imply that there are large mobility costs: output in both sectors would have been double their current levels if these mobility costs had been zero. In addition, we find that demand-side factors, that is, technological change and movements in product and capital prices, were responsible for the growth of the service sector.</p> </abstract>
<abstract> <p> We consider large double auctions with private values. Values need be neither symmetric nor independent. Multiple units may be owned or desired. Participation may be stochastic. We introduce a very mild notion of "a little independence." We prove that all nontrivial equilibria of auctions that satisfy this notion are asymptotically efficient. For any α &gt; 0, inefficiency disappears at rate<tex-math>$1/n^{2-\alpha}$</tex-math>. </p> </abstract>
<abstract> <p>This paper studies the econometrics of computed dynamic models. Since these models generally lack a closed-form solution, their policy functions are approximated by numerical methods. Hence, the researcher can only evaluate an approximated likelihood associated with the approximated policy function rather than the exact likelihood implied by the exact policy function. What are the consequences for inference of the use of approximated likelihoods? First, we find conditions under which, as the approximated policy function converges to the exact policy, the approximated likelihood also converges to the exact likelihood. Second, we show that second order approximation errors in the policy function, which almost always are ignored by researchers, have first order effects on the likelihood function. Third, we discuss convergence of Bayesian and classical estimates. Finally, we propose to use a likelihood ratio test as a diagnostic device for problems derived from the use of approximated likelihoods.</p> </abstract>
<abstract> <p>We develop and analyze a model of random choice and random expected utility. A decision problem is a finite set of lotteries that describe the feasible choices. A random choice rule associates with each decision problem a probability measure over choices. A random utility function is a probability measure over von Neumann-Morgenstern utility functions. We show that a random choice rule maximizes some random utility function if and only if it is mixture continuous, monotone (the probability that a lottery is chosen does not increase when other lotteries are added to the decision problem), extreme (lotteries that are not extreme points of the decision problem are chosen with probability 0), and linear (satisfies the independence axiom).</p> </abstract>
<abstract> <p>This paper provides a first order asymptotic theory for generalized method of moments (GMM) estimators when the number of moment conditions is allowed to increase with the sample size and the moment conditions may be weak. Examples in which these asymptotics are relevant include instrumental variable (IV) estimation with many (possibly weak or uninformed) instruments and some panel data models that cover moderate time spans and have correspondingly large numbers of instruments. Under certain regularity conditions, the GMM estimators are shown to converge in probability but not necessarily to the true parameter, and conditions for consistent GMM estimation are given. A general framework for the GMM limit distribution theory is developed based on epiconvergence methods. Some illustrations are provided, including consistent GMM estimation of a panel model with time varying individual effects, consistent limited information maximum likelihood estimation as a continuously updated GMM estimator, and consistent IV structural estimation using large numbers of weak or irrelevant instruments. Some simulations are reported.</p> </abstract>
<abstract> <p>This paper demonstrates how time consistency of the Ramsey policy-the optimal fiscal and monetary policy under commitment--can be achieved. Each government should leave its successor with a unique maturity structure for nominal and indexed debt, such that the marginal benefit of a surprise inflation exactly balances the marginal cost. Unlike in earlier papers on the topic, the result holds for quite general Ramsey policies, including time-varying polices with positive inflation and positive nominal interest rates. We compare our results with those in Persson, Persson, and Svensson (1987), Calvo and Obstfeld (1990), and Alvarez, Kehoe, and Neumeyer (2004).</p> </abstract>
<abstract> <p> A number of studies, most notably Crémer and McLean (1985, 1988), have shown that in generic type spaces that admit a common prior and are of a fixed finite size, an uninformed seller can design mechanisms that extract all the surplus from privately informed bidders. We show that this result hinges on the nonconvexity of such a family of priors. When the ambient family of priors is convex, generic priors do not allow for full surplus extraction provided that for at least one prior in this family, players' beliefs about other players' types do not pin down the players' own preferences. In particular, full surplus extraction is generically impossible in finite type spaces with a common prior. Similarly, generic priors on the universal type space do not allow for full surplus extraction. </p> </abstract>
<abstract> <p>Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N<sup>1/2</sup>-consistent in general and describe conditions under which matching estimators do attain N<sup>1/2</sup>-consistency. Second, we show that even in settings where matching estimators are N<sup>1/2</sup>-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.</p> </abstract>
<abstract> <p>We extend the standard model of general equilibrium with incomplete markets to allow for default and punishment by thinking of assets as pools. The equilibrating variables include expected delivery rates, along with the usual prices of assets and commodities. By reinterpreting the variables, our model encompasses a broad range of adverse selection and signalling phenomena in a perfectly competitive, general equilibrium framework. Perfect competition eliminates the need for lenders to compute how the size of their loan or the price they quote might affect default rates. It also makes for a simple equilibrium refinement, which we propose in order to rule out irrational pessimism about deliveries of untraded assets. We show that refined equilibrium always exists in our model, and that default, in conjunction with refinement, opens the door to a theory of endogenous assets. The market chooses the promises, default penalties, and quantity constraints of actively traded assets.</p> </abstract>
<abstract> <p>We analyze a game of strategic experimentation with two-armed bandits whose risky arm might yield payoffs after exponentially distributed random times. Free-riding causes an inefficiently low level of experimentation in any equilibrium where the players use stationary Markovian strategies with beliefs as the state variable. We construct the unique symmetric Markovian equilibrium of the game, followed by various asymmetric ones. There is no equilibrium where all players use simple cut-off strategies. Equilibria where players switch finitely often between experimenting and free-riding all yield a similar pattern of information acquisition, greater efficiency being achieved when the players share the burden of experimentation more equitably. When players switch roles infinitely often, they can acquire an approximately efficient amount of information, but still at an inefficient rate. In terms of aggregate payoffs, all these asymmetric equilibria dominate the symmetric one wherever the latter prescribes simultaneous use of both arms.</p> </abstract>
<abstract> <p>We study a one-sided offers bargaining game in which the buyer has private information about the value of the object and the seller has private information about his beliefs about the buyer's valuation. We show that this uncertainty about uncertainties dramatically changes the set of outcomes. In particular, second order beliefs can lead to a delay in reaching agreement even when the seller makes frequent offers. We show that not all types of second order beliefs lead to a delay. When the buyer assigns positive probability to the seller knowing the buyer's value, then delay not only can occur, but it must occur for a class of equilibria. However, in all other cases delay will never occur.</p> </abstract>
<abstract> <p>We show existence of equilibria in distributional strategies for a wide class of private value auctions, including the first general existence result for double auctions. The set of equilibria is invariant to the tie-breaking rule. The model incorporates multiple unit demands, all standard pricing rules, reserve prices, entry costs, and stochastic demand and supply. Valuations can be correlated and asymmetrically distributed. For double auctions, we show further that at least one equilibrium involves a positive volume of trade. The existence proof establishes new connections among existence techniques for discontinuous Bayesian games.</p> </abstract>
<abstract> <p>Fix finite pure strategy sets &lt;tex-math&gt;$S_{1},\ldots ,S_{n}$&lt;/tex-math&gt;, and let &lt;tex-math&gt;$S=S_{1}\times \cdots \times S_{n}$&lt;/tex-math&gt;. In our model of a random game the agents' payoffs are statistically independent, with each agent's payoff uniformly distributed on the unit sphere in &lt;tex-math&gt;${\Bbb R}^{S}$&lt;/tex-math&gt;. For given nonempty &lt;tex-math&gt;$T_{1}\subset S_{1},\ldots ,T_{n}\subset S_{n}$&lt;/tex-math&gt; we give a computationally implementable formula for the mean number of Nash equilibria in which each agent i's mixed strategy has support T&lt;sub&gt;i&lt;/sub&gt;. The formula is the product of two expressions. The first is the expected number of totally mixed equilibria for the truncated game obtained by eliminating pure strategies outside the sets T&lt;sub&gt;i&lt;/sub&gt;. The second may be construed as the "probability" that such an equilibrium remains an equilibrium when the strategies in the sets &lt;tex-math&gt;$S_{i}\backslash T_{i}$&lt;/tex-math&gt; become available.</p> </abstract>
<abstract> <p>We compare three market structures for monetary economies: bargaining (search equilibrium); price taking (competitive equilibrium); and price posting (competitive search equilibrium). We also extend work on the microfoundations of money by allowing a general matching technology and entry. We study how equilibrium and the effects of policy depend on market structure. Under bargaining, trade and entry are both inefficient, and inflation implies first-order welfare losses. Under price taking, the Friedman rule solves the first inefficiency but not the second, and inflation may actually improve welfare. Under posting, the Friedman rule yields the first best, and inflation implies second-order welfare losses.</p> </abstract>
<abstract> <p>This paper considers a general equilibrium model in which the distinction between uncertainty and risk is formalized by assuming agents have incomplete preferences over state-contingent consumption bundles, as in Bewley (1986). Without completeness, individual decision making depends on a set of probability distributions over the state space. A bundle is preferred to another if and only if it has larger expected utility for all probabilities in this set. When preferences are complete this set is a singleton, and the model reduces to standard expected utility. In this setting, we characterize Pareto optima and equilibria, and show that the presence of uncertainty generates robust indeterminacies in equilibrium prices and allocations for any specification of initial endowments. We derive comparative statics results linking the degree of uncertainty with changes in equilibria. Despite the presence of robust indeterminacies, we show that equilibrium prices and allocations vary continuously with underlying fundamentals. Equilibria in a standard risk economy are thus robust to adding small degrees of uncertainty. Finally, we give conditions under which some assets are not traded due to uncertainty aversion.</p> </abstract>
<abstract> <p>The ability of quantile regression models to characterize the heterogeneous impact of variables on different points of an outcome distribution makes them appealing in many economic applications. However, in observational studies, the variables of interest (e.g., education, prices) are often endogenous, making conventional quantile regression inconsistent and hence inappropriate for recovering the causal effects of these variables on the quantiles of economic outcomes. In order to address this problem, we develop a model of quantile treatment effects (QTE) in the presence of endogeneity and obtain conditions for identification of the QTE without functional form assumptions. The principal feature of the model is the imposition of conditions that restrict the evolution of ranks across treatment states. This feature allows us to overcome the endogeneity problem and recover the true QTE through the use of instrumental variables. The proposed model can also be equivalently viewed as a structural simultaneous equation model with nonadditive errors, where QTE can be interpreted as the structural quantile effects (SQE).</p> </abstract>
<abstract> <p>We study the monotonicity of the equilibrium bid with respect to the number of bidders n in affiliated private-value models of first-price sealed-bid auctions and prove the existence of a large class of such models in which the equilibrium bid function is not increasing in n. We moreover decompose the effect of a change in n on the bid level into a competition effect and an affiliation effect. The latter suggests to the winner of the auction that competition is less intense than she had thought before the auction. Since the affiliation effect can occur in both private- and common-value models, a negative relationship between the bid level and n does not allow one to distinguish between the two models and is also not necessarily (only) due to bidders taking account of the winner's curse.</p> </abstract>
<abstract> <p>We develop general model-free adjustment procedures for the calculation of unbiased volatility loss functions based on practically feasible realized volatility benchmarks. The procedures, which exploit recent nonparametric asymptotic distributional results, are both easy-to-implement and highly accurate in empirically realistic situations. We also illustrate that properly accounting for the measurement errors in the volatility forecast evaluations reported in the existing literature can result in markedly higher estimates for the true degree of return volatility predictability.</p> </abstract>
<abstract> <p>What on earth are economic theorists like me trying to accomplish? This paper discusses four dilemmas encountered by an economic theorist: The dilemma of absurd conclusions: Should we abandon a model if it produces absurd conclusions or should we regard a model as a very limited set of assumptions that will inevitably fail in some contexts? The dilemma of responding to evidence: Should our models be judged according to experimental results? The dilemma of modelless regularities: Should models provide the hypothesis for testing or are they simply exercises in logic that have no use in identifying regularities? The dilemma of relevance: Do we have the right to offer advice or to make statements that are intended to influence the real world?</p> </abstract>
<abstract> <p>We present an equilibrium model of the market for higher education. Our model simultaneously predicts student selection into institutions of higher education, financial aid, educational expenditures, and educational outcomes. We show that the model gives rise to a strict hierarchy of colleges that differ by the educational quality provided to the students. We also develop a new estimation procedure that exploits the observed variation in prices within colleges. Identification is based on variation in endowments and technology. It does not rely on observed variation in potentially endogenous characteristics of colleges such as peer quality measures and expenditures. We estimate the structural parameters using data collected by the National Center for Education Statistics and aggregate data from Peterson's and the National Science Foundation.</p> </abstract>
<abstract> <p>This paper provides an analysis of the asymptotic properties of Pareto optimal consumption allocations in a stochastic general equilibrium model with heterogeneous consumers. In particular, we investigate the market selection hypothesis that markets favor traders with more accurate beliefs. We show that in any Pareto-optimal allocation whether each consumer vanishes or survives is determined entirely by discount factors and beliefs. Whereas equilibrium allocations in economies with complete markets are Pareto optimal, our results characterize the limit behavior of these economies. We show that, all else equal, the market selects for consumers who use Bayesian learning with the truth in the support of their prior and selects among Bayesians according to the size of their parameter space. Finally, we show that in economies with incomplete markets, these conclusions may not hold. With incomplete markets, payoff functions can matter for long-run survival, and the market selection hypothesis fails.</p> </abstract>
<abstract> <p>This paper presents a new approach to estimation and inference in panel data models with a general multifactor error structure. The unobserved factors and the individual-specific errors are allowed to follow arbitrary stationary processes, and the number of unobserved factors need not be estimated. The basic idea is to filter the individual-specific regressors by means of cross-section averages such that asymptotically as the cross-section dimension (N) tends to infinity, the differential effects of unobserved common factors are eliminated. The estimation procedure has the advantage that it can be computed by least squares applied to auxiliary regressions where the observed regressors are augmented with cross-sectional averages of the dependent variable and the individual-specific regressors. A number of estimators (referred to as common correlated effects (CCE) estimators) are proposed and their asymptotic distributions are derived. The small sample properties of mean group and pooled CCE estimators are investigated by Monte Carlo experiments, showing that the CCE estimators have satisfactory small sample properties even under a substantial degree of heterogeneity and dynamics, and for relatively small values of N and T.</p> </abstract>
<abstract> <p>Building upon a continuous-time model of search with Nash bargaining in a stationary environment, we analyze the effect of changes in minimum wages on labor market outcomes and welfare. Although minimum wage increases may or may not lead to increases in unemployment in our model, they can be welfare-improving to labor market participants on both the supply and demand sides of the labor market. We discuss identification of the model using Current Population Survey data on accepted wages and unemployment durations, and show that by incorporating a limited amount of information from the demand side of the market it is possible to obtain credible and precise estimates of all primitive parameters. We show that the optimal minimum wage in 1996 depends critically on whether or not contact rates can be considered to be exogenous and we note that the limited variation in minimum wages makes testing this assumption problematic.</p> </abstract>
<abstract> <p>A contract with multiple agents may be susceptible to collusion. We show that agents' collusion imposes no cost in a large class of circumstances with risk neutral agents, including both uncorrelated and correlated types. In those circumstances, any payoff the principal can attain in the absence of collusion, including the second-best level, can be attained in the presence of collusion in a way robust to many aspects of collusion behavior. The collusion-proof implementation generalizes to a setting in which only a subset of agents may collude, provided that noncollusive agents' incentives can be protected via an ex post incentive compatible and ex post individually rational mechanism. Our collusion-proof implementation also sheds light on the extent to which hierarchical delegation of contracts can optimally respond to collusion.</p> </abstract>
<abstract> <p>We characterize dominant-strategy incentive compatibility with multidimensional types. A deterministic social choice function is dominant-strategy incentive compatible if and only if it is weakly monotone (W-Mon). The W-Mon requirement is the following: If changing one agent's type (while keeping the types of other agents fixed) changes the outcome under the social choice function, then the resulting difference in utilities of the new and original outcomes evaluated at the new type of this agent must be no less than this difference in utilities evaluated at the original type of this agent.</p> </abstract>
<abstract> <p>We consider the situation when there is a large number of series, N, each with T observations, and each series has some predictive ability for some variable of interest. A methodology of growing interest is first to estimate common factors from the panel of data by the method of principal components and then to augment an otherwise standard regression with the estimated factors. In this paper, we show that the least squares estimates obtained from these factor-augmented regressions are<tex-math>$\sqrt{T}$</tex-math>consistent and asymptotically normal if<tex-math>$\sqrt{T}/N \rightarrow 0$</tex-math>. The conditional mean predicted by the estimated factors is min[<tex-math>$\sqrt{T}$</tex-math>,<tex-math>$\sqrt{N}$</tex-math>] consistent and asymptotically normal. Except when T/N goes to zero, inference should take into account the effect of "estimated regressors" on the estimated conditional mean. We present analytical formulas for prediction intervals that are valid regardless of the magnitude of N/T and that can also be used when the factors are nonstationary.</p> </abstract>
<abstract> <p>We examine legislative policy making in institutions with two empirically relevant features: agenda setting occurs in real time and the default policy evolves. We demonstrate that these institutions select Condorcet winners when they exist, provided a sufficient number of individuals have opportunities to make proposals. In policy spaces with either pork barrel or pure redistributional politics (where a Condorcet winner does not exist), the last proposer is effectively a dictator or near-dictator under relatively weak conditions.</p> </abstract>
<abstract> <p>School choice has become an increasingly prominent strategy for enhancing academic achievement. To evaluate the impact on participants, we exploit randomized lotteries that determine high school admission in the Chicago Public Schools. Compared to those students who lose lotteries, students who win attend high schools that are better in a number of dimensions, including peer achievement and attainment levels. Nonetheless, we find little evidence that winning a lottery provides any systematic benefit across a wide variety of traditional academic measures. Lottery winners do, however, experience improvements on a subset of nontraditional outcome measures, such as self-reported disciplinary incidents and arrest rates.</p> </abstract>
<abstract> <p>A step toward a strategic foundation for rational expectations equilibrium is taken by considering a double auction with n buyers and m sellers with interdependent values and affiliated private information. If there are sufficiently many buyers and sellers, and their bids are restricted to a sufficiently fine discrete set of prices, then, generically, there is an equilibrium in nondecreasing bidding functions that is arbitrarily close to the unique fully revealing rational expectations equilibrium of the limit market with unrestricted bids and a continuum of agents. In particular, the large double-auction equilibrium is almost efficient and almost fully aggregates the agents' information.</p> </abstract>
<abstract> <p>Comparisons of learning models in repeated games have been a central preoccupation of experimental and behavioral economics over the last decade. Much of this work begins with pooled estimation of the model(s) under scrutiny. I show that in the presence of parameter heterogeneity, pooled estimation can produce a severe bias that tends to unduly favor reinforcement learning relative to belief learning. This occurs when comparisons are based on goodness of fit and when comparisons are based on the relative importance of the two kinds of learning in hybrid structural models. Even mis-specified random parameter estimators can greatly reduce the bias relative to pooled estimation.</p> </abstract>
<abstract> <p>We introduce and solve a new class of "downward-recursive" static portfolio choice problems. An individual simultaneously chooses among ranked stochastic options, and each choice is costly. In the motivational application, just one may be exercised from those that succeed. This often emerges in practice, such as when a student applies to many colleges or when a firm simultaneously tries several technologies. We show that such portfolio choice problems quite generally entail maximizing a submodular function of finite sets-which is NP-hard in general. Still, we show that a greedy algorithm finds the optimal set, finding first the best singleton, then the best single addition to it, and so on. We show that the optimal choices are "less aggressive" than the sequentially optimal ones, but "more aggressive" than the best singletons. Also, the optimal set in general contains gaps. We provide some comparative statics results on the chosen set.</p> </abstract>
<abstract> <p>A seller and a buyer bargain over the terms of trade for an object. The seller receives a perfect signal that determines the value of the object to both players, whereas the buyer remains uninformed. We analyze the infinite-horizon bargaining game in which the buyer makes all the offers. When the static incentive constraints permit first-best efficiency, then under some regularity conditions the outcome of the sequential bargaining game becomes arbitrarily efficient as bargaining frictions vanish. When the static incentive constraints preclude first-best efficiency, the limiting bargaining outcome is not second-best efficient and may even perform worse than the outcome from the one-period bargaining game. With frequent buyer offers, the outcome is then characterized by recurring bursts of high probability of agreement, followed by long periods of delay in which the probability of agreement is negligible.</p> </abstract>
<abstract> <p>Recent discoveries in behavioral economics have led scholars to question the underpinnings of neoclassical economics. We use insights gained from one of the most influential lines of behavioral research-gift exchange-in an attempt to maximize worker effort in two quite distinct tasks: data entry for a university library and door-to-door fundraising for a research center. In support of the received literature, our field evidence suggests that worker effort in the first few hours on the job is considerably higher in the "gift" treatment than in the "nongift" treatment. After the initial few hours, however, no difference in outcomes is observed, and overall the gift treatment yielded inferior aggregate outcomes for the employer: with the same budget we would have logged more data for our library and raised more money for our research center by using the market-clearing wage rather than by trying to induce greater effort with a gift of higher wages.</p> </abstract>
<abstract> <p>This paper examines an exchange economy with heterogeneous indivisible objects that can be substitutable or complementary. We show that a competitive equilibrium exists in such economies, provided that all the objects can be partitioned into two groups, and from the viewpoint of each agent, objects in the same group are substitutes and objects across the two groups are complements. This condition generalizes the well-known Kelso-Crawford gross substitutes condition and is called gross substitutes and complements. We also provide practical and typical examples from which substitutes and complements are both jointly observed.</p> </abstract>
<abstract> <p>Finite population noncooperative games with linear-quadratic utilities, where each player decides how much action she exerts, can be interpreted as a network game with local payoff complementarities, together with a globally uniform payoff substitutability component and an own-concavity effect. For these games, the Nash equilibrium action of each player is proportional to her Bonacich centrality in the network of local complementarities, thus establishing a bridge with the sociology literature on social networks. This Bonacich-Nash linkage implies that aggregate equilibrium increases with network size and density. We then analyze a policy that consists of targeting the key player, that is, the player who, once removed, leads to the optimal change in aggregate activity. We provide a geometric characterization of the key player identified with an intercentrality measure, which takes into account both a player's centrality and her contribution to the centrality of the others.</p> </abstract>
<abstract> <p>The ethic of priority is a compromise between the extremely compensatory ethic of outcome equality and the needs-blind ethic of resource equality. We propose an axiom of priority and characterize resource-allocation rules that are impartial, prioritarian, and solidaristic. They comprise a class of rules that equalize across individuals some index of outcome and resources. Consequently, we provide an ethical rationalization for the many applications in which such indices have been used (e.g., the human development index, the index of primary goods, etc.).</p> </abstract>
<abstract> <p>This note revisits the identification theorems of Brown (1983) and Roehrig (1988). We describe an error in the proofs of the main identification theorems in these papers, and provide an important counterexample to the theorems on the identification of the reduced form. Specifically, the reduced form of a nonseparable simultaneous equations model is not identified even under the assumptions of these papers. We provide conditions under which the reduced form is identified and is recoverable using the distribution of the endogenous variables conditional on the exogenous variables. However, these conditions place substantial limitations on the structural model. We conclude the note with a conjecture that it may be possible to use classical exclusion restrictions to recover some of the key implications of the theorems in more general settings.</p> </abstract>
<abstract> <p>We study preferences over menus which can be represented as if the agent selects an alternative from a menu and experiences regret if her choice is ex post inferior. Since regret arises from comparisons between the alternative selected and the other available alternatives, our axioms reflect the agent's desire to limit her options. We prove that our representation is essentially unique. We also introduce two measures of comparative regret attitudes and relate them to our representation. Finally, we explore the formal connection between the present work and the literature on temptation.</p> </abstract>
<abstract> <p>Suppose that each player in a game is rational, each player thinks the other players are rational, and so on. Also, suppose that rationality is taken to incorporate an admissibility requirement--that is, the avoidance of weakly dominated strategies. Which strategies can be played? We provide an epistemic framework in which to address this question. Specifically, we formulate conditions of rationality and mth-order assumption of rationality (RmAR) and rationality and common assumption of rationality (RCAR). We show that (i) RCAR is characterized by a solution concept we call a "self-admissible set"; (ii) in a "complete" type structure, RmAR is characterized by the set of strategies that survive m + 1 rounds of elimination of inadmissible strategies; (iii) under certain conditions, RCAR is impossible in a complete structure.</p> </abstract>
<abstract> <p>Much evidence suggests that people are heterogeneous with regard to their abilities to make rational, forward-looking decisions. This raises the question as to when the rational types are decisive for aggregate outcomes and when the boundedly rational types shape aggregate results. We examine this question in the context of a long-standing and important economic problem: the adjustment of nominal prices after an anticipated monetary shock. Our experiments suggest that two types of bounded rationality--money illusion and anchoring--are important behavioral forces behind nominal inertia. However, depending on the strategic environment, bounded rationality has vastly different effects on aggregate price adjustment. If agents' actions are strategic substitutes, adjustment to the new equilibrium is extremely quick, whereas under strategic complementarity, adjustment is both very slow and associated with relatively large real effects. This adjustment difference is driven by price expectations, which are very flexible and forward-looking under substitutability but adaptive and sticky under complementarity. Moreover, subjects' expectations are also considerably more rational under substitutability.</p> </abstract>
<abstract> <p>We study a model of lumpy investment wherein establishments face persistent shocks to common and plant-specific productivity, and nonconvex adjustment costs lead them to pursue generalized (S, s) investment rules. We allow persistent heterogeneity in both capital and total factor productivity alongside low-level investments exempt from adjustment costs to develop the first model consistent with the cross-sectional distribution of establishment investment rates. Examining the implications of lumpy investment for aggregate dynamics in this setting, we find that they remain substantial when factor supply considerations are ignored, but are quantitatively irrelevant in general equilibrium. The substantial implications of general equilibrium extend beyond the dynamics of aggregate series. While the presence of idiosyncratic shocks makes the time-averaged distribution of plant-level investment rates largely invariant to market-clearing movements in real wages and interest rates, we show that the dynamics of plants' investments differ sharply in their presence. Thus, model-based estimations of capital adjustment costs involving panel data may be quite sensitive to the assumption about equilibrium. Our analysis also offers new insights about how nonconvex adjustment costs influence investment at the plant. When establishments face idiosyncratic productivity shocks consistent with existing estimates, we find that nonconvex costs do not cause lumpy investments, but act to eliminate them.</p> </abstract>
<abstract> <p>Previous research has argued that debt financing affects equity-holders' investment decisions, producing substantial inefficiency. This paper shows that the size of this inefficiency depends on the degree of investment reversibility. In a dynamic model of financing and investment, the paper provides an upper bound for the inefficiency produced by debt financing. The upper bound is decreasing in the degree of investment reversibility and is zero when investment is perfectly reversible.</p> </abstract>
<abstract> <p>We study a Monte Carlo algorithm for computing marginal and stationary densities of stochastic models with the Markov property, establishing global asymptotic normality and<tex-math>$o_p (n^{ - 1/2} )$</tex-math>convergence. Asymptotic normality is used to derive error bounds in terms of the distribution of the norm deviation.</p> </abstract>
<abstract> <p>This paper studies a general model of holdup in a setting encompassing the models of Segal (1999) and Che and Hausch (1999) among others. It is shown that if renegotiation is modeled as an infinite-horizon noncooperative bargaining game, then, with a simple initial contract, an efficient equilibrium will generally exist. The contract is robust in the sense that it does not depend on fine details of the model. The contract gives authority to one party to set the terms of trade and gives the other party a nonexpiring option to trade at these terms. The difference from standard results arises because the initial contract ensures that the renegotiation game has multiple equilibria; the multiplicity of continuation equilibria can be used to enforce efficient investment.</p> </abstract>
<abstract> <p>We consider a dynamic Bertrand game in which prices are publicly observed and each firm receives a privately observed cost shock in each period. Although cost shocks are independent across firms, within a firm costs follow a first-order Markov process. We analyze the set of collusive equilibria available to firms, emphasizing the best collusive scheme for the firms at the start of the game. In general, there is a trade-off between productive efficiency, whereby the low-cost firm serves the market in a given period, and high prices. We show that when costs are perfectly correlated over time within a firm, if the distribution of costs is log-concave and firms are sufficiently patient, then the optimal collusive scheme entails price rigidity: firms set the same price and share the market equally, regardless of their respective costs. When serial correlation of costs is imperfect, partial productive efficiency is optimal. For the case of two cost types, first-best collusion is possible if the firms are patient relative to the persistence of cost shocks, but not otherwise. We present numerical examples of first-best collusive schemes.</p> </abstract>
<abstract> <p>We show that a simple "reputation-style" test can always identify which of two experts is informed about the true distribution. The test presumes no prior knowledge of the true distribution, achieves any desired degree of precision in some fixed finite time, and does not use "counterfactual" predictions. Our analysis capitalizes on a result of Fudenberg and Levine (1992) on the rate of convergence of supermartingales. We use our setup to shed some light on the apparent paradox that a strategically motivated expert can ignorantly pass any test. We point out that this paradox arises because in the single-expert setting, any mixed strategy for Nature over distributions is reducible to a pure strategy. This eliminates any meaningful sense in which Nature can randomize. Comparative testing reverses the impossibility result because the presence of an expert who knows the realized distribution eliminates the reducibility of Nature's compound lotteries.</p> </abstract>
<abstract> <p>We consider a cross-calibration test of predictions by multiple potential experts in a stochastic environment. This test checks whether each expert is calibrated conditional on the predictions made by other experts. We show that this test is good in the sense that a true expert--one informed of the true distribution of the process--is guaranteed to pass the test no matter what the other potential experts do, and false experts will fail the test on all but a small (category I) set of true distributions. Furthermore, even when there is no true expert present, a test similar to cross-calibration cannot be simultaneously manipulated by multiple false experts, but</p> </abstract>
<abstract> <p>We design experiments to jointly elicit risk and time preferences for the adult Danish population. Since subjects are generally risk averse, we find that joint elicitation provides estimates of discount rates that are significantly lower than those found in previous studies and more in line with what would be considered as a priori reasonable rates. The statistical specification relies on a theoretical framework that involves a latent trade-off between long-run optimization and short-run temptation. Estimation of this specification is undertaken using structural, maximum likelihood methods. Our main results based on exponential discounting are robust to alternative specifications such as hyperbolic discounting. These results have direct implications for attempts to elicit time preferences, as well as debates over the appropriate domain of the utility function when characterizing risk aversion and time consistency.</p> </abstract>
<abstract> <p>We study the provision of dynamic incentives to self-interested politicians who control the allocation of resources in the context of the standard neoclassical growth model. Citizens discipline politicians using elections. We show that the need to provide incentives to the politician in power creates political economy distortions in the structure of production, which resemble aggregate tax distortions. We provide conditions under which the political economy distortions persist or disappear in the long run. If the politicians are as patient as the citizens, the best subgame perfect equilibrium leads to an asymptotic allocation where the aggregate distortions arising from political economy disappear. In contrast, when politicians are less patient than the citizens, political economy distortions remain asymptotically and lead to positive aggregate labor and capital taxes.</p> </abstract>
<abstract> <p>This paper proposes a new method for identifying social interactions using conditional variance restrictions. The method provides a consistent estimate of the social multiplier when social interactions take the "linear-in-means" form (Manski (1993)). When social interactions are not of the linear-in-means form, the estimator, under certain conditions, continues to form the basis of a consistent test of the no social interactions null with correct large sample size. The methods are illustrated using data from the Tennessee class size reduction experiment Project STAR. The application suggests that differences in peer group quality were an important source of individual-level variation in the academic achievement of Project STAR kindergarten students.</p> </abstract>
<abstract> <p>We develop and estimate a model of dynamic interactions in which commitment is limited and contracts are incomplete to explain the patterns of income and consumption growth in village economies of less developed countries. Households can insure each other through both formal contracts and informal agreements, that is, selfenforcing agreements specifying voluntary transfers. This theoretical setting nests the case of complete markets and the case where only informal agreements are available. We derive a system of nonlinear equations for income and consumption growth. A key prediction of our model is that both variables are affected by lagged consumption as a consequence of the interplay of formal and informal contracting possibilities. In a semi-parametric setting, we prove identification, derive testable restrictions, and estimate the model with the use of data from Pakistani villages. Empirical results are consistent with the economic arguments. Incentive constraints due to self-enforcement bind with positive probability and formal contracts are used to reduce this probability.</p> </abstract>
<abstract> <p>This paper studies the asymptotic behavior of Fisher's information for a Lévy process discretely sampled at an increasing frequency. As a result, we derive the optimal rates of convergence of efficient estimators of the different parameters of the process and show that the rates are often nonstandard and differ across parameters. We also show that it is possible to distinguish the continuous part of the process from its jumps part, and even different types of jumps from one another.</p> </abstract>
<abstract> <p>We propose inference procedures for partially identified population features for which the population identification region can be written as a transformation of the Aumann expectation of a properly defined set valued random variable (SVRV). An SVRV is a mapping that associates a set (rather than a real number) with each element of the sample space. Examples of population features in this class include interval-identified scalar parameters, best linear predictors with interval outcome data, and parameters of semiparametric binary models with interval regressor data. We extend the analogy principle to SVRVs and show that the sample analog estimator of the population identification region is given by a transformation of a Minkowski average of SVRVs. Using the results of the mathematics literature on SVRVs, we show that this estimator converges in probability to the population identification region with respect to the Hausdorff distance. We then show that the Hausdorff distance and the directed Hausdorff distance between the population identification region and the estimator, when properly normalized by<tex-math>$\sqrt n $</tex-math>, converge in distribution to functions of a Gaussian process whose covariance kernel depends on parameters of the population identification region. We provide consistent bootstrap procedures to approximate these limiting distributions. Using similar arguments as those applied for vector valued random variables, we develop a methodology to test assumptions about the true identification region and its subsets. We show that these results can be used to construct a confidence collection and a directed confidence collection. Those are (respectively) collection of sets that, when specified as a null hypothesis for the true value (a subset of values) of the population identification region, cannot be rejected by our tests.</p> </abstract>
<abstract> <p>We combine choice data in the ultimatum game with the expectations of proposers elicited by subjective probability questions to estimate a structural model of decision making under uncertainty. The model, estimated using a large representative sample of subjects from the Dutch population, allows both nonlinear preferences for equity and expectations to vary across socioeconomic groups. Our results indicate that inequity aversion to one's own disadvantage is an increasing and concave function of the payoff difference. We also find considerable heterogeneity in the population. Young and highly educated subjects have lower aversion for inequity than other groups. Moreover, the model that uses subjective data on expectations generates much better in- and out-of- sample predictions than a model which assumes that players have rational expectations.</p> </abstract>
<abstract> <p>We prove existence of equilibrium in a continuous-time securities market in which the securities are potentially dynamically complete: the number of securities is at least one more than the number of independent sources of uncertainty. We prove that dynamic completeness of the candidate equilibrium price process follows from mild exogenous assumptions on the economic primitives of the model. Our result is universal, rather than generic: dynamic completeness of the candidate equilibrium price process and existence of equilibrium follow from the way information is revealed in a Brownian filtration, and from a mild exogenous nondegeneracy condition on the terminal security dividends. The nondegeneracy condition, which requires that finding one point at which a determinant of a Jacobian matrix of dividends is nonzero, is very easy to check. We find that the equilibrium prices, consumptions, and trading strategies are well-behaved functions of the stochastic process describing the evolution of information. We prove that equilibria of discrete approximations converge to equilibria of the continuous-time economy.</p> </abstract>
<abstract> <p>Consider two agents who learn the value of an unknown parameter by observing a sequence of private signals. The signals are independent and identically distributed across time but not necessarily across agents. We show that when each agent's signal space is finite, the agents will commonly learn the value of the parameter, that is, that the true value of the parameter will become approximate common knowledge. The essential step in this argument is to express the expectation of one agent's signals, conditional on those of the other agent, in terms of a Markov chain. This allows us to invoke a contraction mapping principle ensuring that if one agent's signals are close to those expected under a particular value of the parameter, then that agent expects the other agent's signals to be even closer to those expected under the parameter value. In contrast, if the agents' observations come from a countably infinite signal space, then this contraction mapping property fails. We show by example that common learning can fail in this case.</p> </abstract>
<abstract> <p>This paper provides conditions for identification of functionals in nonparametric simultaneous equations models with nonadditive unobservable random terms. The conditions are derived from a characterization of observational equivalence between models. We show that, in the models considered, observational equivalence can be characterized by a restriction on the rank of a matrix. The use of the new results is exemplified by deriving previously known results about identification in parametric and nonparametric models as well as new results. A stylized method for analyzing identification, which is useful in some situations, is also presented.</p> </abstract>
<abstract> <p>We develop a framework to assess how successfully standard time series models explain low-frequency variability of a data series. The low-frequency information is extracted by computing a finite number of weighted averages of the original data, where the weights are low-frequency trigonometric series. The properties of these weighted averages are then compared to the asymptotic implications of a number of common time series models. We apply the framework to twenty U.S. macroeconomic and financial time series using frequencies lower than the business cycle.</p> </abstract>
<abstract> <p>Traditional discrete-choice models assume buyers are aware of all products for sale. In markets where products change rapidly, the full information assumption is untenable. I present a discrete-choice model of limited consumer information, where advertising influences the set of products from which consumers choose to purchase. I apply the model to the U.S. personal computer market where top firms spend over $2 billion annually on advertising. I find estimated markups of 19% over production costs, where top firms advertise more than average and earn higher than average markups. High markups are explained to a large extent by informational asymmetries across consumers, where full information models predict markups of one-fourth the magnitude. I find that estimated product demand curves are biased toward being too elastic under traditional models. I show how to use data on media exposure to improve estimated price elasticities in the absence of micro ad data.</p> </abstract>
<abstract> <p>We analyze tender offers where privately informed shareholders are uncertain about the raider's ability to improve firm value. The raider suffers a "lemons problem" in that, for any price offered, only shareholders who are relatively pessimistic about the value of the firm tender their shares. Consequently, the raider finds it too costly to induce shareholders to tender when their information is positive. In the limit as the number of shareholders gets arbitrarily large, when private benefits are relatively low, the tender offer is unsuccessful if the takeover has the potential to create value. The takeover market is therefore inefficient. In contrast, when private benefits of control are high, the tender offer allocates the firm to any value-increasing raider, but may also allow inefficient takeovers to occur. Unlike the case where all information is symmetric, shareholders cannot always extract the entire surplus from the acquisition.</p> </abstract>
<abstract> <p>This paper derives asymptotic power envelopes for tests of the unit root hypothesis in a zero-mean AR(1) model. The power envelopes are derived using the limits of experiments approach and are semiparametric in the sense that the underlying error distribution is treated as an unknown infinite-dimensional nuisance parameter. Adaptation is shown to be possible when the error distribution is known to be symmetric and to be impossible when the error distribution is unrestricted. In the latter case, two conceptually distinct approaches to nuisance parameter elimination are employed in the derivation of the semiparametric power bounds. One of these bounds, derived under an invariance restriction, is shown by example to be sharp, while the other, derived under a similarity restriction, is conjectured not to be globally attainable.</p> </abstract>
<abstract> <p>Rabin (2000) proved that a low level of risk aversion with respect to small gambles leads to a high, and absurd, level of risk aversion with respect to large gambles. Rabin's arguments strongly depend on expected utility theory, but we show that similar arguments apply to general non-expected utility theories.</p> </abstract>
<abstract> <p>We study a definition of subjective beliefs applicable to preferences that allow for the perception of ambiguity, and provide a characterization of such beliefs in terms of market behavior. Using this definition, we derive necessary and sufficient conditions for the efficiency of ex ante trade and show that these conditions follow from the fundamental welfare theorems. When aggregate uncertainty is absent, our results show that full insurance is efficient if and only if agents share some common subjective beliefs. Our results hold for a general class of convex preferences, which contains many functional forms used in applications involving ambiguity and ambiguity aversion. We show how our results can be articulated in the language of these functional forms, confirming results existing in the literature, generating new results, and providing a useful tool for applications.</p> </abstract>
<abstract> <p>We use the control function approach to identify the average treatment effect and the effect of treatment on the treated in models with a continuous endogenous regressor whose impact is heterogeneous. We assume a stochastic polynomial restriction on the form of the heterogeneity, but unlike alternative nonparametric control function approaches, our approach does not require large support assumptions.</p> </abstract>
<abstract> <p>In this paper we revisit the results in Caner and Hansen (2001), where the authors obtained novel limiting distributions of Wald type test statistics for testing for the presence of threshold nonlinearities in autoregressive models containing unit roots. Using the same framework, we obtain a new formulation of the limiting distribution of the Wald statistic for testing for threshold effects, correcting an expression that appeared in the main theorem presented by Caner and Hansen. Subsequently, we show that under a particular scenario that excludes stationary regressors such as lagged dependent variables and despite the presence of a unit root, this same limiting random variable takes a familiar form that is free of nuisance parameters and already tabulated in the literature, thus removing the need to use bootstrap based inferences. This is a novel and unusual occurrence in this literature on testing for the presence of nonlinear dynamics.</p> </abstract>
<abstract> <p>This paper uses revealed preference inequalities to provide the tightest possible (best) nonparametric bounds on predicted consumer responses to price changes using consumer-level data over a finite set of relative price changes. These responses are allowed to vary nonparametrically across the income distribution. This is achieved by combining the theory of revealed preference with the semiparametric estimation of consumer expansion paths (Engel curves). We label these expansion path based bounds on demand responses as E-bounds. Deviations from revealed preference restrictions are measured by preference perturbations which are shown to usefully characterize taste change and to provide a stochastic environment within which violations of revealed preference inequalities can be assessed.</p> </abstract>
<abstract> <p>In the past few decades multistore retailers, especially those with 100 or more stores, have experienced substantial growth. At the same time, there is widely reported public outcry over the impact of these chain stores on other retailers and local communities. This paper develops an empirical model to assess the impact of chain stores on other discount retailers and to quantify the size of the scale economies within a chain. The model has two key features. First, it allows for flexible competition patterns among all players. Second, for chains, it incorporates the scale economies that arise from operating multiple stores in nearby regions. In doing so, the model relaxes the commonly used assumption that entry in different markets is independent. The lattice theory is exploited to solve this complicated entry game among chains and other discount retailers in a large number of markets. It is found that the negative impact of Kmart's presence on Wal-Mart's profit was much stronger in 1988 than in 1997, while the opposite is true for the effect of Wal-Mart's presence on Kmart's profit. Having a chain store in a market makes roughly 50% of the discount stores unprofitable. Wal-Mart's expansion from the late 1980s to the late 1990s explains about 40-50% of the net change in the number of small discount stores and 30-40% for all other discount stores. Scale economies were important for Wal-Mart, but less so for Kmart, and the magnitude did not grow proportionately with the chains' sizes.</p> </abstract>
<abstract> <p>Productivity differences across firms are large and persistent, but the evidence for worker reallocation as an important source of aggregate productivity growth is mixed. The purpose of this paper is to estimate the structure of an equilibrium model of growth through innovation designed to identify and quantify the role of resource reallocation in the growth process. The model is a version of the Schumpeterian theory of firm evolution and growth developed by Klette and Kortum (2004) extended to allow for firm heterogeneity. The data set is a panel of Danish firms that includes information on value added, employment, and wages. The model's fit is good. The estimated model implies that more productive firms in each cohort grow faster and consequently crowd out less productive firms in steady state. This selection effect accounts for 53% of aggregate growth in the estimated version of the model.</p> </abstract>
<abstract> <p>We propose an approximation method for analyzing Ericson and Pakes (1995)-style dynamic models of imperfect competition. We define a new equilibrium concept that we call oblivious equilibrium, in which each firm is assumed to make decisions based only on its own state and knowledge of the long-run average industry state, but where firms ignore current information about competitors' states. The great advantage of oblivious equilibria is that they are much easier to compute than are Markov perfect equilibria. Moreover, we show that, as the market becomes large, if the equilibrium distribution of firm states obeys a certain "light-tail" condition, then oblivious equilibria closely approximate Markov perfect equilibria. This theorem justifies using oblivious equilibria to analyze Markov perfect industry dynamics in Ericson and Pakes (1995)-style models with many firms.</p> </abstract>
<abstract> <p>Our concern is the extension of the theory of the Shapley value to problems involving externalities. Using the standard axiom systems behind the Shapley value leads to the identification of bounds on players' payoffs around an "externality-free" value. The approach determines the direction and maximum size of Pigouvian-like transfers among players, transfers based on the specific nature of externalities that are compatible with basic normative principles. Examples are provided to illustrate the approach and to draw comparisons with previous literature.</p> </abstract>
<abstract> <p>The difficulties in properly anticipating key economic variables may encourage decision makers to rely on experts' forecasts. Professional forecasters, however, may not be reliable and so their forecasts must be empirically tested. This may induce experts to forecast strategically in order to pass the test. A test can be ignorantly passed if a false expert, with no knowledge of the data-generating process, can pass the test. Many tests that are unlikely to reject correct forecasts can be ignorantly passed. Tests that cannot be ignorantly passed do exist, but these tests must make use of predictions contingent on data not yet observed at the time the forecasts are rejected. Such tests cannot be run if forecasters report only the probability of the next period's events on the basis of the actually observed data. This result shows that it is difficult to dismiss false, but strategic, experts who know how theories are tested. This result also shows an important role that can be played by predictions contingent on data not yet observed.</p> </abstract>
<abstract> <p>Numerous psychological and economic experiments have shown that the exchange of promises greatly enhances cooperative behavior in experimental games. This paper seeks to test two theories to explain this effect. The first posits that individuals have a preference for keeping their word. The second assumes that people dislike letting down others' payoff expectations. According to the latter account, promises affect behavior only indirectly, because they lead to changes in the payoff expectations attributed to others. I conduct an experiment designed to distinguish between and test these alternative explanations. The results demonstrate that the effects of promises cannot be accounted for by changes in payoff expectations. This suggests that people have a preference for promise keeping per se.</p> </abstract>
<abstract> <p>This paper shows how to use realized kernels to carry out efficient feasible inference on the ex post variation of underlying equity prices in the presence of simple models of market frictions. The weights can be chosen to achieve the best possible rate of convergence and to have an asymptotic variance which equals that of the maximum likelihood estimator in the parametric version of this problem. Realized kernels can also be selected to (i) be analyzed using endogenously spaced data such as that in data bases on transactions, (ii) allow for market frictions which are endogenous, and (iii) allow for temporally dependent noise. The finite sample performance of our estimators is studied using simulation, while empirical work illustrates their use in practice.</p> </abstract>
<abstract> <p>Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romanoǐ (1994).</p> </abstract>
<abstract> <p>A player's pure strategy is called relevant for an outcome of a game in extensive form with perfect recall if there exists a weakly sequential equilibrium with that outcome for which the strategy is an optimal reply at every information set it does not exclude. The outcome satisfies forward induction if it results from a weakly sequential equilibrium in which players' beliefs assign positive probability only to relevant strategies at each information set reached by a profile of relevant strategies. We prove that if there are two players and payoffs are generic, then an outcome satisfies forward induction if every game with the same reduced normal form after eliminating redundant pure strategies has a sequential equilibrium with an equivalent outcome. Thus in this case forward induction is implied by decision-theoretic criteria.</p> </abstract>
<abstract> <p>We study the role of observability in bargaining with correlated values. Short-run buyers sequentially submit offers to one seller. When previous offers are observable, bargaining is likely to end up in an impasse. In contrast, when offers are hidden, agreement is always reached, although with delay.</p> </abstract>
<abstract> <p>This paper studies a class of games, "all-pay contests," which capture general asymmetries and sunk investments inherent in scenarios such as lobbying, competition for market power, labor-market tournaments, and R&amp;D races. Players compete for one of several identical prizes by choosing a score. Conditional on winning or losing, it is weakly better to do so with a lower score. This formulation allows for differing production technologies, costs of capital, prior investments, attitudes toward risk, and conditional and unconditional investments, among others. I provide a closed-form formula for players' equilibrium payoffs and analyze player participation. A special case of contests is multiprize, complete-information all-pay auctions.</p> </abstract>
<abstract> <p>Consider a weather forecaster predicting a probability of rain for the next day. We consider tests that, given a finite sequence of forecast predictions and outcomes, will ei ther pass or fail the forecaster. Sandroni showed that any test which passes a forecaster who knows the distribution of nature can also be probabilistically passed by a forecaster with no knowledge of future events. We look at the computational complexity of such forecasters and exhibit a linear-time test and distribution of nature such that any forecaster without knowledge of the future who can fool the test must be able to solve computationally difficult problems. Thus, unlike Sandroni's work, a computationally efficient forecaster cannot always fool this test independently of nature.</p> </abstract>
<abstract> <p> This paper applies some general concepts in decision theory to a linear panel data model. A simple version of the model is an autoregression with a separate intercept for each unit in the cross section, with errors that are independent and identically distributed with a normal distribution. There is a parameter of interest γ and a nuisance parameter τ, a N x K matrix, where N is the cross-section sample size. The focus is on dealing with the incidental parameters problem created by a potentially high-dimension nuisance parameter. We adopt a "fixed-effects" approach that seeks to protect against any sequence of incidental parameters. We transform τ to (δ, ρ, ω), where δ is a J x K matrix of coefficients from the least-squares projection of τ on a N x J matrix x of strictly exogenous variables, ρ is a K x K symmetric, positive semidefinite matrix obtained from the residual sums of squares and cross-products in the projection of τ on x, and to is a (N - J) x K matrix whose columns are orthogonal and have unit length. The model is invariant under the actions of a group on the sample space and the parameter space, and we find a maximal invariant statistic. The distribution of the maximal invariant statistic does not depend upon ω. There is a unique invariant distribution for ω. We use this invariant distribution as a prior distribution to obtain an integrated likelihood function. It depends upon the observation only through the maximal invariant statistic. We use the maximal invariant statistic to construct a marginal likelihood function, so we can eliminate ω by integration with respect to the invariant prior distribution or by working with the marginal likelihood function. The two approaches coincide. Decision rules based on the invariant distribution for ω have a minimax property. Given a loss function that does not depend upon ω and given a prior distribution for (γ, δ, ρ), we show how to minimize the average--with respect to the prior distribution for (γ, δ, ρ)--of the maximum risk, where the maximum is with respect to ω. There is a family of prior distributions for (δ, ρ) that leads to a simple closed form for the integrated likelihood function. This integrated likelihood function coincides with the likelihood function for a normal, correlated random-effects model. Under random sampling, the corresponding quasi maximum likelihood estimator is consistent for γ as N → ∞, with a standard limiting distribution. The limit results do not require normality or homoskedasticity (conditional on x) assumptions. </p> </abstract>
<abstract> <p> In dynamic discrete choice analysis, controlling for unobserved heterogeneity is an important issue, and finite mixture models provide flexible ways to account for it. This paper studies nonparametric identifiability of type probabilities and type-specific component distributions in finite mixture models of dynamic discrete choices. We derive sufficient conditions for nonparametric identification for various finite mixture models of dynamic discrete choices used in applied work under different assumptions on the Markov property, stationarity, and type-invariance in the transition process. Three elements emerge as the important determinants of identification: the time-dimension of panel data, the number of values the covariates can take, and the heterogeneity of the response of different types to changes in the covariates. For example, in a simple case where the transition function is type-invariant, a time-dimension of T = 3 is sufficient for identification, provided that the number of values the covariates can take is no smaller than the number of types and that the changes in the covariates induce sufficiently heterogeneous variations in the choice probabilities across types. Identification is achieved even when state dependence is present if a model is stationary first-order Markovian and the panel has a moderate time-dimension (T ≥ 6). </p> </abstract>
<abstract> <p>We create an analytical structure that reveals the long-run risk-return relationship for nonlinear continuous-time Markov environments. We do so by studying an eigenvalue problem associated with a positive eigenfunction for a conveniently chosen family of valuation operators. The members of this family are indexed by the elapsed time between payoff and valuation dates, and they are necessarily related via a mathematical structure called a semigroup. We represent the semigroup using a positive process with three components: an exponential term constructed from the eigenvalue, a martingale, and a transient eigenfunction term. The eigenvalue encodes the risk adjustment, the martingale alters the probability measure to capture long-run approximation, and the eigenfunction gives the long-run dependence on the Markov state. We discuss sufficient conditions for the existence and uniqueness of the relevant eigenvalue and eigenfunction. By showing how changes in the stochastic growth components of cash flows induce changes in the corresponding eigenvalues and eigenfunctions, we reveal a long-run risk-return trade-off.</p> </abstract>
<abstract> <p>We reappraise the significance and robustness of indeterminacy in overlapping-generations models. In any of Gale's example economies with an equilibrium that is not locally unique, for instance, perturbing the economy by judiciously splitting each of Gale's goods into two close substitutes restricts that indeterminacy to each period's allocation of consumption between those substitutes. In particular, prices, interest rates, the commodity value of nominal savings (including money), and utility levels become determinate. Any indeterminacy of equilibrium consumption in the perturbed economy is thus insignificant to consumers, and some forecasting and comparative-statics policy exercises become possible.</p> </abstract>
<abstract> <p>This paper presents simple new multisignal generalizations of the two classic methods used to justify the first-order approach to moral hazard principal-agent problems, and compares these two approaches with each other. The paper first discusses limitations of previous generalizations. Then a state-space formulation is used to obtain a new multisignal generalization of the Jewitt (1988) conditions. Next, using the Mirrlees formulation, new multisignal generalizations of the convexity of the distribution function condition (CDFC) approach of Rogerson (1985) and Sinclair-Desgagné (1994) are obtained. Vector calculus methods are used to derive easy-to-check local conditions for our generalization of the CDFC. Finally, we argue that the Jewitt conditions may generalize more flexibly than the CDFC to the multisignal case. This is because, with many signals, the principal can become very well informed about the agent's action and, even in the one-signal case, the CDFC must fail when the signal becomes very accurate.</p> </abstract>
<abstract> <p>We propose bootstrap methods for a general class of nonlinear transformations of realized volatility which includes the raw version of realized volatility and its logarithmic transformation as special cases. We consider the independent and identically distributed (i.i.d.) bootstrap and the wild bootstrap (WB), and prove their first-order asymptotic validity under general assumptions on the log-price process that allow for drift and leverage effects. We derive Edgeworth expansions in a simpler model that rules out these effects. The i.i.d. bootstrap provides a second-order asymptotic refinement when volatility is constant, but not otherwise. The WB yields a second-order asymptotic refinement under stochastic volatility provided we choose the external random variable used to construct the WB data appropriately. None of these methods provides third-order asymptotic refinements. Both methods improve upon the first-order asymptotic theory in finite samples.</p> </abstract>
<abstract> <p>The property of an allocation rule to be implementable in dominant strategies by a unique payment scheme is called revenue equivalence. We give a characterization of revenue equivalence based on a graph theoretic interpretation of the incentive compatibility constraints. The characterization holds for any (possibly infinite) outcome space and many of the known results are immediate consequences. Moreover, revenue equivalence can be identified in cases where existing theorems are silent.</p> </abstract>
<abstract> <p>Uncertainty appears to jump up after major shocks like the Cuban Missile crisis, the assassination of JFK, the OPEC I oil-price shock, and the 9/11 terrorist attacks. This paper offers a structural framework to analyze the impact of these uncertainty shocks. I build a model with a time-varying second moment, which is numerically solved and estimated using firm-level data. The parameterized model is then used to simulate a macro uncertainty shock, which produces a rapid drop and rebound in aggregate output and employment. This occurs because higher uncertainty causes firms to temporarily pause their investment and hiring. Productivity growth also falls because this pause in activity freezes reallocation across units. In the medium term the increased volatility from the shock induces an overshoot in output, employment, and productivity. Thus, uncertainty shocks generate short sharp recessions and recoveries. This simulated impact of an uncertainty shock is compared to vector autoregression estimations on actual data, showing a good match in both magnitude and timing. The paper also jointly estimates labor and capital adjustment costs (both convex and nonconvex). Ignoring capital adjustment costs is shown to lead to substantial bias, while ignoring labor adjustment costs does not.</p> </abstract>
<abstract> <p>Using many moment conditions can improve efficiency but makes the usual generalized method of moments (GMM) inferences inaccurate. Two-step GMM is biased. Generalized empirical likelihood (GEL) has smaller bias, but the usual standard errors are too small in instrumental variable settings. In this paper we give a new variance estimator for GEL that addresses this problem. It is consistent under the usual asymptotics and, under many weak moment asymptotics, is larger than usual and is consistent. We also show that the Kleibergen (2005) Lagrange multiplier and conditional likelihood ratio statistics are valid under many weak moments. In addition, we introduce a jackknife GMM estimator, but find that GEL is asymptotically more efficient under many weak moments. In Monte Carlo examples we find that t-statistics based on the new variance estimator have nearly correct size in a wide range of cases.</p> </abstract>
<abstract> <p>This paper considers inference in a broad class of nonregular models. The models considered are nonregular in the sense that standard test statistics have asymptotic distributions that are discontinuous in some parameters. It is shown in Andrews and Guggenberger (2009a) that standard fixed critical value, subsampling, and m out of n bootstrap methods often have incorrect asymptotic size in such models. This paper introduces general methods of constructing tests and confidence intervals that have correct asymptotic size. In particular, we consider a hybrid subsampling/fixed-criticalvalue method and size-correction methods. The paper discusses two examples in detail. They are (i) confidence intervals in an autoregressive model with a root that may be close to unity and conditional heteroskedasticity of unknown form and (ii) tests and confidence intervals based on a post-conservative model selection estimator.</p> </abstract>
<abstract> <p>Consider a group consisting of S members facing a common budget constraint P' ξ = 1: any demand vector belonging to the budget set can be (privately or publicly) consumed by the members. Although the intragroup decision process is not known, it is assumed to generate Pareto-efficient outcomes; neither individual consumptions nor intragroup transfers are observable. The paper analyzes when, to what extent, and under which conditions it is possible to recover the underlying structure—individual preferences and the decision process—from the group's aggregate behavior. We show that the general version of the model is not identified. However, a simple exclusion assumption (whereby each member does not consume at least one good) is sufficient to guarantee generic identifiability of the welfare-relevant structural concepts.</p> </abstract>
<abstract> <p>This paper proposes a model of decision under ambiguity deemed vector expected utility, or VEU. In this model, an uncertain prospect, or Savage act, is assessed according to (a) a baseline expected-utility evaluation, and (b) an adjustment that reflects the individual's perception of ambiguity and her attitudes toward it. The adjustment is itself a function of the act's exposure to distinct sources of ambiguity, as well as its variability. The key elements of the VEU model are a baseline probability and a collection of random variables, or adjustment factors, which represent acts exposed to distinct ambiguity sources and also reflect complementarities among ambiguous events. The adjustment to the baseline expected-utility evaluation of an act is a function of the covariance of its utility profile with each adjustment factor, which reflects exposure to the corresponding ambiguity source. A behavioral characterization of the VEU model is provided. Furthermore, an updating rule for VEU preferences is proposed and characterized. The suggested updating rule facilitates the analysis of sophisticated dynamic choice with VEU preferences.</p> </abstract>
<abstract> <p>We develop a theory of optimal stopping under Knightian uncertainty. A suitable martingale theory for multiple priors is derived that extends the classical dynamic programming or Snell envelope approach to multiple priors. We relate the multiple prior theory to the classical setup via a minimax theorem. In a multiple prior version of the classical model of independent and identically distributed random variables, we discuss several examples from microeconomics, operation research, and finance. For monotone payoffs, the worst-case prior can be identified quite easily with the help of stochastic dominance arguments. For more complex payoff structures like barrier options, model ambiguity leads to stochastic changes in the worst-case beliefs.</p> </abstract>
<abstract> <p>Can incentives be effective in encouraging the development of good habits? We investigate the post-intervention effects of paying people to attend a gym a number of times during one month. In two studies we find marked attendance increases after the intervention relative to attendance changes for the respective control groups. This is entirely driven by people who did not previously attend the gym on a regular basis. In our second study, we find improvements on health indicators such as weight, waist size, and pulse rate, suggesting the intervention led to a net increase in total physical activity rather than to a substitution away from nonincentivized ones. We argue that there is scope for financial intervention in habit formation, particularly in the area of health.</p> </abstract>
<abstract> <p>We propose a new Walrasian tâtonnement process called a double-track procedure for efficiently allocating multiple heterogeneous indivisible items in two distinct sets to many buyers who view items in the same set as substitutes but items across the two sets as complements. In each round of the process, a Walrasian auctioneer first announces the current prices for all items, buyers respond by reporting their demands at these prices, and then the auctioneer adjusts simultaneously the prices of items in one set upward but those of items in the other set downward. It is shown that this procedure converges globally to a Walrasian equilibrium in finitely many rounds.</p> </abstract>
<abstract> <p>We propose a new regression method to evaluate the impact of changes in the distribution of the explanatory variables on quantiles of the unconditional (marginal) distribution of an outcome variable. The proposed method consists of running a regression of the (recentered) influence function (RIF) of the unconditional quantile on the explanatory variables. The influence function, a widely used tool in robust estimation, is easily computed for quantiles, as well as for other distributional statistics. Our approach, thus, can be readily generalized to other distributional statistics.</p> </abstract>
<abstract> <p>Two key issues in the literature on female labor supply are (i) whether persistence in employment status is due to unobserved heterogeneity or state dependence, and (ii) whether fertility is exogenous to labor supply. Until recently, the consensus was that unobserved heterogeneity is very important and fertility is endogenous. Hyslop (1999) challenged this. Using a dynamic panel probit model of female labor supply including heterogeneity and state dependence, he found that adding autoregressive errors led to a substantial diminution in the importance of heterogeneity. This, in turn, meant he could not reject that fertility is exogenous. Here, we extend Hyslop (1999) to allow classification error in employment status, using an estimation procedure developed by Keane and Wolpin (2001) and Keane and Sauer (2005). We find that a fairly small amount of classification error is enough to overturn Hyslop's conclusions, leading to overwhelming rejection of the hypothesis of exogenous fertility.</p> </abstract>
<abstract> <p>We develop a model of friendship formation that sheds light on segregation patterns observed in social and economic networks. Individuals have types and see typedependent benefits from friendships. We examine the properties of a steady-state equilibrium of a matching process of friendship formation. We use the model to understand three empirical patterns of friendship formation: (i) larger groups tend to form more same-type ties and fewer other-type ties than small groups, (ii) larger groups form more ties per capita, and (iii) all groups are biased towards same-type relative to demographics, with the most extreme bias coming from middle-sized groups. We show how these empirical observations can be generated by biases in preferences and biases in meetings. We also illustrate some welfare implications of the model.</p> </abstract>
<abstract> <p>We present evidence on the effect of social connections between workers and managers on productivity in the workplace. To evaluate whether the existence of social connections is beneficial to the firm's overall performance, we explore how the effects of social connections vary with the strength of managerial incentives and worker's ability. To do so, we combine panel data on individual worker's productivity from personnel records with a natural field experiment in which we engineered an exogenous change in managerial incentives, from fixed wages to bonuses based on the average productivity of the workers managed. We find that when managers are paid fixed wages, they favor workers to whom they are socially connected irrespective of the worker's ability, but when they are paid performance bonuses, they target their effort toward high ability workers irrespective of whether they are socially connected to them or not. Although social connections increase the performance of connected workers, we find that favoring connected workers is detrimental for the firm's overall performance.</p> </abstract>
<abstract> <p>The English auction is susceptible to tacit collusion when post-auction interbidder resale is allowed. We show this by constructing equilibria where, with positive probability, one bidder wins the auction without any competition and divides the spoils by optimally reselling the good to the other bidders. These equilibria interim Pareto-dominate (among bidders) the standard value-bidding equilibrium without requiring the bidders to make any commitment on bidding behavior or postbidding spoil division.</p> </abstract>
<abstract> <p>We characterize equilibria with endogenous debt constraints for a general equilibrium economy with limited commitment in which the only consequence of default is losing the ability to borrow in future periods. First, we show that equilibrium debt limits must satisfy a simple condition that allows agents to exactly roll over existing debt period by period. Second, we provide an equivalence result, whereby the resulting set of equilibrium allocations with self-enforcing private debt is equivalent to the allocations that are sustained with unbacked public debt or rational bubbles. In contrast to the classic result by Bulow and Rogoff (1989a), positive levels of debt are sustainable in our environment because the interest rate is sufficiently low to provide repayment incentives.</p> </abstract>
<abstract> <p>Comparative advantage, whether driven by technology or factor endowment, is at the core of neoclassical trade theory. Using tools from the mathematics of complementarity, this paper offers a simple yet unifying perspective on the fundamental forces that shape comparative advantage. The main results characterize sufficient conditions on factor productivity and factor supply to predict patterns of international specialization in a multifactor generalization of the Ricardian model which we refer to as an "elementary neoclassical economy." These conditions, which hold for an arbitrarily large number of countries, goods, and factors, generalize and extend many results from the previous trade literature. They also offer new insights about the joint effects of technology and factor endowments on international specialization.</p> </abstract>
<abstract> <p>This paper studies the nonparametric identification of the first-price auction model with risk averse bidders within the private value paradigm. First, we show that the benchmark model is nonindentified from observed bids. We also derive the restrictions imposed by the model on observables and show that these restrictions are weak. Second, we establish the nonparametric identification of the bidders' utility function under exclusion restrictions. Our primary exclusion restriction takes the form of an exogenous bidders' participation, leading to a latent distribution of private values that is independent of the number of bidders. The key idea is to exploit the property that the bid distribution varies with the number of bidders while the private value distribution does not. We then extend these results to endogenous bidders' participation when the exclusion restriction takes the form of instruments that do not affect the bidders' private value distribution. Though derived for a benchmark model, our results extend to more general cases such as a binding reserve price, affiliated private values, and asymmetric bidders. Last, possible estimation methods are proposed.</p> </abstract>
<abstract> <p>This paper considers large N and large T panel data models with unobservable multiple interactive effects, which are correlated with the regressors. In earnings studies, for example, workers' motivation, persistence, and diligence combined to influence the earnings in addition to the usual argument of innate ability. In macroeconomics, interactive effects represent unobservable common shocks and their heterogeneous impacts on cross sections. We consider identification, consistency, and the limiting distribution of the interactive-effects estimator. Under both large N and large T, the estimator is shown to be<tex-math>$\sqrt {NT} $</tex-math>consistent, which is valid in the presence of correlations and heteroskedasticities of unknown form in both dimensions. We also derive the constrained estimator and its limiting distribution, imposing additivity coupled with interactive effects. The problem of testing additive versus interactive effects is also studied. In addition, we consider identification and estimation of models in the presence of a grand mean, time-invariant regressors, and common regressors. Given identification, the rate of convergence and limiting results continue to hold.</p> </abstract>
<abstract> <p>This paper proposes a method for testing complementarities between explanatory and dependent variables in a large class of economic models. The proposed test is based on the monotone comparative statics (MCS) property of equilibria. Our main result is that MCS produces testable implications on the (small and large) quantiles of the dependent variable, despite the presence of multiple equilibria. The key features of our approach are that (i) we work with a nonparametric structural model of a continuous dependent variable in which the unobservable is allowed to be correlated with the explanatory variable in a reasonably general way; (ii) we do not require the structural function to be known or estimable; (iii) we remain fairly agnostic on how an equilibrium is selected. We illustrate the usefulness of our result for policy evaluation within Berry, Levinsohn, and Pakes's (1999) model.</p> </abstract>
<abstract> <p>This paper extends Imbens and Manski's (2004) analysis of confidence intervals for interval identified parameters. The extension is motivated by the discovery that for their final result, Imbens and Manski implicitly assumed locally superefficient estimation of a nuisance parameter. I reanalyze the problem both with assumptions that merely weaken this superefficiency condition and with assumptions that remove it altogether. Imbens and Manski's confidence region is valid under weaker assumptions than theirs, yet superefficiency is required. I also provide a confidence interval that is valid under superefficiency, but can be adapted to the general case. A methodological contribution is to observe that the difficulty of inference comes from a preestimation problem regarding a nuisance parameter, clarifying the connection to other work on partial identification.</p> </abstract>
<abstract> <p>In Bayesian environments with private information, as described by the types of Harsanyi, how can types of agents be (statistically) disassociated from each other and how are such disassociations reflected in the agents' knowledge structure? Conditions studied are (i) subjective independence (the opponents' types are independent conditional on one's own) and (ii) type disassociation under common knowledge (the agents' types are independent, conditional on some common-knowledge variable). Subjective independence is motivated by its implications in Bayesian games and in studies of equilibrium concepts. We find that a variable that disassociates types is more informative than any common-knowledge variable. With three or more agents, conditions (i) and (ii) are equivalent. They also imply that any variable which is common knowledge to two agents is common knowledge to all, and imply the existence of a unique common-knowledge variable that disassociates types, which is the one defined by Aumann.</p> </abstract>
<abstract> <p>We document cash management patterns for households that are at odds with the predictions of deterministic inventory models that abstract from precautionary motives. We extend the Baumol- Tobin cash inventory model to a dynamic environment that allows for the possibility of withdrawing cash at random times at a low cost. This modification introduces a precautionary motive for holding cash and naturally captures developments in withdrawal technology, such as the increasing diffusion of bank branches and ATM terminals. We characterize the solution of the model, which qualitatively reproduces several empirical patterns. We estimate the structural parameters using micro data and show that quantitatively the model captures important economic patterns. The estimates are used to quantify the expenditure and interest rate elasticity of money demand, the impact of financial innovation on money demand, the welfare cost of inflation, and the benefit of ATM ownership.</p> </abstract>
<abstract> <p>We develop a search-theoretic model of financial intermediation in an over-thecounter market and study how trading frictions affect the distribution of asset holdings and standard measures of liquidity. A distinctive feature of our theory is that it allows for unrestricted asset holdings, so market participants can accommodate trading frictions by adjusting their asset positions. We show that these individual responses of asset demands constitute a fundamental feature of illiquid markets: they are a key determinant of trade volume, bid-ask spreads, and trading delays—the dimensions of market liquidity that search-based theories seek to explain.</p> </abstract>
<abstract> <p>We examine the competition between a group of Internet retailers who operate in an environment where a price search engine plays a dominant role. We show that for some products in this environment, the easy price search makes demand tremendously pricesensitive. Retailers, though, engage in obfuscation—practices that frustrate consumer search or make it less damaging to firms— resulting in much less price sensitivity on some other products. We discuss several models of obfuscation and examine its effects on demand and markups empirically.</p> </abstract>
<abstract> <p>We define belief-free equilibria in two-player games with incomplete information as sequential equilibria for which players' continuation strategies are best replies after every history, independently of their beliefs about the state of nature. We characterize a set of payoffs that includes all belief- free equilibrium payoffs. Conversely, any payoff in the interior of this set is a belief-free equilibrium payoff. The characterization is applied to the analysis of reputations.</p> </abstract>
<abstract> <p>Many approaches to estimation of panel models are based on an average or integrated likelihood that assigns weights to different values of the individual effects. Fixed effects, random effects, and Bayesian approaches all fall into this category. We provide a characterization of the class of weights (or priors) that produce estimators that are first-order unbiased. We show that such bias-reducing weights will depend on the data in general unless an orthogonal reparameterization or an essentially equivalent condition is available. Two intuitively appealing weighting schemes are discussed. We argue that asymptotically valid confidence intervals can be read from the posterior distribution of the common parameters when N and T grow at the same rate. Next, we show that random effects estimators are not bias reducing in general and we discuss important exceptions. Moreover, the bias depends on the Kullback-Leibler distance between the population distribution of the effects and its best approximation in the random effects family. Finally, we show that, in general, standard random effects estimation of marginal effects is inconsistent for large T, whereas the posterior mean of the marginal effect is large-T consistent, and we provide conditions for bias reduction. Some examples and Monte Carlo experiments illustrate the results.</p> </abstract>
<abstract> <p>This paper analyzes the general nonlinear optimal income tax for couples, a multidimensional screening problem. Each couple consists of a primary earner who always participates in the labor market, but makes an hours-of-work choice, and a secondary earner who chooses whether or not to work. If second-earner participation is a signal of the couple being better (worse) off, we prove that optimal tax schemes display a positive tax (subsidy) on secondary earnings and that the tax (subsidy) on secondary earnings decreases with primary earnings and converges to zero asymptotically. We present calibrated microsimulations for the United Kingdom showing that decreasing tax rates on secondary earnings is quantitatively significant and consistent with actual income tax and transfer programs.</p> </abstract>
<abstract> <p>I construct a theoretical framework in which firms offer wage-tenure contracts to direct the search by risk-averse workers. All workers can search, on or off the job. I characterize an equilibrium and prove its existence. The equilibrium generates a nondegenerate, continuous distribution of employed workers over the values of contracts, despite that all matches are identical and workers observe all offers. A striking property is that the equilibrium is block recursive; that is, individuals' optimal decisions and optimal contracts are independent of the distribution of workers. This property makes the equilibrium analysis tractable. Consistent with stylized facts, the equilibrium predicts that (i) wages increase with tenure, (ii) job-to-job transitions decrease with tenure and wages, and (iii) wage mobility is limited in the sense that the lower the worker's wage, the lower the future wage a worker will move to in the next job transition. Moreover, block recursivity implies that changes in the unemployment benefit and the minimum wage have no effect on an employed worker's job-to-job transitions and contracts.</p> </abstract>
<abstract> <p>We propose a test of the hypothesis of stochastic monotonicity. This hypothesis is of interest in many applications in economics. Our test is based on the supremum of a rescaled U -statistic. We show that its asymptotic distribution is Gumbel. The proof is difficult because the approximating Gaussian stochastic process contains both a stationary and a nonstationary part, and so we have to extend existing results that only apply to either one or the other case. We also propose a refinement to the asymptotic approximation that we show works much better in finite samples. We apply our test to the study of intergenerational income mobility.</p> </abstract>
<abstract> <p>This paper describes a direct revelation mechanism for eliciting agents' subjective probabilities. The game induced by the mechanism has a dominant strategy equilibrium in which the players reveal their subjective probabilities.</p> </abstract>
<abstract> <p>Learning-by-doing and organizational forgetting are empirically important in a variety of industrial settings. This paper provides a general model of dynamic competition that accounts for these fundamentals and shows how they shape industry structure and dynamics. We show that forgetting does not simply negate learning. Rather, they are distinct economic forces that interact in subtle ways to produce a great variety of pricing behaviors and industry dynamics. In particular, a model with learning and forgetting can give rise to aggressive pricing behavior, varying degrees of long-run industry concentration ranging from moderate leadership to absolute dominance, and multiple equilibria.</p> </abstract>
<abstract> <p>We examine the labor market effects of incomplete information about the workers' own job-finding process. Search outcomes convey valuable information, and learning from search generates endogenous heterogeneity in workers' beliefs about their jobfinding probability. We characterize this process and analyze its interactions with job creation and wage determination. Our theory sheds new light on how unemployment can affect workers' labor market outcomes and wage determination, providing a rational explanation for discouragement as the consequence of negative search outcomes. In particular, longer unemployment durations are likely to be followed by lower reemployment wages because a worker's beliefs about his job-finding process deteriorate with unemployment duration. Moreover, our analysis provides a set of useful results on dynamic programming with optimal learning.</p> </abstract>
<abstract> <p>We investigate the role of search frictions in markets with price competition and how it leads to sorting of heterogeneous agents. There are two aspects of value creation: the match value when two agents actually trade and the probability of trading governed by the search technology. We show that positive assortative matching obtains when complementarities in the former outweigh complementarities in the latter. This happens if and only if the match-value function is root-supermodular, that is, its nth root is supermodular, where n reflects the elasticity of substitution of the search technology. This condition is weaker than the condition required for positive assortative matching in markets with random search.</p> </abstract>
<abstract> <p>Single equation instrumental variable models for discrete outcomes are shown to be set identifying, not point identifying, for the structural functions that deliver the values of the discrete outcome. Bounds on identified sets are derived for a general nonparametric model and sharp set identification is demonstrated in the binary outcome case. Point identification is typically not achieved by imposing parametric restrictions. The extent of an identified set varies with the strength and support of instruments, and typically shrinks as the support of a discrete outcome grows. The paper extends the analysis of structural quantile functions with endogenous arguments to cases in which there are discrete outcomes.</p> </abstract>
<abstract> <p>Unlike the prediction of a frictionless open economy model, long-term average savings and investment rates are highly correlated across countries—a puzzle first identified by Feldstein and Horioka (1980). We quantitatively investigate the impact of two types of financial frictions on this correlation. One is limited enforcement, where contracts are enforced by the threat of default penalties. The other is limited spanning, where the only asset available is noncontingent bonds. We find that the calibrated model with both frictions produces a savings-investment correlation and a volume of capital flows close to the data. To solve the puzzle, the limited enforcement friction needs low default penalties under which capital flows are much lower than those in the data, and the limited spanning friction needs to exogenously restrict capital flows to the observed level. When combined, the two frictions interact to endogenously restrict capital flows and thereby solve the Feldstein-Horioka puzzle.</p> </abstract>
<abstract> <p>The deferred acceptance algorithm is often used to allocate indivisible objects when monetary transfers are not allowed. We provide two characterizations of agentproposing deferred acceptance allocation rules. Two new axioms—individually rational monotonicity and weak Maskin monotonicity—are essential to our analysis. An allocation rule is the agent-proposing deferred acceptance rule for some acceptant substitutable priority if and only if it satisfies non-wastefulness and individually rational monotonicity. An alternative characterization is in terms of non-wastefulness, population monotonicity, and weak Maskin monotonicity. We also offer an axiomatization of the deferred acceptance rule generated by an exogenously specified priority structure. We apply our results to characterize efficient deferred acceptance rules.</p> </abstract>
<abstract> <p>The subjective likelihood of a contingency often depends on the manner in which it is described to the decision maker. To accommodate this dependence, we introduce a model of decision making under uncertainty that takes as primitive a family of preferences indexed by partitions of the state space. Each partition corresponds to a description of the state space. We characterize the following partition-dependent expected utility representation. The decision maker has a nonadditive set function v over events. Given a partition of the state space, she computes expected utility with respect to her partition-dependent belief, which weights each cell in the partition by v. Nonadditivity of v allows the probability of an event to depend on the way in which the state space is described. We propose behavioral definitions for those events that are transparent to the decision maker and those that are completely overlooked, and connect these definitions to conditions on the representation.</p> </abstract>
<abstract> <p>This paper considers model averaging as a way to construct optimal instruments for the two-stage least squares (2SLS), limited information maximum likelihood (LIML), and Fuller estimators in the presence of many instruments. We propose averaging across least squares predictions of the endogenous variables obtained from many different choices of instruments and then use the average predicted value of the endogenous variables in the estimation stage. The weights for averaging are chosen to minimize the asymptotic mean squared error of the model averaging version of the 2SLS, LIML, or Fuller estimator. This can be done by solving a standard quadratic programming problem.</p> </abstract>
<abstract> <p>A model for binary panel data is introduced which allows for state dependence and unobserved heterogeneity beyond the effect of available covariates. The model is of quadratic exponential type and its structure closely resembles that of the dynamic logit model. However, it has the advantage of being easily estimable via conditional likelihood with at least two observations (further to an initial observation) and even in the presence of time dummies among the regressors.</p> </abstract>
<abstract> <p>This paper introduces a novel bootstrap procedure to perform inference in a wide class of partially identified econometric models. We consider econometric models defined by finitely many weak moment inequalities, which encompass many applications of economic interest. The objective of our inferential procedure is to cover the identified set with a prespecified probability. We compare our bootstrap procedure, a competing asymptotic approximation, and subsampling procedures in terms of the rate at which they achieve the desired coverage level, also known as the error in the coverage probability. Under certain conditions, we show that our bootstrap procedure and the asymptotic approximation have the same order of error in the coverage probability, which is smaller than that obtained by using subsampling. This implies that inference based on our bootstrap and asymptotic approximation should eventually be more precise than inference based on subsampling. A Monte Carlo study confirms this finding in a small sample simulation.</p> </abstract>
<abstract> <p>A decision maker (DM) is characterized by two binary relations. The first reflects choices that are rational in an "objective" sense: the DM can convince others that she is right in making them. The second relation models choices that are rational in a "subjective" sense: the DM cannot be convinced that she is wrong in making them. In the context of decision under uncertainty, we propose axioms that the two notions of rationality might satisfy. These axioms allow a joint representation by a single set of prior probabilities and a single utility index. It is "objectively rational" to choose f in the presence of g if and only if the expected utility of f is at least as high as that of g given each and every prior in the set. It is "subjectively rational" to choose f rather than g if and only if the minimal expected utility of f (with respect to all priors in the set) is at least as high as that of g. In other words, the objective and subjective rationality relations admit, respectively, a representation a la Bewley (2002) and a la Gilboa and Schmeidler (1989). Our results thus provide a bridge between these two classic models, as well as a novel foundation for the latter.</p> </abstract>
<abstract> <p>We consider truthful implementation of the socially efficient allocation in an independent private-value environment in which agents receive private information over time. We propose a suitable generalization of the pivot mechanism, based on the marginal contribution of each agent. In the dynamic pivot mechanism, the ex post incentive and ex post participation constraints are satisfied for all agents after all histories. In an environment with diverse preferences it is the unique mechanism satisfying ex post incentive, ex post participation, and efficient exit conditions. We develop the dynamic pivot mechanism in detail for a repeated auction of a single object in which each bidder learns over time her true valuation of the object. The dynamic pivot mechanism here is equivalent to a modified second price auction.</p> </abstract>
<abstract> <p>We consider a class of mechanism games in which there are multiple principals and three or more agents. For a mechanism game in this class, a sort of folk theorem holds: there is a threshold value for each of the principals such that an allocation is achieved at a pure-strategy sequential equilibrium of the game if and only if (i) it is incentive compatible and (ii) it attains an expected utility for each principal that is greater than or equal to the threshold value for the principal.</p> </abstract>
<abstract> <p>We present a comprehensive framework for Bayesian estimation of structural nonlinear dynamic economic models on sparse grids to overcome the curse of dimensionality for approximations. We apply sparse grids to a global polynomial approximation of the model solution, to the quadrature of integrals arising as rational expectations, and to three new nonlinear state space filters which speed up the sequential importance resampling particle filter. The posterior of the structural parameters is estimated by a new Metropolis-Hastings algorithm with mixing parallel sequences. The parallel extension improves the global maximization property of the algorithm, simplifies the parameterization for an appropriate acceptance ratio, and allows a simple implementation of the estimation on parallel computers. Finally, we provide all algorithms in the open source software JBendge for the solution and estimation of a general class of models.</p> </abstract>
<abstract> <p>Fudenberg and Levine (1993a) introduced the notion of self-confirming equilibrium, which is generally less restrictive than Nash equilibrium. Fudenberg and Levine also defined a concept of consistency, and claimed in their Theorem 4 that with consistency and other conditions on beliefs, a self-confirming equilibrium has a Nash equilibrium outcome. We provide a counterexample that disproves Theorem 4 and prove an alternative by replacing consistency with a more restrictive concept, which we call strong consistency. In games with observed deviators, self-confirming equilibria are strongly consistent self-confirming equilibria. Hence, our alternative theorem ensures that despite the counterexample, the corollary of Theorem 4 is still valid.</p> </abstract>
<abstract> <p>Recursive procedures which are based on iterating on the best response mapping have difficulties converging to all equilibria in multi-player games. We illustrate these difficulties by revisiting the asymptotic properties of the iterative nested pseudo maximum likelihood method for estimating dynamic games introduced by Aguirregabiria and Mira (2007). An example shows that the iterative method may not be consistent.</p> </abstract>
<abstract> <p>We show that in repeated interactions the avenues for effective provision of incentives depend crucially on the type of information players observe. We establish this conclusion for general repeated two-player games in which information arrives via a continuous-time stationary process that has a continuous multidimensional Brownian component and a Poisson component, and in which the players act frequently. The Poisson jumps can be used to effectively provide incentives both with transfers and value burning, while continuous Brownian information can be used to provide incentives only with transfers.</p> </abstract>
<abstract> <p>This paper formulates and estimates multistage production functions for children's cognitive and noncognitive skills. Skills are determined by parental environments and investments at different stages of childhood. We estimate the elasticity of substitution between investments in one period and stocks of skills in that period to assess the benefits of early investment in children compared to later remediation. We establish nonparametric identification of a general class of production technologies based on nonlinear factor models with endogenous inputs. A by-product of our approach is a framework for evaluating childhood and schooling interventions that does not rely on arbitrarily scaled test scores as outputs and recognizes the differential effects of the same bundle of skills in different tasks. Using the estimated technology, we determine optimal targeting of interventions to children with different parental and personal birth endowments. Substitutability decreases in later stages of the life cycle in the production of cognitive skills. It is roughly constant across stages of the life cycle in the production of noncognitive skills. This finding has important implications for the design of policies that target the disadvantaged. For most configurations of disadvantage it is optimal to invest relatively more in the early stages of childhood than in later stages.</p> </abstract>
<abstract> <p>This paper combines dynamic social choice and strategic experimentation to study the following question: How does a society, a committee, or, more generally, a group of individuals with potentially heterogeneous preferences, experiment with new opportunities? Each voter recognizes that, during experimentation, other voters also learn about their preferences. As a result, pivotal voters today are biased against experimentation because it reduces their likelihood of remaining pivotal. This phenomenon reduces equilibrium experimentation below the socially efficient level, and may even result in a negative option value of experimentation. However, one can restore efficiency by designing a voting rule that depends deterministically on time. Another main result is that even when payoffs of a reform are independently distributed across the population, good news about any individual's payoff increases other individuals' incentives to experiment with that reform, due to a positive voting externality.</p> </abstract>
<abstract> <p>This paper develops a framework to assess how fear of miscoordination affects the sustainability of cooperation. Building on theoretical insights from Carlsson and van Damme (1993), it explores the effect of small amounts of private information on a class of dynamic cooperation games with exit. Lack of common knowledge leads players to second guess each other's behavior and makes coordination difficult. This restricts the range of equilibria and highlights the role of miscoordination payoffs in determining whether cooperation is sustainable or not. The paper characterizes the range of perfect Bayesian equilibria as the players' information becomes arbitrarily precise. Unlike in one-shot two-by-two games, the global games information structure does not yield equilibrium uniqueness.</p> </abstract>
<abstract> <p>This paper provides a novel approach to ordering signals based on the property that more informative signals lead to greater variability of conditional expectations. We define two nested information criteria (supermodular precision and integral precision) by combining this approach with two variability orders (dispersive and convex orders). We relate precision criteria with orderings based on the value of information to a decision maker. We then use precision to study the incentives of an auctioneer to supply private information. Using integral precision, we obtain two results: (i) a more precise signal yields a more efficient allocation; (ii) the auctioneer provides less than the efficient level of information. Supermodular precision allows us to extend the previous analysis to the case in which supplying information is costly and to obtain an additional finding; (iii) there is a complementarity between information and competition, so that both the socially efficient and the auctioneer's optimal choice of precision increase with the number of bidders.</p> </abstract>
<abstract> <p>Much of the extensive empirical literature on insurance markets has focused on whether adverse selection can be detected. Once detected, however, there has been little attempt to quantify its welfare cost or to assess whether and what potential government interventions may reduce these costs. To do so, we develop a model of annuity contract choice and estimate it using data from the U. K. annuity market. The model allows for private information about mortality risk as well as heterogeneity in preferences over different contract options. We focus on the choice of length of guarantee among individuals who are required to buy annuities. The results suggest that asymmetric information along the guarantee margin reduces welfare relative to a first-best symmetric information benchmark by about £ 127 million per year or about 2 percent of annuitized wealth. We also find that by requiring that individuals choose the longest guarantee period allowed, mandates could achieve the first-best allocation. However, we estimate that other mandated guarantee lengths would have detrimental effects on welfare. Since determining the optimal mandate is empirically difficult, our findings suggest that achieving welfare gains through mandatory social insurance may be harder in practice than simple theory may suggest.</p> </abstract>
<abstract> <p>This paper proposes a method to address the longstanding problem of lack of monotonicity in estimation of conditional and structural quantile functions, also known as the quantile crossing problem (Bassett and Koenker (1982)). The method consists in sorting or monotone rearranging the original estimated non-monotone curve into a monotone rearranged curve. We show that the rearranged curve is closer to the true quantile curve than the original curve in finite samples, establish a functional delta method for rearrangement-related operators, and derive functional limit theory for the entire rearranged curve and its functionals. We also establish validity of the bootstrap for estimating the limit law of the entire rearranged curve and its functionals. Our limit results are generic in that they apply to every estimator of a monotone function, provided that the estimator satisfies a functional central limit theorem and the function satisfies some smoothness conditions. Consequently, our results apply to estimation of other econometric functions with monotonicity restrictions, such as demand, production, distribution, and structural distribution functions. We illustrate the results with an application to estimation of structural distribution and quantile functions using data on Vietnam veteran status and earnings.</p> </abstract>
<abstract> <p>This paper proves the existence and uniqueness of a fixed point for local contractions without assuming the family of contraction coefficients to be uniformly bounded away from 1. More importantly it shows how this fixed-point result can apply to study the existence and uniqueness of solutions to some recursive equations that arise in economic dynamics.</p> </abstract>
<abstract> <p>Does expertise in strategic behavior obtained in the field transfer to the abstract setting of the laboratory? Palacios-Huerta and Volij (2008) argued that the behavior of professional soccer players in mixed-strategy games conforms closely to minimax play, while the behavior of students (who are presumably novices in strategic situations requiring unpredictability) does not. We reexamine their data, showing that the play of professionals is inconsistent with the minimax hypothesis in several respects: (i) professionals follow nonstationary mixtures, with action frequencies that are negatively correlated between the first and the second half of the experiment, (ii) professionals tend to switch between under-and overplaying an action relative to its equilibrium frequency, and (iii) the distribution of action frequencies across professionals is far from the distribution implied by minimax. In each respect, the behavior of students conforms more closely to the minimax hypothesis.</p> </abstract>
<abstract> <p>Sannikov (2007) investigated properties of perfect public equilibria in continuoustime repeated games. This note points out that the proof of Lemma 6, required for the proof of the main theorem (Theorem 2), contains an error in computing a Hessian matrix. A correct proof of Lemma 6 is provided using an additional innocuous assumption and a generalized version of Lemma 5.</p> </abstract>
<abstract> <p>This paper provides a directed search model designed to explain the residual part of wage variation left over after the impact of education and other observable worker characteristics have been removed. orkers have private information about their characteristics at the time they apply for jobs. Firms value these characteristics differently and can observe them once workers apply. They hire the worker they most prefer. However, the characteristics are not contractible, so firms cannot condition their wages on them. This paper shows how to extend arguments from directed search to handle this, allowing for arbitrary distributions of worker and firm types. The model is used to provide a functional relationship that ties together the wage distribution and the wageduration function. This relationship provides a testable implication of the model. This relationship suggests a common property of wage distributions that guarantees that workers who leave unemployment at the highest wages also have the shortest unemployment duration. This is in strict contrast to the usual (and somewhat implausible) directed search story in which high wages are always accompanied by higher probability of unemployment.</p> </abstract>
<abstract> <p>This paper develops a technique for studying incentive problems with unidimensional hidden characteristics in a way that is independent of whether the type set is finite, the type distribution has a continuous density, or the type distribution has both mass points and an atomless part. By this technique, the proposition that optimal incentive schemes induce no distortion "at the top" and downward distortions "below the top" is extended to arbitrary type distributions. However, mass points in the interior of the type set require pooling with adjacent higher types and, unless there are other complications, a discontinuous jump in the transition from adjacent lower types.</p> </abstract>
<abstract> <p>This paper develops a new framework for examining the determinants of wage distributions that emphasizes within-industry reallocation, labor market frictions, and differences in workforce composition across firms. More productive firms pay higher wages and exporting increases te wage paid by a firm with a given productivity. The opening of trade enhances wage inequality and can either raise or reduce unemployment. While wage inequality is higher in a trade equilibrium than in autarky, gradual trade liberalization first increases and later decreases inequality.</p> </abstract>
<abstract> <p>We study preferences over menus which can be represented as if the individual is uncertain of her tastes, but is able to engage in costly contemplation before selecting an alternative from a menu. Since contemplation is costly, our key axiom, aversion to contingent planning, reflects the individual's preference to learn the menu from which she will be choosing prior to engaging in contemplation about her tastes for the alternatives. Our representation models contemplation strategies as subjective signals over a subjective state space. The subjectivity of the state space and the information structure in our representation makes it difficult to identify them from the preference. To overcome this issue, we show that each signal can be modeled in reduced form as a measure over ex post utility functions without reference to a state space. We show that in this reduced-form representation, the set of measures and their costs are uniquely identified. Finally, we provide a measure of comparative contemplation costs and characterize the special case of our representation where contemplation is costless.</p> </abstract>
<abstract> <p>We provide theoretical foundations for several common (nested) representations of intrinsic linear habit formation. Our axiomatization introduces an intertemporal theory of weaning a decision-maker from her habits using the device of compensation. We clarify differences across specifications of the model, provide measures of habit-forming tendencies, and suggest methods for axiomatizing time-nonseparable preferences.</p> </abstract>
<abstract> <p>It has long been recognized that there is considerable heterogeneity in individual risk taking behavior, but little is known about the distribution of risk taking types. We present a parsimonious characterization of risk taking behavior by estimating a finite mixture model for three different experimental data sets, two Swiss and one Chinese, over a large number of real gains and losses. We find two major types of individuals: In all three data sets, the choices of roughly 80% of the subjects exhibit significant deviations from linear probability weighting of varying strength, consistent with prospect theory. Twenty percent of the subjects weight probabilities near linearly and behave essentially as expected value maximizers. Moreover, individuals are cleanly assigned to one type with probabilities close to unity. The reliability and robustness of our classification suggest using a mix of preference theories in applied economic modeling.</p> </abstract>
<abstract> <p>The minimax argument represents game theory in its most elegant form: simple but with stark predictions. Although some of these predictions have been met with reasonable success in the field, experimental data have generally not provided results close to the theoretical predictions. In a striking study, Palacios-Huerta and Volij (2008) presented evidence that potentially resolves this puzzle: both amateur and professional soccer players play nearly exact minimax strategies in laboratory experiments. In this paper, we establish important bounds on these results by examining the behavior of four distinct subject pools: college students, bridge professionals, world-class poker players, who have vast experience with high-stakes randomization in card games, and American professional soccer players. In contrast to Palacios-Huerta and Volij's results, we find little evidence that real-world experience transfers to the lab in these games— indeed, similar to previous experimental results, all four subject pools provide choices that are generally not close to minimax predictions. We use two additional pieces of evidence to explore why professionals do not perform well in the lab: (i) complementary experimental treatments that pit professionals against preprogrammed computers and (ii) post-experiment questionnaires. The most likely explanation is that these professionals are unable to transfer their skills at randomization from the familiar context of the field to the unfamiliar context of the lab.</p> </abstract>
<abstract> <p>We use a second-price common-value auction, called the maximal game, to experimentally study whether the winner's curse (WC) can be explained by models which retain best-response behavior but allow for inconsistent beliefs. We compare behavior in a regular version of the maximal game, where the WC can be explained by inconsistent beliefs, to behavior in versions where such explanations are less plausible. We find little evidence of differences in behavior. Overall, our study casts a serious doubt on theories that posit the WC is driven by beliefs.</p> </abstract>
<abstract> <p>Behavioral choice models generate inequalities which, when combined with additional assumptions, can be used as a basis for estimation. This paper considers two sets of such assumptions and uses them in two empirical examples. The second example examines the structure of payments resulting from the upstream interactions in a vertical market. I then mimic the empirical setting for this example in a numerical analysis which computes actual equilibria, examines how their characteristics vary with the market setting, and compares them to the empirical results. The final section uses the numerical results in a Monte Carlo analysis of the robustness of the two approaches to estimation to their underlying assumptions.</p> </abstract>
<abstract> <p>We study economies with adverse selection, plus the frictions in competitive search theory. With competitive search, principals post terms of trade (contracts), then agents choose where to apply, and they match bilaterally. Search allows us to analyze the effects of private information on both the intensive and extensive margins (the terms and probability of trade). There always exists a separating equilibrium where each type applies to a different contract. The equilibrium is unique in terms of payoffs. It is not generally efficient. We provide an algorithm for constructing equilibrium. Three applications illustrate the usefulness of the approach, and contrast our results with those in standard contract and search theory.</p> </abstract>
<abstract> <p>This study utilizes regression discontinuity to examine the long-run impacts of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812. Results indicate that a mita effect lowers household consumption by around 25% and increases the prevalence of stunted growth in children by around 6 percentage points in subjected districts today. Using data from the Spanish Empire and Peruvian Republic to trace channels of institutional persistence, I show that the mita's influence has persisted through its impacts on land tenure and public goods provision. Mita districts historically had fewer large landowners and lower educational attainment. Today, they are less integrated into road networks and their residents are substantially more likely to be subsistence farmers.</p> </abstract>
<abstract> <p>We prove—in the standard independent private-values model—that the outcome, in terms of interim expected probabilities of trade and interim expected transfers, of any Bayesian mechanism can also be obtained with a dominant-strategy mechanism.</p> </abstract>
<abstract> <p>Harsanyi's impartial observer must consider two types of lotteries: imaginary identity lotteries ("accidents of birth") that she faces as herself and the real outcome lotteries ("life chances") to be faced by the individuals she imagines becoming. If we maintain a distinction between identity and outcome lotteries, then Harsanyi-like axioms yield generalized utilitarianism, and allow us to accommodate concerns about different individuals' risk attitudes and concerns about fairness. Requiring an impartial observer to be indifferent as to which individual should face similar risks restricts her social welfare function, but still allows her to accommodate fairness. Requiring an impartial observer to be indifferent between identity and outcome lotteries, however, forces her to ignore both fairness and different risk attitudes, and yields a new axiomatization of Harsanyi's utilitarianism.</p> </abstract>
<abstract> <p>Experimental evidence suggests that individuals are more risk averse when they perceive risk that is gradually resolved over time. We address these findings by studying a decision maker who has recursive, nonexpected utility preferences over compound lotteries. The decision maker has preferences for one-shot resolution of uncertainty if he always prefers any compound lottery to be resolved in a single stage. We establish an equivalence between dynamic preferences for one-shot resolution of uncertainty and static preferences that are identified with commonly observed behavior in Allais-type experiments. The implications of this equivalence on preferences over information systems are examined. We define the gradual resolution premium and demonstrate its magnifying effect when combined with the usual risk premium.</p> </abstract>
<abstract> <p>Simple exchange experiments have revealed that participants trade their endowment less frequently than standard demand theory would predict. List (2003a) found that the most experienced dealers acting in a well functioning market are not subject to this exchange asymmetry, suggesting that a significant amount of market experience is required to overcome it. To understand this market-experience effect, we introduce a distinction between two types of uncertainty—choice uncertainty and trade uncertainty—both of which could lead to exchange asymmetry. We conjecture that trade uncertainty is most important for exchange asymmetry. To test this conjecture, we design an experiment where the two treatments impact differently on trade uncertainty, while controlling for choice uncertainty. Supporting our conjecture, we find that "forcing" subjects to give away their endowment in a series of exchanges eliminates exchange asymmetry in a subsequent test. We discuss why markets might not provide sufficient incentives for learning to overcome exchange asymmetry.</p> </abstract>
<abstract> <p>In weighted moment condition models, we show a subtle link between identification and estimability that limits the practical usefulness of estimators based on these models. In particular, if it is necessary for (point) identification that the weights take arbitrarily large values, then the parameter of interest, though point identified, cannot be estimated at the regular (parametric) rate and is said to be irregularly identified. This rate depends on relative tail conditions and can be as slow in some examples as $n^{ - (1/4)}$ This nonstandard rate of convergence can lead to numerical instability and/or large standard errors. We examine two weighted model examples: (i) the binary response model under mean restriction introduced by Lewbel (1997) and further generalized to cover endogeneity and selection, where the estimator in this class of models is weighted by the density of a special regressor, and (ii) the treatment effect model under exogenous selection (Rosenbaum and Rubin (1983)), where the resulting estimator of the average treatment effect is one that is weighted by a variant of the propensity score. Without strong relative support conditions, these models, similar to well known "identified at infinity" models, lead to estimators that converge at slower than parametric rate, since essentially, to ensure point identification, one requires some variables to take values on sets with arbitrarily small probabilities, or thin sets. For the two models above, we derive some rates of convergence and propose that one conducts inference using rate adaptive procedures that are analogous to Andrews and Schafgans (1998) for the sample selection model.</p> </abstract>
<abstract> <p>A unifying framework to test for causal effects in nonlinear models is proposed. We consider a generalized linear-index regression model with endogenous regressors and no parametric assumptions on the error disturbances. To test the significance of the effect of an endogenous regressor, we propose a statistic that is a kernel-weighted version of the rank correlation statistic (tau) of Kendall (1938). The semiparametric model encompasses previous cases considered in the literature (continuous endogenous regressors (Blundell and Powell (2003)) and a single binary endogenous regressor (Vytlacil and Yildiz (2007))), but the testing approach is the first to allow for (i) multiple discrete endogenous regressors, (ii) endogenous regressors that are neither discrete nor continuous (e.g., a censored variable), and (iii) an arbitrary "mix" of endogenous regressors (e.g., one binary regressor and one continuous regressor).</p> </abstract>
<abstract> <p>We study optimal taxation when consumers have temptation and self-control problems. Embedding the class of preferences developed by Gul and Pesendorfer into a standard macroeconomic setting, we first prove, in a two-period model, that the optimal policy is to subsidize savings when consumers are tempted by "excessive" impatience. The savings subsidy improves welfare because it makes succumbing to temptation less attractive. We then study an economy with a long but finite horizon which nests, as a special case, the Phelps-Pollak-Laibson multiple-selves model (thereby providing guidance on how to evaluate welfare in this model). We prove that when period utility is logarithmic, the optimal savings subsidies increase over time for any finite horizon. Moreover, as the horizon grows large, the optimal policy prescribes a constant subsidy, in contrast to the well known Chamley-Judd result.</p> </abstract>
<abstract> <p>Two Ellsberg-style thought experiments are described that reflect on the smooth ambiguity decision model developed by Klibanoff, Marinacci, and Mukerji (2005). The first experiment poses difficulties for the model's axiomatic foundations and, as a result, also for its interpretation, particularly for the claim that the model achieves a separation between ambiguity and the attitude toward ambiguity. Given the problematic nature of its foundations, the behavioral content of the model and how it differs from multiple priors, for example, are not clear. The second thought experiment casts some light on these questions.</p> </abstract>
<abstract> <p>This paper extends the static analysis of oligopoly structure into an infinite-horizon setting with sunk costs and demand uncertainty. The observation that exit rates decline with firm age motivates the assumption of last-in first-out dynamics: An entrant expects to produce no longer than any incumbent. This selects an essentially unique Markovperfect equilibrium. With mild restrictions on the demand shocks, sequences of thresholds describe firms' equilibrium entry and survival decisions. Bresnahan and Reiss' (1993) empirical analysis of oligopolists' entry and exit assumes that such thresholds govern the evolution of the number of competitors. Our analysis provides an infinitehorizon game-theoretic foundation for that structure.</p> </abstract>
<abstract> <p>We discuss the identification and estimation of discrete games of complete information. Following Bresnahan and Reiss (1990,1991), a discrete game is a generalization of a standard discrete choice model where utility depends on the actions of other players. Using recent algorithms to compute all of the Nash equilibria to a game, we propose simulation-based estimators for static, discrete games. We demonstrate that the model is identified under weak functional form assumptions using exclusion restrictions and an identification at infinity approach. Monte Carlo evidence demonstrates that the estimator can perform well in moderately sized samples. As an application, we study entry decisions by construction contractors to bid on highway projects in California. We find that an equilibrium is more likely to be observed if it maximizes joint profits, has a higher Nash product, uses mixed strategies, and is not Pareto dominated by another equilibrium.</p> </abstract>
<abstract> <p>This paper studies the identification and estimation of preferences and technologies in equilibrium hedonic models. In it, we identify nonparametric structural relationships with nonadditive heterogeneity. We determine what features of hedonic models can be identified from equilibrium observations in a single market under weak assumptions about the available information. We then consider use of additional information about structural functions and heterogeneity distributions. Separability conditions facilitate identification of consumer marginal utility and firm marginal product functions. We also consider how identification is facilitated using multimarket data.</p> </abstract>
<abstract> <p>The coalitional Nash bargaining solution is defined to be the core allocation for which the product of players' payoffs is maximal. We consider a non-cooperative model with discounting in which one team may form and every player is randomly selected to make a proposal in every period. The grand team, consisting of all players, generates the largest surplus. But a smaller team may form. We show that as players get more patient if an efficient and stationary equilibrium exists, it must deliver payoffs that correspond to the coalitional Nash bargaining solution. We also characterize when an efficient and stationary equilibrium exists, which requires conditions that go beyond the nonemptiness of the core.</p> </abstract>
<abstract> <p>The random priority (random serial dictatorship) mechanism is a common method for assigning objects. The mechanism is easy to implement and strategy-proof. However, this mechanism is inefficient, because all agents may be made better off by another mechanism that increases their chances of obtaining more preferred objects. This form of inefficiency is eliminated by a mechanism called probabilistic serial, but this mechanism is not strategy-proof. Thus, which mechanism to employ in practical applications is an open question. We show that these mechanisms become equivalent when the market becomes large. More specifically, given a set of object types, the random assignments in these mechanisms converge to each other as the number of copies of each object type approaches infinity. Thus, the inefficiency of the random priority mechanism becomes small in large markets. Our result gives some rationale for the common use of the random priority mechanism in practical problems such as student placement in public schools.</p> </abstract>
<abstract> <p>This paper studies repeated games with imperfect public monitoring where the players are uncertain both about the payoff functions and about the relationship between the distribution of signals and the actions played. We introduce the concept of perfect public ex post equilibrium (PPXE), and show that it can be characterized with an extension of the techniques used to study perfect public equilibria. We develop identifiability conditions that are sufficient for a folk theorem; these conditions imply that there are PPXE in which the payoffs are approximately the same as if the monitoring structure and payoff functions were known. Finally, we define perfect type-contingently public ex post equilibria (PTXE), which allows players to condition their actions on their initial private information, and we provide its linear programming characterization.</p> </abstract>
<abstract> <p>We offer an axiomatization of the serial cost-sharing method of Friedman and Moulin (1999). The key property in our axiom system is Group Demand Monotonicity, asking that when a group of agents raise their demands, not all of them should pay less.</p> </abstract>
<abstract> <p>Consider an environment with a finite number of alternatives, and agents with private values and quasilinear utility functions. A domain of valuation functions for an agent is a monotonicity domain if every finite-valued monotone randomized allocation rule defined on it is implementable in dominant strategies. We fully characterize the set of all monotonicity domains.</p> </abstract>
<abstract> <p>We study the evolution of market-oriented policies over time and across countries. We consider a model in which own and neighbors' past experiences influence policy choices through their effect on policymakers' beliefs. We estimate the model using a large panel of countries and find that it fits a large fraction of the policy choices observed in the postwar data, including the slow adoption of liberal policies. Our model also predicts that there would be reversals to state intervention if nowadays the world was hit by a shock of the size of the Great Depression.</p> </abstract>
<abstract> <p>This paper axiomatizes the robust control criterion of multiplier preferences introduced by Hansen and Sargent (2001). The axiomatization relates multiplier preferences to other classes of preferences studied in decision theory, in particular, the variational preferences recently introduced by Maccheroni, Marinacci, and Rustichini (2006a). This paper also establishes a link between the parameters of the multiplier criterion and the observable behavior of the agent. This link enables measurement of the parameters on the basis of observable choice data and provides a useful tool for applications.</p> </abstract>
<abstract> <p>This paper develops a tractable version of the Lucas and Prescott (1974) search model. Each of a continuum of industries produces a heterogeneous good using a production technology that is continually hit by idiosyncratic shocks. In response to adverse shocks, some workers search for new industries while others are rest unemployed, waiting for their industry's condition to improve. We obtain closed-form expressions for key aggregate variables and use them to evaluate the model's quantitative predictions for unemployment and wages. Both search and rest unemployment are important for understanding the behavior of wages at the industry level.</p> </abstract>
<abstract> <p>We study the dynamics of the distribution of wealth in an overlapping generation economy with finitely lived agents and intergenerational transmission of wealth. Financial markets are incomplete, exposing agents to both labor and capital income risk. We show that the stationary wealth distribution is a Pareto distribution in the right tail and that it is capital income risk, rather than labor income, that drives the properties of the right tail of the wealth distribution. We also study analytically the dependence of the distribution of wealth—of wealth inequality in particular—on various fiscal policy instruments like capital income taxes and estate taxes, and on different degrees of social mobility. We show that capital income and estate taxes can significantly reduce wealth inequality, as do institutions favoring social mobility. Finally, we calibrate the economy to match the Lorenz curve of the wealth distribution of the U.S. economy.</p> </abstract>
<abstract> <p>This paper investigates an empirical puzzle in technology adoption for developing countries: the low adoption rates of technologies like hybrid maize that increase average farm profits dramatically. I offer a simple explanation for this: benefits and costs of technologies are heterogeneous, so that farmers with low net returns do not adopt the technology. I examine this hypothesis by estimating a correlated random coefficient model of yields and the corresponding distribution of returns to hybrid maize. This distribution indicates that the group of farmers with the highest estimated gross returns does not use hybrid, but their returns are correlated with high costs of acquiring the technology (due to poor infrastructure). Another group of farmers has lower returns and adopts, while the marginal farmers have zero returns and switch in and out of use over the sample period. Overall, adoption decisions appear to be rational and well explained by (observed and unobserved) variation in heterogeneous net benefits to the technology.</p> </abstract>
<abstract> <p>This paper develops a tractable econometric model of optimal migration, focusing on expected income as the main economic influence on migration. The model improves on previous work in two respects: it covers optimal sequences of location decisions (rather than a single once-for-all choice) and it allows for many alternative location choices. The model is estimated using panel data from the National Longitudinal Survey of Youth on white males with a high-school education. Our main conclusion is that interstate migration decisions are influenced to a substantial extent by income prospects. The results suggest that the link between income and migration decisions is driven both by geographic differences in mean wages and by a tendency to move in search of a better locational match when the income realization in the current location is unfavorable.</p> </abstract>
<abstract> <p>The rollout of Wal-Mart store openings followed a pattern that radiated from the center outward, with Wal-Mart maintaining high store density and a contiguous store network all along the way. This paper estimates the benefits of such a strategy to Wal-Mart, focusing on the savings in distribution costs afforded by a dense network of stores. The paper takes a revealed preference approach, inferring the magnitude of density economies from how much sales cannibalization of closely packed stores Wal-Mart is willing to suffer to achieve density economies. The model is dynamic with rich geographic detail on the locations of stores and distribution centers. Given the enormous number of possible combinations of store-opening sequences, it is difficult to directly solve Wal-Mart's problem, making conventional approaches infeasible. The moment inequality approach is used instead and works well. The estimates show the benefits to Wal-Mart of high store density are substantial and likely extend significantly beyond savings in trucking costs.</p> </abstract>
<abstract> <p>Instrumental variables are widely used in applied econometrics to achieve identification and carry out estimation and inference in models that contain endogenous explanatory variables. In most applications, the function of interest (e. g., an Engel curve or demand function) is assumed to be known up to finitely many parameters (e.g., a linear model), and instrumental variables are used to identify and estimate these parameters. However, linear and other finite-dimensional parametric models make strong assumptions about the population being modeled that are rarely if ever justified by economic theory or other a priori reasoning and can lead to seriously erroneous conclusions if they are incorrect. This paper explores what can be learned when the function of interest is identified through an instrumental variable but is not assumed to be known up to finitely many parameters. The paper explains the differences between parametric and nonparametric estimators that are important for applied research, describes an easily implemented nonparametric instrumental variables estimator, and presents empirical examples in which nonparametric methods lead to substantive conclusions that are quite different from those obtained using standard, parametric estimators.</p> </abstract>
<abstract> <p>The asymptotic validity of tests is usually established by making appropriate primitive assumptions, which imply the weak convergence of a specific function of the data, and an appeal to the continuous mapping theorem. This paper, instead, takes the weak convergence of some function of the data to a limiting random element as the starting point and studies efficiency in the class of tests that remain asymptotically valid for all models that induce the same weak limit. It is found that efficient tests in this class are simply given by efficient tests in the limiting problem— that is, with the limiting random element assumed observed—evaluated at sample analogues. Efficient tests in the limiting problem are usually straightforward to derive, even in nonstandard testing problems. What is more, their evaluation at sample analogues typically yields tests that coincide with suitably robustified versions of optimal tests in canonical parametric versions of the model. This paper thus establishes an alternative and broader sense of asymptotic efficiency for many previously derived tests in econometrics, such as tests for unit roots, parameter stability tests, and tests about regression coefficients under weak instruments.</p> </abstract>
<abstract> <p>This paper shows that the semiparametric efficiency bound for a parameter identified by an unconditional moment restriction with data missing at random (MAR) coincides with that of a particular augmented moment condition problem. The augmented system consists of the inverse probability weighted (IPW) original moment restriction and an additional conditional moment restriction which exhausts all other implications of the MAR assumption. The paper also investigates the value of additional semiparametric restrictions on the conditional expectation function (CEF) of the original moment function given always observed covariates. In the program evaluation context, for example, such restrictions are implied by semiparametric models for the potential outcome CEFs given baseline covariates. The efficiency bound associated with this model is shown to also coincide with that of a particular moment condition problem. Some implications of these results for estimation are briefly discussed.</p> </abstract>
<abstract> <p>This paper introduces the model confidence set (MCS) and applies it to the selection of models. A MCS is a set of models that is constructed such that it will contain the best model with a given level of confidence. The MCS is in this sense analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data, such that uninformative data yield a MCS with many models, whereas informative data yield a MCS with only a few models. The MCS procedure does not assume that a particular model is the true model; in fact, the MCS procedure can be used to compare more general objects, beyond the comparison of models. We apply the MCS procedure to two empirical problems. First, we revisit the inflation forecasting problem posed by Stock and Watson (1999), and compute the MCS for their set of inflation forecasts. Second, we compare a number of Taylor rule regressions and determine the MCS of the best regression in terms of in-sample likelihood criteria.</p> </abstract>
<abstract> <p>We generalize Athey's (2001) and McAdams' (2003) results on the existence of monotone pure-strategy equilibria in Bayesian games. We allow action spaces to be compact locally complete metric semilattices and type spaces to be partially ordered probability spaces. Our proof is based on contractibility rather than convexity of bestreply sets. Several examples illustrate the scope of the result, including new applications to multi-unit auctions with risk-averse bidders.</p> </abstract>
<abstract> <p>The majority of labor transactions throughout much of history and a significant fraction of such transactions in many developing countries today are "coercive," in the sense that force or the threat of force plays a central role in convincing workers to accept employment or its terms. We propose a tractable principal-agent model of coercion, based on the idea that coercive activities by employers, or "guns," affect the participation constraint of workers. We show that coercion and effort are complements, so that coercion increases effort, but coercion always reduces utilitarian social welfare. Better outside options for workers reduce coercion because of the complementarity between coercion and effort: workers with a better outside option exert lower effort in equilibrium and thus are coerced less. Greater demand for labor increases coercion because it increases equilibrium effort. We investigate the interaction between outside options, market prices, and other economic variables by embedding the (coercive) principal-agent relationship in a general equilibrium setup, and studying when and how labor scarcity encourages coercion. General (market) equilibrium interactions working through the price of output lead to a positive relationship between labor scarcity and coercion along the lines of ideas suggested by Domar, while interactions those working through the outside option lead to a negative relationship similar to ideas advanced in neo-Malthusian historical analyses of the decline of feudalism. In net, a decline in available labor increases coercion in general equilibrium if and only if its direct (partial equilibrium) effect is to increase the price of output by more than it increases outside options. Our model also suggests that markets in slaves make slaves worse off, conditional on enslavement, and that coercion is more viable in industries that do not require relationship-specific investment by workers.</p> </abstract>
<abstract> <p>Gul and Pesendorfer (2001) model the static behavior of an agent who ranks menus prior to the experience of temptation. This paper models the dynamic behavior of an agent whose ranking of menus itself is subject to temptation. The representation for the agent's dynamically inconsistent choice behavior views him as possessing a dynamically consistent view of what choices he "should" make (a normative preference) and being tempted by menus that contain tempting alternatives. Foundations for the model require a departure from Gul and Pesendorfer's idea that temptation creates a preference for commitment. Instead, it is hypothesized that distancing an agent from the consequences of his choices separates normative preference and temptation.</p> </abstract>
<abstract> <p>We characterize, in the Anscombe-Aumann framework, the preferences for which there are a utility function u on outcomes and an ambiguity index c on the set of probabilities on the states of the world such that, for all acts f and g,<tex-math>$f \succsim g \Leftrightarrow {min \atop p} \bigg(\int u(f)dp + c(p)\bigg) \geq {min\atop p} \bigg(\int u(g)dp + c(p)\bigg)$</tex-math>. The function u represents the decision maker's risk attitudes, while the index c captures his ambiguity attitudes. These preferences include the multiple priors preferences of Gilboa and Schmeidler and the multiplier preferences of Hansen and Sargent. This provides a rigorous decision-theoretic foundation for the latter model, which has been widely used in macroeconomics and finance.</p> </abstract>
<abstract> <p>We prove the folk theorem for discounted repeated games under private, almost-perfect monitoring. Our result covers all finite, n-player games that satisfy the usual full-dimensionality condition. Mixed strategies are allowed in determining the individually rational payoffs. We assume no cheap-talk communication between players and no public randomization device.</p> </abstract>
<abstract> <p>We propose a framework for out-of-sample predictive ability testing and forecast selection designed for use in the realistic situation in which the forecasting model is possibly misspecified, due to unmodeled dynamics, unmodeled heterogeneity, incorrect functional form, or any combination of these. Relative to the existing literature (Diebold and Mariano (1995) and West (1996)), we introduce two main innovations: (i) We derive our tests in an environment where the finite sample properties of the estimators on which the forecasts may depend are preserved asymptotically. (ii) We accommodate conditional evaluation objectives (can we predict which forecast will be more accurate at a future date?), which nest unconditional objectives (which forecast was more accurate on average?), that have been the sole focus of previous literature. As a result of (i), our tests have several advantages: they capture the effect of estimation uncertainty on relative forecast performance, they can handle forecasts based on both nested and nonnested models, they allow the forecasts to be produced by general estimation methods, and they are easy to compute. Although both unconditional and conditional approaches are informative, conditioning can help fine-tune the forecast selection to current economic conditions. To this end, we propose a two-step decision rule that uses current information to select the best forecast for the future date of interest. We illustrate the usefulness of our approach by comparing forecasts from leading parameter-reduction methods for macroeconomic forecasting using a large number of predictors.</p> </abstract>
<abstract> <p>We examine experimentally the impact of communication on trust and cooperation. Our design admits observation of promises, lies, and beliefs. The evidence is consistent with people striving to live up to others' expectations so as to avoid guilt, as can be modeled using psychological game theory. When players exhibit such guilt aversion, communication may influence motivation and behavior by influencing beliefs about beliefs. Promises may enhance trustworthy behavior, which is what we observe. We argue that guilt aversion may be relevant for understanding strategic interaction in a variety of settings, and that it may shed light on the role of language, discussions, agreements, and social norms in these contexts.</p> </abstract>
<abstract> <p>We study a repeated game with asymmetric information about a dynamic state of nature. In the course of the game, the better-informed player can communicate some or all of his information to the other. Our model covers costly and/or bounded communication. We characterize the set of equilibrium payoffs and contrast these with the communication equilibrium payoffs, which by definition entail no communication costs.</p> </abstract>
<abstract> <p>Temporary price reductions (sales) are common for many goods and naturally result in large increases in the quantity sold. Demand estimation based on temporary price reductions may mismeasure the long-run responsiveness to prices. In this paper we quantify the extent of the problem and assess its economic implications. We structurally estimate a dynamic model of consumer choice using two years of scanner data on the purchasing behavior of a panel of households. The results suggest that static demand estimates, which neglect dynamics, (i) overestimate own-price elasticities by 30 percent, (ii) underestimate cross-price elasticities by up to a factor of 5, and (iii) overestimate the substitution to the no-purchase or outside option by over 200 percent. This suggests that policy analysis based on static elasticity estimates will underestimate price-cost margins and underpredict the effects of mergers.</p> </abstract>
<abstract> <p>In 1971, President Nixon declared war on cancer. Thirty years later, many declared this war a failure: the age-adjusted mortality rate from cancer in 2000 was essentially the same as in the early 1970s. Meanwhile the age-adjusted mortality rate from cardiovascular disease fell dramatically. Since the causes that underlie cancer and cardiovascular disease are likely dependent, the decline in mortality rates from cardiovascular disease may partially explain the lack of progress in cancer mortality. Because competing risks models (used to model mortality from multiple causes) are fundamentally unidentified, it is difficult to estimate cancer trends. We derive bounds for aspects of the underlying distributions without assuming that the underlying risks are independent. We then estimate changes in cancer and cardiovascular mortality since 1970. The bounds for the change in duration until death for either cause are fairly tight and suggest much larger improvements in cancer than previously estimated.</p> </abstract>
<abstract> <p>In this paper a bootstrap algorithm for a reduced rank vector autoregressive model with a restricted linear trend and independent, identically distributed errors is analyzed. For testing the cointegration rank, the asymptotic distribution under the hypothesis is the same as for the usual likelihood ratio test, so that the bootstrap is consistent. It is furthermore shown that a bootstrap procedure for determining the rank is asymptotically consistent in the sense that the probability of choosing the rank smaller than the true one converges to zero.</p> </abstract>
<abstract> <p>A convex, compact, and possibly discontinuous better reply secure game has a Nash equilibrium. We introduce a very weak notion of continuity that can be used to establish that a game is better reply secure and we show that this notion of continuity is satisfied by a large class of games.</p> </abstract>
<abstract> <p>The sensitivity of Bayesian implementation to agents' beliefs about others suggests the use of more robust notions of implementation such as ex post implementation, which requires that each agent's strategy be optimal for every possible realization of the types of other agents. We show that the only deterministic social choice functions that are ex post implementable in generic mechanism design frameworks with multidimensional signals, interdependent valuations, and transferable utilities are constant functions. In other words, deterministic ex post implementation requires that the same alternative must be chosen irrespective of agents' signals. The proof shows that ex post implementability of a nontrivial deterministic social choice function implies that certain rates of information substitution coincide for all agents. This condition amounts to a system of differential equations that are not satisfied by generic valuation functions.</p> </abstract>
<abstract> <p>Identification of dynamic nonlinear panel data models is an important and delicate problem in econometrics. In this paper we provide insights that shed light on the identification of parameters of some commonly used models. Using these insights, we are able to show through simple calculations that point identification often fails in these models. On the other hand, these calculations also suggest that the model restricts the parameter to lie in a region that is very small in many cases, and the failure of point identification may, therefore, be of little practical importance in those cases. Although the emphasis is on identification, our techniques are constructive in that they can easily form the basis for consistent estimates of the identified sets.</p> </abstract>
<abstract> <p>This paper studies the problem of identification and estimation in nonparametric regression models with a misclassified binary regressor where the measurement error may be correlated with the regressors. We show that the regression function is nonparametrically identified in the presence of an additional random variable that is correlated with the unobserved true underlying variable but unrelated to the measurement error. Identification for semiparametric and parametric regression functions follows straightforwardly from the basic identification result. We propose a kernel estimator based on the identification strategy, derive its large sample properties, and discuss alternative estimation procedures. We also propose a test for misclassification in the model based on an exclusion restriction that is straightforward to implement.</p> </abstract>
<abstract> <p>In this paper, I analyze a decentralized search and matching economy with transferable utility composed of heterogeneous agents. I explore whether Becker's assortative matching result generalizes to an economy where agents engage in costly search. In an economy with explicit additive search costs, complementarities in joint production (supermodularity of the joint production function) lead to assortative matching. This is in contrast to previous literature, which had shown that in a search economy with discounting, assortative matching may fail even when the joint production function is supermodular.</p> </abstract>
<abstract> <p>This paper considers the problem of conducting inference on the regression coefficient in a bivariate regression model with a highly persistent regressor. Gaussian asymptotic power envelopes are obtained for a class of testing procedures that satisfy a conditionality restriction. In addition, the paper proposes testing procedures that attain these power envelopes whether or not the innovations of the regression model are normally distributed.</p> </abstract>
<abstract> <p>This paper considers tests of the parameter on an endogenous variable in an instrumental variables regression model. The focus is on determining tests that have some optimal power properties. We start by considering a model with normally distributed errors and known error covariance matrix. We consider tests that are similar and satisfy a natural rotational invariance condition. We determine a two-sided power envelope for invariant similar tests. This allows us to assess and compare the power properties of tests such as the conditional likelihood ratio (CLR), the Lagrange multiplier, and the Anderson-Rubin tests. We find that the CLR test is quite close to being uniformly most powerful invariant among a class of two-sided tests. The finite-sample results of the paper are extended to the case of unknown error covariance matrix and possibly nonnormal errors via weak instrument asymptotics. Strong instrument asymptotic results also are provided because we seek tests that perform well under both weak and strong instruments.</p> </abstract>
<abstract> <p>In standard auctions resale creates a role for a speculator-a bidder who is commonly known to have no use value for the good on sale. We study this issue in environments with symmetric independent private-value bidders. For second-price and English auctions the efficient value-bidding equilibrium coexists with a continuum of inefficient equilibria in which the speculator wins the auction and makes positive profits. First-price and Dutch auctions have an essentially unique equilibrium, and whether or not the speculator wins the auction and distorts the final allocation depends on the number of bidders, the value distribution, and the discount factor. Speculators do not make profits in first-price or Dutch auctions.</p> </abstract>
<abstract> <p>Building on the Ramsey-de Finetti idea of event exchangeability, we derive a characterization of probabilistic sophistication without requiring any of the various versions of monotonicity, continuity, or comparative likelihood assumptions imposed by Savage (1954), Machina and Schmeidler (1992), and Grant (1995). Our characterization identifies a unique and finitely-additive subjective probability measure over an algebra of events.</p> </abstract>
<abstract> <p>In this paper, we introduce a kernel-based estimation principle for nonparametric models named local partitioned regression (LPR). This principle is a nonparametric generalization of the familiar partition regression in linear models. It has several key advantages: First, it generates estimators for a very large class of semi- and nonparametric models. A number of examples that are particularly relevant for economic applications will be discussed in this paper. This class contains the additive, partially linear, and varying coefficient models as well as several other models that have not been discussed in the literature. Second, LPR-based estimators achieve optimality criteria: They have optimal speed of convergence and are oracle-efficient. Moreover, they are simple in structure, widely applicable, and computationally inexpensive. A Monte Carlo simulation highlights these advantages.</p> </abstract>
<abstract> <p>This paper studies the estimation of dynamic discrete games of incomplete information. Two main econometric issues appear in the estimation of these models: the indeterminacy problem associated with the existence of multiple equilibria and the computational burden in the solution of the game. We propose a class of pseudo maximum likelihood (PML) estimators that deals with these problems, and we study the asymptotic and finite sample properties of several estimators in this class. We first focus on two-step PML estimators, which, although they are attractive for their computational simplicity, have some important limitations: they are seriously biased in small samples; they require consistent nonparametric estimators of players' choice probabilities in the first step, which are not always available; and they are asymptotically inefficient. Second, we show that a recursive extension of the two-step PML, which we call nested pseudo likelihood (NPL), addresses those drawbacks at a relatively small additional computational cost. The NPL estimator is particularly useful in applications where consistent nonparametric estimates of choice probabilities either are not available or are very imprecise, e.g., models with permanent unobserved heterogeneity. Finally, we illustrate these methods in Monte Carlo experiments and in an empirical application to a model of firm entry and exit in oligopoly markets using Chilean data from several retail industries.</p> </abstract>
<abstract> <p>This paper develops a theoretical framework for studying contract and enforcement in settings with nondurable trading opportunities and complete but unverifiable information. The framework explicitly accounts for the parties' individual trade actions. The sets of implementable state-contingent payoffs, under various assumptions about renegotiation opportunities, are characterized and compared. The results indicate the benefit of modeling trade actions as individual, rather than as public, and they highlight the usefulness of a structured game-theoretic framework for applied research.</p> </abstract>
<abstract> <p>Families, primarily female-headed minority households with children, living in high-poverty public housing projects in five U.S. cities were offered housing vouchers by lottery in the Moving to Opportunity program. Four to seven years after random assignment, families offered vouchers lived in safer neighborhoods that had lower poverty rates than those of the control group not offered vouchers. We find no significant overall effects of this intervention on adult economic self-sufficiency or physical health. Mental health benefits of the voucher offers for adults and for female youth were substantial. Beneficial effects for female youth on education, risky behavior, and physical health were offset by adverse effects for male youth. For outcomes that exhibit significant treatment effects, we find, using variation in treatment intensity across voucher types and cities, that the relationship between neighborhood poverty rate and outcomes is approximately linear.</p> </abstract>
<abstract> <p>We show experimentally that fairness concerns may have a decisive impact on the actual and optimal choice of contracts in a moral hazard context. Bonus contracts that offer a voluntary and unenforceable bonus for satisfactory performance provide powerful incentives and are superior to explicit incentive contracts when there are some fair-minded players, but trust contracts that pay a generous wage up front are less efficient than incentive contracts. The principals understand this and predominantly choose the bonus contracts. These results are consistent with recently developed theories of fairness, which offer important new insights into the interaction of contract choices, fairness, and incentives.</p> </abstract>
<abstract> <p> Consider a decentralized, dynamic market with an infinite horizon and participation costs in which both buyers and sellers have private information concerning their values for the indivisible traded good. Time is discrete, each period has length δ, and, each unit of time, continuums of new buyers and sellers consider entry. Traders whose expected utility is negative choose not to enter. Within a period each buyer is matched anonymously with a seller and each seller is matched with zero, one, or more buyers. Every seller runs a first price auction with a reservation price and, if trade occurs, both the seller and the winning buyer exit the market with their realized utility. Traders who fail to trade continue in the market to be rematched. We characterize the steady-state equilibria that are perfect Bayesian. We show that, as δ converges to zero, equilibrium prices at which trades occur converge to the Walrasian price and the realized allocations converge to the competitive allocation. We also show the existence of equilibria for δ sufficiently small, provided the discount rate is small relative to the participation costs. </p> </abstract>
<abstract> <p>This paper establishes that instruments enable the identification of nonparametric regression models in the presence of measurement error by providing a closed form solution for the regression function in terms of Fourier transforms of conditional expectations of observable variables. For parametrically specified regression functions, we propose a root n consistent and asymptotically normal estimator that takes the familiar form of a generalized method of moments estimator with a plugged-in nonparametric kernel density estimate. Both the identification and the estimation methodologies rely on Fourier analysis and on the theory of generalized functions. The finite-sample properties of the estimator are investigated through Monte Carlo simulations.</p> </abstract>
<abstract> <p>Consider a Bayesian collective decision problem in which the preferences of agents are private information. We provide a general demonstration that the utility costs associated with incentive constraints become negligible when the decision problem is linked with a large number of independent copies of itself. This is established by defining a mechanism in which agents must budget their representations of preferences so that the frequency of preferences across problems mirrors the underlying distribution of preferences, and then arguing that agents' incentives are to satisfy their budget by being as truthful as possible. We also show that all equilibria of the linking mechanisms converge to the target utility levels. The mechanisms do not require transferable utility or interpersonal comparisons of utility, and are immune to manipulations by coalitions.</p> </abstract>
<abstract> <p>This paper develops estimators for quantile treatment effects under the identifying restriction that selection to treatment is based on observable characteristics. Identification is achieved without requiring computation of the conditional quantiles of the potential outcomes. Instead, the identification results for the marginal quantiles lead to an estimation procedure for the quantile treatment effect parameters that has two steps: nonparametric estimation of the propensity score and computation of the difference between the solutions of two separate minimization problems. Root-N consistency, asymptotic normality, and achievement of the semiparametric efficiency bound are shown for that estimator. A consistent estimation procedure for the variance is also presented. Finally, the method developed here is applied to evaluation of a job training program and to a Monte Carlo exercise. Results from the empirical application indicate that the method works relatively well even for a data set with limited overlap between treated and controls in the support of covariates. The Monte Carlo study shows that, for a relatively small sample size, the method produces estimates with good precision and low bias, especially for middle quantiles.</p> </abstract>
<abstract> <p>This paper analyzes a tax enforcement field experiment in Denmark. In the base year, a stratified and representative sample of over 40,000 individual income tax filers was selected for the experiment. Half of the tax filers were randomly selected to be thoroughly audited, while the rest were deliberately not audited. The following year, threat-of-audit letters were randomly assigned and sent to tax filers in both groups. We present three main empirical findings. First, using baseline audit data, we find that the tax evasion rate is close to zero for income subject to third-party reporting, but substantial for self-reported income. Since most income is subject to third-party reporting, the overall evasion rate is modest. Second, using quasi-experimental variation created by large kinks in the income tax schedule, we find that marginal tax rates have a positive impact on tax evasion for self-reported income, but that this effect is small in comparison to legal avoidance and behavioral responses. Third, using the randomization of enforcement, we find that prior audits and threat-of-audit letters have significant effects on self-reported income, but no effect on third-party reported income. All these empirical results can be explained by extending the standard model of (rational) tax evasion to allow for the key distinction between self-reported and third-party reported income.</p> </abstract>
<abstract> <p>This paper provides an empirical analysis of the effects of employer-provided health insurance, Medicare, and Social Security on retirement behavior. Using data from the Health and Retirement Study, we estimate a dynamic programming model of retirement that accounts for both saving and uncertain medical expenses. Our results suggest that Medicare is important for understanding retirement behavior, and that uncertainty and saving are both important for understanding the labor supply responses to Medicare. Half the value placed by a typical worker on his employer-provided health insurance is the value of reduced medical expense risk. Raising the Medicare eligibility age from 65 to 67 leads individuals to work an additional 0.074 years over ages 60-69. In comparison, eliminating 2 years worth of Social Security benefits increases years of work by 0.076 years.</p> </abstract>
<abstract> <p>This paper proposes that idiosyncratic firm-level shocks can explain an important part of aggregate movements and provide a microfoundation for aggregate shocks. Existing research has focused on using aggregate shocks to explain business cycles, arguing that individual firm shocks average out in the aggregate. I show that this argument breaks down if the distribution of firm sizes is fat-tailed, as documented empirically. The idiosyncratic movements of the largest 100 firms in the United States appear to explain about one-third of variations in output growth. This "granular" hypothesis suggests new directions for macroeconomic research, in particular that macroeconomic questions can be clarified by looking at the behavior of large firms. This paper's ideas and analytical results may also be useful for thinking about the fluctuations of other economic aggregates, such as exports or the trade balance.</p> </abstract>
<abstract> <p>We study reputation dynamics in continuous-time games in which a large player (e.g., government) faces a population of small players (e.g., households) and the large player's actions are imperfectly observable. The major part of our analysis examines the case in which public signals about the large player's actions are distorted by a Brownian motion and the large player is either a normal type, who plays strategically, or a behavioral type, who is committed to playing a stationary strategy. We obtain a clean characterization of sequential equilibria using ordinary differential equations and identify general conditions for the sequential equilibrium to be unique and Markovian in the small players' posterior belief. We find that a rich equilibrium dynamics arises when the small players assign positive prior probability to the behavioral type. By contrast, when it is common knowledge that the large player is the normal type, every public equilibrium of the continuous-time game is payoff-equivalent to one in which a static Nash equilibrium is played after every history. Finally, we examine variations of the model with Poisson signals and multiple behavioral types.</p> </abstract>
<abstract> <p>Repeated games with imperfect private monitoring have a wide range of applications, but a complete characterization of all equilibria in this class of games has yet to be obtained. The existing literature has identified a relatively tractable subset of equilibria. The present paper introduces the notion of weakly belief-free equilibria for repeated games with imperfect private monitoring. This is a tractable class which subsumes, as a special case, a major part of the existing literature (the belief-free equilibria). It is shown that this class can outperform the equilibria identified by the previous work.</p> </abstract>
<abstract> <p>We study the effects of deliberation on collective decisions. In a series of experiments, we vary groups' preference distributions (between common and conflicting interests) and the institutions by which decisions are reached (simple majority, two-thirds majority, and unanimity). Without deliberation, different institutions generate significantly different outcomes, tracking the theoretical comparative statics. Deliberation, however, significantly diminishes institutional differences and uniformly improves efficiency. Furthermore, communication protocols exhibit an array of stable attributes: messages are public, consistently reveal private information, provide a good predictor for ultimate group choices, and follow particular (endogenous) sequencing.</p> </abstract>
<abstract> <p>We show that democratic change may be triggered by transitory economic shocks. Our approach uses within-country variation in rainfall as a source of transitory shocks to sub-Saharan African economies. We find that negative rainfall shocks are followed by significant improvement in democratic institutions. This result is consistent with the economic approach to political transitions, where transitory negative shocks can open a window of opportunity for democratic improvement. Instrumental variables estimates indicate that following a transitory negative income shock of 1 percent, democracy scores improve by 0.9 percentage points and the probability of a democratic transition increases by 1.3 percentage points.</p> </abstract>
<abstract> <p>This paper studies the special case of the triangular system of equations in Vytlacil and Yildiz (2007), where both dependent variables are binary but without imposing the restrictive support condition required by Vytlacil and Yildiz (2007) for identification of the average structural function (ASF) and the average treatment effect (ATE). Under weak regularity conditions, we derive upper and lower bounds on the ASF and the ATE. We show further that the bounds on the ASF and ATE are sharp under some further regularity conditions and an additional restriction on the support of the covariates and the instrument.</p> </abstract>
<abstract> <p>Postel-Vinay and Robin's (2002) sequential auction model is extended to allow for aggregate productivity shocks. Workers exhibit permanent differences in ability while firms are identical. Negative aggregate productivity shocks induce job destruction by driving the surplus of matches with low ability workers to negative values. Endogenous job destruction coupled with worker heterogeneity thus provides a mechanism for amplifying productivity shocks that offers an original solution to the unemployment volatility puzzle (Shimer (2005)). Moreover, positive or negative shocks may lead employers and employees to renegotiate low wages up and high wages down when agents' individual surpluses become negative. The model delivers rich business cycle dynamics of wage distributions and explains why both low wages and high wages are more procyclical than wages in the middle of the distribution.</p> </abstract>
<abstract> <p>This paper uses a structural model to understand, predict, and evaluate the impact of an exogenous microcredit intervention program, the Thai Million Baht Village Fund program. We model household decisions in the face of borrowing constraints, income uncertainty, and high-yield indivisible investment opportunities. After estimation of parameters using preprogram data, we evaluate the model's ability to predict and interpret the impact of the village fund intervention. Simulations from the model mirror the data in yielding a greater increase in consumption than credit, which is interpreted as evidence of credit constraints. A cost-benefit analysis using the model indicates that some households value the program much more than its per household cost, but overall the program costs 30 percent more than the sum of these benefits.</p> </abstract>
<abstract> <p>This paper studies whether removing barriers to trade induces efficiency gains for producers. Like almost all empirical work which relies on a production function to recover productivity measures, I do not observe physical output at the firm level. Therefore, it is imperative to control for unobserved prices and demand shocks. I develop an empirical model that combines a demand system with a production function to generate estimates of productivity. I rely on my framework to identify the productivity effects from reduced trade protection in the Belgian textile market. This trade liberalization provides me with observed demand shifters that are used to separate out the associated price, scale, and productivity effects. Using a matched plant-product level data set and detailed quota data, I find that correcting for unobserved prices leads to substantially lower productivity gains. More specifically, abolishing all quota protections increases firm-level productivity by only 2 percent as opposed to 8 percent when relying on standard measures of productivity. My results beg for a serious réévaluation of a long list of empirical studies that document productivity responses to major industry shocks and, in particular, to opening up to trade. My findings imply the need to study the impact of changes in the operating environment on productivity together with market power and prices in one integrated framework. The suggested method and identification strategy are quite general and can be applied whenever it is important to distinguish between revenue productivity and physical productivity.</p> </abstract>
<abstract> <p>We examine the sales of French manufacturing firms in 113 destinations, including France itself. Several regularities stand out: (i) the number of French firms selling to a market, relative to French market share, increases systematically with market size; (ii) sales distributions are similar across markets of very different size and extent of French participation; (iii) average sales in France rise systematically with selling to less popular markets and to more markets. We adopt a model of firm heterogeneity and export participation which we estimate to match moments of the French data using the method of simulated moments. The results imply that over half the variation across firms in market entry can be attributed to a single dimension of underlying firm heterogeneity: efficiency. Conditional on entry, underlying efficiency accounts for much less of the variation in sales in any given market. We use our results to simulate the effects of a 10 percent counterfactual decline in bilateral trade barriers on French firms. While total French sales rise by around $ 16 billion (U.S.), sales by the top decile of firms rise by nearly $23 billion (U.S.). Every lower decile experiences a drop in sales, due to selling less at home or exiting altogether.</p> </abstract>
<abstract> <p>This paper studies the nonparametric identification of a contract model with adverse selection and moral hazard. Specifically, we consider the false moral hazard model developed by Laffont and Tirole (1986). We first extend this model to allow for general random demand and cost functions. We establish the nonparametric identification of the demand, cost, deterministic transfer, and effort disutility functions as well as the joint distribution of the random elements of the model, which are the firm's type and the demand, cost, and transfer shocks. The cost of public funds is identified with the help of an instrument. Testable restrictions of the model are characterized.</p> </abstract>
<abstract> <p>The focus of this paper is the nonparametric estimation of an instrumental regression function defined by conditional moment restrictions that stem from a structural econometric model E[Y — (Z) | W] = 0, and involve endogenous variables Y and Z and instruments W. The function is the solution of an ill-posed inverse problem and we propose an estimation procedure based on Tikhonov regularization. The paper analyzes identification and overidentification of this model, and presents asymptotic properties of the estimated nonparametric instrumental regression function.</p> </abstract>
<abstract> <p>The coefficient of relative risk aversion is a key parameter for analyses of behavior toward risk, but good estimates of this parameter do not exist. A promising place for reliable estimation is rare macroeconomic disasters, which have a major influence on the equity premium. The premium depends on the probability and size distribution of disasters, gauged by proportionate declines in per capita consumption or gross domestic product. Long-term national-accounts data for 36 countries provide a large sample of disasters of magnitude 10% or more. A power-law density provides a good fit to the size distribution, and the upper-tail exponent, α, is estimated to be around 4. A higher α signifies a thinner tail and, therefore, a lower equity premium, whereas a higher coefficient of relative risk aversion, γ, implies a higher premium. The premium is finite if α &gt; γ. The observed premium of 5% generates an estimated γ close to 3, with a 95% confidence interval of 2 to 4. The results are robust to uncertainty about the values of the disaster probability and the equity premium, and can accommodate seemingly paradoxical situations in which the equity premium may appear to be infinite.</p> </abstract>
<abstract> <p>It is common for a majority of people to rank themselves as better than average on simple tasks and worse than average on difficult tasks. The literature takes for granted that this apparent misconfidence is problematic. We argue, however, that this behavior is consistent with purely rational Bayesian updaters. In fact, better-than-average data alone cannot be used to show overconfidence; we indicate which type of data can be used. Our theory is consistent with empirical patterns found in the literature.</p> </abstract>
<abstract> <p>We introduce entropy techniques to study the classical reputation model in which a long-run player faces a series of short-run players. The long-run player's actions are possibly imperfectly observed. We derive explicit lower and upper bounds on the equilibrium payoffs to the long-run player.</p> </abstract>
<abstract> <p>We provide a pure Nash equilibrium existence theorem for games with discontinuous payoffs whose hypotheses are in a number of ways weaker than those of the theorem of Reny (1999). In comparison with Reny's argument, our proof is brief. Our result subsumes a prior existence result of Nishimura and Friedman (1981) that is not covered by his theorem. We use the main result to prove the existence of pure Nash equilibrium in a class of finite games in which agents' pure strategies are subsets of a given set, and in turn use this to prove the existence of stable configurations for games, similar to those used by Schelling (1971, 1972) to study residential segregation, in which agents choose locations.</p> </abstract>
<abstract> <p>Rational herd behavior and informationally efficient security prices have long been considered to be mutually exclusive but for exceptional cases. In this paper we describe the conditions on the underlying information structure that are necessary and sufficient for informational herding and contrarianism. In a standard sequential security trading model, subject to sufficient noise trading, people herd if and only if, loosely, their information is sufficiently dispersed so that they consider extreme outcomes more likely than moderate ones. Likewise, people act as contrarians if and only if their information leads them to concentrate on middle values. Both herding and contrarianism generate more volatile prices, and they lower liquidity. They are also resilient phenomena, although by themselves herding trades are self-enforcing whereas contrarian trades are self-defeating. We complete the characterization by providing conditions for the absence of herding and contrarianism.</p> </abstract>
<abstract> <p>We study testable implications for the dynamics of consumption and income of models in which first-best allocations are not achieved because of a moral hazard problem with hidden saving. We show that in this environment, agents typically achieve more insurance than that obtained under self-insurance with a single asset. Consumption allocations exhibit "excess smoothness," as found and defined by Campbell and Deaton (1989). We argue that excess smoothness, in this context, is equivalent to a violation of the intertemporal budget constraint considered in a Bewley economy (with a single asset). We also show parameterizations of our model in which we can obtain a closedform solution for the efficient insurance contract and where the excess smoothness parameter has a structural interpretation in terms of the severity of the moral hazard problem. We present tests of excess smoothness, applied to U.K. microdata and constructed using techniques proposed by Hansen, Roberds, and Sargent (1991) to test the intertemporal budget constraint. Our theoretical model leads us to interpret them as tests of the market structure faced by economic agents. We also construct a test based on the dynamics of the cross-sectional variances of consumption and income that is, in a precise sense, complementary to that based on Hansen, Roberds, and Sargent (1991) and that allows us to estimate the same structural parameter. The results we report are consistent with the implications of the model and are internally coherent.</p> </abstract>
<abstract> <p>The standard gravity model predicts that trade flows increase in proportion to importer and exporter total income, regardless of how income is divided into income per capita and population. Bilateral trade data, however, show that trade grows strongly with income per capita and is largely unresponsive to population. I develop a general equilibrium Ricardian model of trade that allows the elasticity of trade with respect to income per capita and with respect to population to diverge. Goods are of various types, which differ in their income elasticity of demand and in the extent to which there is heterogeneity in their production technologies. I estimate the model using bilateral trade data of 162 countries and compare it to a special case that delivers the gravity equation. The general model improves the restricted model's predictions regarding variations in trade due to size and income. I experiment with counterfactuals. A positive technology shock in China makes poor and rich countries better off and middle-income countries worse off.</p> </abstract>
<abstract> <p>One of the most dramatic economic transformations of the past century has been the entry of women into the labor force. While many theories explain why this change took place, we investigate the process of transition itself. We argue that local information transmission generates changes in participation that are geographically heterogeneous, locally correlated, and smooth in the aggregate, just like those observed in our data. In our model, women learn about the effects of maternal employment on children by observing nearby employed women. When few women participate in the labor force, data are scarce and participation rises slowly. As information accumulates in some regions, the effects of maternal employment become less uncertain and more women in that region participate. Learning accelerates, labor force participation rises faster, and regional participation rates diverge. Eventually, information diffuses throughout the economy, beliefs converge to the truth, participation flattens out, and regions become more similar again. To investigate the empirical relevance of our theory, we use a new county-level data set to compare our calibrated model to the time series and geographic patterns of participation.</p> </abstract>
<abstract> <p>Golosov and Lucas recently argued that a menu-cost model, when made consistent with salient features of the microdata, predicts approximate monetary neutrality. I argue here that their model misses, in fact, two important features of the data. First, the distribution of the size of price changes in the data is very dispersed. Second, in the data many price changes are temporary. I study an extension of the simple menu-cost model to a multiproduct setting in which firms face economies of scope in adjusting posted and regular prices. The model, because of its ability to replicate this additional set of microeconomic facts, predicts real effects of monetary policy shocks that are much greater than those in Golosov and Lucas and nearly as large as those in the Calvo model. Although episodes of sales account for roughly 40% of all goods sold in retail stores, the model predicts that these episodes do not contribute much to the flexibility of the aggregate price level.</p> </abstract>
<abstract> <p>In this paper, we introduce the extended method of moments (XMM) estimator. This estimator accommodates a more general set of moment restrictions than the standard generalized method of moments (GMM) estimator. More specifically, the XMM differs from the GMM in that it can handle not only uniform conditional moment restrictions (i.e., valid for any value of the conditioning variable), but also local conditional moment restrictions valid for a given fixed value of the conditioning variable. The local conditional moment restrictions are of special relevance in derivative pricing to reconstruct the pricing operator on a given day by using the information in a few cross sections of observed traded derivative prices and a time series of underlying asset returns. The estimated derivative prices are consistent for a large time series dimension, but a fixed number of cross sectionally observed derivative prices. The asymptotic properties of the XMM estimator are nonstandard, since the combination of uniform and local conditional moment restrictions induces different rates of convergence (parametric and nonparametric) for the parameters.</p> </abstract>
<abstract> <p>This paper studies the design of optimal contracts in dynamic environments where agents have private information that is persistent. In particular, I focus on a continuoustime version of a benchmark insurance problem where a risk-averse agent would like to borrow from a risk-neutral lender to stabilize his utility. The agent privately observes a persistent state variable, typically either income or a taste shock, and he makes reports to the principal. I give verifiable sufficient conditions showing that incentive-compatible contracts can be written recursively, conditioning on the report and two additional state variables: the agent's promised utility and promised marginal utility of the private state. I then study two examples where the optimal contracts can be solved in closed form, showing how persistence alters the nature of the contract. Unlike the previous discretetime models with independent and identically distributed (i.i.d.) private information, the agent's consumption under the contract may grow over time. Furthermore, in my setting the efficiency losses due to private information increase with the persistence of the private information, and the distortions vanish as I approximate an i.i.d. environment.</p> </abstract>
<abstract> <p>We present an algorithm to compute the set of perfect public equilibrium payoffs as the discount factor tends to 1 for stochastic games with observable states and public (but not necessarily perfect) monitoring when the limiting set of (long-run players') equilibrium payoffs is independent of the initial state. This is the case, for instance, if the Markov chain induced by any Markov strategy profile is irreducible. We then provide conditions under which a folk theorem obtains: if in each state the joint distribution over the public signal and next period's state satisfies some rank condition, every feasible payoff vector above the minmax payoff is sustained by a perfect public equilibrium with low discounting.</p> </abstract>
<abstract> <p>The increase in female employment and participation rates is one of the most dramatic changes to have taken place in the economy during the last century. However, while the employment rate of married women more than doubled during the last 50 years, that of unmarried women remained almost constant. To empirically analyze these trends, we estimate a female dynamic labor supply model using an extended version of Eckstein and Wolpin (1989) to compare the various explanations in the literature for the observed trends. This dynamic model provides a much better fit to the life-cycle employment pattern than a static version of the model and a standard static reduced form model (Heckman (1979)). The main finding using the dynamic model is that the rise in education levels accounts for about 33 percent of the increase in female employment, and the rise in wages and narrowing of the gender wage gap account for another 20 percent, while about 40 percent remains unexplained by observed household characteristics. We show that this unexplained portion can be empirically attributed to cohort-specific changes in preferences or the costs of child-rearing and household maintenance. Finally, the decline in fertility and the increase in divorce rates account for only a small share of the increase in female employment rates.</p> </abstract>
<abstract> <p>We propose a new and flexible nonparametric framework for estimating the jump tails of Itô semimartingale processes. The approach is based on a relatively simple-toimplement set of estimating equations associated with the compensator for the jump measure, or its intensity, that only utilizes the weak assumption of regular variation in the jump tails, along with in-fill asymptotic arguments for directly estimating the "large" jumps. The procedure assumes that the large-sized jumps are identically distributed, but otherwise allows for very general dynamic dependencies in jump occurrences, and, importantly, does not restrict the behavior of the "small" jumps or the continuous part of the process and the temporal variation in the stochastic volatility. On implementing the new estimation procedure with actual high-frequency data for the S&amp;P 500 aggregate market portfolio, we find strong evidence for richer and more complex dynamic dependencies in the jump tails than hitherto entertained in the literature.</p> </abstract>
<abstract> <p>We provide a tractable characterization of the sharp identification region of the parameter vector θ in a broad class of incomplete econometric models. Models in this class have set-valued predictions that yield a convex set of conditional or unconditional moments for the observable model variables. In short, we call these models with convex moment predictions. Examples include static, simultaneous-move finite games of complete and incomplete information in the presence of multiple equilibria; best linear predictors with interval outcome and covariate data; and random utility models of multinomial choice in the presence of interval regressors data. Given a candidate value for 0, we establish that the convex set of moments yielded by the model predictions can be represented as the Aumann expectation of a properly defined random set. The sharp identification region of θ, denoted Θ₁, can then be obtained as the set of minimizers of the distance from a properly specified vector of moments of random variables to this Aumann expectation. Algorithms in convex programming can be exploited to efficiently verify whether a candidate θ is in Θ₁. We use examples analyzed in the literature to illustrate the gains in identification and computational tractability afforded by our method.</p> </abstract>
<abstract> <p>We adapt the expectation-maximization algorithm to incorporate unobserved heterogeneity into conditional choice probability (CCP) estimators of dynamic discrete choice problems. The unobserved heterogeneity can be time-invariant or follow a Markov chain. By developing a class of problems where the difference in future value terms depends on a few conditional choice probabilities, we extend the class of dynamic optimization problems where CCP estimators provide a computationally cheap alternative to full solution methods. Monte Carlo results confirm that our algorithms perform quite well, both in terms of computational time and in the precision of the parameter estimates.</p> </abstract>
<abstract> <p>A seller can trade an endowment of a perfectly divisible good, the quality of which she privately knows. Buyers compete by offering menus of nonexclusive contracts, so that the seller can privately trade with several buyers. In this setting, we show that an equilibrium exists under mild conditions and that aggregate equilibrium allocations are generically unique. Although the good for sale is divisible, in equilibrium the seller ends up trading her whole endowment or not trading at all. Trades take place at a price equal to the expected quality of the good, conditional on the seller being ready to trade at that price. Our model thus provides a novel strategic foundation for Akerlof's (1970) results. It also contrasts with competitive screening models in which contracts are assumed to be exclusive, as in Rothschild and Stiglitz (1976). Latent contracts that are issued but not traded in equilibrium play an important role in our analysis.</p> </abstract>
<abstract> <p>A finite number of sellers (n) compete in schedules to supply an elastic demand. The cost of each seller is random, with common and private value components, and the seller receives a private signal about it. A Bayesian supply function equilibrium is characterized: The equilibrium is privately revealing and the incentives to rely on private signals are preserved. Supply functions are steeper with higher correlation among the cost parameters. For high (positive) correlation, supply functions are downward sloping, price is above the Cournot level, and as we approach the common value case, price tends to the collusive level. As correlation becomes maximally negative, we approach the competitive outcome. With positive correlation, private information coupled with strategic behavior induces additional distortionary market power above full information levels. Efficiency can be restored with appropriate subsidy schemes or with a precise enough public signal about the common value component. As the market grows large with the number of sellers, the equilibrium becomes price-taking, bid shading is on the order of 1/n, and the order of magnitude of welfare losses is 1/n². The results extend to inelastic demand, demand uncertainty, and demand schedule competition. A range of applications in product and financial markets is presented.</p> </abstract>
<abstract> <p>This paper examines repeated implementation of a social choice function (SCF) with infinitely lived agents whose preferences are determined randomly in each period. An SCF is repeatedly implementable in Nash equilibrium if there exists a sequence of (possibly history-dependent) mechanisms such that its Nash equilibrium set is nonempty and every equilibrium outcome path results in the desired social choice at every possible history of past play and realizations of uncertainty. We show, with minor qualifications, that in the complete information environment an SCF is repeatedly implementable in Nash equilibrium if and only if it is efficient. We also discuss several extensions of our analysis.</p> </abstract>
<abstract> <p>This paper studies dynamic identification of parameters of a dynamic stochastic general equilibrium model from the first and second moments of the data. Classical results for dynamic simultaneous equations do not apply because the state space solution of the model does not constitute a standard reduced form. Full rank of the Jacobian matrix of derivatives of the solution parameters with respect to the parameters of interest is necessary but not sufficient for identification. We use restrictions implied by observational equivalence to obtain two sets of rank and order conditions: one for stochastically singular models and another for nonsingular models. Measurement errors, mean, long-run, and a priori restrictions can be accommodated. An example is considered to illustrate the results.</p> </abstract>
<abstract> <p>The standard dual-self model of self-control, with a shorter-run self who cares only about the current period, is excessively sensitive to the timing of decisions and to the interpolation of additional "no-action" time periods in between the dates when decisions are made. We show that when the shorter-run self is not completely myopic, this excess sensitivity goes away. To accommodate the combination of short time periods and convex costs of self-control, we introduce a cognitive resource variable that tracks how the control cost depends on the self-control that has been used in the recent past. We consider models with both linear and convex control costs, illustrating the theory through a series of examples. We examine when opportunities to consume will be avoided or delayed, and we consider the way in which the marginal interest declines with delay.</p> </abstract>
<abstract> <p>This paper shows that information imperfections and common values can solve coordination problems in multicandidate elections. We analyze an election in which (i) the majority is divided between two alternatives and (ii) the minority backs a third alternative, which the majority views as strictly inferior. Standard analyses assume voters have a fixed preference ordering over candidates. Coordination problems cannot be overcome in such a case, and it is possible that inferior candidates win. In our setup the majority is also divided as a result of information imperfections. The majority thus faces two problems: aggregating information and coordinating to defeat the minority candidate. We show that when the common value component is strong enough, approval voting produces full information and coordination equivalence: the equilibrium is unique and solves both problems. Thus, the need for information aggregation helps resolve the majority's coordination problem under approval voting. This is not the case under standard electoral systems.</p> </abstract>
<abstract> <p>We study elections that simultaneously decide multiple issues, where voters have independent private values over bundles of issues. The innovation is in considering nonseparable preferences, where issues may be complements or substitutes. Voters face a political exposure problem: the optimal vote for a particular issue will depend on the resolution of the other issues. Moreover, the probabilities that the other issues will pass should be conditioned on being pivotal. We prove that equilibrium exists when distributions over values have full support or when issues are complements. We then study large elections with two issues. There exists a nonempty open set of distributions where the probability of either issue passing fails to converge to either 1 or 0 for all limit equilibria. Thus, the outcomes of large elections are not generically predictable with independent private values, despite the fact that there is no aggregate uncertainty regarding fundamentals. While the Condorcet winner is not necessarily the outcome of a multi-issue election, we provide sufficient conditions that guarantee the implementation of the Condorcet winner.</p> </abstract>
<abstract> <p>This paper studies the inference of interaction effects in discrete simultaneous games with incomplete information. We propose a test for the signs of state-dependent interaction effects that does not require parametric specifications of players' payoffs, the distributions of their private signals, or the equilibrium selection mechanism. The test relies on the commonly invoked assumption that players' private signals are independent conditional on observed states. The procedure is valid in (but does not rely on) the presence of multiple equilibria in the data-generating process (DGP). As a by-product, we propose a formal test for multiple equilibria in the DGP. We also implement the test using data on radio programming of commercial breaks in the United States, and infer stations' incentives to synchronize their commercial breaks. Our results support the earlier finding by Sweeting (2009) that stations have stronger incentives to coordinate and air commercials at the same time during rush hours and in smaller markets.</p> </abstract>
<abstract> <p>This paper examines the problem of testing and confidence set construction for one-dimensional functions of the coefficients in autoregressive (AR(p)) models with potentially persistent time series. The primary example concerns inference on impulse responses. A new asymptotic framework is suggested and some new theoretical properties of known procedures are demonstrated. I show that the likelihood ratio (LR) and LR± statistics for a linear hypothesis in an AR(p) can be uniformly approximated by a weighted average of local-to-unity and normal distributions. The corresponding weights depend on the weight placed on the largest root in the null hypothesis. The suggested approximation is uniform over the set of all linear hypotheses. The same family of distributions approximates the LR and LR± statistics for tests about impulse responses, and the approximation is uniform over the horizon of the impulse response. I establish the size properties of tests about impulse responses proposed by Inoue and Kilian (2002) and Gospodinov (2004), and theoretically explain some of the empirical findings of Pesavento and Rossi (2007). An adaptation of the grid bootstrap for impulse response functions is suggested and its properties are examined.</p> </abstract>
<abstract> <p>This paper develops methods for hypothesis testing in a nonparametric instrumental variables setting within a partial identification framework. We construct and derive the asymptotic distribution of a test statistic for the hypothesis that at least one element of the identified set satisfies a conjectured restriction. The same test statistic can be employed under identification, in which case the hypothesis is whether the true model satisfies the posited property. An almost sure consistent bootstrap procedure is provided for obtaining critical values. Possible applications include testing for semiparametric specifications as well as building confidence regions for certain functional on the identified set. As an illustration we obtain confidence intervals for the level and slope of Brazilian fuel Engel curves. A Monte Carlo study examines finite sample performance.</p> </abstract>
<abstract> <p>This paper studies nonparametric estimation of conditional moment restrictions in which the generalized residual functions can be nonsmooth in the unknown functions of endogenous variables. This is a nonparametric nonlinear instrumental variables (IV) problem. We propose a class of penalized sieve minimum distance (PSMD) estimators, which are minimizers of a penalized empirical minimum distance criterion over a collection of sieve spaces that are dense in the infinite-dimensional function parameter space. Some of the PSMD procedures use slowly growing finite-dimensional sieves with flexible penalties or without any penalty; others use large dimensional sieves with lower semicompact and/or convex penalties. We establish their consistency and the convergence rates in Banach space norms (such as a sup-norm or a root mean squared norm), allowing for possibly noncompact infinite-dimensional parameter spaces. For both mildly and severely ill-posed nonlinear inverse problems, our convergence rates in Hubert space norms (such as a root mean squared norm) achieve the known minimax optimal rate for the nonparametric mean IV regression. We illustrate the theory with a nonparametric additive quantile IV regression. We present a simulation study and an empirical application of estimating nonparametric quantile IV Engel curves.</p> </abstract>
<abstract> <p>We study matching and coalition formation environments allowing complementarities and peer effects. Agents have preferences over coalitions, and these preferences vary with an underlying, and commonly known, state of nature. Assuming that there is substantial variability of preferences across states of nature, we show that there exists a core stable coalition structure in every state if and only if agents' preferences are pairwise-aligned in every state. This implies that there is a stable coalition structure if agents' preferences are generated by Nash bargaining over coalitional outputs. We further show that all stability-inducing rules for sharing outputs can be represented by a profile of agents' bargaining functions and that agents match assortatively with respect to these bargaining functions. This framework allows us to show how complementarities and peer effects overturn well known comparative statics of many-to-one matching.</p> </abstract>
<abstract> <p>This paper analyzes Bayesian normal form games in which players write contracts that condition their actions on the contracts of other players. These contracts are required to be representable in a formal language. This is accomplished by constructing contracts which are definable functions of the Godei code of every other player's contract. We provide a complete characterization of the set of allocations supportable as pure-strategy Bayesian equilibria of this contracting game. When information is complete, this characterization provides a folk theorem. In general, the set of supportable allocations is smaller than the set supportable by a centralized mechanism designer.</p> </abstract>
<abstract> <p>We show by example that empirical likelihood and other commonly used tests for moment restrictions are unable to control the (exponential) rate at which the probability of a Type I error tends to zero unless the possible distributions for the observed data are restricted appropriately. From this, it follows that for the optimality claim for empirical likelihood in Kitamura (2001) to hold, additional assumptions and qualifications are required. Under stronger assumptions than those in Kitamura (2001), we establish the following optimality result: (i) empirical likelihood controls the rate at which the probability of a Type I error tends to zero and (ii) among all procedures for which the probability of a Type I error tends to zero at least as fast, empirical likelihood maximizes the rate at which the probability of a Type II error tends to zero for most alternatives. This result further implies that empirical likelihood maximizes the rate at which the probability of a Type II error tends to zero for all alternatives among a class of tests that satisfy a weaker criterion for their Type I error probabilities.</p> </abstract>
<abstract> <p>The delta method and continuous mapping theorem are among the most extensively used tools in asymptotic derivations in econometrics. Extensions of these methods are provided for sequences of functions that are commonly encountered in applications and where the usual methods sometimes fail. Important examples of failure arise in the use of simulation-based estimation methods such as indirect inference. The paper explores the application of these methods to the indirect inference estimator (IIE) in first order autoregressive estimation. The IIE uses a binding function that is sample size dependent. Its limit theory relies on a sequence-based delta method in the stationary case and a sequence-based implicit continuous mapping theorem in unit root and local to unity cases. The new limit theory shows that the IIE achieves much more than (partial) bias correction. It changes the limit theory of the maximum likelihood estimator (MLE) when the autoregressive coefficient is in the locality of unity, reducing the bias and the variance of the MLE without affecting the limit theory of the MLE in the stationary case. Thus, in spite of the fact that the IIE is a continuously differentiable function of the MLE, the limit distribution of the IIE is not simply a scale multiple of the MLE, but depends implicitly on the full binding function mapping. The unit root case therefore represents an important example of the failure of the delta method and shows the need for an implicit mapping extension of the continuous mapping theorem.</p> </abstract>
<abstract> <p>It is generally presumed that stronger legal enforcement of lender rights increases credit access for all borrowers because it expands the set of incentive compatible loan contracts. This result relies on an assumption that the supply of credit is infinitely elastic. In contrast, with inelastic supply, stronger enforcement generates general equilibrium effects that may reduce credit access for small borrowers and expand it for wealthy borrowers. In a firm-level panel, we find evidence that an Indian judicial reform that increased banks' ability to recover nonperforming loans had such an adverse distributive impact.</p> </abstract>
<abstract> <p>We propose a novel generalized recursive smooth ambiguity model which permits a three-way separation among risk aversion, ambiguity aversion, and intertemporal substitution. We apply this utility model to a consumption-based asset-pricing model in which consumption and dividends follow hidden Markov regime-switching processes. Our calibrated model can match the mean equity premium, the mean risk-free rate, and the volatility of the equity premium observed in the data. In addition, our model can generate a variety of dynamic asset-pricing phenomena, including the procyclical variation of price-dividend ratios, the countercyclical variation of equity premia and equity volatility, the leverage effect, and the mean reversion of excess returns. The key intuition is that an ambiguity-averse agent behaves pessimistically by attaching more weight to the pricing kernel in bad times when his continuation values are low.</p> </abstract>
<abstract> <p>We propose a theory of task trade between countries that have similar relative factor endowments and technological capabilities, but may differ in size. Firms produce differentiated goods by performing a continuum of tasks, each of which generates local spillovers. Tasks can be performed at home or abroad, but offshoring entails costs that vary by task. In equilibrium, the tasks with the highest offshoring costs may not be traded. Among the remainder, those with the relatively higher offshoring costs are performed in the country that has the higher wage and the higher aggregate output. We discuss the relationship between equilibrium wages, equilibrium outputs, and relative country size.</p> </abstract>
<abstract> <p>Weinstein and Yildiz (2007) have shown that in static games, only very weak predictions are robust to perturbations of higher order beliefs. These predictions are precisely those provided by interim correlated rationalizability (ICR). This negative result is obtained under the assumption that agents have no information on payoffs. This assumption is unnatural in many settings. It is therefore natural to ask whether Weinstein and Yildiz's results remain true under more general information structures. This paper characterizes the "robust predictions" in static and dynamic games, under arbitrary information structures. This characterization is provided by an extensive form solution concept: interim sequential rationalizability (ISR). In static games, ISR coincides with ICR and does not depend on the assumptions on agents' information. Hence the "no information" assumption entails no loss of generality in these settings. This is not the case in dynamic games, where ISR refines ICR and depends on the details of the information structure. In these settings, the robust predictions depend on the assumptions on agents' information. This reveals a hitherto neglected interaction between information and higher order uncertainty, raising novel questions of robustness.</p> </abstract>
<abstract> <p>We study the question of whether local incentive constraints are sufficient to imply full incentive compatibility in a variety of mechanism design settings, allowing for probabilistic mechanisms. We give a unified approach that covers both continuous and discrete type spaces. On many common preference domains—including any convex domain of cardinal or ordinal preferences, single-peaked ordinal preferences, and successive single-crossing ordinal preferences—local incentive compatibility (suitably defined) implies full incentive compatibility. On domains of cardinal preferences that satisfy a strong nonconvexity condition, local incentive compatibility is not sufficient. Our sufficiency results hold for dominant-strategy and Bayesian Nash solution concepts, and allow for some interdependence in preferences.</p> </abstract>
<abstract> <p>This paper investigates the effects of market size on the ability of price to aggregate traders' private information. To account for heterogeneity in correlation of trader values, a Gaussian model of double auction is introduced that departs from the standard information structure based on a common (fundamental) shock. The paper shows that markets are informationally efficient only if correlations of values coincide across all bidder pairs. As a result, with heterogeneously interdependent values, price informativeness may not increase monotonically with market size. As a necessary and sufficient condition for the monotonicity, price informativeness increases with the number of traders if the implied reduction in (the absolute value of) an average correlation statistic of an information structure is sufficiently small.</p> </abstract>
<abstract> <p>This paper develops a new estimation procedure for characteristic-based factor models of stock returns. We treat the factor model as a weighted additive nonparametric regression model, with the factor returns serving as time-varying weights and a set of univariate nonparametric functions relating security characteristic to the associated factor betas. We use a time-series and cross-sectional pooled weighted additive nonparametric regression methodology to simultaneously estimate the factor returns and characteristic-beta functions. By avoiding the curse of dimensionality, our methodology allows for a larger number of factors than existing semiparametric methods. We apply the technique to the three-factor Fama-French model, Carhart's four-factor extension of it that adds a momentum factor, and a five-factor extension that adds an own-volatility factor. We find that momentum and own-volatility factors are at least as important, if not more important, than size and value in explaining equity return comovements. We test the multifactor beta pricing theory against a general alternative using a new nonparametric test.</p> </abstract>
<abstract> <p>A large-sample approximation of the posterior distribution of partially identified structural parameters is derived for models that can be indexed by an identifiable finitedimensional reduced-form parameter vector. It is used to analyze the differences between Bayesian credible sets and frequentist confidence sets. We define a plug-in estimator of the identified set and show that asymptotically Bayesian highest-posteriordensity sets exclude parts of the estimated identified set, whereas it is well known that frequentist confidence sets extend beyond the boundaries of the estimated identified set. We recommend reporting estimates of the identified set and information about the conditional prior along with Bayesian credible sets. A numerical illustration for a two-player entry game is provided.</p> </abstract>
<abstract> <p>We study mixed hitting-time models that specify durations as the first time a Lévy process—a continuous-time process with stationary and independent increments—crosses a heterogeneous threshold. Such models are of substantial interest because they can be deduced from optimal-stopping models with heterogeneous agents that do not naturally produce a mixed proportional hazards structure. We show how strategies for analyzing the identifiability of the mixed proportional hazards model can be adapted to prove identifiability of a hitting-time model with observed covariates and unobserved heterogeneity. We discuss inference from censored data and give examples of structural applications. We conclude by discussing the relative merits of both models as complementary frameworks for econometric duration analysis.</p> </abstract>
<abstract> <p>This paper studies the asymptotic properties of the quasi-maximum likelihood estimator of (generalized autoregressive conditional heteroscedasticity) GARCH(1,1) models without strict stationarity constraints and considers applications to testing problems. The estimator is unrestricted in the sense that the value of the intercept, which cannot be consistently estimated in the explosive case, is not fixed. A specific behavior of the estimator of the GARCH coefficients is obtained at the boundary of the stationarity region, but, except for the intercept, this estimator remains consistent and asymptotically normal in every situation. The asymptotic variance is different in the stationary and nonstationary situations, but is consistently estimated with the same estimator in both cases. Tests of strict stationarity and nonstationarity are proposed. The tests developed for the classical GARCH(1,1) model are able to detect nonstationarity in more general GARCH models. A numerical illustration based on stock indices and individual stock returns is proposed.</p> </abstract>
<abstract> <p>While vote-buying is common, little is known about how politicians determine who to target. We argue that vote-buying can be sustained by an internalized norm of reciprocity. Receiving money engenders feelings of obligation. Combining survey data on vote-buying with an experiment-based measure of reciprocity, we show that politicians target reciprocal individuals. Overall, our findings highlight the importance of social preferences in determining political behavior.</p> </abstract>
<abstract> <p>We report an experiment on effects of varying institutional format and dynamic structure of centipede games and Dutch auctions. Centipede games with a clock format unravel, as predicted by theory but not reported in previous literature on two-player tree-format centipede games. Dutch auctions with a tree format produce bids close to risk neutral Nash equilibrium bids, unlike previous literature on clock-format Dutch auctions. Our data provide a new, expanded set of stylized facts which may provide a foundation for unified modeling of play in a class of games that includes centipede games and Dutch auctions.</p> </abstract>
<abstract> <p>I explore the equilibrium value implications of economic models that incorporate responses to a stochastic environment with growth. I propose dynamic valuation decompositions (DVD's) designed to distinguish components of an underlying economic model that influence values over long investment horizons from components that impact only the short run. A DVD represents the values of stochastically growing claims to consumption payoffs or cash flows using a stochastic discount process that both discounts the future and adjusts for risk. It is enabled by constructing operators indexed by the elapsed time between the trading date and the date of the future realization of the payoff. Thus formulated, methods from applied mathematics permit me to characterize valuation behavior and the term structure of risk prices in a revealing manner. I apply this approach to investigate how investor beliefs and the associated uncertainty are reflected in current-period values and risk-price elasticities.</p> </abstract>
<abstract> <p>How can price elasticities be identified when agents face optimization frictions such as adjustment costs or inattention? I derive bounds on structural price elasticities that are a function of the observed effect of a price change on demand, the size of the price change, and the degree of frictions. The degree of frictions is measured by the utility losses agents tolerate to deviate from the frictionless optimum. The bounds imply that frictions affect intensive margin elasticities much more than extensive margin elasticities. I apply these bounds to the literature on labor supply. The utility costs of ignoring the tax changes used to identify intensive margin labor supply elasticities are typically less than 1% of earnings. As a result, small frictions can explain the differences between micro and macro elasticities, extensive and intensive margin elasticities, and other disparate findings. Pooling estimates from existing studies, I estimate a Hicksian labor supply elasticity of 0.33 on the intensive margin and 0.25 on the extensive margin after accounting for frictions.</p> </abstract>
<abstract> <p>The typical cost analysis of an environmental regulation consists of an engineering estimate of the compliance costs. In industries where fixed costs are an important determinant of market structure, this static analysis ignores the dynamic effects of the regulation on entry, investment, and market power. I evaluate the welfare costs of the 1990 Amendments to the Clean Air Act on the U.S. Portland cement industry, accounting for these effects through a dynamic model of oligopoly in the tradition of Ericson and Pakes (1995). Using the two-step estimator of Bajari, Benkard, and Levin (2007), I recover the entire cost structure of the industry, including the distributions of sunk entry costs and capacity adjustment costs. My primary finding is that the Amendments have significantly increased the sunk cost of entry, leading to a loss of between $ 810M and $ 3.2B in product market surplus. A static analysis misses the welfare penalty on consumers, and obtains the wrong sign of the welfare effects on incumbent firms.</p> </abstract>
<abstract> <p>Does switching the composition of jobs between low-paying and high-paying industries have important effects on wages in other sectors? In this paper, we build on search and bargaining theory to clarify a key general equilibrium channel through which changes in industrial composition could have substantial effects on wages in all sectors. In this class of models, wage determination takes the form of a social interaction problem and we illustrate how the implied sectoral linkages can be empirically explored using U.S. Census data. We find that sector-level wages interact as implied by the model and that the predicted general equilibrium effects are present and substantial. We interpret our results as highlighting the relevance of search and bargaining theory for understanding the determination of wages, and we argue that the results provide support for the view that industrial composition is important for understanding wage outcomes.</p> </abstract>
<abstract> <p>We introduce and derive the asymptotic behavior of a new measure constructed from high-frequency data which we call the realized Laplace transform of volatility. The statistic provides a nonparametric estimate for the empirical Laplace transform function of the latent stochastic volatility process over a given interval of time and is robust to the presence of jumps in the price process. With a long span of data, that is, under joint long-span and infill asymptotics, the statistic can be used to construct a nonparametric estimate of the volatility Laplace transform as well as of the integrated joint Laplace transform of volatility over different points of time. We derive feasible functional limit theorems for our statistic both under fixed-span and infill asymptotics as well as under joint long-span and infill asymptotics which allow us to quantify the precision in estimation under both sampling schemes.</p> </abstract>
<abstract> <p>We analyze the identification and estimation of parameters β satisfying the incomplete linear moment restrictions E (z T (×β — y)) = E(z T u(z)), where z is a set of instruments and u(z) an unknown bounded scalar function. We first provide empirically relevant examples of such a setup. Second, we show that these conditions set identify β where the identified set B is bounded and convex. We provide a sharp characterization of the identified set not only when the number of moment conditions is equal to the number of parameters of interest, but also in the case in which the number of conditions is strictly larger than the number of parameters. We derive a necessary and sufficient condition of the validity of supernumerary restrictions which generalizes the familiar Sargan condition. Third, we provide new results on the asymptotics of analog estimates constructed from the identification results. When B is a strictly convex set, we also construct a test of the null hypothesis, β₀ ∊ B, whose size is asymptotically correct and which relies on the minimization of the support function of the set B — (β₀). Results of some Monte Carlo experiments are presented.</p> </abstract>
<abstract> <p>Checking parameter stability of econometric models is a long-standing problem. Almost all existing structural change tests in econometrics are designed to detect abrupt breaks. Little attention has been paid to smooth structural changes, which may be more realistic in economics. We propose a consistent test for smooth structural changes as well as abrupt structural breaks with known or unknown change points. The idea is to estimate smooth time-varying parameters by local smoothing and compare the fitted values of the restricted constant parameter model and the unrestricted time-varying parameter model. The test is asymptotically pivotal and does not require prior information about the alternative. A simulation study highlights the merits of the proposed test relative to a variety of popular tests for structural changes. In an application, we strongly reject the stability of univariate and multivariate stock return prediction models in the postwar and post-oil-shocks periods.</p> </abstract>
<abstract> <p>This paper studies a dynamic model of perfectly competitive price posting under demand uncertainty. Firms must produce output in advance. After observing aggregate sales in prior periods, firms post prices for their unsold output. In each period, the demand of a new batch of consumers is randomly activated. Existing customers who have not yet bought and then new customers arrive at the market in random order, observe the posted prices, and either purchase at the lowest available price or delay their purchase decision. We construct a sequential equilibrium in which the output produced and its allocation across consumers is efficient. Thus consumers endogenously sort themselves efficiently, with the highest valuations purchasing first. Transaction prices in each period rise continuously, as firms become more optimistic about demand, followed by a market correction. By the last period, prices are market clearing.</p> </abstract>
<abstract> <p>We study the existence of dynamic equilibria with endogenously complete markets in continuous-time, heterogenous agents economies driven by diffusion processes. Our main results show that under appropriate conditions on the transition density of the state variables, market completeness can be deduced from the primitives of the economy. In particular, we prove that a sufficient condition for market completeness is that the volatility of dividends be invertible and provide higher order conditions that apply when this condition fails as is the case in the presence of fixed income securities. In contrast to previous research, our formulation does not require that securities pay terminal dividends, and thus allows for both finite and infinite horizon economies.</p> </abstract>
<abstract> <p>We study the random Strotz model, a version of the Strotz (1955) model with uncertainty about the nature of the temptation that will strike. We show that the random Strotz representation is unique and characterize a comparative notion of "more temptation averse." Also, we demonstrate an unexpected connection between the random Strotz model and a generalization of the Gul-Pesendorfer (GP) (2001) model of temptation which allows for the temptation to be uncertain and which we call random GP. In particular, a preference over menus has a random GP representation if and only if it also has a representation via a random Strotz model with sufficiently smooth uncertainty about the intensity of temptation. We also show that choices of menus combined with choices from menus can distinguish the random GP and random Strotz models.</p> </abstract>
<abstract> <p>We find that Epstein's (2010) Ellsberg-style thought experiments pose, contrary to his claims, no paradox or difficulty for the smooth ambiguity model of decision making under uncertainty developed by Klibanoff, Marinacci, and Mukerji (2005). Not only are the thought experiments naturally handled by the smooth ambiguity model, but our reanalysis shows that they highlight some of its strengths compared to models such as the maxmin expected utility model (Gilboa and Schmeidler (1989)). In particular, these examples pose no challenge to the model's foundations—interpretation of the model as affording a separation of ambiguity and ambiguity attitude or the potential for calibrating ambiguity attitude in the model.</p> </abstract>
<abstract> <p>Philosophers, psychologists, and economists have long argued that certain decision rights carry not only instrumental value but may also be valuable for their own sake. The ideas of autonomy, freedom, and liberty derive their intuitive appeal—at least partly—from an assumed positive intrinsic value of decision rights. Providing clean evidence for the existence of this intrinsic value and measuring its size, however, is intricate. Here, we develop a method capable of achieving these goals. The data reveal that the large majority of our subjects intrinsically value decision rights beyond their instrumental benefit. The intrinsic valuation of decision rights has potentially important consequences for corporate governance, human resource management, and optimal job design: it may explain why managers value power, why employees appreciate jobs with task discretion, why individuals sort into self-employment, and why the reallocation of decision rights is often very difficult and cumbersome. Our method and results may also prove useful in developing an empirical revealed preference foundation for concepts such as "freedom of choice" and "individual autonomy."</p> </abstract>
<abstract> <p>It is costly to learn about market conditions elsewhere, especially in developing countries. This paper examines how such information frictions affect trade. Using data on regional agricultural trade in the Philippines, I first document a number of observed patterns in trade flows and prices that suggest the presence of information frictions. I then incorporate information frictions into a perfect competition trade model by embedding a process whereby heterogeneous producers engage in a costly sequential search process to determine where to sell their produce. I show that introducing information frictions reconciles the theory with the observed patterns in the data. Structural estimation of the model finds that information frictions are quantitatively important: roughly half the observed regional price dispersion is due to information frictions. Furthermore, incorporating information frictions improves the out-of-sample predictive power of the model.</p> </abstract>
<abstract> <p>This paper uses the information contained in the joint dynamics of individuals' labor earnings and consumption-choice decisions to quantify both the amount of income risk that individuals face and the extent to which they have access to informal insurance against this risk. We accomplish this task by using indirect inference to estimate a structural consumption-savings model, in which individuals both learn about the nature of their income process and partly insure shocks via informal mechanisms. In this framework, we estimate (i) the degree of partial insurance, (ii) the extent of systematic differences in income growth rates, (iii) the precision with which individuals know their own income growth rates when they begin their working lives, (iv) the persistence of typical labor income shocks, (v) the tightness of borrowing constraints, and (vi) the amount of measurement error in the data. In implementing indirect inference, we find that an auxiliary model that approximates the true structural equations of the model (which are not estimable) works very well, with negligible small sample bias. The main substantive findings are that income shocks are moderately persistent, systematic differences in income growth rates are large, individuals have substantial amounts of information about their income growth rates, and about one-half of income shocks are smoothed via partial insurance. Putting these findings together, the amount of uninsurable lifetime income risk that individuals perceive is substantially smaller than what is typically assumed in calibrated macroeconomic models with incomplete markets.</p> </abstract>
<abstract> <p>Studying Native American reservations, and their historical formation, I find that their forced integration of autonomous polities into a system of shared governance had large negative long-run consequences, even though the affected people were ethnically and linguistically homogenous. Reservations that combined multiple sub-tribal bands when they were formed are 30% poorer today, even when conditioning on prereservation political traditions. The results hold with tribe fixed effects, identifying only off within-tribe variation across reservations. I also provide estimates from an instrumental variable strategy based on historical mining rushes that led to exogenously more centralized reservations. Data on the timing of economic divergence and on contemporary political conflict suggest that the primary mechanism runs from persistent social divisions through the quality of local governance to the local economic environment.</p> </abstract>
<abstract> <p>U.S. data reveal three facts: (1) the share of goods in total expenditure declines at a constant rate over time, (2) the price of goods relative to services declines at a constant rate over time, and (3) poor households spend a larger fraction of their budget on goods than do rich households. I provide a macroeconomic model with non-Gorman preferences that rationalizes these facts, along with the aggregate Kaldor facts. The model is parsimonious and admits an analytical solution. Its functional form allows a decomposition of U.S. structural change into an income and substitution effect. Estimates from micro data show each of these effects to be of roughly equal importance.</p> </abstract>
<abstract> <p>We show that deterioration in household balance sheets, or the housing net worth channel, played a significant role in the sharp decline in U.S. employment between 2007 and 2009. Counties with a larger decline in housing net worth experience a larger decline in non-tradable employment. This result is not driven by industry-specific supplyside shocks, exposure to the construction sector, policy-induced business uncertainty, or contemporaneous credit supply tightening. We find little evidence of labor market adjustment in response to the housing net worth shock. There is no significant expansion of the tradable sector in counties with the largest decline in housing net worth. Further, there is little evidence of wage adjustment within or emigration out of the hardest hit counties.</p> </abstract>
<abstract> <p>Subjects in a laboratory experiment withdraw earnings from a cash reserve evolving according to an arithmetic Brownian motion in near-continuous time. Aggressive withdrawal policies expose subjects to risk of bankruptcy, but the policy that maximizes expected earnings need not maximize the odds of survival. When profit maximization is consistent with high rates of survival (HS parameters), subjects adjust decisively towards the optimum. When survival and profit maximization are sharply at odds (LS parameters), subjects persistently (and sub-optimally) hoard excess cash in an evident effort to improve survival rates. The design ensures that this hoarding is not due to standard risk aversion. Analysis of period-to-period adjustments in strategies suggests instead that hoarding is due to a widespread bias towards survival in the subject population. Robustness treatments varying feedback, parameters, and framing fail to eliminate the bias.</p> </abstract>
<abstract> <p>Before choosing among two actions with state-dependent payoffs, a Bayesian decision-maker with a finite memory sees a sequence of informative signals, ending each period with fixed chance. He summarizes information observed with a finite-state automaton. I characterize the optimal protocol as an equilibrium of a dynamic game of imperfect recall; a new player runs each memory state each period. Players act as if maximizing expected payoffs in a common finite action decision problem. I characterize equilibrium play with many multinomial signals. The optimal protocol rationalizes many behavioral phenomena, like "stickiness," salience, confirmation bias, and belief polarization.</p> </abstract>
<abstract> <p>In the regression-discontinuity (RD) design, units are assigned to treatment based on whether their value of an observed covariate exceeds a known cutoff. In this design, local polynomial estimators are now routinely employed to construct confidence intervals for treatment effects. The performance of these confidence intervals in applications, however, may be seriously hampered by their sensitivity to the specific bandwidth employed. Available bandwidth selectors typically yield a "large" bandwidth, leading to data-driven confidence intervals that may be biased, with empirical coverage well below their nominal target. We propose new theory-based, more robust confidence interval estimators for average treatment effects at the cutoff in sharp RD, sharp kink RD, fuzzy RD, and fuzzy kink RD designs. Our proposed confidence intervals are constructed using a bias-corrected RD estimator together with a novel standard error estimator. For practical implementation, we discuss mean squared error optimal bandwidths, which are by construction not valid for conventional confidence intervals but are valid with our robust approach, and consistent standard error estimators based on our new variance formulas. In a special case of practical interest, our procedure amounts to running a quadratic instead of a linear local regression. More generally, our results give a formal justification to simple inference procedures based on increasing the order of the local polynomial estimator employed. We find in a simulation study that our confidence intervals exhibit close-to-correct empirical coverage and good empirical interval length on average, remarkably improving upon the alternatives available in the literature. All results are readily available in R and STATA using our companion software packages described in Calonico, Cattaneo, and Titiunik (2014d, 2014b).</p> </abstract>
<abstract> <p>This paper develops the fixed-smoothing asymptotics in a two-step generalized method of moments (GMM) framework. Under this type of asymptotics, the weighting matrix in the second-step GMM criterion function converges weakly to a random matrix and the two-step GMM estimator is asymptotically mixed normal. Nevertheless, the Wald statistic, the GMM criterion function statistic, and the Lagrange multiplier statistic remain asymptotically pivotal. It is shown that critical values from the fixed-smoothing asymptotic distribution are high order correct under the conventional increasing-smoothing asymptotics. When an orthonormal series covariance estimator is used, the critical values can be approximated very well by the quantiles of a noncentral F distribution. A simulation study shows that statistical tests based on the new fixed-smoothing approximation are much more accurate in size than existing tests.</p> </abstract>
<abstract> <p>We study dominant strategy incentive compatibility in a mechanism design setting with contingent contracts where the payoff of each agent is observed by the principal and can be contracted upon. Our main focus is on the class of linear contracts (one of the most commonly used contingent contracts) which consist of a transfer and a flat rate of profit sharing. We characterize outcomes implementable by linear contracts and provide a foundation for them by showing that, in finite type spaces, every social choice function that can be implemented using a more general nonlinear contingent contract can also be implemented using a linear contract. We then qualitatively describe the set of implementable outcomes. We show that a general class of social welfare criteria can be implemented. This class contains social choice functions (such as the Rawlsian) which cannot be implemented using (uncontingent) transfers. Under additional conditions, we show that only social choice functions in this class are implementable.</p> </abstract>
<abstract> <p>We introduce methods for estimating nonparametric, nonadditive models with simultaneity. The methods are developed by directly connecting the elements of the structural system to be estimated with features of the density of the observable variables, such as ratios of derivatives or averages of products of derivatives of this density. The estimators are therefore easily computed functionals of a nonparametric estimator of the density of the observable variables. We consider in detail a model where to each structural equation there corresponds an exclusive regressor and a model with one equation of interest and one instrument that is included in a second equation. For both models, we provide new characterizations of observational equivalence on a set, in terms of the density of the observable variables and derivatives of the structural functions. Based on those characterizations, we develop two estimation methods. In the first method, the estimators of the structural derivatives are calculated by a simple matrix inversion and matrix multiplication, analogous to a standard least squares estimator, but with the elements of the matrices being averages of products of derivatives of non-parametric density estimators. In the second method, the estimators of the structural derivatives are calculated in two steps. In a first step, values of the instrument are found at which the density of the observable variables satisfies some properties. In the second step, the estimators are calculated directly from the values of derivatives of the density of the observable variables evaluated at the found values of the instrument. We show that both pointwise estimators are consistent and asymptotically normal.</p> </abstract>
<abstract> <p>This study provides causal evidence that a shock to the relative supply of inputs to production can (1) affect the direction of technological progress and (2) lead to a rebound in the relative price of the input that became relatively more abundant (the strong induced-bias hypothesis). I exploit the impact of the U.S. Civil War on the British cotton textile industry, which reduced supplies of cotton from the Southern United States, forcing British producers to shift to lower-quality Indian cotton. Using detailed new data, I show that this shift induced the development of new technologies that augmented Indian cotton. As these new technologies became available, I show that the relative price of Indian/U.S. cotton rebounded to its pre-war level, despite the increased relative supply of Indian cotton. This is the first paper to establish both of these patterns empirically, lending support to the two key predictions of leading directed technical change theories.</p> </abstract>
<abstract> <p>Are there times when durable spending is less responsive to economic stimulus? We argue that aggregate durable expenditures respond more sluggishly to economic shocks during recessions because microeconomic frictions lead to declines in the frequency of households' durable adjustment. We show this by first using indirect inference to estimate a heterogeneous agent incomplete markets model with fixed costs of durable adjustment to match consumption dynamics in PSID microdata. We then show that aggregating this model delivers an extremely procyclical Impulse Response Function (IRF) of durable spending to aggregate shocks. For example, the response of durable spending to an income shock in 1999 is estimated to be almost twice as large as if it occurred in 2009. This procyclical IRF holds in response to standard business cycle shocks as well as in response to various policy shocks, and it is robust to general equilibrium. After estimating this robust theoretical implication of micro frictions, we provide additional direct empirical evidence for its importance using both cross-sectional and time-series data.</p> </abstract>
<abstract> <p>Internet advertising has been the fastest growing advertising channel in recent years, with paid search ads comprising the bulk of this revenue. We present results from a series of large-scale field experiments done at eBay that were designed to measure the causal effectiveness of paid search ads. Because search clicks and purchase intent are correlated, we show that returns from paid search are a fraction of non-experimental estimates. As an extreme case, we show that brand keyword ads have no measurable short-term benefits. For non-brand keywords, we find that new and infrequent users are positively influenced by ads but that more frequent users whose purchasing behavior is not influenced by ads account for most of the advertising expenses, resulting in average returns that are negative.</p> </abstract>
<abstract> <p>For an arbitrary data set D = {(p, x)} ⊂̠ (ℝm+\{0}) × ℝm+, finite or infinite, it is shown that the following three conditions are equivalent: (a) D satisfies GARP; (b) D can be rationalized by a utility function; (c) D can be rationalized by a utility function that is quasiconcave, nondecreasing, and that strictly increases when all its coordinates strictly increase. Examples of infinite data sets satisfying GARP are provided for which every utility rationalization fails to be lower semicontinuous, upper semicontinuous, or concave. Thus condition (c) cannot be substantively improved upon.</p> </abstract>
<abstract> <p>The paper analyzes dynamic principal-agent models with short period lengths. The two main contributions are: (i) an analytic characterization of the values of optimal contracts in the limit as the period length goes to 0, and (ii) the construction of relatively simple (almost) optimal contracts for fixed period lengths. Our setting is flexible and includes the pure hidden action or pure hidden information models as special cases. We show how such details of the underlying information structure affect the optimal provision of incentives and the value of the contracts. The dependence is very tractable and we obtain sharp comparative statics results. The results are derived with a novel method that uses a quadratic approximation of the Pareto boundary of the equilibrium value set.</p> </abstract>
<abstract> <p>This paper axiomatizes an intertemporal version of the maxmin expected-utility model. It employs two axioms specific to a dynamic setting. The first requires that smoothing consumption across states of the world is more beneficial to the individual than smoothing consumption across time. Such behavior is viewed as the intertemporal manifestation of ambiguity aversion. The second axiom extends Koopmans' notion of stationarity from deterministic to stochastic environments.</p> </abstract>
<abstract> <p>We develop a model of the market for federal funds that explicitly accounts for its two distinctive features: banks have to search for a suitable counterparty, and once they meet, both parties negotiate the size of the loan and the repayment. The theory is used to answer a number of positive and normative questions: What are the determinants of the fed funds rate? How does the market reallocate funds? Is the market able to achieve an efficient reallocation of funds? We also use the model for theoretical and quantitative analyses of policy issues facing modern central banks.</p> </abstract>
<abstract> <p>This paper provides conditions under which the inequality constraints generated by either single agent optimizing behavior or the best response condition of multiple agent problems can be used as a basis for estimation and inference. An application illustrates how the use of these inequality constraints can simplify the analysis of complex behavioral models.</p> </abstract>
<abstract> <p>A sequence of experiments documents static and dynamic "preference reversals" between sooner-smaller and later-larger rewards, when the sooner reward could be immediate. The theoretically motivated design permits separate identification of time consistent, stationary, and time invariant choices. At least half of the subjects are time consistent, but only three-quarters of them exhibit stationary choices. About half of subjects with time inconsistent choices have stationary preferences. These results challenge the view that present-bias preferences are the main source of time inconsistent choices.</p> </abstract>
<abstract> <p>In this paper, I construct players' prior beliefs and show that these prior beliefs lead the players to learn to play an approximate Nash equilibrium uniformly in any infinitely repeated slightly perturbed game with discounting and perfect monitoring. That is, given any ε &gt; 0, there exists a (single) profile of players' prior beliefs that leads play to almost surely converge to an ε-Nash equilibrium uniformly for any (finite normal form) stage game with slight payoff perturbation and any discount factor less than 1.</p> </abstract>
<abstract> <p>Ivanov, Levin, and Niederle (2010) use a common-value second-price auction experiment to reject beliefs-based explanations for the winner's curse. ILN's conclusion, however, stems from the misuse of theoretical arguments. Beliefs-based models are even compatible with some observations from ILN's experiment.</p> </abstract>
<abstract> <p>This paper studies the introduction of electronic voting technology in Brazilian elections. Estimates exploiting a regression discontinuity design indicate that electronic voting reduced residual (error-ridden and uncounted) votes and promoted a large de facto enfranchisement of mainly less educated citizens. Estimates exploiting the unique pattern of the technology's phase-in across states over time suggest that, as predicted by political economy models, it shifted government spending toward health care, which is particularly beneficial to the poor. Positive effects on both the utilization of health services (prenatal visits) and newborn health (low-weight births) are also found for less educated mothers, but not for the more educated.</p> </abstract>
<abstract> <p>Is African politics characterized by concentrated power in the hands of a narrow group (ethnically determined) that then fluctuates from one extreme to another via frequent coups? Employing data on the ethnicity of cabinet ministers since independence, we show that African ruling coalitions are surprisingly large and that political power is allocated proportionally to population shares across ethnic groups. This holds true even restricting the analysis to the subsample of the most powerful ministerial posts. We argue that the likelihood of revolutions from outsiders and coup threats from insiders are major forces explaining allocations within these regimes. Alternative allocation mechanisms are explored. Counterfactual experiments that shed light on the role of Western policies in affecting African national coalitions and leadership group premia are performed.</p> </abstract>
<abstract> <p>We examine the link between the threat of violence and democratization in the context of the Great Reform Act passed by the British Parliament in 1832. We georeference the so-called Swing riots, which occurred between the 1830 and 1831 parliamentary elections, and compute the number of these riots that happened within a 10 km radius of the 244 English constituencies. Our empirical analysis relates this constituency-specific measure of the threat perceptions held by the 344,000 voters in the Unreformed Parliament to the share of seats won in each constituency by pro-reform politicians in 1831. We find that the Swing riots induced voters to vote for pro-reform politicians after experiencing first-hand the violence of the riots.</p> </abstract>
<abstract> <p>We formalize the Keynesian insight that aggregate demand driven by sentiments can generate output fluctuations under rational expectations. When production decisions must be made under imperfect information about demand, optimal decisions based on sentiments can generate stochastic self-fulfilling rational expectations equilibria in standard economies without persistent informational frictions, externalities, nonconvexities, or strategic complementarities in production. The models we consider are deliberately simple, but could serve as benchmarks for more complicated equilibrium models with additional features.</p> </abstract>
<abstract> <p>We study social dilemmas in (quasi-) continuous-time experiments, comparing games with different durations and termination rules. We discover a stark qualitative contrast in behavior in continuous time as compared to previously studied behavior in discrete-time games: cooperation is easier to achieve and sustain with deterministic horizons than with stochastic ones, and end-game effects emerge, but subjects postpone them with experience. Analysis of individual strategies provides a basis for a simple reinforcement learning model that proves to be consistent with this evidence. An additional treatment lends further support to this explanation.</p> </abstract>
<abstract> <p>We consider empirical measurement of equivalent variation (EV) and compensating variation (CV) resulting from price change of a discrete good using individual-level data when there is unobserved heterogeneity in preferences. We show that for binary and unordered multinomial choice, the marginal distributions of EV and CV can be expressed as simple closed-form functionals of conditional choice probabilities under essentially unrestricted preference distributions. These results hold even when the distribution and dimension of unobserved heterogeneity are neither known nor identified, and utilities are neither quasilinear nor parametrically specified. The welfare distributions take simple forms that are easy to compute in applications. In particular, average EV for a price rise equals the change in average Marshallian consumer surplus and is smaller than average CV for a normal good. These nonparametric point-identification results fail for ordered choice if the unit price is identical for all alternatives, thereby providing a connection to Hausman—Newey's (2014) partial identification results for the limiting case of continuous choice.</p> </abstract>
<abstract> <p>We characterize a generalization of discounted logistic choice that incorporates a parameter to capture different views the agent might have about the costs and benefits of larger choice sets. The discounted logit model used in the empirical literature is the special case that displays a "preference for flexibility" in the sense that the agent always prefers to add additional items to a menu. Other cases display varying levels of "choice aversion," where the agent prefers to remove items from a menu if their ex ante value is below a threshold. We show that higher choice aversion, as measured by dislike of bigger menus, also corresponds to an increased preference for putting off decisions as late as possible.</p> </abstract>
<abstract> <p>Many violations of the independence axiom of expected utility can be traced to subjects' attraction to risk-free prospects. The key axiom in this paper, negative certainty independence (Dillenberger (2010)), formalizes this tendency. Our main result is a utility representation of all preferences over monetary lotteries that satisfy negative certainty independence together with basic rationality postulates. Such preferences can be represented as if the agent were unsure of how to evaluate a given lottery p; instead, she has in mind a set of possible utility functions over outcomes and displays a cautious behavior: she computes the certainty equivalent of p with respect to each possible function in the set and picks the smallest one. The set of utilities is unique in a well defined sense. We show that our representation can also be derived from a "cautious" completion of an incomplete preference relation.</p> </abstract>
<abstract> <p>This paper presents a new method for the analysis of moral hazard principal-agent problems. The new approach avoids the stringent assumptions on the distribution of outcomes made by the classical first-order approach and instead only requires the agent's expected utility to be a rational function of the action. This assumption allows for a reformulation of the agent's utility maximization problem as an equivalent system of equations and inequalities. This reformulation in turn transforms the principal's utility maximization problem into a nonlinear program. Under the additional assumptions that the principal's expected utility is a polynomial and the agent's expected utility is rational in the wage, the final nonlinear program can be solved to global optimality. The paper also shows how to first approximate expected utility functions that are not rational by polynomials, so that the polynomial optimization approach can be applied to compute an approximate solution to nonpolynomial problems. Finally, the paper demonstrates that the polynomial optimization approach extends to principal-agent models with multidimensional action sets.</p> </abstract>
<abstract> <p>This paper considers nonstandard hypothesis testing problems that involve a nuisance parameter. We establish an upper bound on the weighted average power of all valid tests, and develop a numerical algorithm that determines a feasible test with power close to the bound. The approach is illustrated in six applications: inference about a linear regression coefficient when the sign of a control coefficient is known; small sample inference about the difference in means from two independent Gaussian samples from populations with potentially different variances; inference about the break date in structural break models with moderate break magnitude; predictability tests when the regressor is highly persistent; inference about an interval identified parameter; and inference about a linear regression coefficient when the necessity of a control is in doubt.</p> </abstract>
<abstract> <p>It is well known that the finite-sample properties of tests of hypotheses on the cointegrating vectors in vector autoregressive models can be quite poor, and that current solutions based on Bartlett-type corrections or bootstrap based on unrestricted parameter estimators are unsatisfactory, in particular in those cases where also asymptotic X² tests fail most severely. In this paper, we solve this inference problem by showing the novel result that a bootstrap test where the null hypothesis is imposed on the bootstrap sample is asymptotically valid. That is, not only does it have asymptotically correct size, but, in contrast to what is claimed in existing literature, it is consistent under the alternative. Compared to the theory for bootstrap tests on the co-integration rank (Cavaliere, Rahbek, and Taylor (2012)), establishing the validity of the bootstrap in the framework of hypotheses on the co-integrating vectors requires new theoretical developments, including the introduction of multivariate Ornstein-Uhlenbeck processes with random (reduced rank) drift parameters. Finally, as documented by Monte Carlo simulations, the bootstrap test outperforms existing methods.</p> </abstract>
<abstract> <p>We study markets in which agents first make investments and are then matched into potentially productive partnerships. Equilibrium investments and the equilibrium matching will be efficient if agents can simultaneously negotiate investments and matches, but we focus on markets in which agents must first sink their investments before matching. Additional equilibria may arise in this sunk-investment setting, even though our matching market is competitive. These equilibria exhibit inefficiencies that we can interpret as coordination failures. All allocations satisfying a constrained efficiency property are equilibria, and the converse holds if preferences satisfy a separability condition. We identify sufficient conditions (most notably, quasiconcave utilities) for the investments of matched agents to satisfy an exchange efficiency property as well as sufficient conditions (most notably, a single crossing property) for agents to be matched positive assortatively, with these conditions then forming the core of sufficient conditions for the efficiency of equilibrium allocations.</p> </abstract>
<abstract> <p>This paper studies two-sided matching markets with non-transferable utility when the number of market participants grows large. We consider a model in which each agent has a random preference ordering over individual potential matching partners, and agents' types are only partially observed by the econometrician. We show that in a large market, the inclusive value is a sufficient statistic for an agent's endogenous choice set with respect to the probability of being matched to a spouse of a given observable type. Furthermore, while the number of pairwise stable matchings for a typical realization of random utilities grows at a fast rate as the number of market participants increases, the inclusive values resulting from any stable matching converge to a unique deterministic limit. We can therefore characterize the limiting distribution of the matching market as the unique solution to a fixed-point condition on the inclusive values. Finally we analyze identification and estimation of payoff parameters from the asymptotic distribution of observable characteristics at the level of pairs resulting from a stable matching.</p> </abstract>
<abstract> <p>When people interact in familiar settings, social conventions usually develop so that people tend to disregard alternatives outside the convention. For rational players to usually restrict attention to a block of conventional strategies, no player should prefer to deviate from the block when others are likely to act conventionally and rationally inside the block. We explore two set-valued concepts, coarsely and finely tenable blocks, that formalize this notion for finite normal-form games. We then identify settled equilibria, which are Nash equilibria with support in minimal tenable blocks. For a generic class of normal-form games, our coarse and fine concepts are equivalent, and yet they differ from standard solution concepts on open sets of games. We demonstrate the nature and power of the solutions by way of examples. Settled equilibria are closely related to persistent equilibria but are strictly more selective on an open set of games. With fine tenability, we obtain invariance under the insertion of a subgame with a unique totally mixed payoff-equivalent equilibrium, a property that other related concepts have not satisfied.</p> </abstract>
<abstract> <p>Harsanyi (1974) criticized the von Neumann-Morgenstern (vNM) stable set for its presumption that coalitions are myopic about their prospects. He proposed a new dominance relation incorporating farsightedness, but retained another feature of the stable set: that a coalition S can impose any imputation as long as its restriction to S is feasible for it. This implicitly gives an objecting coalition complete power to arrange the payoffs of players elsewhere, which is clearly unsatisfactory. While this assumption is largely innocuous for myopic dominance, it is of crucial significance for its farsighted counterpart. Our modification of the Harsanyi set respects "coalitional sovereignty." The resulting farsighted stable set is very different from both the Harsanyi and the vNM sets. We provide a necessary and sufficient condition for the existence of a farsighted stable set containing just a single-payoff allocation. This condition roughly establishes an equivalence between core allocations and the union of allocations over all single-payoff farsighted stable sets. We then conduct a comprehensive analysis of the existence and structure of farsighted stable sets in simple games. This last exercise throws light on both single-payoff and multi-payoff stable sets, and suggests that they do not coexist.</p> </abstract>
<abstract> <p>This paper considers inference on functional of semi/nonparametric conditional moment restrictions with possibly nonsmooth generalized residuals, which include all of the (nonlinear) nonparametric instrumental variables (IV) as special cases. These models are often ill-posed and hence it is difficult to verify whether a (possibly nonlinear) functional is root-n estimable or not. We provide computationally simple, unified inference procedures that are asymptotically valid regardless of whether a functional is root-estimable or not. We establish the following new useful results: (1) the asymptotic normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a (possibly nonlinear) functional; (2) the consistency of simple sieve variance estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square distribution of the sieve Wald statistic; (3) the asymptotic chi-square distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test under the null hypothesis; (4) the asymptotic tight distribution of a non-optimally weighted sieve QLR statistic under the null; (5) the consistency of generalized residual bootstrap sieve Wald and QLR tests; (6) local power properties of sieve Wald and QLR tests and of their bootstrap versions; (7) asymptotic properties of sieve Wald and SQLR for functionals of increasing dimension. Simulation studies and an empirical illustration of a nonparametric quantile IV regression are presented.</p> </abstract>
<abstract> <p>We develop a new parametric estimation procedure for option panels observed with error. We exploit asymptotic approximations assuming an ever increasing set of option prices in the moneyness (cross-sectional) dimension, but with a fixed time span. We develop consistent estimators for the parameters and the dynamic realization of the state vector governing the option price dynamics. The estimators converge stably to a mixed-Gaussian law and we develop feasible estimators for the limiting variance. We also provide semiparametric tests for the option price dynamics based on the distance between the spot volatility extracted from the options and one constructed nonparametrically from high-frequency data on the underlying asset. Furthermore, we develop new tests for the day-by-day model fit over specific regions of the volatility surface and for the stability of the risk-neutral dynamics over time. A comprehensive Monte Carlo study indicates that the inference procedures work well in empirically realistic settings. In an empirical application to S&amp; P 500 index options, guided by the new diagnostic tests, we extend existing asset pricing models by allowing for a flexible dynamic relation between volatility and priced jump tail risk. Importantly, we document that the priced jump tail risk typically responds in a more pronounced and persistent manner than volatility to large negative market shocks.</p> </abstract>
<abstract> <p>This paper introduces time-varying grouped patterns of heterogeneity in linear panel data models. A distinctive feature of our approach is that group membership is left unrestricted. We estimate the parameters of the model using a "grouped fixed-effects" estimator that minimizes a least squares criterion with respect to all possible groupings of the cross-sectional units. Recent advances in the clustering literature allow for fast and efficient computation. We provide conditions under which our estimator is consistent as both dimensions of the panel tend to infinity, and we develop inference methods. Finally, we allow for grouped patterns of unobserved heterogeneity in the study of the link between income and democracy across countries.</p> </abstract>
<abstract> <p>I consider nonparametric identification of nonseparable instrumental variables models with continuous endogenous variables. If both the outcome and first stage equations are strictly increasing in a scalar unobservable, then many kinds of continuous, discrete, and even binary instruments can be used to point-identify the levels of the outcome equation. This contrasts sharply with related work by Imbens and Newey (2009) that requires continuous instruments with large support. One implication is that assumptions about the dimension of heterogeneity can provide nonparametric point-identification of the distribution of treatment response for a continuous treatment in a randomized controlled experiment with partial compliance.</p> </abstract>
<abstract> <p>We study the identification through instruments of a nonseparable function that relates a continuous outcome to a continuous endogenous variable. Using group and dynamical systems theories, we show that full identification can be achieved under strong exogeneity of the instrument and a dual monotonicity condition, even if the instrument is discrete. When identified, the model is also testable. Our results therefore highlight the identifying power of strong exogeneity when combined with monotonicity restrictions.</p> </abstract>
<abstract> <p>We develop a strategic theory of counterfeiting as a multi-market large game. Bad guys choose whether to counterfeit, and what quality to produce. Opposing them is a continuum of good guys who select a costly verification effort. In equilibrium, counterfeiters produce better quality at higher notes, but verifiers try sufficiently harder that verification still improves. We develop a graphical framework for deducing comparative statics. Passed and counterfeiting rates vanish for low and high notes. Our predictions are consistent with time series and cross-sectional patterns in a unique data set assembled largely from the Secret Service.</p> </abstract>
<abstract> <p>Levy (2013) presented examples of discounted stochastic games that do not have stationary equilibria. The second named author has pointed out that one of these examples is incorrect. In addition to describing the details of this error, this note presents a new example by the first named author that succeeds in demonstrating that discounted stochastic games with absolutely continuous transitions can fail to have stationary equilibria.</p> </abstract>
<abstract> <p>This paper studies regulated health insurance markets known as exchanges, motivated by the increasingly important role they play in both public and private insurance provision. We develop a framework that combines data on health outcomes and insurance plan choices for a population of insured individuals with a model of a competitive insurance exchange to predict outcomes under different exchange designs. We apply this framework to examine the effects of regulations that govern insurers' ability to use health status information in pricing. We investigate the welfare implications of these regulations with an emphasis on two potential sources of inefficiency: (i) adverse selection and (ii) premium reclassification risk. We find substantial adverse selection leading to full unraveling of our simulated exchange, even when age can be priced. While the welfare cost of adverse selection is substantial when health status cannot be priced, that of reclassification risk is five times larger when insurers can price based on some health status information. We investigate several extensions including (i) contract design regulation, (ii) self-insurance through saving and borrowing, and (iii) insurer risk adjustment transfers.</p> </abstract>
<abstract> <p>Using an exhaustive data set on claims held by trade creditors (suppliers) on failed trade debtors (customers), we quantify the importance of trade credit chains for the propagation of corporate bankruptcy. We show that trade creditors experience significant trade credit losses due to trade debtor failures and that creditors' bankruptcy risks increase in the size of incurred losses. By exploring the roles of financial constraints and creditor-debtor dependences, we infer that the trade credit failure propagation mechanism is driven by both credit losses and demand shrinkage. Finally, we show that the documented propagation mechanism constitutes a significant part of the overall bankruptcy frequency, suggesting that it has measurable implications for the aggregate level.</p> </abstract>
<abstract> <p>This paper develops a new model for empirically analyzing dynamic matching in the marriage market and then applies that model to recent changes in the U.S. marriage distribution. Its primary objective is to estimate gains by age from being married today (till death of at least one spouse) relative to remaining single for that same time period. An empirical methodology that relies on the model's equilibrium outcomes identifies the marriage gains using a single cross-section of observed aggregate matches. This behavioral dynamic model rationalizes a new marriage matching function. The model also solves the inverse problem of computing the vector of aggregate marriages, given a new distribution of available single individuals and estimated preferences. Finally, this paper develops a simple test of the model's empirical validity. Using aggregate data of new marriages and available single men and women in the United States over two decades from 1970 to 1990,I investigate the changes in marriage gains over this period.</p> </abstract>
<abstract> <p>This paper develops a theory of optimal provision of commitment devices to people who value both commitment and flexibility and whose preferences differ in the degree of time inconsistency. If time inconsistency is observable, both a planner and a monopolist provide devices that help each person commit to the efficient level of flexibility. However, the combination of unobservable time inconsistency and preference for flexibility causes an adverse-selection problem. To solve this problem, the monopolist and (possibly) the planner curtail flexibility in the device for a more inconsistent person at both ends of the efficient choice range; moreover, they may have to add unused options to the device for a less inconsistent person and also distort his actual choices. This theory has normative and positive implications for private and public provision of commitment devices.</p> </abstract>
<abstract> <p>We develop a behavioral axiomatic characterization of subjective expected utility (SEU) under risk aversion. Given is an individual agent's behavior in the market: assume a finite collection of asset purchases with corresponding prices. We show that such behavior satisfies a "revealed preference axiom" if and only if there exists a SEU model (a subjective probability over states and a concave utility function over money) that accounts for the given asset purchases.</p> </abstract>
<abstract> <p>We propose a novel technique to boost the power of testing a high-dimensional vector H : θ = 0 against sparse alternatives where the null hypothesis is violated by only a few components. Existing tests based on quadratic forms such as the Wald statistic often suffer from low powers due to the accumulation of errors in estimating high-dimensional parameters. More powerful tests for sparse alternatives such as thresholding and extreme value tests, on the other hand, require either stringent conditions or bootstrap to derive the null distribution and often suffer from size distortions due to the slow convergence. Based on a screening technique, we introduce a "power enhancement component," which is zero under the null hypothesis with high probability, but diverges quickly under sparse alternatives. The proposed test statistic combines the power enhancement component with an asymptotically pivotal statistic, and strengthens the power under sparse alternatives. The null distribution does not require stringent regularity conditions, and is completely determined by that of the pivotal statistic. The proposed methods are then applied to testing the factor pricing models and validating the cross-sectional independence in panel data models.</p> </abstract>
<abstract> <p>In this paper, we study the least squares (LS) estimator in a linear panel regression model with unknown number of factors appearing as interactive fixed effects. Assuming that the number of factors used in estimation is larger than the true number of factors in the data, we establish the limiting distribution of the LS estimator for the regression coefficients as the number of time periods and the number of cross-sectional units jointly go to infinity. The main result of the paper is that under certain assumptions, the limiting distribution of the LS estimator is independent of the number of factors used in the estimation as long as this number is not underestimated. The important practical implication of this result is that for inference on the regression coefficients, one does not necessarily need to estimate the number of interactive fixed effects consistently.</p> </abstract>
<abstract> <p>This paper presents a test of the exogeneity of a single explanatory variable in a multivariate model. It does not require the exogeneity of the other regressors or the existence of instrumental variables. The fundamental maintained assumption is that the model must be continuous in the explanatory variable of interest. This test has power when unobservable confounders are discontinuous with respect to the explanatory variable of interest, and it is particularly suitable for applications in which that variable has bunching points. An application of the test to the problem of estimating the effects of maternal smoking in birth weight shows evidence of remaining endogeneity, even after controlling for the most complete covariate specification in the literature.</p> </abstract>
<abstract> <p>This paper studies the dynamics of long-term contracts in repeated principal-agent relationships with an impatient agent. Despite the absence of exogenous uncertainty, Pareto-optimal dynamic contracts generically oscillate between favoring the principal and favoring the agent.</p> </abstract>
<abstract> <p>Mechanism design enables a social planner to obtain a desired outcome by leveraging the players' rationality and their beliefs. It is thus a fundamental, but yet unproven, intuition that the higher the level of rationality of the players, the better the set of obtainable outcomes. In this paper, we prove this fundamental intuition for players with possibilistic beliefs, a model long considered in epistemic game theory. Specifically, • We define a sequence of monotonically increasing revenue benchmarks for singlegood auctions, G⁰ ≤ G¹ ≤ G² ≤ · · ·, where each Gi is defined over the players' beliefs and G⁰ is the second-highest valuation (i.e., the revenue benchmark achieved by the second-price mechanism). • We (1) construct a single, interim individually rational, auction mechanism that, without any clue about the rationality level of the players, guarantees revenue Gk if all players have rationality levels ≥ k + 1, and (2) prove that no such mechanism can guarantee revenue even close to Gk when at least two players are at most level-k: rational.</p> </abstract>
<abstract> <p>This paper concerns the two-stage game introduced in Nash (1953). It formalizes a suggestion made (but not pursued) by Nash regarding equilibrium selection in that game, and hence offers an arguably more solid foundation for the "Nash bargaining with endogenous threats" solution. Analogous reasoning is then applied to an infinite horizon game to provide equilibrium selection in two-person repeated games with contracts. In this setting, issues about enforcement of threats are much less problematic than in Nash's static setting. The analysis can be extended to stochastic games with contracts.</p> </abstract>
<abstract> <p>There is a widely held view within the general public that large corporations should act in the interests of a broader group of agents than just their shareholders (the stakeholder view). This paper presents a framework where this idea can be justified. The point of departure is the observation that a large firm typically faces endogenous risks that may have a significant impact on the workers it employs and the consumers it serves. These risks generate externalities on these stakeholders which are not internalized by shareholders. As a result, in the competitive equilibrium, there is underinvestment in the prevention of these risks. We suggest that this under-investment problem can be alleviated if firms are instructed to maximize the total welfare of their stakeholders rather than shareholder value alone (stakeholder equilibrium). The stakeholder equilibrium can be implemented by introducing new property rights (employee rights and consumer rights) and instructing managers to maximize the total value of the firm (the value of these rights plus shareholder value). If there is only one firm, the stakeholder equilibrium is Pareto optimal. However, this is not true with more than one firm and/or heterogeneous agents, which illustrates some of the limits of the stakeholder model.</p> </abstract>
<abstract> <p>We analyze the Vickrey mechanism for auctions of multiple identical goods when the players have both Knightian uncertainty over their own valuations and incomplete preferences. In this model, the Vickrey mechanism is no longer dominant-strategy, and we prove that all dominant-strategy mechanisms are inadequate. However, we also prove that, in undominated strategies, the social welfare produced by the Vickrey mechanism in the worst case is not only very good, but also essentially optimal.</p> </abstract>
<abstract> <p>We consider a group of strategic agents who must each repeatedly take one of two possible actions. They learn which of the two actions is preferable from initial private signals and by observing the actions of their neighbors in a social network. We show that the question of whether or not the agents learn efficiently depends on the topology of the social network. In particular, we identify a geometric "egalitarianism" condition on the social network that guarantees learning in infinite networks, or learning with high probability in large finite networks, in any equilibrium. We also give examples of nonegalitarian networks with equilibria in which learning fails.</p> </abstract>
<abstract> <p>This paper characterizes an equilibrium payoff subset for dynamic Bayesian games as discounting vanishes. Monitoring is imperfect, transitions may depend on actions, types may be correlated, and values may be interdependent. The focus is on equilibria in which players report truthfully. The characterization generalizes that for repeated games, reducing the analysis to static Bayesian games with transfers. With independent private values, the restriction to truthful equilibria is without loss, except for the punishment level: if players withhold their information during punishment-like phases, a folk theorem obtains.</p> </abstract>
<abstract> <p>We consider a large market where auctioneers with private reservation values compete for bidders by announcing cheap-talk messages. If auctioneers run efficient first-price auctions, then there always exists an equilibrium in which each auctioneer truthfully reveals her type. The equilibrium is constrained efficient, assigning more bidders to auctioneers with larger gains from trade. The choice of the trading mechanism is crucial for the result. Most notably, the use of second-price auctions (equivalently, ex post bidding) leads to the nonexistence of any informative equilibrium. We examine the robustness of our finding in various dimensions, including finite markets and equilibrium selection.</p> </abstract>
<abstract> <p>We argue that poverty can perpetuate itself by undermining the capacity for self-control. In line with a distinguished psychological literature, we consider modes of selfcontrol that involve the self-imposed use of contingent punishments and rewards. We study settings in which consumers with quasi-hyperbolic preferences confront an otherwise standard intertemporal allocation problem with credit constraints. Our main result demonstrates that low initial assets can limit self-control, trapping people in poverty, while individuals with high initial assets can accumulate indefinitely. Thus, even temporary policies that initiate accumulation among the poor may be effective. We examine implications concerning the effect of access to credit on saving, the demand for commitment devices, the design of financial accounts to promote accumulation, and the variation of the marginal propensity to consume across income from different sources. We also explore the nature of optimal self-control, demonstrating that it has a simple and behaviorally plausible structure that is immune to self-renegotiation.</p> </abstract>
<abstract> <p>This paper analyzes South Africa's Free Basic Water Policy, under which households receive a free water allowance equal to the World Health Organization's recommended minimum. I estimate residential water demand, evaluate the welfare effects of free water, and provide optimal price schedules derived from a social planner's problem. I use a data set of monthly metered billing data for 60,000 households for 2002-2009 from a particularly disadvantaged suburb of Pretoria, with rich price variation across 20 different nonlinear tariff schedules. I find that the free allowance acts as a lump-sum subsidy, without large effects on water consumption. However, it is possible to reallocate the current subsidy to form an optimal tariff without a free allowance, which would increase welfare while leaving the water provider's profit unchanged. This optimal tariff would also reduce the number of households consuming low quantities of water, a desirable policy goal according to the WHO.</p> </abstract>
<abstract> <p>This paper makes the following original contributions to the literature. (i) We develop a simpler analytical characterization and numerical algorithm for Bayesian inference in structural vector autoregressions (VARs) that can be used for models that are overidentified, just-identified, or underidentified. (ii) We analyze the asymptotic properties of Bayesian inference and show that in the underidentified case, the asymptotic posterior distribution of contemporaneous coefficients in an n-variable VAR is confined to the set of values that orthogonalize the population variance-covariance matrix of ordinary least squares residuals, with the height of the posterior proportional to the height of the prior at any point within that set. For example, in a bivariate VAR for supply and demand identified solely by sign restrictions, if the population correlation between the VAR residuals is positive, then even if one has available an infinite sample of data, any inference about the demand elasticity is coming exclusively from the prior distribution. (iii) We provide analytical characterizations of the informative prior distributions for impulse-response functions that are implicit in the traditional sign-restriction approach to VARs, and we note, as a special case of result (ii), that the influence of these priors does not vanish asymptotically. (iv) We illustrate how Bayesian inference with informative priors can be both a strict generalization and an unambiguous improvement over frequentist inference in just-identified models. (v) We propose that researchers need to explicitly acknowledge and defend the role of prior beliefs in influencing structural conclusions and we illustrate how this could be done using a simple model of the U.S. labor market.</p> </abstract>
<abstract> <p>We propose a method to set identify bounds on the sharing rule for a general collective household consumption model. Unlike the effects of distribution factors, the level of the sharing rule cannot be uniquely identified without strong assumptions on preferences across households. Our new results show that, though not point identified without these assumptions, strong bounds on the sharing rule can be obtained. We get these bounds by applying revealed preference restrictions implied by the collective model to the household's continuous aggregate demand functions. We obtain informative bounds even if nothing is known about whether each good is public, private, or assignable within the household, though having such information tightens the bounds. We apply our method to US PSID data, obtaining narrow bounds that yield useful conclusions regarding the effects of income and wages on intrahousehold resource sharing, and on the prevalence of individual (as opposed to household level) poverty.</p> </abstract>
<abstract> <p>This paper develops a specification test for instrument validity in the heterogeneous treatment effect model with a binary treatment and a discrete instrument. The strongest testable implication for instrument validity is given by the condition for nonnegativity of point-identifiable compilers' outcome densities. Our specification test infers this testable implication using a variance-weighted Kolmogorov-Smirnov test statistic. The test can be applied to both discrete and continuous outcome cases, and an extension of the test to settings with conditioning covariates is provided.</p> </abstract>
<abstract> <p>Strategic choice data from a carefully chosen set of ring-network games are used to obtain individual-level estimates of higher-order rationality. The experimental design exploits a natural exclusion restriction that is considerably weaker than the assumptions underlying alternative designs in the literature. In our data set, 93 percent of subjects are rational, 71 percent are rational and believe others are rational, 44 percent are rational and hold second-order beliefs that others are rational, and 22 percent are rational and hold at least third-order beliefs that others are rational.</p> </abstract>
<abstract> <p>Both aristocratic privileges and constitutional constraints in traditional monarchies can be derived from a ruler's incentive to minimize expected costs of moral-hazard rents for high officials. We consider a dynamic moral-hazard model of governors serving a sovereign prince, who must deter them from rebellion and hidden corruption which could cause costly crises. To minimize costs, a governor's rewards for good performance should be deferred up to the maximal credit that the prince can be trusted to pay. In the long run, we find that high officials can become an entrenched aristocracy with low turnover and large claims on the ruler. Dismissals for bad performance should be randomized to avoid inciting rebellions, but the prince can profit from reselling vacant offices, and so his decisions to dismiss high officials require institutionalized monitoring. A soft budget constraint that forgives losses for low-credit governors can become efficient when costs of corruption are low.</p> </abstract>
<abstract> <p>This paper develops a quantitative model of internal city structure that features agglomeration and dispersion forces and an arbitrary number of heterogeneous city blocks. The model remains tractable and amenable to empirical analysis because of stochastic shocks to commuting decisions, which yield a gravity equation for commuting flows. To structurally estimate agglomeration and dispersion forces, we use data on thousands of city blocks in Berlin for 1936, 1986, and 2006 and exogenous variation from the city's division and reunification. We estimate substantial and highly localized production and residential externalities. We show that the model with the estimated agglomeration parameters can account both qualitatively and quantitatively for the observed changes in city structure. We show how our quantitative framework can be used to undertake counterfactuals for changes in the organization of economic activity within cities in response, for example, to changes in the transport network.</p> </abstract>
<abstract> <p>Our paper provides a complete characterization of leverage and default in binomial economies with financial assets serving as collateral. Our Binomial No-Default Theorem states that any equilibrium is equivalent (in real allocations and prices) to another equilibrium in which there is no default. Thus actual default is irrelevant, though the potential for default drives the equilibrium and limits borrowing. This result is valid with arbitrary preferences and endowments, contingent or noncontingent promises, many assets and consumption goods, production, and multiple periods. We also show that only no-default equilibria would be selected if there were the slightest cost of using collateral or handling default. Our Binomial Leverage Theorem shows that equilibrium Loan to Value (LTV) for noncontingent debt contracts is the ratio of the worst-case return of the asset to the riskless gross rate of interest. In binomial economies, leverage is determined by down risk and not by volatility.</p> </abstract>
<abstract> <p>We develop a parsimonious model to study the equilibrium and socially optimal decisions of banks to enter, trade in, and possibly exit, an OTC market. Although we endow all banks with the same trading technology, banks' optimal entry and trading decisions endogenously lead to a realistic market structure composed of dealers and customers with distinct trading patterns. We decompose banks' entry incentives into incentives to hedge risk and incentives to make intermediation profits. We show that dealer banks enter more than is socially optimal. In the face of large negative shocks, they may also exit more than is socially optimal when markets are not perfectly resilient.</p> </abstract>
<abstract> <p>This paper develops a generalized Roy model with human capital accumulation, moral hazard, and career concerns. We identify and estimate the model with a large panel that matches data on publicly listed firms to information on their executives. The structural estimates obtained are used to decompose the firm-size pay gap. We find that although total compensation and incentive pay increase with firm size, certaintyequivalent pay decreases with firm size. In larger firms, and for more highly ranked executives, weaker signal quality about effort results in higher risk premiums. This risk premium accounts for roughly 80 percent of the firm-size gap in total compensation. Larger firms are also willing to pay more than smaller ones to attract executives. Finally, the estimated coefficients on human capital accumulation from formal education and experience gained from different firms are individually significant, but their collective effect on firm-size pay differentials nets out.</p> </abstract>
<abstract> <p>Perturbed utility functions—the sum of expected utility and a nonlinear perturbation function—provide a simple and tractable way to model various sorts of stochastic choice. We provide two easily understood conditions each of which characterizes this representation: One condition generalizes the acyclicity condition used in revealed preference theory, and the other generalizes Luce's IIA condition. We relate the discrimination or selectivity of choice rules to properties of their associated perturbations, both across different agents and across decision problems. We also show that these representations correspond to a form of ambiguity-averse preferences for an agent who is uncertain about her true utility.</p> </abstract>
<abstract> <p>This paper examines some of the recent literature on the estimation of production functions. We focus on techniques suggested in two recent papers, Olley and Pakes (1996) and Levinsohn and Petrin (2003). While there are some solid and intuitive identification ideas in these papers, we argue that the techniques can suffer from functional dependence problems. We suggest an alternative approach that is based on the ideas in these papers, but does not suffer from the functional dependence problems and produces consistent estimates under alternative data generating processes for which the original procedures do not.</p> </abstract>
<abstract> <p>We consider nonparametric identification and estimation in a nonseparable model where a continuous regressor of interest is a known, deterministic, but kinked function of an observed assignment variable. We characterize a broad class of models in which a sharp "Regression Kink Design" (RKD or RK Design) identifies a readily interpretable treatment-on-the-treated parameter (Florens, Heckman, Meghir, and Vytlaèil (2008)). We also introduce a "fuzzy regression kink design" generalization that allows for omitted variables in the assignment rule, noncompliance, and certain types of measurement errors in the observed values of the assignment variable and the policy variable. Our identifying assumptions give rise to testable restrictions on the distributions of the assignment variable and predetermined covariates around the kink point, similar to the restrictions delivered by Lee (2008) for the regression discontinuity design. Using a kink in the unemployment benefit formula, we apply a fuzzy RKD to empirically estimate the effect of benefit rates on unemployment durations in Austria.</p> </abstract>
<abstract> <p>We demonstrate the asymptotic equivalence between commonly used test statistics for out-of-sample forecasting performance and conventional Wald statistics. This equivalence greatly simplifies the computational burden of calculating recursive outof-sample test statistics and their critical values. For the case with nested models, we show that the limit distribution, which has previously been expressed through stochastic integrals, has a simple representation in terms of X²-distributed random variables and we derive its density. We also generalize the limit theory to cover local alternatives and characterize the power properties of the test.</p> </abstract>
<abstract> <p>We estimate demand for residential broadband using high-frequency data from subscribers facing a three-part tariff. The three-part tariff makes data usage during the billing cycle a dynamic problem, thus generating variation in the (shadow) price of usage. We provide evidence that subscribers respond to this variation, and we use their dynamic decisions to estimate a flexible distribution of willingness to pay for different plan characteristics. Using the estimates, we simulate demand under alternative pricing and find that usage-based pricing eliminates low-value traffic. Furthermore, we show that the costs associated with investment in fiber-optic networks are likely recoverable in some markets, but that there is a large gap between social and private incentives to invest.</p> </abstract>
<abstract> <p>This paper examines how prices, markups, and marginal costs respond to trade liberalization. We develop a framework to estimate markups from production data with multi-product firms. This approach does not require assumptions on the market structure or demand curves faced by firms, nor assumptions on how firms allocate their inputs across products. We exploit quantity and price information to disentangle markups from quantity-based productivity, and then compute marginal costs by dividing observed prices by the estimated markups. We use India's trade liberalization episode to examine how firms adjust these performance measures. Not surprisingly, we find that trade liberalization lowers factory-gate prices and that output tariff declines have the expected pro-competitive effects. However, the price declines are small relative to the declines in marginal costs, which fall predominantly because of the input tariff liberalization. The reason for this incomplete cost pass-through to prices is that firms offset their reductions in marginal costs by raising markups. Our results demonstrate substantial heterogeneity and variability in markups across firms and time and suggest that producers benefited relative to consumers, at least immediately after the reforms.</p> </abstract>
<abstract> <p>The U.S. Prohibition experience shows a remarkable policy reversal. In only 14 years, a drastic shift in public opinion required two constitutional amendments. I develop and estimate a model of endogenous law enforcement, determined by beliefs about the Prohibition-crime nexus and alcohol-related moral views. In turn, the policy outcomes shape subsequent learning about Prohibition enforcement costs. I estimate the model through maximum likelihood on Prohibition Era city-level data on police enforcement, crime, and alcohol-related legislation. The model can account for the variation in public opinion changes, and the heterogeneous responses of law enforcement and violence across cities. Results show that a 15% increase in the homicide rate can be attributed to Prohibition enforcement. The subsequent learning-driven adjustment of local law enforcement allowed for the alcohol market to rebound to 60% of its pre-Prohibition size. I conclude with counterfactual exercises exploring the welfare implications of policy learning, prior beliefs, preference polarization, and alternative political environments. Results illustrate the importance of incorporating the endogenous nature of law enforcement into our understanding of policy failure and policy success.</p> </abstract>
<abstract> <p>The question of whether and how mutual fund managers provide valuable services for their clients motivates one of the largest literatures in finance. One candidate explanation is that funds process information about future asset values and use that information to invest in high-valued assets. But formal theories are scarce because information choice models with many assets are difficult to solve as well as difficult to test. This paper tackles both problems by developing a new attention allocation model that uses the state of the business cycle to predict information choices, which in turn, predict observable patterns of portfolio investments and returns. The predictions about fund portfolios' covariance with payoff shocks, cross-fund portfolio and return dispersion, and their excess returns are all supported by the data. These findings offer new evidence that some investment managers have skill and that attention is allocated rationally.</p> </abstract>
<abstract> <p>We study how long it takes for large populations of interacting agents to come close to Nash equilibrium when they adapt their behavior using a stochastic better reply dynamic. Prior work considers this question mainly for 2 × 2 games and potential games; here we characterize convergence times for general weakly acyclic games, including coordination games, dominance solvable games, games with strategic complementarities, potential games, and many others with applications in economics, biology, and distributed control. If players' better replies are governed by idiosyncratic shocks, the convergence time can grow exponentially in the population size; moreover, this is true even in games with very simple payoff structures. However, if their responses are sufficiently correlated due to aggregate shocks, the convergence time is greatly accelerated; in fact, it is bounded for all sufficiently large populations. We provide explicit bounds on the speed of convergence as a function of key structural parameters including the number of strategies, the length of the better reply paths, the extent to which players can influence the payoffs of others, and the desired degree of approximation to Nash equilibrium.</p> </abstract>
<abstract> <p>This paper studies how the abolition of an elite recruitment system—China's civil exam system that lasted over 1,300 years—affects political stability. Employing a panel data set across 262 prefectures and exploring the variations in the quotas on the entrylevel exam candidates, we find that higher quotas per capita were associated with a higher probability of revolution participation after the abolition and a higher incidence of uprisings in 1911 that marked the end of the 2,000 years of imperial rule. This finding is robust to various checks including using the number of small rivers and short-run exam performance before the quota system as instruments. The patterns in the data appear most consistent with the interpretation that in regions with higher quotas per capita under the exam system, more would-be elites were negatively affected by the abolition. In addition, we document that modern human capital in the form of those studying in Japan also contributed to the revolution and that social capital strengthened the effect of quotas on revolution participation.</p> </abstract>
<abstract> <p>We show that firms' individually optimal liquidity management results in socially inefficient boom-and-bust patterns. Financially constrained firms decide on the level of their liquid resources facing cash-flow shocks and time-varying investment opportunities. Firms' liquidity management decisions generate simultaneous waves in aggregate cash holdings and investment, even if technology remains constant. These investment waves are not constrained efficient in general, because the social and private value of liquidity differs. The resulting pecuniary externality affects incentives differentially depending on the state of the economy, and often overinvestment occurs during booms and underinvestment occurs during recessions. In general, policies intended to mitigate underinvestment raise prices during recessions, making overinvestment during booms worse. However, a well-designed price-support policy will increase welfare in both booms and recessions.</p> </abstract>
<abstract> <p>Propensity score matching estimators (Rosenbaum and Rubin (1983)) are widely used in evaluation research to estimate average treatment effects. In this article, we derive the large sample distribution of propensity score matching estimators. Our derivations take into account that the propensity score is itself estimated in a first step, prior to matching. We prove that first step estimation of the propensity score affects the large sample distribution of propensity score matching estimators, and derive adjustments to the large sample variances of propensity score matching estimators of the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The adjustment for the ATE estimator is negative (or zero in some special cases), implying that matching on the estimated propensity score is more efficient than matching on the true propensity score in large samples. However, for the ATET estimator, the sign of the adjustment term depends on the data generating process, and ignoring the estimation error in the propensity score may lead to confidence intervals that are either too large or too small.</p> </abstract>
<abstract> <p>We present a methodology for estimating the distributional effects of an endogenous treatment that varies at the group level when there are group-level unobservables, a quantile extension of Hausman and Taylor (1981). Because of the presence of grouplevel unobservables, standard quantile regression techniques are inconsistent in our setting even if the treatment is independent of unobservables. In contrast, our estimation technique is consistent as well as computationally simple, consisting of group-by-group quantile regression followed by two-stage least squares. Using the Bahadur representation of quantile estimators, we derive weak conditions on the growth of the number of observations per group that are sufficient for consistency and asymptotic zero-mean normality of our estimator. As in Hausman and Taylor (1981), micro-level covariates can be used as internal instruments for the endogenous group-level treatment if they satisfy relevance and exogeneity conditions. Our approach applies to a broad range of settings including labor, public finance, industrial organization, urban economics, and development; we illustrate its usefulness with several such examples. Finally, an empirical application of our estimator finds that low-wage earners in the United States from 1990 to 2007 were significantly more affected by increased Chinese import competition than high-wage earners.</p> </abstract>
<abstract> <p>We consider contests with many, possibly heterogeneous, players and prizes, and show that the equilibrium outcomes of such contests are approximated by the outcomes of mechanisms that implement the assortative allocation in an environment with a single agent that has a continuum of possible types. This makes it possible to easily approximate the equilibria of contests whose exact equilibrium characterization is complicated, as well as the equilibria of contests for which there is no existing equilibrium characterization.</p> </abstract>
<abstract> <p>We analyze the implications of household-level adjustment costs for the dynamics of aggregate consumption. We show that an economy in which agents have "consumption commitments" is approximately equivalent to a habit formation model in which the habit stock is a weighted average of past consumption if idiosyncratic risk is large relative to aggregate risk. Consumption commitments can thus explain the empirical regularity that consumption is excessively sensitive and excessively smooth, findings that are typically attributed to habit formation. Unlike habit formation and other theories, but consistent with empirical evidence, the consumption commitments model also predicts that excess sensitivity and smoothness vanish for large shocks. These results suggest that behavior previously attributed to habit formation may be better explained by adjustment costs. We develop additional testable predictions to further distinguish the commitment and habit models and show that the two models have different welfare implications.</p> </abstract>
<abstract> <p>This paper develops a dynamic model of neighborhood choice along with a computationally light multi-step estimator. The proposed empirical framework captures observed and unobserved preference heterogeneity across households and locations in a flexible way. We estimate the model using a newly assembled data set that matches demographic information from mortgage applications to the universe of housing transactions in the San Francisco Bay Area from 1994 to 2004. The results provide the first estimates of the marginal willingness to pay for several non-marketed amenities—neighborhood air pollution, violent crime, and racial composition—in a dynamic framework. Comparing these estimates with those from a static version of the model highlights several important biases that arise when dynamic considerations are ignored.</p> </abstract>
<abstract> <p>An endogenous growth model is developed where each period firms invest in researching and developing new ideas. An idea increases a firm's productivity. By how much depends on the technological propinquity between an idea and the firm's line of business. Ideas can be bought and sold on a market for patents. A firm can sell an idea that is not relevant to its business or buy one if it fails to innovate. The developed model is matched up with stylized facts about the market for patents in the United States. The analysis gauges how efficiency in the patent market affects growth.</p> </abstract>
<abstract> <p>We develop an econometric methodology to infer the path of risk premia from a large unbalanced panel of individual stock returns. We estimate the time-varying risk premia implied by conditional linear asset pricing models where the conditioning includes both instruments common to all assets and asset-specific instruments. The estimator uses simple weighted two-pass cross-sectional regressions, and we show its consistency and asymptotic normality under increasing cross-sectional and time series dimensions. We address consistent estimation of the asymptotic variance by hard thresholding, and testing for asset pricing restrictions induced by the no-arbitrage assumption. We derive the restrictions given by a continuum of assets in a multi-period economy under an approximate factor structure robust to asset repackaging. The empirical analysis on returns for about ten thousand U.S. stocks from July 1964 to December 2009 shows that risk premia are large and volatile in crisis periods. They exhibit large positive and negative strays from time-invariant estimates, follow the macroeconomic cycles, and do not match risk premia estimates on standard sets of portfolios. The asset pricing restrictions are rejected for a conditional four-factor model capturing market, size, value, and momentum effects.</p> </abstract>
<abstract> <p>We test for the existence of housing bubbles associated with a failure of the transversality condition that requires the present value of payments occurring infinitely far in the future to be zero. The most prominent such bubble is the classic rational bubble. We study housing markets in the United Kingdom and Singapore, where residential property ownership takes the form of either leaseholds or freeholds. Leaseholds are finite-maturity, pre-paid, and tradeable ownership contracts with maturities often exceeding 700 years. Freeholds are infinite-maturity ownership contracts. The price difference between leaseholds with extremely-long maturities and freeholds reflects the present value of a claim to the freehold after leasehold expiry, and is thus a direct empirical measure of the transversality condition. We estimate this price difference, and find no evidence of failures of the transversality condition in housing markets in the U.K. and Singapore, even during periods when a sizable bubble was regularly thought to be present.</p> </abstract>
<abstract> <p>We develop an equilibrium framework that relaxes the standard assumption that people have a correctly specified view of their environment. Each player is characterized by a (possibly misspecified) subjective model, which describes the set of feasible beliefs over payoff-relevant consequences as a function of actions. We introduce the notion of a Berk–Nash equilibrium: Each player follows a strategy that is optimal given her belief, and her belief is restricted to be the best fit among the set of beliefs she considers possible. The notion of best fit is formalized in terms of minimizing the Kullback–Leibler divergence, which is endogenous and depends on the equilibrium strategy profile. Standard solution concepts such as Nash equilibrium and self-confirming equilibrium constitute special cases where players have correctly specified models. We provide a learning foundation for Berk–Nash equilibrium by extending and combining results from the statistics literature on misspecified learning and the economics literature on learning in games.</p> </abstract>
<abstract> <p>I highlight how reputational concerns provide a natural explanation for "deadline effects," the high frequency of deals prior to a deadline in bargaining. Rational agents imitate the demands of obstinate behavioral types and engage in brinkmanship in the face of uncertainty about the deadline's arrival. I also identify how surplus is divided when the prior probability of behavioral types is vanishingly small. If behavioral types are committed to fixed demands, outcomes converge to the Nash bargaining solution regardless of agents' respective impatience. If behavioral types can adopt more complex demand strategies, outcomes converge to the solution of an alternating offers game without behavioral types for the deadline environment.</p> </abstract>
<abstract> <p>This paper proposes a method for aggregating individual preferences in the context of uncertainty. Individuals are assumed to abide by Savage's model of Subjective Expected Utility, in which everyone has his/her own utility and subjective probability. Disagreement on probabilities among individuals gives rise to uncertainty at the societal level, and thus society may entertain a set of probabilities rather than only one. We assume that social preference admits a Maxmin Expected Utility representation. In this context, two Pareto-type conditions are shown to be equivalent to social utility being a weighted average of individual utilities and the social set of priors containing only weighted averages of individual priors. Thus, society respects consensus among individuals' beliefs and does not add ambiguity beyond disagreement on beliefs. We also deal with the case in which society does not rule out any individual belief.</p> </abstract>
<abstract> <p>We examine the role of stochastic feasibility in consumer choice using a random conditional choice set rule (RCCSR) and uniquely characterize the model from conditions on stochastic choice data. Feasibility is modeled to permit correlation in availability of alternatives. This provides a natural way to examine substitutability/complementarity. We show that an RCCSR generalizes the random consideration set rule of Manzini and Mariotti (2014). We then relate this model to existing literature. In particular, an RCCSR is not a random utility model.</p> </abstract>
<abstract> <p>Individual heterogeneity is an important source of variation in demand. Allowing for general heterogeneity is needed for correct welfare comparisons. We consider general heterogeneous demand where preferences and linear budget sets are statistically independent. Only the marginal distribution of demand for each price and income is identified from cross-section data where only one price and income is observed for each individual. Thus, objects that depend on varying price and/or income for an individual are not generally identified, including average exact consumer surplus. We use bounds on income effects to derive relatively simple bounds on the average surplus, including for discrete/continuous choice. We also sketch an approach to bounding surplus that does not use income effect bounds. We apply the results to gasoline demand. We find tight bounds for average surplus in this application, but wider bounds for average deadweight loss.</p> </abstract>
<abstract> <p>Conventional tests for composite hypotheses in minimum distance models can be unreliable when the relationship between the structural and reduced-form parameters is highly nonlinear. Such nonlinearity may arise for a variety of reasons, including weak identification. In this note, we begin by studying the problem of testing a "curved null" in a finite-sample Gaussian model. Using the curvature of the model, we develop new finite-sample bounds on the distribution of minimum-distance statistics. These bounds allow us to construct tests for composite hypotheses which are uniformly asymptotically valid over a large class of data generating processes and structural models.</p> </abstract>
<abstract> <p>Life insurers use reinsurance to move liabilities from regulated and rated companies that sell policies to shadow reinsurers, which are less regulated and unrated offbalance-sheet entities within the same insurance group. U.S. life insurance and annuity liabilities ceded to shadow reinsurers grew from $11 billion in 2002 to $364 billion in 2012. Life insurers using shadow insurance, which capture half of the market share, ceded 25 cents of every dollar insured to shadow reinsurers in 2012, up from 2 cents in 2002. By relaxing capital requirements, shadow insurance could reduce the marginal cost of issuing policies and thereby improve retail market efficiency. However, shadow insurance could also reduce risk-based capital and increase expected loss for the industry. We model and quantify these effects based on publicly available data and plausible assumptions.</p> </abstract>
<abstract> <p>In a number of interesting environments, dynamic screening involves positive selection: in contrast with Coasian dynamics, only the most motivated remain over time. The paper provides conditions under which the principal's commitment optimum is time consistent and uses this result to derive testable predictions under permanent or transient shocks. It also identifies environments in which time consistency does not hold despite positive selection, and yet simple equilibrium characterizations can be obtained.</p> </abstract>
<abstract> <p>Using the intuition that financial markets transfer risks in business time, "market microstructure invariance" is defined as the hypotheses that the distributions of risk transfers ("bets") and transaction costs are constant across assets when measured per unit of business time. The invariance hypotheses imply that bet size and transaction costs have specific, empirically testable relationships to observable dollar volume and volatility. Portfolio transitions can be viewed as natural experiments for measuring transaction costs, and individual orders can be treated as proxies for bets. Empirical tests based on a data set of 400,000+ portfolio transition orders support the invariance hypotheses. The constants calibrated from structural estimation imply specific predictions for the arrival rate of bets ("market velocity"), the distribution of bet sizes, and transaction costs.</p> </abstract>
<abstract> <p>We study a continuous-time contracting problem under hidden action, where the principal has ambiguous beliefs about the project cash flows. The principal designs a robust contract that maximizes his utility under the worst-case scenario subject to the agent's incentive and participation constraints. Robustness generates endogenous belief heterogeneity and induces a tradeoff between incentives and ambiguity sharing so that the incentive constraint does not always bind. We implement the optimal contract by cash reserves, debt, and equity. In addition to receiving ordinary dividends when cash reserves reach a threshold, outside equity holders also receive special dividends or inject cash in the cash reserves to hedge against model uncertainty and smooth dividends. The equity premium and the credit yield spread generated by ambiguity aversion are state dependent and high for distressed firms with low cash reserves.</p> </abstract>
<abstract> <p>We extend Kyle's (1985) model of insider trading to the case where noise trading volatility follows a general stochastic process. We determine conditions under which, in equilibrium, price impact and price volatility are both stochastic, driven by shocks to uninformed volume even though the fundamental value is constant. The volatility of price volatility appears 'excessive' because insiders choose to trade more aggressively (and thus more information is revealed) when uninformed volume is higher and price impact is lower. This generates a positive relation between price volatility and trading volume, giving rise to an endogenous subordinate stochastic process for prices.</p> </abstract>
<abstract> <p>What is the role of a country's financial system in determining technology adoption? To examine this, a dynamic contract model is embedded into a general equilibrium setting with competitive intermediation. The terms of finance are dictated by an intermediary's ability to monitor and control a firm's cash flow, in conjunction with the structure of the technology that the firm adopts. It is not always profitable to finance promising technologies. A quantitative illustration is presented where financial frictions induce entrepreneurs in India and Mexico to adopt less-promising ventures than in the United States, despite lower input prices.</p> </abstract>
<abstract> <p>We develop and estimate a general equilibrium search and matching model that accounts for key business cycle properties of macroeconomic aggregates, including labor market variables. In sharp contrast to leading New Keynesian models, we do not impose wage inertia. Instead we derive wage inertia from our specification of how firms and workers negotiate wages. Our model outperforms a variant of the standard New Keynesian Calvo sticky wage model. According to our estimated model, there is a critical interaction between the degree of price stickiness, monetary policy, and the duration of an increase in unemployment benefits.</p> </abstract>
<abstract> <p>This paper shows that the problem of testing hypotheses in moment condition models without any assumptions about identification may be considered as a problem of testing with an infinite-dimensional nuisance parameter. We introduce a sufficient statistic for this nuisance parameter in a Gaussian problem and propose conditional tests. These conditional tests have uniformly correct asymptotic size for a large class of models and test statistics. We apply our approach to construct tests based on quasilikelihood ratio statistics, which we show are efficient in strongly identified models and perform well relative to existing alternatives in two examples.</p> </abstract>
<abstract> <p>We propose a semiparametric two-step inference procedure for a finite-dimensional parameter based on moment conditions constructed from high-frequency data. The population moment conditions take the form of temporally integrated functionals of state-variable processes that include the latent stochastic volatility process of an asset. In the first step, we nonparametrically recover the volatility path from high-frequency asset returns. The nonparametric volatility estimator is then used to form sample moment functions in the second-step GMM estimation, which requires the correction of a high-order nonlinearity bias from the first step. We show that the proposed estimator is consistent and asymptotically mixed Gaussian and propose a consistent estimator for the conditional asymptotic variance. We also construct a Bierens-type consistent specification test. These infill asymptotic results are based on a novel empirical-process-type theory for general integrated functionals of noisy semimartingale processes.</p> </abstract>
<abstract> <p>We analyze money and credit as competing payment instruments in decentralized exchange. In natural environments, we show the economy does not need both: if credit is easy, money is irrelevant; if credit is tight, money is essential, but credit becomes irrelevant. Changes in credit conditions are neutral because real balances respond endogenously to keep total liquidity constant. This is true for both exogenous and endogenous debt limits and policy limits, secured and unsecured lending, and general pricing mechanisms. While we show how to overturn some of these results, the benchmark model suggests credit might matter less than people think.</p> </abstract>
<abstract> <p>This paper studies competitive equilibria of economies where assets are heterogeneous and traders have heterogeneous information about them. Markets are defined by a price and a procedure for clearing trades, and any asset can, in principle, be traded in any market. Buyers can use their information to impose acceptance rules which specify which assets they are willing to trade in each market. The set of markets where trade takes place is derived endogenously. The model can be applied to find conditions under which these economies feature fire sales, contagion, and flights to quality.</p> </abstract>
<abstract> <p>We study takeovers of firms whose ownership structure is a mixture of minority block-holders and small shareholders. We show that the combination of dispersed private information on the side of small shareholders and the presence of a large shareholder can facilitate profitable takeovers. Furthermore, our analysis implies that even if some model of takeovers predicts a profit for the raider, for example, due to private benefits, the profit will be underestimated unless the large shareholder and the dispersion of information among the small shareholders are modeled.</p> </abstract>
<abstract> <p>Most countries have automatic rules in their tax-and-transfer systems that are partly intended to stabilize economic fluctuations. This paper measures their effect on the dynamics of the business cycle. We put forward a model that merges the standard incomplete-markets model of consumption and inequality with the new Keynesian model of nominal rigidities and business cycles, and that includes most of the main potential stabilizers in the U.S. data and the theoretical channels by which they may work. We find that the conventional argument that stabilizing disposable income will stabilize aggregate demand plays a negligible role in the dynamics of the business cycle, whereas tax-and-transfer programs that affect inequality and social insurance can have a larger effect on aggregate volatility. However, as currently designed, the set of stabilizers in place in the United States has had little effect on the volatility of aggregate output fluctuations or on their welfare costs despite stabilizing aggregate consumption. The stabilizers have a more important role when monetary policy is constrained by the zero lower bound, and they affect welfare significantly through the provision of social insurance.</p> </abstract>
<abstract> <p>We provide a theoretical and empirical analysis of the link between financial and real health care markets. This link is important as financial returns drive investment in medical research and development (R&amp;D), which, in turn, affects real spending growth. We document a "medical innovation premium" of 4-6% annually for equity returns of firms in the health care sector. We interpret this premium as compensating investors for government-induced profit risk, and we provide supportive evidence for this hypothesis through company filings and abnormal return patterns surrounding threats of government intervention. We quantify the implications of the premium for the growth in real health care spending by calibrating our model to match historical trends, predicting the share of gross domestic product (GDP) devoted to health care to be 32% in the long run. Policies that had removed government risk would have led to more than a doubling of medical R&amp;D and would have increased the current share of health care spending by more than 3% of GDP.</p> </abstract>
<abstract> <p>This paper analyzes a sequential search model with adverse selection. We study information aggregation by the price—how close the equilibrium prices are to the fullinformation prices—when search frictions are small. We identify circumstances under which prices fail to aggregate information well even when search frictions are small. We trace this to a strong form of the winner's curse that is present in the sequential search model. The failure of information aggregation may result in inefficient allocations.</p> </abstract>
<abstract> <p>This paper investigates relational incentive contracts with continuous, privately observed agent types that are persistent over time. With fixed agent types, full separation is not possible when continuation equilibrium payoffs following revelation are on the Pareto frontier of attainable payoffs. This result is related to the ratchet effect in that: (1) a type imitating a less productive type receives an information rent, and (2) with full separation, one imitating a more productive type receives the same future payoff as that more productive type. However, the reason for (2) is fundamentally different than with the ratchet effect. It arises from the dynamic enforcement requirement in relational contracts, not from the principal having all the bargaining power, and applies whatever the distribution between principal and agent of the future gains from the relationship (i.e., whatever the point on the Pareto frontier). This result extends to sufficiently persistent types under certain conditions.</p> </abstract>
<abstract> <p>This paper considers equilibrium quit turnover in a frictional labor market with costly hiring by firms, where large firms employ many workers and face both aggregate and firm specific productivity shocks. There is exogenous firm turnover as new (small) startups enter the market over time, while some existing firms fail and exit. Individual firm growth rates are disperse and evolve stochastically. The paper highlights how dynamic monopsony, where firms trade off lower wages against higher (endogenous) employee quit rates, yields excessive job-to-job quits. Such quits directly crowd out the reemployment prospects of the unemployed. With finite firm productivity states, stochastic equilibrium is fully tractable and can be computed using standard numerical techniques.</p> </abstract>
<abstract> <p>We revisit the comparison of mathematical programming with equilibrium constraints (MPEC) and nested fixed point (NFXP) algorithms for estimating structural dynamic models by Su and Judd (2012). Their implementation of the nested fixed point algorithm used successive approximations to solve the inner fixed point problem (NFXP-SA). We redo their comparison using the more efficient version of NFXP proposed by Rust (1987), which combines successive approximations and Newton-Kantorovich iterations to solve the fixed point problem (NFXP-NK). We show that MPEC and NFXP are similar in speed and numerical performance when the more efficient NFXP-NK variant is used.</p> </abstract>
<abstract> <p>We propose a theory of monetary policy and macroprudential interventions in financial markets. We focus on economies with nominal rigidities in goods and labor markets and subject to constraints on monetary policy, such as the zero lower bound or fixed exchange rates. We identify an aggregate demand externality that can be corrected by macroprudential interventions in financial markets. Ex post, the distribution of wealth across agents affects aggregate demand and output. Ex ante, however, these effects are not internalized in private financial decisions. We provide a simple formula for the required financial interventions that depends on a small number of measurable sufficient statistics. We also characterize optimal monetary policy. We extend our framework to incorporate pecuniary externalities, providing a unified approach to both externalities. Finally, we provide a number of applications which illustrate the relevance of our theory.</p> </abstract>
<abstract> <p>We estimate a dynamic model of employment, human capital accumulation—including education, and savings for women in the United Kingdom, exploiting tax and benefit reforms, and use it to analyze the effects of welfare policy. We find substantial elasticities for labor supply and particularly for lone mothers. Returns to experience, which are important in determining the longer-term effects of policy, increase with education, but experience mainly accumulates when in full-time employment. Tax credits are welfare improving in the U.K., increase lone-mother labor supply and marginally reduce educational attainment, but the employment effects do not extend beyond the period of eligibility. Marginal increases in tax credits improve welfare more than equally costly increases in income support or tax cuts.</p> </abstract>
<abstract> <p>I estimate a search-and-bargaining model of a decentralized market to quantify the effects of trading frictions on asset allocations, asset prices, and welfare, and to quantify the effects of intermediaries that facilitate trade. Using business-aircraft data, I find that, relative to the Walrasian benchmark, 18.3 percent of the assets are misallocated; prices are 19.2 percent lower; and the aggregate welfare losses equal 23.9 percent. Dealers play an important role in reducing trading frictions: In a market with no dealers, a larger fraction of assets would be misallocated, and prices would be higher. However, dealers reduce aggregate welfare because their operations are costly, and they impose a negative externality by decreasing the number of agents' direct transactions.</p> </abstract>
<abstract> <p>Call an economic model incomplete if it does not generate a probabilistic prediction even given knowledge of all parameter values. We propose a method of inference about unknown parameters for such models that is robust to heterogeneity and dependence of unknown form. The key is a Central Limit Theorem for belief functions; robust confidence regions are then constructed in a fashion paralleling the classical approach. Monte Carlo simulations support tractability of the method and demonstrate its enhanced robustness relative to existing methods.</p> </abstract>
<abstract> <p>Two fundamental axioms in social choice theory are consistency with respect to a variable electorate and consistency with respect to components of similar alternatives. In the context of traditional non-probabilistic social choice, these axioms are incompatible with each other. We show that in the context of probabilistic social choice, these axioms uniquely characterize a function proposed by Fishburn (1984). Fishburn's function returns so-called maximal lotteries, that is, lotteries that correspond to optimal mixed strategies in the symmetric zero-sum game induced by the pairwise majority margins. Maximal lotteries are guaranteed to exist due to von Neumann's Minimax Theorem, are almost always unique, and can be efficiently computed using linear programming.</p> </abstract>
<abstract> <p>We study families of normal-form games with fixed preferences over pure action profiles but varied preferences over lotteries. That is, we subject players' utilities to monotone but nonlinear transformations and examine changes in the rationalizable set and set of equilibria. Among our results: The rationalizable set always grows under concave transformations (risk aversion) and shrinks under convex transformations (risk love). The rationalizable set reaches an upper bound under extreme risk aversion, and lower bound under risk love, and both of these bounds are characterized by elimination processes. For generic two-player games, under extreme risk love or aversion, all Nash equilibria are close to pure and the limiting set of equilibria can be described using preferences over pure action profiles.</p> </abstract>
<abstract> <p>We consider a decision maker who ranks actions according to the smooth ambiguity criterion of Klibanoff, Marinacci, and Mukerji (2005). An action is justifiable if it is a best reply to some belief over probabilistic models. We show that higher ambiguity aversion expands the set of justifiable actions. A similar result holds for risk aversion. Our results follow from a generalization of the duality lemma of Wald (1949) and Pearce (1984).</p> </abstract>
<abstract> <p>The farm household model has played a central role in improving the understanding of small-scale agricultural households and non-farm enterprises. Under the assumptions that all current and future markets exist and that farmers treat all prices as given, the model simplifies households' simultaneous production and consumption decisions into a recursive form in which production can be treated as independent of preferences of household members. These assumptions, which are the foundation of a large literature in labor and development, have been tested and not rejected in several important studies including Benjamin (1992). Using multiple waves of longitudinal survey data from Central Java, Indonesia, this paper tests a key prediction of the recursive model: demand for farm labor is unrelated to the demographic composition of the farm household. The prediction is unambiguously rejected. The rejection cannot be explained by contamination due to unobserved heterogeneity that is fixed at the farm level, local area shocks, or farm-specific shocks that affect changes in household composition and farm labor demand. We conclude that the recursive form of the farm household model is not consistent with the data. Developing empirically tractable models of farm households when markets are incomplete remains an important challenge.</p> </abstract>
<abstract> <p>IO economists often estimate demand for differentiated products using data sets with a small number of large markets. This paper addresses the question of consistency and asymptotic distributions of instrumental variables estimates as the number of products increases in some commonly used models of demand under conditions on economic primitives. I show that, in a Bertrand-Nash equilibrium, product characteristics lose their identifying power as price instruments in the limit in certain cases, leading to inconsistent estimates. The reason is that product characteristic instruments achieve identification through correlation with markups, and, depending on the model of demand, the supply side can constrain markups to converge to a constant quickly relative to sampling error. I find that product characteristic instruments can yield consistent estimates in many of the cases I consider, but care must be taken in modeling demand and choosing instruments. A Monte Carlo study confirms that the asymptotic results are relevant in market sizes of practical importance.</p> </abstract>
<abstract> <p>We consider an agent who chooses an option after receiving some private information. This information, however, is unobserved by an analyst, so from the latter's perspective, choice is probabilistic or random. We provide a theory in which information can be fully identified from random choice. In addition, the analyst can perform the following inferences even when information is unobservable: (1) directly compute ex ante valuations of menus from random choice and vice versa, (2) assess which agent has better information by using choice dispersion as a measure of informativeness, (3) determine if the agent's beliefs about information are dynamically consistent, and (4) test to see if these beliefs are well-calibrated or rational.</p> </abstract>
<abstract> <p>Consider a group of individuals with unobservable perspectives (subjective prior beliefs) about a sequence of states. In each period, each individual receives private information about the current state and forms an opinion (a posterior belief). She also chooses a target individual and observes the target's opinion. This choice involves a trade-off between well-informed targets, whose signals are precise, and well-understood targets, whose perspectives are well known. Opinions are informative about the target's perspective, so observed individuals become better understood over time. We identify a simple condition under which long-run behavior is history independent. When this fails, each individual restricts attention to a small set of experts and observes the most informed among these. A broad range of observational patterns can arise with positive probability, including opinion leadership and information segregation. In an application to areas of expertise, we show how these mechanisms generate own field bias and large field dominance.</p> </abstract>
<abstract> <p>The past forty years have seen a rapid rise in top income inequality in the United States. While there is a large number of existing theories of the Pareto tail of the longrun income distributions, almost none of these address the fast rise in top inequality observed in the data. We show that standard theories, which build on a random growth mechanism, generate transition dynamics that are too slow relative to those observed in the data. We then suggest two parsimonious deviations from the canonical model that can explain such changes: "scale dependence" that may arise from changes in skill prices, and "type dependence," that is, the presence of some "high-growth types." These deviations are consistent with theories in which the increase in top income inequality is driven by the rise of "superstar" entrepreneurs or managers.</p> </abstract>
<abstract> <p>We analyze the internal consistency of using the market price of a firm's equity to trigger a contractual change in the firm's capital structure, given that the value of the equity itself depends on the firm's capital structure. Of particular interest is the case of contingent capital for banks, in the form of debt that converts to equity, when conversion is triggered by a decline in the bank's stock price. We analyze the problem of existence and uniqueness of equilibrium values for a firm's liabilities in this context, meaning values consistent with a market-price trigger. Discrete-time dynamics allow multiple equilibria. In contrast, we show that the possibility of multiple equilibria can largely be ruled out in continuous time, where the price of the triggering security adjusts in anticipation of breaching the trigger. Our main condition for existence of an equilibrium requires that the consequences of triggering a conversion be consistent with the direction in which the trigger is crossed. For the design of contingent capital with a stock price trigger, this condition may be interpreted to mean that conversion should be disadvantageous to shareholders, and it is satisfied by setting the trigger sufficiently high. Uniqueness follows provided the trigger is sufficiently accessible by all candidate equilibria. We illustrate precise formulations of these conditions with a variety of applications.</p> </abstract>
<abstract> <p>We introduce the class of conditional linear combination tests, which reject null hypotheses concerning model parameters when a data-dependent convex combination of two identification-robust statistics is large. These tests control size under weak identification and have a number of optimality properties in a conditional problem. We show that the conditional likelihood ratio test of Moreira (2003) is a conditional linear combination test in models with one endogenous regressor, and that the class of conditional linear combination tests is equivalent to a class of quasi-conditional likelihood ratio tests. We suggest using minimax regret conditional linear combination tests and propose a computationally tractable class of tests that plug in an estimator for a nuisance parameter. These plug-in tests perform well in simulation and have optimal power in many strongly identified models, thus allowing powerful identification-robust inference in a wide range of linear and nonlinear models without sacrificing efficiency if identification is strong.</p> </abstract>
<abstract> <p>Confidence intervals are commonly used to describe parameter uncertainty. In nonstandard problems, however, their frequentist coverage property does not guarantee that they do so in a reasonable fashion. For instance, confidence intervals may be empty or extremely short with positive probability, even if they are based on inverting powerful tests. We apply a betting framework and a notion of bet-proofness to formalize the "reasonableness" of confidence intervals as descriptions of parameter uncertainty, and use it for two purposes. First, we quantify the violations of bet-proofness for previously suggested confidence intervals in nonstandard problems. Second, we derive alternative confidence sets that are bet-proof by construction. We apply our framework to several nonstandard problems involving weak instruments, near unit roots, and moment inequalities. We find that previously suggested confidence intervals are not bet-proof, and numerically determine alternative bet-proof confidence sets.</p> </abstract>
<abstract> <p>This paper provides a novel mechanism for identifying and estimating latent group structures in panel data using penalized techniques. We consider both linear and nonlinear models where the regression coefficients are heterogeneous across groups but homogeneous within a group and the group membership is unknown. Two approaches are considered—penalized profile likelihood (PPL) estimation for the general nonlinear models without endogenous regressors, and penalized GMM (PGMM) estimation for linear models with endogeneity. In both cases, we develop a new variant of Lasso called classifier-Lasso (C-Lasso) that serves to shrink individual coefficients to the unknown group-specific coefficients. C-Lasso achieves simultaneous classification and consistent estimation in a single step and the classification exhibits the desirable property of uniform consistency. For PPL estimation, C-Lasso also achieves the oracle property so that group-specific parameter estimators are asymptotically equivalent to infeasible estimators that use individual group identity information. For PGMM estimation, the oracle property of C-Lasso is preserved in some special cases. Simulations demonstrate good finite-sample performance of the approach in both classification and estimation. Empirical applications to both linear and nonlinear models are presented.</p> </abstract>
<abstract> <p>We present a noncooperative game model of coalitional bargaining, closely based on that of Gul (1989) but solvable by backward induction. In this game, Gul's condition of "value additivity" does not suffice to ensure the existence of a subgame perfect Nash equilibrium that supports the Shapley value, but a related condition—"no positive value-externalities"—does. Multiple equilibria can arise only in the event of ties, and with a mild restriction on tie-break rules these equilibria all support the Shapley value.</p> </abstract>
<abstract> <p>This paper examines changes in the distribution of wages using bounds to allow for the impact of nonrandom selection into work. We show that worst case bounds can be informative. However, because employment rates in the United Kingdom are often low, they are not informative about changes in educational or gender wage differen- tials. Thus we explore ways to tighten these bounds using restrictions motivated from economic theory. With these assumptions, we find convincing evidence of an increase in inequality within education groups, changes in educational differentials, and increases in the relative wages of women.</p> </abstract>
<abstract> <p>Rationalizability is a central solution concept of game theory. Economic models often have many rationalizable outcomes, motivating economists to use refinements of rationalizability, including equilibrium refinements. In this paper we try to achieve a general understanding of when this multiplicity occurs and how one should deal with it. Assuming that the set of possible payoff functions and belief structures is sufficiently rich, we establish a revealing structure of the correspondence of beliefs to sets of rationalizable outcomes. We show that, for any rationalizable action a of any type, we can perturb the beliefs of the type in such a way that a is uniquely rationalizable for the new type. This unique outcome will be robust to further small changes. When multiplicity occurs, then we are in a "knife-edge" case, where the unique rationalizable outcome changes, sandwiched between open sets of types where each of the rationalizable actions is uniquely rationalizable. As an immediate application of this result, we characterize, for any refinement of rationalizability, the predictions that are robust to small misspecifications of interim beliefs. These are only those predictions that are true for all rationalizable strategies, that is, the predictions that could have been made without the refinement.</p> </abstract>
<abstract> <p>This paper develops and applies some new results in the theory of monotone comparative statics. Let f be a real-valued function defined on<tex-math>$R^l$</tex-math>and consider the problem of maximizing f(x) when x is constrained to lie in some subset C of<tex-math>$R^l$</tex-math>. We develop a natural way to order the constraint sets C and find the corresponding restrictions on the objective function f that guarantee that optimal solutions increase with the constraint set. We apply our techniques to problems in consumer, producer, and portfolio theory. We also use them to generalize Rybcsynski's theorem and the LeChatelier principle.</p> </abstract>
<abstract> <p>Does individual behavior in a laboratory setting provide a reliable indicator of behavior in a naturally occurring setting? We consider this general methodological question in the context of eliciting risk attitudes. The controls that are typically employed in laboratory settings, such as the use of abstract lotteries, could lead subjects to employ behavioral rules that differ from the ones they employ in the field. Because it is field behavior that we are interested in understanding, those controls might be a confound in themselves if they result in differences in behavior. We find that the use of artificial monetary prizes provides a reliable measure of risk attitudes when the natural counterpart outcome has minimal uncertainty, but that it can provide an unreliable measure when the natural counterpart outcome has background risk. Behavior tended to be moderately risk averse when artificial monetary prizes were used or when there was minimal uncertainty in the natural nonmonetary outcome, but subjects drawn from the same population were much more risk averse when their attitudes were elicited using the natural nonmonetary outcome that had some background risk. These results are consistent with conventional expected utility theory for the effects of background risk on attitudes to risk.</p> </abstract>
<abstract> <p>This paper considers issues related to estimation, inference, and computation with multiple structural changes that occur at unknown dates in a system of equations. Changes can occur in the regression coefficients and/or the covariance matrix of the errors. We also allow arbitrary restrictions on these parameters, which permits the analysis of partial structural change models, common breaks that occur in all equations, breaks that occur in a subset of equations, and so forth. The method of estimation is quasi-maximum likelihood based on Normal errors. The limiting distributions are obtained under more general assumptions than previous studies. For testing, we propose likelihood ratio type statistics to test the null hypothesis of no structural change and to select the number of changes. Structural change tests with restrictions on the parameters can be constructed to achieve higher power when prior information is present. For computation, an algorithm for an efficient procedure is proposed to construct the estimates and test statistics. We also introduce a novel locally ordered breaks model, which allows the breaks in different equations to be related yet not occurring at the same dates.</p> </abstract>
<abstract> <p>An extension to Ellsberg's experiment demonstrates that attitudes to ambiguity and compound objective lotteries are tightly associated. The sample is decomposed into three main groups: subjective expected utility subjects, who reduce compound objective lotteries and are ambiguity neutral, and two groups that exhibit different forms of association between preferences over compound lotteries and ambiguity, corresponding to alternative theoretical models that account for ambiguity averse or seeking behavior.</p> </abstract>
<abstract> <p>This paper considers identification and estimation of the effect of a mismeasured binary regressor in a nonparametric or semiparametric regression, or the conditional average effect of a binary treatment or policy on some outcome where treatment may be misclassified. Failure to account for misclassification is shown to result in attenuation bias in the estimated treatment effect. An identifying assumption that overcomes this bias is the existence of an instrument for the binary regressor that is conditionally independent of the treatment effect. A discrete instrument suffices for nonparametric identification.</p> </abstract>
<abstract> <p>We provide a nonparametric characterization of a general collective model for household consumption, which includes externalities and public consumption. Next, we establish testable necessary and sufficient conditions for data consistency with collective rationality that only include observed price and quantity information. These conditions have a similar structure as the generalized axiom of revealed preference for the unitary model, which is convenient from a testing point of view. In addition, we derive the minimum number of goods and observations that enable the rejection of collectively rational household behavior.</p> </abstract>
<abstract> <p>In this article we introduce efficient Wald tests for testing the null hypothesis of the unit root against the alternative of the fractional unit root. In a local alternative framework, the proposed tests are locally asymptotically equivalent to the optimal Robinson Lagrange multiplier tests. Our results contrast with the tests for fractional unit roots, introduced by Dolado, Gonzalo, and Mayoral, which are inefficient. In the presence of short range serial correlation, we propose a simple and efficient two-step test that avoids the estimation of a nonlinear regression model. In addition, the first-order asymptotic properties of the proposed tests are not affected by the preestimation of short or long memory parameters.</p> </abstract>
<abstract> <p>Dekel, Lipman and Rustichini (2001) (henceforth DLR) axiomatically characterized three representations of preferences that allow for a desire for flexibility and/or commitment. In one of these representations (ordinal expected utility), the independence axiom is stated in a weaker form than is necessary to obtain the representation; in another (additive expected utility), the continuity axiom is too weak. In this erratum we provide examples showing that the axioms used by DLR are not sufficient, and provide stronger versions of these axioms that, together with the other axioms used by DLR, are necessary and sufficient for these two representations.</p> </abstract>
<abstract> <p> This paper applies some general concepts in decision theory to a simple instrumental variables model. There are two endogenous variables linked by a single structural equation; k of the exogenous variables are excluded from this structural equation and provide the instrumental variables (IV). The reduced-form distribution of the endogenous variables conditional on the exogenous variables corresponds to independent draws from a bivariate normal distribution with linear regression functions and a known covariance matrix. A canonical form of the model has parameter vector (ρ, ϕ, ω), where ϕ is the parameter of interest and is normalized to be a point on the unit circle. The reduced-form coefficients on the instrumental variables are split into a scalar parameter ρ and a parameter vector ω, which is normalized to be a point on the (k-1)-dimensional unit sphere; ρ measures the strength of the association between the endogenous variables and the instrumental variables, and ω is a measure of direction. A prior distribution is introduced for the IV model. The parameters ϕ, ρ, and ω are treated as independent random variables. The distribution for ϕ is uniform on the unit circle; the distribution for ω is uniform on the unit sphere with dimension k-1. These choices arise from the solution of a minimax problem. The prior for ρ is left general. It turns out that given any positive value for ρ, the Bayes estimator of ω does not depend on ρ; it equals the maximum-likelihood estimator. This Bayes estimator has constant risk; because it minimizes average risk with respect to a proper prior, it is minimax. The same general concepts are applied to obtain confidence intervals. The prior distribution is used in two ways. The first way is to integrate out the nuisance parameter ω in the IV model. This gives an integrated likelihood function with two scalar parameters, ϕ and ρ. Inverting a likelihood ratio test, based on the integrated likelihood function, provides a confidence interval for ϕ. This lacks finite sample optimality, but invariance arguments show that the risk function depends only on ρ and not on ϕ or ω. The second approach to confidence sets aims for finite sample optimality by setting up a loss function that trades off coverage against the length of the interval. The automatic uniform priors are used for ϕ, and ω, but a prior is also needed for the scalar ρ, and no guidance is offered on this choice. The Bayes rule is a highest posterior density set. Invariance arguments show that the risk function depends only on ρ and not on ϕ or ω. The optimality result combines average risk and maximum risk. The confidence set minimizes the average-with respect to the prior distribution for ρ-of the maximum risk, where the maximization is with respect to ϕ and ω. </p> </abstract>
<abstract> <p>Consider a two-person intertemporal bargaining problem in which players choose actions and offers each period, and collect payoffs (as a function of that period's actions) while bargaining proceeds. This can alternatively be viewed as an infinitely repeated game wherein players can offer one another enforceable contracts that govern play for the rest of the game. Theory is silent with regard to how the surplus is likely to be split, because a folk theorem applies. Perturbing such a game with a rich set of behavioral types for each player yields a specific asymptotic prediction for how the surplus will be divided, as the perturbation probabilities approach zero. Behavioral types may follow nonstationary strategies and respond to the opponent's play. In equilibrium, rational players initially choose a behavioral type to imitate and a war of attrition ensues. How much should a player try to get and how should she behave while waiting for the resolution of bargaining? In both respects she should build her strategy around the advice given by the "Nash bargaining with threats" (NBWT) theory developed for two-stage games. In any perfect Bayesian equilibrium, she can guarantee herself virtually her NBWT payoff by imitating a behavioral type with the following simple strategy: in every period, ask for (and accept nothing less than) that player's NBWT share and, while waiting for the other side to concede, take the action Nash recommends as a threat in his two-stage game. The results suggest that there are forces at work in some dynamic games that favor certain payoffs over all others. This is in stark contrast to the classic folk theorems, to the further folk theorems established for repeated games with two-sided reputational perturbations, and to the permissive results obtained in the literature on bargaining with payoffs as you go.</p> </abstract>
<abstract> <p>Global games of regime change-coordination games of incomplete information in which a status quo is abandoned once a sufficiently large fraction of agents attack it-have been used to study crises phenomena such as currency attacks, bank runs, debt crises, and political change. We extend the static benchmark examined in the literature by allowing agents to take actions in many periods and to learn about the underlying fundamentals over time. We first provide a simple recursive algorithm for the characterization of monotone equilibria. We then show how the interaction of the knowledge that the regime survived past attacks with the arrival of information over time, or with changes in fundamentals, leads to interesting equilibrium properties. First, multiplicity may obtain under the same conditions on exogenous information that guarantee uniqueness in the static benchmark. Second, fundamentals may predict the eventual fate of the regime but not the timing or the number of attacks. Finally, equilibrium dynamics can alternate between phases of tranquility-where no attack is possible-and phases of distress-where a large attack can occur-even without changes in fundamentals.</p> </abstract>
<abstract> <p>In this paper, we consider the nonparametric identification and estimation of the average effect of a dummy endogenous regressor in models where the regressors are weakly but not additively separable from the error term. The model is not required to be strictly increasing in the error term, and the class of models considered includes limited dependent variable models such as discrete choice models. Conditions are established conditions under which it is possible to identify the average effect of the dummy endogenous regressor in a weakly separable model without relying on parametric functional form or distributional assumptions and without the use of large support conditions.</p> </abstract>
<abstract> <p>We propose a simple method to help researchers develop quantitative models of economic fluctuations. The method rests on the insight that many models are equivalent to a prototype growth model with time-varying wedges that resemble productivity, labor and investment taxes, and government consumption. Wedges that correspond to these variables-efficiency, labor, investment, and government consumption wedges-are measured and then fed back into the model so as to assess the fraction of various fluctuations they account for. Applying this method to U.S. data for the Great Depression and the 1982 recession reveals that the efficiency and labor wedges together account for essentially all of the fluctuations; the investment wedge plays a decidedly tertiary role, and the government consumption wedge plays none. Analyses of the entire postwar period and alternative model specifications support these results. Models with frictions manifested primarily as investment wedges are thus not promising for the study of U.S. business cycles.</p> </abstract>
<abstract> <p>As the exchange rate, foreign demand, and production costs evolve, domestic producers are continually faced with two choices: whether to be an exporter and, if so, how much to export. We develop a dynamic structural model of export supply that characterizes these two decisions. The model embodies plant-level heterogeneity in export profits, uncertainty about the determinants of future profits, and market entry costs for new exporters. Using a Bayesian Monte Carlo Markov chain estimator, we fit this model to plant-level panel data on three Colombian manufacturing industries. We obtain profit function and sunk entry cost coefficients, and use them to simulate export responses to shifts in the exchange-rate process and several types of export subsidies. In each case, the aggregate export response depends on entry costs, expectations about the exchange rate process, prior exporting experience, and producer heterogeneity. Export revenue subsidies are far more effective at stimulating exports than policies that subsidize entry costs.</p> </abstract>
<abstract> <p>When two parties have different prior beliefs about some future event, they can realize gains through speculative trade. Can these gains be realized when the parties' prior beliefs are not common knowledge? We examine a simple example in which two parties having heterogeneous prior beliefs, independently drawn from some distribution, bet on what future action one of them will choose. We define a notion of "constrained interim-efficient" best and ask whether they can be implemented in Bayesian equilibrium by some mechanism. Our main result establishes that as the costs of unilaterally manipulating the bet's outcome become more symmetric across states, implementation becomes easier. In particular, when these costs are equal in both states, implementation is possible for any distribution.</p> </abstract>
<abstract> <p> We analyze a cheap talk game, à la Crawford and Sobel, in a multidimensional state and policy space. A feature of the multidimensional state space is that communication on one dimension often reveals information on others. We show how this feature imposes bounds on communication. </p> </abstract>
<abstract> <p>We report recent advances concerning the package allocation problem, in which traders seek to buy or sell combinations of goods. The problems are most difficult when some goods are not substitutes. In that case, competitive equilibrium typically fail to exist but the core is non-empty and comprises the competitive solutions. Also in that case, the Vickrey auction fails to select core allocations and yield revenues that are less than competitive. The Ausubel-Milgrom auction generally selects core allocations and, when goods are substitutes, prescribes the Vickrey allocation. We also evaluate the problems and promise of mechanisms for the package exchange problem.</p> </abstract>
<abstract> <p>In this paper I demonstrate how one can generalize finitely many examples to statement about (infinite) classes of economic models. If there exist upper bounds on the number of connected components of one-dimensional linear subsets of the set of parameters for which a conjecture is true, one can conclude that it is correct for all parameter values in the class considered, except for a small residual set, once one has verified the conjecture for a predetermined finite set of points. I show how to apply this insight to computational experiments and spell out assumptions on the economic fundamentals that ensure that the necessary bounds on the number of connected components exist. I argue that these methods can be fruitfully utilized in applied general equilibrium analysis. I provide general assumptions on preferences and production sets that ensure that economic conjectures define sets with a bounded number of connected components. Using the theoretical results, I give an example of how one can explore qualitative and quantitative implications of general equilibrium models using computational experiments. Finally, I show how random algorithms can be used for generalizing examples in high-dimensional problems.</p> </abstract>
<abstract> <p> Many tests of asset-pricing models address only the pricing predictions, but these pricing predictions rest on portfolio choice predictions that seem obviously wrong. This paper suggests a new approach to asset pricing and portfolio choices based on unobserved heterogeneity. This approach yields the standard pricing conclusions of classical models but is consistent with very different portfolio choices. Novel econometric tests link the price and portfolio predictions and take into account the general equilibrium effects of sample-size bias. This paper works through the approach in detail for the case of the classical capital asset pricing model (CAPM), producing a model called CAPM + ε. When these econometric tests are applied to data generated by large-scale laboratory asset markets that reveal both prices and portfolio choices, CAPM + ∊ is not rejected. </p> </abstract>
<abstract> <p>In this paper, we generalize the notion of Pareto efficiency to make it applicable to environments with endogenous populations. Two efficiency concepts are proposed: P-efficiency and A-efficiency. The two concepts differ in how they treat potential agents that are not born. We show that these concepts are closely related to the notion of Pareto efficiency when fertility is exogenous. We prove a version of the first welfare theorem for Barro-Becker type fertility choice models and discuss how this result can be generalized. Finally, we study examples of equilibrium settings in which fertility decisions are not efficient, and we classify them into settings where inefficiencies arise inside the family and settings where they arise across families.</p> </abstract>
<abstract> <p>We present sufficient conditions for monotone matching in environments where utility is not fully transferable between partners. These conditions involve not only complementarity in types of the total payoff to a match, as in the transferable utility case, but also monotonicity in type of the degree of transferability between partners. We apply our conditions to study some models of risk sharing and incentive problems, deriving new results for predicted matching patterns in those contexts.</p> </abstract>
<abstract> <p>This paper analyzes equilibrium and welfare for a tractable class of economies (games) that have externalities, strategic complementarity or substitutability, and heterogeneous information. First, we characterize the equilibrium use of information: complementarity heightens the sensitivity of equilibrium actions to public information, raising aggregate volatility, whereas substitutability heightens the sensitivity to private information, raising cross-sectional dispersion. Next, we define and characterize an efficiency benchmark designed to address whether the equilibrium use of information is optimal from a social perspective; the efficient use of information reflects the social value of aligning choices across agents. Finally, we examine the comparative statics of equilibrium welfare with respect to the information structure; the social value of information is best understood by classifying economies according to the inefficiency, if any, in the equilibrium use of information. We conclude with a few applications, including production externalities, beauty contests, business cycles, and large Cournot and Bertrand games.</p> </abstract>
<abstract> <p>We study how to evaluate allocations independently of individual preferences over unavailable commodities. We prove impossibility results that suggest that such evaluations encounter serious difficulties. This is related to the well known problem of performing international comparisons of standard of living across countries with different consumption goods. We show how possibility results can be retrieved with restrictions on the domain of preferences, on the application of the independence axiom, or on the set of allocations to be ranked. Such restrictions appear more plausible when the objects of evaluation are allocations of composite commodities, characteristics, or human functionings rather than ordinary commodities.</p> </abstract>
<abstract> <p>This paper considers the problem of selection of weights for averaging across least squares estimates obtained from a set of models. Existing model average methods are based on exponential Akaike information criterion (AIC) and Bayesian information criterion (BIC) weights. In distinction, this paper proposes selecting the weights by minimizing a Mallows criterion, the latter an estimate of the average squared error from the model average fit. We show that our new Mallows model average (MMA) estimator is asymptotically optimal in the sense of achieving the lowest possible squared error in a class of discrete model average estimators. In a simulation experiment we show that the MMA estimator compares favorably with those based on AIC and BIC weights. The proof of the main result is an application of the work of Li (1987).</p> </abstract>
<abstract> <p>We consider nonparametric estimation of a regression function that is identified by requiring a specified quantile of the regression "error" conditional on an instrumental variable to be zero. The resulting estimating equation is a nonlinear integral equation of the first kind, which generates an ill-posed inverse problem. The integral operator and distribution of the instrumental variable are unknown and must be estimated nonparametrically. We show that the estimator is mean-square consistent, derive its rate of convergence in probability, and give conditions under which this rate is optimal in a minimax sense. The results of Monte Carlo experiments show that the estimator behaves well in finite samples.</p> </abstract>
<abstract> <p>For vectors z and w and scalar v, let r(v, z, w) be a function that can be nonparametrically estimated consistently and asymptotically normally, such as a distribution, density, or conditional mean regression function. We provide consistent, asymptotically normal nonparametric estimators for the functions G and H, where r(v, z, w) = H[vG(z), w], and some related models. This framework encompasses homothetic and homothetically separable functions, and transformed partly additive models r(v, z, w) = h[v + g(z), w] for unknown functions g and h. Such models reduce the curse of dimensionality, provide a natural generalization of linear index models, and are widely used in utility, production, and cost function applications. We also provide an estimator of G that is oracle efficient, achieving the same performance as an estimator based on local least squares when H is known.</p> </abstract>
<abstract> <p>This paper develops a framework for performing estimation and inference in econo- metric models with partial identification, focusing particularly on models character- ized by moment inequalities and equalities. Applications of this framework include the analysis of game-theoretic models, revealed preference restrictions, regressions with missing and corrupted data, auction models, structural quantile regressions, and asset pricing models. Specifically, we provide estimators and confidence regions for the set of minimizers $\Theta_I$ of an econometric criterion function $Q(\Theta)$. In applications, the criterion function embodies testable restrictions on economic models. A parameter value Θ that describes an economic model satisfies these restrictions if $Q(\Theta)$ attains its minimum at this value. Interest therefore focuses on the set of minimizers, called the identified set. We use the inversion of the sample analog, $Q_n(\Theta)$, of the population criterion, $Q(\Theta)$, to construct estimators and confidence regions for the identified set, and develop consistency, rates of convergence, and inference results for these estimators and regions. To derive these results, we develop methods for analyzing the asymptotic properties of sample criterion functions under set identification.</p> </abstract>
<abstract> <p>This paper investigates a new class of two-player games in continuous time, in which the players' observations of each other's actions are distorted by Brownian motions. These games are analogous to repeated games with imperfect monitoring in which the players take actions frequently. Using a differential equation, we find the set ε(r) of payoff pairs achievable by all public perfect equilibria of the continuous-time game, where r is the discount rate. The same differential equation allows us to find public perfect equilibria that achieve any value pair on the boundary of the set ε(r). These public perfect equilibria are based on a pair of continuation values as a state variable, which moves along the boundary of ε(r) during the course of the game. In order to give players incentives to take actions that are not static best responses, the pair of continuation values is stochastically driven by the players' observations of each other's actions along the boundary of the set ε(r).</p> </abstract>
<abstract> <p>We describe a two-step algorithm for estimating dynamic games under the assumption that behavior is consistent with Markov perfect equilibrium. In the first step, the policy functions and the law of motion for the state variables are estimated. In the second step, the remaining structural parameters are estimated using the optimality conditions for equilibrium. The second step estimator is a simple simulated minimum distance estimator. The algorithm applies to a broad class of models, including industry competition models with both discrete and continuous controls such as the Ericson and Pakes (1995) model. We test the algorithm on a class of dynamic discrete choice models with normally distributed errors and a class of dynamic oligopoly models similar to that of Pakes and McGuire (1994).</p> </abstract>
<abstract> <p>This paper presents three sets of results about equilibrium bias of technology. First, I show that when the menu of technological possibilities only allows for factor-augmenting technologies, the increase in the supply of a factor induces technological change relatively biased toward that factor-meaning that the induced technological change increases the relative marginal product of the factor becoming more abundant. Moreover, this induced bias can be strong enough to make the relative marginal product of a factor increasing in response to an increase in its supply, thus leading to an upward-sloping relative demand curve. I also show that these results about relative bias do not generalize when more general menus of technological possibilities are considered. Second, I prove that under mild assumptions, the increase in the supply of a factor induces technological change that is absolutely biased toward that factor-meaning that it increases its marginal product at given factor proportions. The third and most important result in the paper establishes the possibility of and conditions for strong absolute equilibrium bias-whereby the price (marginal product) of a factor increases in response to an increase in its supply. I prove that, under some regularity conditions, there will be strong absolute equilibrium bias if and only if the aggregate production function of the economy fails to be jointly concave in factors and technology. This type of failure of joint concavity is possible in economies where equilibrium factor demands and technologies result from the decisions of different agents.</p> </abstract>
<abstract> <p>The purpose of this paper is to provide theoretical justification for some existing methods for constructing confidence intervals for the sum of coefficients in autoregressive models. We show that the methods of Stock (1991), Andrews (1993), and Hansen (1999) provide asymptotically valid confidence intervals, whereas the subsampling method of Romano and Wolf (2001) does not. In addition, we generalize the three valid methods to a larger class of statistics. We also clarify the difference between uniform and pointwise asymptotic approximations, and show that a pointwise convergence of coverage probabilities for all values of the parameter does not guarantee the validity of the confidence set.</p> </abstract>
<abstract> <p>This paper takes steps toward integrating firm theory in the spirit of Alchian and Demsetz (1972) and Grossman and Hart (1986), contract theory in the spirit of Holmstrom (1979), and general equilibrium theory in the spirit of Arrow and Debreu (1954) and McKenzie (1959). In the model presented here, the set of firms that form and the contractual arrangements that appear, the assignments of agents to firms, the prices faced by firms for inputs and outputs, and the incentives to agents are all determined endogenously at equilibrium. Agents choose consumption-but they also choose which firms to join, which roles to occupy in those firms, and which actions to take in those roles. Agents interact anonymously with the (large) market, but strategically within the (small) firms they join. The model accommodates moral hazard, adverse selection, signaling, and insurance. Equilibria may be Pareto ranked.</p> </abstract>
<abstract> <p>This study reports evidence from a field experiment that was conducted to investigate the relevance of gift exchange in a natural setting. In collaboration with a charitable organization, we sent roughly 10,000 solicitation letters to potential donors. One-third of the letters contained no gift, one-third contained a small gift, and one-third contained a large gift. Treatment assignment was random. The results confirm the economic importance of gift exchange. Compared to the no gift condition, the relative frequency of donations increased by 17 percent if a small gift was included and by 75 percent for a large gift. The study extends the current body of research on gift exchange, which is almost exclusively confined to laboratory studies.</p> </abstract>
<abstract> <p>Nonseparable models do not impose any type of additivity between the unobserved part and the observable regressors, and are therefore ideal for many economic applications. To identify these models using the entire joint distribution of the data as summarized in regression quantiles, monotonicity in unobservables has frequently been assumed. This paper establishes that in the absence of monotonicity, the quantiles identify local average structural derivatives of nonseparable models.</p> </abstract>
<abstract> <p>We study, theoretically and quantitatively, the general equilibrium of an economy in which households smooth consumption by means of both a riskless asset and unsecured loans with the option to default. The default option resembles a bankruptcy filing under Chapter 7 of the U.S. Bankruptcy Code. Competitive financial intermediaries offer a menu of loan sizes and interest rates wherein each loan makes zero profits. We prove the existence of a steady-state equilibrium and characterize the circumstances under which a household defaults on its loans. We show that our model accounts for the main statistics regarding bankruptcy and unsecured credit while matching key macro-economic aggregates, and the earnings and wealth distributions. We use this model to address the implications of a recent policy change that introduces a form of "means testing" for households contemplating a Chapter 7 bankruptcy filing. We find that this policy change yields large welfare gains.</p> </abstract>
<abstract> <p>We study a two-player one-arm bandit problem in discrete time, in which the risky arm can have two possible types, high and low, the decision to stop experimenting is irreversible, and players observe each other's actions but not each other's payoffs. We prove that all equilibria are in cutoff strategies and provide several qualitative results on the sequence of cutoffs.</p> </abstract>
<abstract> <p>This paper studies a shape-invariant Engel curve system with endogenous total expenditure, in which the shape-invariant specification involves a common shift parameter for each demographic group in a pooled system of nonparametric Engel curves. We focus on the identification and estimation of both the nonparametric shapes of the Engel curves and the parametric specification of the demographic scaling parameters. The identification condition relates to the bounded completeness and the estimation procedure applies the sieve minimum distance estimation of conditional moment restrictions, allowing for endogeneity. We establish a new root mean squared convergence rate for the nonparametric instrumental variable regression when the endogenous regressor could have unbounded support. Root-n asymptotic normality and semiparametric efficiency of the parametric components are also given under a set of "low-level" sufficient conditions. Our empirical application using the U.K. Family Expenditure Survey shows the importance of adjusting for endogeneity in terms of both the nonparametric curvatures and the demographic parameters of systems of Engel curves.</p> </abstract>
<abstract> <p>We analyze use of a quasi-likelihood ratio statistic for a mixture model to test the null hypothesis of one regime versus the alternative of two regimes in a Markov regime-switching context. This test exploits mixture properties implied by the regime-switching process, but ignores certain implied serial correlation properties. When formulated in the natural way, the setting is nonstandard, involving nuisance parameters on the boundary of the parameter space, nuisance parameters identified only under the alternative, or approximations using derivatives higher than second order. We exploit recent advances by Andrews (2001) and contribute to the literature by extending the scope of mixture models, obtaining asymptotic null distributions different from those in the literature. We further provide critical values for popular models or bounds for tail probabilities that are useful in constructing conservative critical values for regime-switching tests. We compare the size and power of our statistics to other useful tests for regime switching via Monte Carlo methods and find relatively good performance. We apply our methods to reexamine the classic cartel study of Porter (1983) and reaffirm Porter's findings.</p> </abstract>
<abstract> <p>This paper proposes a structural nonequilibrium model of initial responses to incomplete-information games based on "level-k" thinking, which describes behavior in many experiments with complete-information games. We derive the model's implications in first- and second-price auctions with general information structures, compare them to equilibrium and Eyster and Rabin's (2005) "cursed equilibrium," and evaluate the model's potential to explain nonequilibrium bidding in auction experiments. The level-k model generalizes many insights from equilibrium auction theory. It allows a unified explanation of the winner's curse in common-value auctions and overbidding in those independent-private-value auctions without the uniform value distributions used in most experiments.</p> </abstract>
<abstract> <p>A new panel data model is proposed to represent the behavior of economies in transition, allowing for a wide range of possible time paths and individual heterogeneity. The model has both common and individual specific components, and is formulated as a nonlinear time varying factor model. When applied to a micro panel, the decomposition provides flexibility in idiosyncratic behavior over time and across section, while retaining some commonality across the panel by means of an unknown common growth component. This commonality means that when the heterogeneous time varying idiosyncratic components converge over time to a constant, a form of panel convergence holds, analogous to the concept of conditional sigma convergence. The paper provides a framework of asymptotic representations for the factor components that enables the development of econometric procedures of estimation and testing. In particular, a simple regression based convergence test is developed, whose asymptotic properties are analyzed under both null and local alternatives, and a new method of clustering panels into club convergence groups is constructed. These econometric methods are applied to analyze convergence in cost of living indices among 19 U.S. metropolitan cities.</p> </abstract>
<abstract> <p>A general equilibrium search model makes layoff costs affect the aggregate unemployment rate in ways that depend on equilibrium proportions of frictional and structural unemployment that in turn depend on the generosity of government unemployment benefits and skill losses among newly displaced workers. The model explains how, before the 1970s, lower flows into unemployment gave Europe lower unemployment rates than the United States and also how, after 1980, higher durations have kept unemployment rates in Europe persistently higher than in the United States. These outcomes arise from the way Europe's higher firing costs and more generous unemployment compensation make its unemployment rate respond to bigger skill losses among newly displaced workers. Those bigger skill losses also explain why U.S. workers have experienced more earnings volatility since 1980 and why, especially among older workers, hazard rates of gaining employment in Europe now fall sharply with increases in the duration of unemployment.</p> </abstract>
<abstract> <p>This paper develops a nonparametric theory of preferences over one's own and others' monetary payoffs. We introduce "more altruistic than" (MAT), a partial ordering over such preferences, and interpret it with known parametric models. We also introduce and illustrate "more generous than" (MGT), a partial ordering over opportunity sets. Several recent studies focus on two-player extensive form games of complete information in which the first mover (FM) chooses a more or less generous opportunity set for the second mover (SM). Here reciprocity can be formalized as the assertion that an MGT choice by the FM will elicit MAT preferences in the SM. A further assertion is that the effect on preferences is stronger for acts of commission by FM than for acts of omission. We state and prove propositions on the observable consequences of these assertions. Finally, empirical support for the propositions is found in existing data from investment and dictator games, the carrot and stick game, and the Stackelberg duopoly game and in new data from Stackelberg mini-games.</p> </abstract>
<abstract> <p>We study how professional players and college students play zero-sum two-person strategic games in a laboratory setting. We first ask professionals to play a 2 x 2 game that is formally identical to a strategic interaction situation that they face in their natural environment. Consistent with their behavior in the field, they play very close to the equilibrium of the game. In particular, (i) they equate their strategies' payoffs to the equilibrium ones and (ii) they generate sequences of choices that are serially independent. In sharp contrast, however, we find that college students play the game far from the equilibrium predictions. We then study the behavior of professional players and college students in the classic O'Neill 4 x 4 zero-sum game, a game that none of the subjects has encountered previously, and find the same differences in the behavior of these two pools of subjects. The transfer of skills and experience from the familiar field to the unfamiliar laboratory observed for professional players is relevant to evaluate the circumstances under which behavior in a laboratory setting may be a reliable indicator of behavior in a naturally occurring setting. From a cognitive perspective, it is useful for research on recognition processes, intuition, and similarity as a basis for inductive reasoning.</p> </abstract>
<abstract> <p>There are typically multiple equilibrium outcomes in the Crawford-Sobel (CS) model of strategic information transmission. This paper identifies a simple condition on equilibrium payoffs, called NITS (no incentive to separate), that selects among CS equilibria. Under a commonly used regularity condition, only the equilibrium with the maximal number of induced actions satisfies NITS. We discuss various justifications for NITS, including perturbed cheap-talk games with nonstrategic players or costly lying. We also apply NITS to other models of cheap talk, illustrating its potential beyond the CS framework.</p> </abstract>
<abstract> <p>Experimental studies have found that a decision maker prefers spreading good and bad outcomes evenly over time. We propose, in an axiomatic framework, a new model of discount factors that captures this preference for spread. The model provides a refinement of the discounted utility model while maintaining dynamic consistency. The derived discount factors incorporate gain/loss asymmetry recursively: the difference between average future utility and current utility defines a gain or a loss, and gains are discounted more than losses. This notion of utility smoothing can induce a preference for spread: if bad outcomes are concentrated on future periods, moving one of the bad outcomes to today would be beneficial because such an operation eliminates a large loss and replaces it with a small gain.</p> </abstract>
<abstract> <p>The conventional heteroskedasticity-robust (HR) variance matrix estimator for cross-sectional regression (with or without a degrees-of-freedom adjustment), applied to the fixed-effects estimator for panel data with serially uncorrelated errors, is incon- sistent if the number of time periods T is fixed (and greater than 2) as the number of entities n increases. We provide a bias-adjusted HR estimator that is<tex-math>$\sqrt{nT}$</tex-math>-consistent under any sequences (n, T) in which n and/or T increase to ∞. This estimator can be extended to handle serial correlation of fixed order.</p> </abstract>
<abstract> <p> This paper considers studentized tests in time series regressions with nonparametrically autocorrelated errors. The studentization is based on robust standard errors with truncation lag M = bT for some constant b ∊ (0, 1] and sample size T. It is shown that the nonstandard fixed-b limit distributions of such nonparametrically studentized tests provide more accurate approximations to the finite sample distributions than the standard small-b limit distribution. We further show that, for typical economic time series, the optimal bandwidth that minimizes a weighted average of type I and type II errors is larger by an order of magnitude than the bandwidth that minimizes the asymptotic mean squared error of the corresponding long-run variance estimator. A plug-in procedure for implementing this optimal bandwidth is suggested and simulations (not reported here) confirm that the new plug-in procedure works well in finite samples. </p> </abstract>
<abstract> <p>While the literature on nonclassical measurement error traditionally relies on the availability of an auxiliary data set containing correctly measured observations, we establish that the availability of instruments enables the identification of a large class of nonclassical nonlinear errors-in-variables models with continuously distributed variables. Our main identifying assumption is that, conditional on the value of the true regressors, some "measure of location" of the distribution of the measurement error (e.g., its mean, mode, or median) is equal to zero. The proposed approach relies on the eigenvalue-eigenfunction decomposition of an integral operator associated with specific joint probability densities. The main identifying assumption is used to "index" the eigenfunctions so that the decomposition is unique. We propose a convenient sievebased estimator, derive its asymptotic properties, and investigate its finite-sample behavior through Monte Carlo simulations.</p> </abstract>
</article>